{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19a3956d",
   "metadata": {},
   "source": [
    "# Necessary Tools and Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57e1172e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./BERTopicVenv/lib/python3.10/site-packages (2.0.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./BERTopicVenv/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./BERTopicVenv/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in ./BERTopicVenv/lib/python3.10/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./BERTopicVenv/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in ./BERTopicVenv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./BERTopicVenv/lib/python3.10/site-packages (from matplotlib) (23.1)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Using cached fonttools-4.41.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Using cached contourpy-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "Collecting pyparsing<3.1,>=2.3.1\n",
      "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./BERTopicVenv/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.4.4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "Requirement already satisfied: numpy>=1.20 in ./BERTopicVenv/lib/python3.10/site-packages (from matplotlib) (1.24.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./BERTopicVenv/lib/python3.10/site-packages (from matplotlib) (10.0.0)\n",
      "Requirement already satisfied: six>=1.5 in ./BERTopicVenv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.1.0 cycler-0.11.0 fonttools-4.41.1 kiwisolver-1.4.4 matplotlib-3.7.2 pyparsing-3.0.9\n",
      "Requirement already satisfied: numpy in ./BERTopicVenv/lib/python3.10/site-packages (1.24.4)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.3/293.3 KB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib!=3.6.1,>=3.1 in ./BERTopicVenv/lib/python3.10/site-packages (from seaborn) (3.7.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in ./BERTopicVenv/lib/python3.10/site-packages (from seaborn) (1.24.4)\n",
      "Requirement already satisfied: pandas>=0.25 in ./BERTopicVenv/lib/python3.10/site-packages (from seaborn) (2.0.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./BERTopicVenv/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.1.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./BERTopicVenv/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.41.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in ./BERTopicVenv/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (10.0.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./BERTopicVenv/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.4)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in ./BERTopicVenv/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./BERTopicVenv/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./BERTopicVenv/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (23.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./BERTopicVenv/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./BERTopicVenv/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./BERTopicVenv/lib/python3.10/site-packages (from pandas>=0.25->seaborn) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in ./BERTopicVenv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.12.2\n",
      "Collecting unzip\n",
      "  Using cached unzip-1.0.0-py3-none-any.whl\n",
      "Installing collected packages: unzip\n",
      "Successfully installed unzip-1.0.0\n",
      "Collecting gensim\n",
      "  Using cached gensim-4.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.4 MB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Using cached smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in ./BERTopicVenv/lib/python3.10/site-packages (from gensim) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in ./BERTopicVenv/lib/python3.10/site-packages (from gensim) (1.11.1)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.3.1 smart-open-6.3.0\n",
      "Requirement already satisfied: nltk in ./BERTopicVenv/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: joblib in ./BERTopicVenv/lib/python3.10/site-packages (from nltk) (1.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./BERTopicVenv/lib/python3.10/site-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in ./BERTopicVenv/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: click in ./BERTopicVenv/lib/python3.10/site-packages (from nltk) (8.1.6)\n",
      "Collecting wordcloud\n",
      "  Using cached wordcloud-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (455 kB)\n",
      "Requirement already satisfied: numpy>=1.6.1 in ./BERTopicVenv/lib/python3.10/site-packages (from wordcloud) (1.24.4)\n",
      "Requirement already satisfied: pillow in ./BERTopicVenv/lib/python3.10/site-packages (from wordcloud) (10.0.0)\n",
      "Requirement already satisfied: matplotlib in ./BERTopicVenv/lib/python3.10/site-packages (from wordcloud) (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./BERTopicVenv/lib/python3.10/site-packages (from matplotlib->wordcloud) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./BERTopicVenv/lib/python3.10/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in ./BERTopicVenv/lib/python3.10/site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./BERTopicVenv/lib/python3.10/site-packages (from matplotlib->wordcloud) (23.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./BERTopicVenv/lib/python3.10/site-packages (from matplotlib->wordcloud) (4.41.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in ./BERTopicVenv/lib/python3.10/site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./BERTopicVenv/lib/python3.10/site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: six>=1.5 in ./BERTopicVenv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.2\n",
      "Collecting spacy\n",
      "  Using cached spacy-3.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
      "Requirement already satisfied: setuptools in ./BERTopicVenv/lib/python3.10/site-packages (from spacy) (59.6.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy) (1.24.4)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.9-py3-none-any.whl (17 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Using cached srsly-2.4.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (492 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy) (23.1)\n",
      "Collecting typer<0.10.0,>=0.3.0\n",
      "  Using cached typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Collecting thinc<8.2.0,>=8.1.8\n",
      "  Using cached thinc-8.1.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (913 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Using cached wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Collecting pathy>=0.10.0\n",
      "  Using cached pathy-0.10.2-py3-none-any.whl (48 kB)\n",
      "Requirement already satisfied: jinja2 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy) (2.31.0)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached cymem-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Using cached murmurhash-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Using cached preshed-3.0.8-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./BERTopicVenv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./BERTopicVenv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./BERTopicVenv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./BERTopicVenv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./BERTopicVenv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Using cached confection-0.1.0-py3-none-any.whl (34 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Using cached blis-0.7.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./BERTopicVenv/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./BERTopicVenv/lib/python3.10/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Installing collected packages: cymem, wasabi, typer, spacy-loggers, spacy-legacy, pydantic, murmurhash, langcodes, catalogue, blis, srsly, preshed, pathy, confection, thinc, spacy\n",
      "Successfully installed blis-0.7.10 catalogue-2.0.9 confection-0.1.0 cymem-2.0.7 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.2 preshed-3.0.8 pydantic-1.10.12 spacy-3.6.0 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.7 thinc-8.1.10 typer-0.9.0 wasabi-1.1.2\n",
      "Collecting spacy_download\n",
      "  Using cached spacy_download-1.1.0-py3-none-any.whl (3.6 kB)\n",
      "Requirement already satisfied: spacy<4.0.0,>=3.0.0 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy_download) (3.6.0)\n",
      "Requirement already satisfied: setuptools in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_download) (59.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_download) (23.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_download) (0.10.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_download) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_download) (2.0.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_download) (2.31.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_download) (1.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_download) (2.0.9)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_download) (6.3.0)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_download) (0.9.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_download) (3.0.12)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_download) (3.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_download) (4.65.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_download) (3.0.8)\n",
      "Requirement already satisfied: jinja2 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_download) (3.1.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_download) (1.0.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_download) (2.4.7)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_download) (1.24.4)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_download) (8.1.10)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<4.0.0,>=3.0.0->spacy_download) (1.10.12)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./BERTopicVenv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacy_download) (4.7.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./BERTopicVenv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_download) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./BERTopicVenv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_download) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./BERTopicVenv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_download) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./BERTopicVenv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_download) (2.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./BERTopicVenv/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.0.0->spacy_download) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./BERTopicVenv/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.0.0->spacy_download) (0.1.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./BERTopicVenv/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy_download) (8.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./BERTopicVenv/lib/python3.10/site-packages (from jinja2->spacy<4.0.0,>=3.0.0->spacy_download) (2.1.3)\n",
      "Installing collected packages: spacy_download\n",
      "Successfully installed spacy_download-1.1.0\n",
      "Collecting pyLDAvis\n",
      "  Using cached pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
      "Collecting numexpr\n",
      "  Using cached numexpr-2.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (381 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./BERTopicVenv/lib/python3.10/site-packages (from pyLDAvis) (1.3.1)\n",
      "Requirement already satisfied: pandas>=2.0.0 in ./BERTopicVenv/lib/python3.10/site-packages (from pyLDAvis) (2.0.3)\n",
      "Requirement already satisfied: jinja2 in ./BERTopicVenv/lib/python3.10/site-packages (from pyLDAvis) (3.1.2)\n",
      "Collecting funcy\n",
      "  Using cached funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: scipy in ./BERTopicVenv/lib/python3.10/site-packages (from pyLDAvis) (1.11.1)\n",
      "Requirement already satisfied: setuptools in ./BERTopicVenv/lib/python3.10/site-packages (from pyLDAvis) (59.6.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in ./BERTopicVenv/lib/python3.10/site-packages (from pyLDAvis) (1.3.0)\n",
      "Requirement already satisfied: gensim in ./BERTopicVenv/lib/python3.10/site-packages (from pyLDAvis) (4.3.1)\n",
      "Requirement already satisfied: numpy>=1.24.2 in ./BERTopicVenv/lib/python3.10/site-packages (from pyLDAvis) (1.24.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./BERTopicVenv/lib/python3.10/site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./BERTopicVenv/lib/python3.10/site-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./BERTopicVenv/lib/python3.10/site-packages (from pandas>=2.0.0->pyLDAvis) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./BERTopicVenv/lib/python3.10/site-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.2.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in ./BERTopicVenv/lib/python3.10/site-packages (from gensim->pyLDAvis) (6.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./BERTopicVenv/lib/python3.10/site-packages (from jinja2->pyLDAvis) (2.1.3)\n",
      "Requirement already satisfied: six>=1.5 in ./BERTopicVenv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n",
      "Installing collected packages: funcy, numexpr, pyLDAvis\n",
      "Successfully installed funcy-2.0 numexpr-2.8.4 pyLDAvis-3.4.1\n",
      "Collecting PyStemmer\n",
      "  Using cached PyStemmer-2.2.0.1-cp310-cp310-linux_x86_64.whl\n",
      "Installing collected packages: PyStemmer\n",
      "Successfully installed PyStemmer-2.2.0.1\n",
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "Collecting en-core-web-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in ./BERTopicVenv/lib/python3.10/site-packages (from en-core-web-sm==3.6.0) (3.6.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.65.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.10)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.12)\n",
      "Requirement already satisfied: setuptools in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (59.6.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.24.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: jinja2 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: pathy>=0.10.0 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./BERTopicVenv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./BERTopicVenv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./BERTopicVenv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./BERTopicVenv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./BERTopicVenv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./BERTopicVenv/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./BERTopicVenv/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./BERTopicVenv/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./BERTopicVenv/lib/python3.10/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.6.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "! pip install pandas\n",
    "! pip install matplotlib\n",
    "! pip install numpy\n",
    "! pip install seaborn\n",
    "! pip install unzip\n",
    "! pip install gensim\n",
    "! pip install nltk\n",
    "! pip install wordcloud\n",
    "! pip install spacy\n",
    "! pip install spacy_download\n",
    "! pip install pyLDAvis\n",
    "! pip install PyStemmer\n",
    "\n",
    "! python3 -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "893b4ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/dev/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/dev/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/dev/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.6.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.7.0,>=3.6.0 in ./BERTopicVenv/lib/python3.10/site-packages (from en-core-web-sm==3.6.0) (3.6.0)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.24.4)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.10)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.12)\n",
      "Requirement already satisfied: setuptools in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (59.6.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: jinja2 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in ./BERTopicVenv/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./BERTopicVenv/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./BERTopicVenv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./BERTopicVenv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./BERTopicVenv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./BERTopicVenv/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in ./BERTopicVenv/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in ./BERTopicVenv/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in ./BERTopicVenv/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./BERTopicVenv/lib/python3.10/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "## Importing PD and Others\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "## Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "## NLTK\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import nltk.stem\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['#', '`', '\"', '@'])\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import spacy\n",
    "spacy.cli.download('en_core_web_sm')\n",
    "\n",
    "\n",
    "\n",
    "## Visualization\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c80025",
   "metadata": {},
   "source": [
    "# Import data and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "093e46bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<p>When dealing with small projects, what do you feel is the break even point for storing data in simple text files, hash tables, etc., versus using a real database?  For small projects with simple data management requirements, a real database is unnecessary complexity and violates YAGNI.  However, at some point the complexity of a database is obviously worth it.  What are some signs that your problem is too complex for simple ad-hoc techniques and needs a real database?</p>\\n\\n<p>Note:  To people used to enterprise environments, this will probably sound like a weird question.  However, my problem domain is bioinformatics.  Most of my programming is prototypes, not production code.  I'm primarily a domain expert and secondarily a programmer.  Most of my code is algorithm-centric, not data management-centric.  The purpose of this question is largely for me to figure out how much work I might save in the long run if I learn to use proper databases in my code instead of the more ad-hoc techniques I typically use.</p>\\n-Databases versus plain text-<database><complexity-theory>\",\n",
       " \"<p>Is there a way to know if a table is locked and what kind of lock is currently on a table? I was hoping for something through the DBC tables in teradata, but I can't find any reference to anything like this. I have normal user access and the DBA is no help. Thanks.</p>\\n-Teradata locks - How to know if a table is locked?-<sql><locking><teradata>\",\n",
       " '<p>Is anyone out there used Rapidminer for sentiment analysis... Is this a right combination???</p>\\n\\n<p>If not how do I get started with a simple sentiment analysis application??</p>\\n-rapidminer and sentiment analysis-<nlp>',\n",
       " \"<p>I was able to do fill tables with data from Excel file or text files using GUI utility Teradata Sql assistant. But now I have a requirement to import data into teradata tables from excel file using a bteq script. I have been trying to do that using \\n<p>.IMPORT REPORT <P>.IMPORT DATA <P>.IMPORT VARTEXT and I have tried other things also, but of no use. I have referred to some answers in teradataforum and googled for the same but my script is not working. Please help me with a script which will import data from excel file or atleast text file using BTEQ script.My script is as follows...\\n<p></p>\\n\\n<pre><code>.LOGON XXXX/XXXXXX,XXXX\\n.import data FILE = D:\\\\XX\\\\XXXX.xls ;\\n.QUIET ON \\n.REPEAT * \\nUSING COL1  (CHAR(1))\\n     ,COL2  (CHAR(1))\\n     ,COL3 (VARCHAR(100))\\n\\nINSERT INTO DATABASE.TABLE\\n    ( COL1\\n     ,COL2\\n     ,COL3)\\nVALUES ( :COL1\\n        ,:COL2\\n        ,:COL3);\\n.QUIT\\n</code></pre>\\n\\n<p><p><strong>EDIT:</strong>\\n<p>Till now I came this long. I have successfully loaded data from comma separated text file using the following code. But how to do it in Excel?</p>\\n\\n<pre><code>.LOGON xxxx/xxxx,xxxx\\n.IMPORT VARTEXT ',' FILE=xxxxx.TXT;\\n.QUIET ON \\n.REPEAT * \\nUSING \\n(   col1 VARCHAR(2)\\n    ,col2 VARCHAR(1)\\n    ,col3 VARCHAR(60)\\n)        \\nINSERT INTO database.table\\n    ( col1\\n     ,col2\\n     ,col3)\\nVALUES ( :col1\\n    ,:col2\\n    ,:col3);\\n.QUIT\\n</code></pre>\\n\\n<p>Sample comma separated text file being</p>\\n\\n<pre><code>1,B,status1\\n2,B,status2\\n3,B,status3\\n</code></pre>\\n\\n<hr>\\n\\n<p>etc.<br>\\nPlease help me if possible to load the same with Excel file.</p>\\n-How to import data from Excel file to Teradata table using BTEQ scripts?-<excel><import><teradata>\",\n",
       " \"<p>Im trying to import data into tables from a file using BTEQ import.</p>\\n\\n<p>im facing weird errors while doing this</p>\\n\\n<p>Like:</p>\\n\\n<p>if im using text file as input data file with ',' as delimiter as filed seperator im getting the error as below:</p>\\n\\n<p>*** Failure 2673 The source parcel length does not match data that was defined.</p>\\n\\n<p>or</p>\\n\\n<p>if im using EXCEL file as input data file im getting the error as below:</p>\\n\\n<p><strong>* Growing Buffer to 53200\\n*</strong> Error: Import data size does not agree with byte length.\\nThe cause may be:\\n1) IMPORT DATA vs. IMPORT REPORT\\n2) incorrect incoming data\\n3) import file has reached end-of-file.\\n*** Warning: Out of data.</p>\\n\\n<p>please help me out by giving the syntax for BTEQ import using txt file as input data file and also the syntax if we use EXCEL file as the input data file</p>\\n\\n<p>Also is there any specific format for the input data file for correct reading of data from it.\\nif so please give me the info about that.</p>\\n\\n<p>Thanks in advance:)\\n<br><br><br></p>\\n\\n<h2>EDIT</h2>\\n\\n<p>sorry for not posting the script in first.\\nIm new to teradata and yet to explore other tools.\\nI was asked to write the script for BTEQ import</p>\\n\\n<p><b>.LOGON TDPD/XXXXXXX,XXXXXX<br>\\n.import VARTEXT ',' FILE = D:\\\\cc\\\\PDATA.TXT <br>\\n.QUIET ON <br>\\n.REPEAT * <br>\\nUSING <br> COL1 (VARCHAR(2)) <br> ,COL2 (VARCHAR(1)) <br>,COL3 (VARCHAR(56)) <br> </p>\\n\\n<p>INSERT INTO  <br> ( COL1 <br>,COL2 <br>,COL3) <br> VALUES ( :COL1 <br>,:COL2 <br>,:COL3); <br>\\n.QUIT<br></b>\\nI executed the above script and it is successful using a txt(seperating the fileds with comma) file and giving the datatype as varchar.<br></p>\\n\\n<p>sample input txt file:<br>\\n1,b,helloworld1<br>\\n2,b,helloworld2<br>\\n3,D,helloworld1<br>\\n12,b,helloworld1<br></p>\\n\\n<p>I also tried to do the same using tab(\\\\t) as the field seperator but it giving the same old error.<br>\\nQ) Does this work only for comma seperated txt files?<br><br></p>\\n\\n<p>Please could u tell me where can i find the BTEQ manual...</p>\\n\\n<p>Thanks a lot</p>\\n-How to import data into teradata tables from an excel file using BTEQ import?-<excel><teradata>\",\n",
       " '<p>I have a REXX job that needs to read from both Teradata (using BTEQ) and DB2. At present, I can get it to either read from Teradata or DB2, but not both. When I try to read from both, the Teradata one (which runs first) works fine but the DB2 read gives an error of RC(1) upon attempting to open the cursor.</p>\\n\\n<p>Code to read from Teradata (by and large copied from <a href=\"http://www.teradataforum.com/teradata/20040928_131203.htm\" rel=\"nofollow noreferrer\">http://www.teradataforum.com/teradata/20040928_131203.htm</a>):</p>\\n\\n<pre><code>ADDRESS TSO \"DELETE BLAH.TEMP\"\\n\"ALLOC FI(SYSPRINT) DA(BLAH.TEMP) NEW CATALOG SP(10 10) TR RELEASE\",\\n   \"UNIT(SYSDA) RECFM(F B A) LRECL(133) BLKSIZE(0) REUSE\"\\n\"ATTRIB FBATTR LRECL(220)\"\\n\"ALLOC F(SYSIN) UNIT(VIO) TRACKS SPACE(10,10) USING(FBATTR)\"\\n/* Set up BTEQ script */\\nQUEUE \".RUN FILE=LOGON\"\\nQUEUE \"SELECT COLUMN1 FROM TABLE1;\"\\n/* Run BTEQ script */\\n\"EXECIO * DISKW SYSIN (FINIS\"\\n\"CALL \\'SYS3.TDP.APPLOAD(BTQMAIN)\\'\"; bteq_rc = rc\\n\"FREE FI(SYSPRINT SYSIN)\"\\n/* Read and parse BTEQ output */\\n\"EXECIO * DISKR SYSPRINT (STEM BTEQOUT. FINIS\"\\nDO I = 1 to BTEQOUT.0\\n   ...\\nEND\\n</code></pre>\\n\\n<p>Code to read from DB2:</p>\\n\\n<pre><code>ADDRESS TSO \"SUBCOM DSNREXX\"\\nIF RC THEN rcDB2 = RXSUBCOM(\\'ADD\\',\\'DSNREXX\\',\\'DSNREXX\\')\\nADDRESS DSNREXX \"CONNECT \" subsys\\n\\nsqlQuery = \"SELECT COLUMN2 FROM TABLE2;\"\\n\\nADDRESS DSNREXX \"EXECSQL DECLARE C001 CURSOR FOR S001\"\\nIF SQLCODE &lt;&gt; 0 THEN DO\\n   SAY \\'DECLARE C001 SQLCODE = \\' SQLCODE\\n   EXIT 12\\nEND\\nADDRESS DSNREXX \"EXECSQL PREPARE S001 FROM :sqlQuery\"\\nIF SQLCODE &lt;&gt; 0 THEN DO\\n   SAY \\'PREPARE S001 SQLCODE = \\' SQLCODE SQLERROR\\n      EXIT 12\\nEND\\nADDRESS DSNREXX \"EXECSQL OPEN C001\"\\nIF SQLCODE &lt;&gt; 0 THEN DO\\n   SAY \\'OPEN C001 SQLCODE = \\' SQLCODE\\n   EXIT 12\\nEND\\n\\nADDRESS DSNREXX \"EXECSQL FETCH C001 INTO :col2\"\\nIF SQLCODE &lt;&gt; 0 THEN DO\\n   SAY \\'FETCH C001 SQLCODE = \\' SQLCODE\\n   EXIT 12\\nEND\\n</code></pre>\\n\\n<p>I suspect that this has something to do with my use of SYSPRINT and SYSIN. Anyone know how I can get this to work?</p>\\n\\n<p>Thanks.</p>\\n\\n<p><strong>Edit</strong></p>\\n\\n<p>The question as stated was actually wrong. Apologies for not correcting this earlier.</p>\\n\\n<p>What I had really done was have this:</p>\\n\\n<pre><code>ADDRESS TSO \"SUBCOM DSNREXX\" \\nIF RC THEN rcDB2 = RXSUBCOM(\\'ADD\\',\\'DSNREXX\\',\\'DSNREXX\\') \\nADDRESS DSNREXX \"CONNECT \" subsys \\n</code></pre>\\n\\n<p>...followed by a small read from DB2, then followed by the code to read from Teradata, followed by more code to read from DB2. When this was changed to reading from Teradata first before having anything to do with DB2 at all, it worked.</p>\\n-Problems using REXX to access both Teradata output and DB2 output-<db2><mainframe><teradata><rexx>',\n",
       " \"<p>I have two tables table1 and table2. Table2 is having less number of rows than table1. In these two tables there are two date columns caldate1 in table1 and caldate2 in table2. So now I need to join on these two tables and get the maximum of the two date columns and keep that in new table. But if we inner join on these two tables the table1 rows which are not there in table2 will not go into the final table. So we need some thing like </p>\\n\\n<pre><code>table1 \\nleft outer join\\ntable2\\n</code></pre>\\n\\n<p>But there is a situation that the two dates are having nulls. So Can I use coalesce to get the correct data in the below scenarios..\\n<p>1. row in table1 is not there in table2 -> then the caldate1 in table1 should go into final table.\\n<p>2. row in table1 is there in table2 and the caldate1 of table1 and caldate2 of table2 are nulls -> then null should come into final table's date column\\n<p>3. row in table1 is there in table2 and caldate1 is not null and caldate2 is null -> then caldate1 should come into final table.\\n<p>4. row in table1 is there in table2 and caldate1 is null and caldate2 is not null -> then caldate2 should come into final table\\n<p>5. row in table1 is there in table2 and caldate1 is greater than caldate2 -> caldate1 should come into final table\\n<p>6. row in table1 is there in table2 and caldate2 is greater than caldate1 -> caldate2 should come into final table\\n<p>We dont need to consider the rows in table2 which are not matching with table1. So basically i need all table1 rows with latest of the caldate if a particular row is there in both tables. Thanks in advance. I am not able to get correct function to do it.Is it coalesce?</p>\\n-Get max of two dates from two different tables in teradata - scenario?-<left-join><max><teradata><coalesce>\",\n",
       " '<p>For a university search engine project, I am using MonetDB with Tijah extensions. I\\'ve got a list of nodes, returned from a search string:</p>\\n\\n<pre><code>let $qid := tijah:queryall-id($nexi)\\nlet $nodes := tijah:nodes($qid)\\n</code></pre>\\n\\n<p><code>$nodes</code> now contains a list of elements, e.g.: </p>\\n\\n<pre><code>&lt;book&gt;Design Patterns&lt;/book&gt;\\n&lt;book&gt;AntiPatterns&lt;/book&gt;\\n</code></pre>\\n\\n<p>I can calculate and return the scores for this list with the following FLWOR expression:</p>\\n\\n<pre><code>for $book in $nodes\\n  let $score := tijah:score($qid, $book)\\n  order by $score descending\\n  return &lt;book score=\"{$score}\"&gt;{$book/title}&lt;/book&gt;\\n</code></pre>\\n\\n<p>However, I want to use the list of nodes in a new search query. To do this, I have to generate a string from this list with the following formatting:</p>\\n\\n<pre><code>Design Patterns {0.2937} Antipatterns {0.43984}\\n</code></pre>\\n\\n<p>In this formatting the scores (returned by <code>tijah:score</code> and the names are combined. I wanted to generate this string with a recursive function, but the MonetDB Algebra engine I need to use doesn\\'t support recursive functions. </p>\\n\\n<p>Can I generate the same result with a non-recursive (possibly FLWOR) expression?</p>\\n-Concat a list of elements and scores without using a recursive function in XQuery with Tijah extensions-<xquery><monetdb>',\n",
       " \"<p>I'm looking at building some data warehousing/querying infrastructure, right now on top of Map/Reduce solutions like Hadoop.</p>\\n\\n<p>However, it strikes me that all the M/R work is just repeating what the RDBMS guys have solved for the last 20 years with parallel SQL databases. Parallel SQL implementations scale reads and writes across nodes, just like M/R, but additionally already contains the niceties from regular databases (SQL, existing integration libraries, etc).</p>\\n\\n<p>The problem is: you don't seem to find the customers of those companies posting much online. So, does anyone here have experience with those kinds of solutions, and can give me some insight and/or links?</p>\\n-Netezza, Teradata, DB2 Parallel/Enterprise, ... versus Hadoop or others?-<db2><data-warehouse><mapreduce><teradata><netezza>\",\n",
       " \"<p>We have a few tables with persisted computed columns in SQL Server.</p>\\n\\n<p>Is there an equivalent of this in Teradata?  And, if so, what is the syntax and are there any limitations?</p>\\n\\n<p>The particular computed columns I am looking at conform some account numbers by removing leading zeros - an index is also created on this conformed account number:</p>\\n\\n<pre><code>ACCT_NUM_std AS ISNULL(CONVERT(varchar(39),\\n                               SUBSTRING(LTRIM(RTRIM([ACCT_NUM])),\\n                                         PATINDEX('%[^0]%',\\n                                                  LTRIM(RTRIM([ACCT_NUM])) + '.'\\n                                                 ),\\n                                         LEN(LTRIM(RTRIM([ACCT_NUM])))\\n                                        )\\n                              ),\\n                       ''\\n                      ) PERSISTED\\n</code></pre>\\n\\n<p>With the Teradata TRIM function, the trimming part would be a little simpler:</p>\\n\\n<pre><code>ACCT_NUM_std AS COALESCE(CAST(TRIM(LEADING '0' FROM TRIM(BOTH FROM ACCT_NUM))) AS varchar(39)),\\n                         ''\\n                        )\\n</code></pre>\\n\\n<p>I guess I could just make this a normal column and put the code to standardize the account numbers in all the processes which insert into the table.  We did this to put the standardization code in one place.</p>\\n-Teradata equivalent of persisted computed column (in SQL Server)-<sql><sql-server><database-design><calculated-columns><teradata>\",\n",
       " '<p>When you need to compare two tables to see what the differences are, are there any tools or shortcuts you use, or do you handcode the SQL to compare the two tables?</p>\\n\\n<p>Basically the core features of a product like Red Gate SQL Data Compare (schemas for my tables typically always match).</p>\\n\\n<p>Background: In my SQL Server environment, I created a stored procedure which inspects the metadata of the two tables/views, creates a query (as dynamic sql) which joins the two tables on the specified key columns, and compares data in the compare columns, reporting key differences and data differences. The query can either be printed and modified/copied or just excecuted as is. We are not allowed to create stored procedures in our Teradata environment, unfortunately.</p>\\n-Best way to compare contents of two tables in Teradata?-<sql><teradata><data-comparison>',\n",
       " \"<p>I would like to be able to produce a file by running a command or batch which basically exports a table or view (SELECT * FROM tbl), in text form (default conversions to text for dates, numbers, etc are fine), tab-delimited, with NULLs being converted to empty field (i.e. a NULL colum would have no space between tab characters, with appropriate line termination (CRLF or Windows), preferably also with column headings.</p>\\n\\n<p>This is the same export I can get in SQL Assistant 12.0, but choosing the export option, using tab delimiter, setting my NULL value to '' and including column headings.</p>\\n\\n<p>I have been unable to find the right combination of options - the closest I have gotten is by building a single column with CAST and '09'XC, but the rows still have a leading 2-byte length indicator in most settings I have tried. I would prefer not to have to build large strings for the various different tables.</p>\\n-Best way to produce automated exports in tab-delimited form from Teradata?-<sql><etl><teradata>\",\n",
       " '<p>My team is thinking about developing a real time application (a bunch of charts, gauges etc) reading from the database. At the backend we have a high volume Teradata database. We expect some other applications to be constantly feeding in data into this database. \\nNow we are wondering about how to feed in the changes from the database to the application. Polling from the application would not be a viable option in our case. </p>\\n\\n<p>Are there any tools that are available within Teradata that would help us achieve this? </p>\\n\\n<p>Any directions on this would be greatly appreciated</p>\\n-Teradata Change data capture-<teradata><capture>',\n",
       " \"<p>I am trying to produce a results table with the last completed course date for each course code, as well as the last completed course code overall for each employee. Below is my query:</p>\\n\\n<pre><code>SELECT employee_number,\\n       MAX(course_completion_date) \\n           OVER (PARTITION BY course_code) AS max_course_date,\\n       MAX(course_completion_date) AS max_date\\nFROM employee_course_completion\\nWHERE course_code IN ('M910303', 'M91301R', 'M91301P')\\nGROUP BY employee_number\\n</code></pre>\\n\\n<p>This query produces the following error:</p>\\n\\n<pre><code>3504 : Selected non-aggregate values must be part of the associated group\\n</code></pre>\\n\\n<p>If I remove the MAX() OVER (PARTITION BY...) line, the query executes just fine, so I've isolated the problem to that line, but after searching these forums and the internet I can't see what I'm doing wrong. Can anyone help?</p>\\n-MAX() and MAX() OVER PARTITION BY produces error 3504 in Teradata Query-<sql><aggregate-functions><teradata><database-partitioning>\",\n",
       " '<p>I have two tables in a teradata database that look like this</p>\\n\\n<pre><code>accounts\\naccount_number integer\\ndate_updated_last datetime\\ndelinquency_code varchar(3)\\n\\npayments\\naccount_number integer\\nstatement_date datetime\\ndelinquency_code varchar(3)\\n</code></pre>\\n\\n<p>the delinquency code column is populated in accounts, but is not populated in payments. I would like to update payments with the delinquency code based on date_updated_last and statement_date. the problem is that statement_date is sequential, say a given account was opened in july of 2009, there would be one record for every month between then and now, but an account record is only added when the information changes, so there may be, for example, only 3 records in the account table for the same account. Say, august 2009, january 2010, and march 2010.  so I would want to update all the payment records between august 2009 and january 2010 with the data from the august 2009 record in accounts. can anyone point me to an easy way to do this?</p>\\n\\n<p>thank you :)</p>\\n\\n<p>-C</p>\\n-how to update rows by a range of dates in teradata-<sql><teradata>',\n",
       " '<p>Here is my code:</p>\\n\\n<pre><code>&lt;?php\\nrequire_once \\'Swift/lib/swift_required.php\\';\\n\\n$transport = Swift_SmtpTransport::newInstance(\\'smtp.gmail.com\\', 465)\\n  -&gt;setUsername(\\'me@ff.com\\')\\n  -&gt;setPassword(\\'pass\\');\\n\\n$mailer = Swift_Mailer::newInstance($transport);\\n$message = Swift_Message::newInstance(\\'Wonderful Subject\\')\\n  -&gt;setFrom(array(\\'me@ff.com\\' =&gt; \\'MY NAME\\'))\\n  -&gt;setTo(array(\\'you@ss.com\\' =&gt; \\'YOU\\'))\\n  -&gt;setBody(\\'This is the text of the mail send by Swift using SMTP transport.\\');\\n//$attachment = Swift_Attachment::newInstance(file_get_contents(\\'path/logo.png\\'), \\'logo.png\\');  \\n//$message-&gt;attach($attachment);\\n$numSent = $mailer-&gt;send($message);\\nprintf(\"Sent %d messages\\\\n\", $numSent);\\n?&gt;\\n</code></pre>\\n\\n<p>AFter RUNNING GOT THIS ERROR...</p>\\n\\n<blockquote>\\n  <p>Fatal error: Uncaught exception \\'Swift_TransportException\\' with message \\'Expected response code 220 but got code \"\", with message \"\"\\' in /home/sitenyou/public_html/Swift/lib/classes/Swift/Transport/AbstractSmtpTransport.php:406 </p>\\n</blockquote>\\n\\n<pre><code>Stack trace: \\n#0 /home/sitenyou/public_html/Swift/lib/classes/Swift/Transport/AbstractSmtpTransport.php(299): Swift_Transport_AbstractSmtpTransport-&gt;_assertResponseCode(\\'\\', Array) \\n#1 /home/sitenyou/public_html/Swift/lib/classes/Swift/Transport/AbstractSmtpTransport.php(107): Swift_Transport_AbstractSmtpTransport-&gt;_readGreeting() \\n#2 /home/sitenyou/public_html/Swift/lib/classes/Swift/Mailer.php(74): Swift_Transport_AbstractSmtpTransport-&gt;start() \\n#3 /home/sitenyou/public_html/sgmail.php(16): Swift_Mailer-&gt;send(Object(Swift_Message)) \\n#4 {main} thrown in /home/sitenyou/public_html/Swift/lib/classes/Swift/Transport/AbstractSmtpTransport.php on line 406\\n</code></pre>\\n-trying to send mail using swift mailer, gmail smtp, php-<php><smtp><swiftmailer>',\n",
       " \"<p>oI have a teradata table with about 10 million records in it, that stores a numeric id field as a varchar.  i need to transfer the values in this field to a bigint column in another table, but i can't simply say cast(id_field as bigint) because i get an invalid character error.  looking through the values, i find that there could be a character at any position in the string, so let's say the string is varchar(18) i could filter out invalid rows like so :</p>\\n\\n<pre><code>     where substr(id_field,1,1) not in (/*big,ugly array of non-numeric chars*/)\\n     and substr(id_field,2,1) not in (/*big,ugly array of non-numeric chars*/)\\n\\netc, etc... \\n</code></pre>\\n\\n<p>then the cast would work, but this is not feasible in the long run. it's slow and if the string has 18 possible characters, it makes the query unreadable.  how can i filter out rows that have a value in this field that will not cast as a bigint without checking each character individually for an array of non-numeric characters?</p>\\n\\n<p>example values would be </p>\\n\\n<pre><code>   123abc464\\n   a2.3v65\\n   a_356087\\n   ........\\n   000000000\\n   BOB KNIGHT\\n   1235468099\\n</code></pre>\\n\\n<p>the values follow no specific patterns, I simply need to filter out the ones that contain ANY non-numeric data.\\n          123456789 is okay but 123.abc_c3865 is not...</p>\\n-how do i filter out non-numeric values in a text field in teradata?-<sql><types><filtering><teradata>\",\n",
       " '<p>I use teradata and the below query outputs \"Altlüd\" when run using a teradata client.</p>\\n\\n<pre><code>select name as name  from MYTABLE where selector=?\\n</code></pre>\\n\\n<p>Whereas, I get \"Altl?d\" as the output when I try to execute the query using a java client(jdbc with teradata drivers). I am using \"UTF-8\" charset and I have also tried Latin charset with no luck.</p>\\n\\n<p>I have also tried this to troubleshoot.</p>\\n\\n<pre><code>while (rs.next()) {\\n System.out.println(rs.getString(1));\\n Reader rd = rs.getCharacterStream(1);\\n int charr = rd.read();\\n while (charr &gt;= 0) {\\n    System.out.println(charr + \" = \" + ((char) charr));\\n    charr = rd.read();\\n }\\n}\\n</code></pre>\\n\\n<p>And the output is </p>\\n\\n<p>Altl?dersdorf\\n65 = A\\n108 = l\\n116 = t\\n108 = l\\n65533 = ?\\n100 = d</p>\\n\\n<p>If you look at the output produced, the int value for the spl character is 65533 which shouldn\\'t be the case. </p>\\n\\n<p>Infact it returns 65533 for all the special characters.</p>\\n\\n<p>Any clues/pointers will be appreciated. Thanks!!!</p>\\n-Problem reading special characters from teradata - JDBC-<java><jdbc><teradata>',\n",
       " \"<p>I'm using the Apache BasicDataSource for both commons DBCP and connection pool:</p>\\n\\n<pre><code>org.apache.commons.dbcp.BasicDataSource\\n</code></pre>\\n\\n<p>and managing it through Spring:</p>\\n\\n<pre><code>org.springframework.jdbc.datasource.DataSourceTransactionManager\\n</code></pre>\\n\\n<p>While using this combination with the Teradata JDBC driver if my database goes down or there is a network glitch I receive the following error:</p>\\n\\n<blockquote>\\n  <p>08S01 804 : I/O Error, Socket closed.  Packet stream write error</p>\\n</blockquote>\\n\\n<p>Which reflects the situation correctly but the problem is with reconnecting. When the error is ironed out physically or DB comes back up and my program tries to query it, I still end up getting the same error for some time period which varies e.g. 20 minutes, hours, never.\\nThe problem goes away when either I restart my JVM or change the connection string e.g. use IP address instead of hostname.</p>\\n\\n<p>Is there any setting in the DataSource or the Spring Transaction Manager that can rectify this? or maybe a TCP/IP setting?</p>\\n-JDBC reconnect problems with Teradata driver using Spring and Apache DataSource-<spring><jdbc><datasource><teradata>\",\n",
       " '<p>I\\'d like to create a table out of the dataset generated by teradata\\'s \"help table\" function so i can add some more information about the table, and be able to filter the rows by conditions. the table has 400+ columns, so this would be very convenient for management.  I\\'d like to be able to do something similar to creating a table as select, but it doesn\\'t work with the help table syntax. short of exporting the data to excel, then manually creating the table schema and importing the table back in, does anyone know how to convert the output of a help table query into a table in teradata?</p>\\n-How to filter teradata help table-<teradata>',\n",
       " '<p>I am currently trying to optimize some bobj reports where our backend is Teradata. The <strong>Teradata optimizer</strong> seems very finicky and I was wondering if anyone has come up with a <strong>solution or a workaround</strong> to get the optimizer to <strong>treat likes in a similar regard to equals</strong>.</p>\\n\\n<pre><code>My issue is that we allow the user to input one of two methods:\\n 1. Enter the Number:\\n    or\\n 2. Enter a Number like:\\n</code></pre>\\n\\n<p>Option one performs like a dream while option two is dragging our query times from 6 seconds to 2 minutes.</p>\\n\\n<p>In addition to this; does anyone know of any good articles, discussions, vidoes, etc.. on optimizing SQL statements for the teradata optimizer?</p>\\n-Teradata Optimizer Equal vs Like in SQL-<sql><teradata>',\n",
       " '<p>I\\'m trying to query a table that has a varchar(100) \"VALUE\" column. This column can hold anything from a letter, a number or, in this case, a date.</p>\\n\\n<p>The date will always be entered in the table as \\'YYYY-mm-dd\\'. However, when I run the following query:</p>\\n\\n<pre><code>select * from myTable\\nwhere VALUE =  \\'2009-12-11\\' (Date, Format \\'yyyy-mm-dd\\')  \\n</code></pre>\\n\\n<p>I receive the following error:</p>\\n\\n<pre><code>Invalid date supplied for myTable.VALUE.\\n</code></pre>\\n\\n<p>Example of the value table:\\n(1,\\'122\\')\\n(2,\\'red\\')\\n(3,\\'2009-12-11\\')</p>\\n\\n<p>Any ideas as to what might be causing this?</p>\\n\\n<p>Thanks!</p>\\n-Teradata - Invalid Date supplied for FIELD-<sql><teradata>',\n",
       " '<p>I have am working on a .net application that integrates with a Teradata database version 12. Currently, we are using Odbc at the DAL to interface with the database. </p>\\n\\n<p>Teradata have come out with the .Net Managed provider recently. I was wondering if there are any performance benchmarks available to compare ODBC with the .Net Managed provider. Does anyone know (from experience) if we are likely to see any significant performance gain by switching to the .Net Managed Provider for Teradata? </p>\\n\\n<p>Any insights are highly appreciated! </p>\\n\\n<p>Regards</p>\\n\\n<p>AC</p>\\n-.Net Data Access integration with Teradata 12.0-<.net><odbc><managed><provider><teradata>',\n",
       " '<p>I need to alter a macro the way that a parameter can be passed to control the max size of the result set.</p>\\n\\n<p>My idea was this SQL:</p>\\n\\n<pre><code>REPLACE MACRO myMacro\\n( maxRows INTEGER DEFAULT 100 )\\nAS\\n(\\n    SELECT TOP :maxRows\\n    FROM myTable;\\n);\\n</code></pre>\\n\\n<p>But all I get is the message:</p>\\n\\n<blockquote>\\n  <p>[SQLState 42000] Syntax error,Expected something like an Integer or decimal number between \\'top\\' and \\':\\'.</p>\\n</blockquote>\\n\\n<p><strong>It\\'s not possible for me to do this in any other way than a macro.</strong></p>\\n\\n<p>How can I do that?</p>\\n-Dynamic \"SELECT TOP :parameter\" in Teradata Macro-<sql><teradata>',\n",
       " '<p>The title is horrible but that\\'s the best I could do.  What I have is something like this:</p>\\n\\n<pre><code>Country  Tag    Weight\\n-----------------------\\n1        1      20\\n1        2      30\\n1        3      77\\n2        1      10\\n2        2      11\\n2        3      100\\n</code></pre>\\n\\n<p>Or, in a human readable form:</p>\\n\\n<pre><code>Country  Tag    Weight\\n-----------------------\\nUSA      Rock   20\\nUSA      Pop    30\\nUSA      Metal  77\\nSpain    Rock   10\\nSpain    Pop    11\\nSpain    Metal  100\\n</code></pre>\\n\\n<p>Using either SQL (to create a view) or Data Manipulation Tools in Knime, I need to manipulate the data into this form:</p>\\n\\n<pre><code>Country  Rock   Pop   Metal\\n----------------------------\\nUSA      20     30    77\\nSpain    10     11    100\\n</code></pre>\\n\\n<p>Essentially, Tag entries (unique ones) become the columns and countries (unique ones) become the row id\\'s with the weight values sticking with their country/tag.</p>\\n\\n<p>I\\'ve tried everything I can think of in Knime and no raw SQL query springs to mind.  In Knime, I successfully created the structure of the matrix I want (Country x Tag), but I have no idea how to populate the actual Weight values, they\\'re all question marks.  My working solution is to simply output the data into a CSV file in the form I want instead of into the database.  But that\\'s klunky and annoying to keep in sync.  Any ideas?</p>\\n-SQL/Knime - Transpose Table with \"Group By\"-<mysql><sql><pivot><knime>',\n",
       " '<p>I have a simple script that I use to automate CLI calls to our software (the Moab Workload Manager) in testing, to avoid having to use the \\'--xml\\' flag to get xml output and then pipe it through tidy so it\\'s easily readable. It uses a <code>subprocess.Popen</code> call to run the command, then uses <code>str.strip()</code> and <code>str.replace()</code> to do a minor cleanup on the returned xml to make it easy to visually inspect. The code in question is here:</p>\\n\\n<hr />\\n\\n<pre><code>cmdString = \"%s --xml\" % cmd\\ncmdList = cmdString.split()\\n\\ncmdRun = subprocess.Popen(cmdList,\\n    stdout=subprocess.PIPE,\\n    stderr=subprocess.PIPE)\\n\\ncrOut,crErr = cmdRun.communicate()\\n\\nxmlOutput = crOut.strip().replace(\"&gt;&lt;\",\"&gt;\\\\n&lt;\").replace(\"\\\\\" \",\"\\\\\"\\\\n\")\\n</code></pre>\\n\\n<p><hr />\\nWhen I run this (I recently upgraded my Python to Python 3.1.2) I now get the following error:</p>\\n\\n<hr />\\n\\n<pre><code>Traceback (most recent call last):\\n  File \"/usr/local/bin/xmlcmd\", line 50, in &lt;module&gt;\\n    runXMLCmd(getCmd())\\n  File \"/usr/local/bin/xmlcmd\", line 45, in runXMLCmd\\n    xmlOutput = crOut.strip().replace(\"&gt;&lt;\",\"&gt;\\\\n&lt;\")\\nTypeError: expected an object with the buffer interface\\n</code></pre>\\n\\n<p><hr />\\nIt appears that the communicate() call is returning byte arrays, but in the python interpreter, if I do <code>dir(bytes)</code> I can still see the strip() and replace() functions. Anybody know how to make this right?</p>\\n\\n<p>Thanks.</p>\\n-Python 3 subprocess.PIPE output error-<python><python-3.x>',\n",
       " '<p>What makes my situation tricky is that I don\\'t have a single column key, with a simple list of primary keys to delete (for instance, \"delete from table where key in ([list])\").  I have multiple columns together as the primary key, and would need to join on all of them.</p>\\n\\n<p>Using what I know of other databases, I thought this might be done as:</p>\\n\\n<pre><code>DELETE FROM\\n    table1 t1\\n  LEFT OUTER JOIN\\n      table2 t2\\n    ON\\n      t2.key1 = t1.key1 AND\\n      t2.key2 = t1.key2\\n  WHERE\\n    t2.key1 IS NULL;\\n</code></pre>\\n\\n<p>But Teradata (v12) responds with error number 3706, saying \"Syntax error: Joined Tables are not allowed in FROM clause.\"</p>\\n-How to delete rows in a Teradata table that are not in another table?-<sql><teradata>',\n",
       " '<p>I have been using Rapidminer and created a series of processes which preform a standard set of tasks. Now, I want allow the user to dynamically set the parameters of a process at the start. </p>\\n\\n<p>For example, when writing  a CSV, I want to prompt the user to type a string containing the location where it should be saved via some prompt (either at the start of the script, or at some other stage during the process.</p>\\n\\n<p>Is this possible via Rapidminer, or should I be creating some script to generate and runt he process on the fly?</p>\\n-Runtime pompt for Rapidminer-<data-mining><text-mining><rapidminer>',\n",
       " '<p>I have a collection of data that looks as follows:</p>\\n\\n<pre><code>id   name     c1    c2    c3    c4   ...  c50\\n-----------------------------------------------\\n1    string1  0.1   0.32  0.54 -1.2  ...  2.3\\n2    string2  0.12  0.12 -0.34  2.45 ...  1.3\\n...\\n(millions of records)\\n</code></pre>\\n\\n<p>So I have an id column, a string column, then 50 floating point columns.</p>\\n\\n<p>There will be only one type of query run on this data that in a traditional SQL SELECT statement would look like this:</p>\\n\\n<p><code>SELECT name FROM table WHERE ((a1-c1)+(a2-c2)+(a3-c3)+...+(a50-c50)) &gt; 1;</code> where <code>a1,a2,a3,etc</code> are values that are generated before the query is sent (not housed in the data table).</p>\\n\\n<p>My question is this: <strong>Does anyone have any recommendations as to what type of database would handle this type of query the fastest.</strong>  I have used <code>SQL server</code> (which is majorly slow), so I am looking for other opinions.</p>\\n\\n<p>Would there be a way to optimize SQL server for this type of query? I have also been curious about column store databases such as <code>MonetDB</code>. Or perhaps a document store database such as <code>MongoDB</code>. Does anyone have any suggestions?</p>\\n\\n<p>Many thanks,\\nBrett</p>\\n-Database recommendation-<sql><sql-server><mongodb><monetdb><database>',\n",
       " '<p>what is the difference between minus and except in teradata?</p>\\n-difference between minus and except in Teradata-<sql><teradata>',\n",
       " '<p>I am wondering what I should use to connect to TD via VB.NET. Whether or not I should use ODBC, etc.... My server is TDDEV, database BCPM_DDBO. Also, passwords are changed every so often by DB Admin so I would not want to hard code a password in the connection string. I know that ODBC will prompt for user/pass. Is this the route to go? Thanks so much!</p>\\n-vb.net to Teradata Connection string-<sql><vb.net><teradata>',\n",
       " \"<p>I installed Oracle Server Express 10g on my computer (WinXP). I want to create a database link to Teradata using ODBC. I've created (non-ODBC) database links to other Oracle databases successfully. However, I can't seem to get the Teradata database link to work.</p>\\n\\n<p>Here's what I did:</p>\\n\\n<p>1) Created an ODBC Connection in Windows to Teradata using Teradata's ODBC driver version 13. Tested that it works by connecting to the database using Teradata SQL Assistant. Called the connection LPS_PROD_VIEW. I saved my Login details in the ODBC settings.</p>\\n\\n<p>2) Edited listener.ora \\nIn the SID_LIST_LISTENER section: </p>\\n\\n<pre><code>(SID_DESC =\\n  (SID_NAME = LPS_PROD_VIEW)\\n  (ORACLE_HOME = C:\\\\oraclexe\\\\app\\\\oracle\\\\product\\\\10.2.0\\\\server)\\n  (PROGRAM = hsodbc)\\n</code></pre>\\n\\n<p>In the LISTENER section</p>\\n\\n<pre><code>     (ADDRESS = (PROTOCOL = TCP)(HOST = localhost)(PORT = 1524)\\n</code></pre>\\n\\n<p>3) In the ...hs\\\\admin\\\\ folder, added initLPS_PROD_VIEW.ora file. Contents:</p>\\n\\n<pre><code>HS_FDS_CONNECT_INFO = LPS_PROD_VIEW\\nHS_FDS_TRACE_LEVEL = ON\\n</code></pre>\\n\\n<p>4) Added an entry in TNSnames.ora (both in the XE server directory and a seperate 10g directory which I had previously before installing Oracle XE).</p>\\n\\n<pre><code>BMW = \\n  (DESCRIPTION = \\n  (ADDRESS_LIST = \\n  (ADDRESS = (PROTOCOL = TCP)(Host = localhost)(Port = 1524))\\n  )(CONNECT_DATA = \\n  (SID = LPS_PROD_VIEW)(HS=OK)\\n  )\\n</code></pre>\\n\\n<p>5) Restarted Oracle listener services through services.msc.</p>\\n\\n<p>6) Connected to local database to create the database link by doing</p>\\n\\n<pre><code>Create database link TERADATA connect to &lt;username&gt; identified by &lt;password&gt; using 'LPS_PROD_VIEW' \\n</code></pre>\\n\\n<p>7) Attempt to run queries but get an ORA-12154: TNS: Could not resolve the connect identifier specified. </p>\\n\\n<p>What am I doing wrong? Does HS support Teradata ODBC ver 13? </p>\\n\\n<p>Thanks in advance and appreciate your help!</p>\\n-Oracle XE Database link to Teradata using ODBC-<oracle><odbc><teradata><heterogeneous-services>\",\n",
       " '<p>This is almost the same problem as <a href=\"https://stackoverflow.com/questions/920207/informix-defining-an-interval-with-a-parameter\">Informix defining an INTERVAL with a parameter</a> but is in Teradata.</p>\\n\\n<p>I\\'m creating a macro that accepts a string in the form hh:mm:ss to be used as an interval.</p>\\n\\n<p>The macro wants to do something with a timestamp in the past hh:mm:ss.</p>\\n\\n<p>Here\\'s the basic sql</p>\\n\\n<pre><code>CREATE MACRO TEST_MACRO (\\n    HHMMSS CHAR(8)\\n)\\nAS\\n(\\nSELECT \\n    CAST(CURRENT_TIME AS TIMESTAMP(0)),\\n    CAST(CURRENT_TIME - INTERVAL :HHMMSS HOUR TO SECOND AS TIMESTAMP(0))\\n)\\n</code></pre>\\n\\n<p>I get the error <code>Failed 3707: Syntax error, expected something like a string or a Unicode character literal between the \\'INTERVAL\\' keyword and \\':\\'</code>.</p>\\n\\n<p>Is there a way around this?</p>\\n-Teradata macro parameter as time interval-<sql><teradata>',\n",
       " '<p>My question is quite similar to this one, but in Teradata:</p>\\n\\n<p><a href=\"https://stackoverflow.com/questions/859277/sql-server-equivalent-of-mysqls-using\">SQL Server equivalent of MySQL&#39;s USING</a></p>\\n\\n<p>Is there any equivalent shortcut to this query?</p>\\n\\n<pre><code>SELECT * \\n  FROM t1 \\n  JOIN t2 \\n    ON (t1.column = t2.column)\\n</code></pre>\\n-Teradata equivalent of MySQL\\'s USING-<mysql><join><using><teradata>',\n",
       " '<p>I need to query data in Tera database from SQLSERVER 2005. I have IP address, userid, password, db name of the tera data base. how do i query a view in teradata from sqlserver.</p>\\n-how to select values from teradata server view from MS sqlserver?-<sql-server><teradata>',\n",
       " '<pre><code>CREATE TABLE \"CMCAPACITY\"(\"CMPATH\" VARCHAR(4000)NOT NULL,\\n        \"CAPACITY\" FLOAT(128), \"USAGE\" SMALLINT NOT NULL); \\n</code></pre>\\n\\n<p>I am getting \"Precision error in FLOAT type constant or during implicit conversions.\"</p>\\n-What\\'s the max value for the Float data type in teradata?-<types><teradata>',\n",
       " \"<p>I would like to know if there is any way of limiting the number of output results in monetdb, like it's done in Oracle for example, using:</p>\\n\\n<pre><code>.SET RETCANCEL ON\\n.SET RETLIMIT -1\\n</code></pre>\\n\\n<p>EDIT: I have found the way of limiting the number of the output rows adding the following sql after query:</p>\\n\\n<pre><code>limit 1 offset 0;\\n</code></pre>\\n\\n<p>But I'm not sure if it does the same thing.. It's important because I'm measuring the speed of different databases and such kind of things are quite important.</p>\\n\\n<p>Thanks a lot,\\nSerhiy.</p>\\n-Monetdb limit results displayed after query-<sql><monetdb>\",\n",
       " '<p>I\\'m currently learning how to make some plugins for eclipse.</p>\\n\\n<p>I first developed a plugin for the <a href=\"http://www.knime.org\" rel=\"nofollow\">KNIME</a> project . This project contains a plugin.xml and a MANIFEST.MF file:</p>\\n\\n<pre><code>(...)\\nBundle-Activator: project1.MyNodePlugin\\n(...)\\n</code></pre>\\n\\n<p>The second project is a \"preferences panel\" that was generated with an eclipse wizard. It also contains a MANIFEST.MF &amp; a plugin.xml:</p>\\n\\n<pre><code>(...)\\nBundle-Activator: project2.Activator\\n(...)\\n</code></pre>\\n\\n<p>I want my project project1 (KNIME) to use some data from my preference panel. </p>\\n\\n<p>How should I merge the two projects (MANIFEST.MF ?) ?</p>\\n\\n<p>How can I get the preferences of the project2 from the project1 ?</p>\\n\\n<p>Thanks</p>\\n-Merging two eclipse plugins-<java><eclipse><plugins><eclipse-plugin><knime>',\n",
       " \"<p>you can fill a cell with '=cos(0)' and you get displayed its value '1'.</p>\\n\\n<p>is there some similar function(ality) to choose the font-color?</p>\\n\\n<p>something like '=COLOR(the text to display, #FF0000)'.</p>\\n\\n<p>if not, how could you achieve something like that?</p>\\n\\n<p>here is where I come from: I write data to an excel-file (using KNIME btw) and I want to choose the font-color.</p>\\n-is specifying a cell's color (in an excel spreadsheet) by its very textual content possible?-<excel><knime>\",\n",
       " '<p>Simple Problem:\\nI want to connect my linux based C++ program to a Teradata database.\\nHow do I accomplish this?</p>\\n\\n<p>I searched the web but I only found some ADO and ODBC based solutions for .net or JDBC drivers. Is there any lib out there that can do the job on linux without ODBC and .net?</p>\\n\\n<p>Greetings,\\nLars</p>\\n-How to connect to Teradata with C++ on linux-<c++><database-connection><teradata>',\n",
       " \"<p>I've imported a datset into Rapidminer 5 and one of the columns that was supposed to be nominal or polynomial was set as a numeric. My data set has over 500 attributes so I don't really want to have to reimport my data every time I realize I've made a mistake like this. Is there some way to either automate the import process so that it saves the column types I set each time or can I go back and edit my already imported data set attribute types?</p>\\n-In Rapidminer once I import a data set how do I change the type of a column?-<data-mining><rapidminer>\",\n",
       " '<p>I have a problem importing a CSV file with RapidMiner.\\nFloating point values are written with commas instead of the separating dot between the integer and decimal values.</p>\\n\\n<p>Anyone know how to import correctly the values formatted in this way?</p>\\n\\n<p>sample data:</p>\\n\\n<p><code>\\nBMI;1;0;1;1;1;blue;-0,138812155;0,520378909;5;0;50;107;0;9;0;other;good;2011\\nBMI;1;0;1;1;1;pink;-0,624654696;;8;0;73;120;1;3;0,882638889;other;good;2011\\n</code></p>\\n\\n<p>Rapid miner actually interprets it as \"polynomial\". Forcing it to \"real\" leads only to a correct interpretation of the \"0\" value.</p>\\n\\n<p>thanks</p>\\n-Rapid miner: CSV with real numbers with commas instead of dots-<csv><floating-point><machine-learning><data-mining><rapidminer>',\n",
       " \"<p>there is a dataset in excel containing some labels in column A(I call it cluster label) and some attributes in column B(I call them cluster component). These data show the best clustering result. </p>\\n\\n<p>But I don't know how to compute recall and precision of other clustering method using these data in rapidminer!</p>\\n\\n<p>can anybody help me?</p>\\n-recall and precision in rapidminer-<cluster-analysis><rapidminer><precision-recall>\",\n",
       " \"<p>I'm using JDBC to query a Teradata server. There are up to 100 simultaneous requests, each one using a fresh connection, and closing it at the end. After some hours of work, some of the threads performing the requests get stuck indefinitely. Eventually a system restart is needed.\\nFrom inspecting the call stacks, I see that the threads are in a socket read state, and that it happens when <strong>preparing a statement</strong> or when <strong>closing the connection</strong>:</p>\\n\\n<p>Case 1:</p>\\n\\n<pre><code>java.lang.Thread.State: RUNNABLE\\n               at java.net.SocketInputStream.socketRead0(Native Method)\\n               at java.net.SocketInputStream.read(SocketInputStream.java:129)\\n               at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.read(TDNetworkIOIF.java:649)\\n               at com.teradata.jdbc.jdbc_4.io.TDPacketStream.readStream(TDPacketStream.java:818)\\n               at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.action(StatementReceiveState.java:125)\\n               at com.teradata.jdbc.jdbc_4.statemachine.StatementController.runBody(StatementController.java:112)\\n               at com.teradata.jdbc.jdbc_4.statemachine.StatementController.run(StatementController.java:103)\\n               at com.teradata.jdbc.jdbc_4.Statement.executeStatement(Statement.java:340)\\n               at com.teradata.jdbc.jdbc_4.Statement.prepareRequest(Statement.java:507)\\n               - locked &lt;0x00002aab4f787518&gt; (a com.teradata.jdbc.jdbc_4.PreparedStatement)\\n               at com.teradata.jdbc.jdbc_4.PreparedStatement.&lt;init&gt;(PreparedStatement.java:66)\\n               at com.teradata.jdbc.jdbc_4.TDSession.createPreparedStatement(TDSession.java:723)\\n               at com.teradata.jdbc.jdbc_3.ifjdbc_4.TeraLocalPreparedStatement.&lt;init&gt;(TeraLocalPreparedStatement.java:89)\\n               at com.teradata.jdbc.jdbc_3.ifjdbc_4.TeraLocalConnection.prepareStatement(TeraLocalConnection.java:333)\\n               at com.teradata.jdbc.jdbc_3.ifjdbc_4.TeraLocalConnection.prepareStatement(TeraLocalConnection.java:152)\\n...\\n</code></pre>\\n\\n<p>Case 2:</p>\\n\\n<pre><code>java.lang.Thread.State: RUNNABLE\\n               at java.net.SocketInputStream.socketRead0(Native Method)\\n               at java.net.SocketInputStream.read(SocketInputStream.java:129)\\n               at com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF.read(TDNetworkIOIF.java:649)\\n               at com.teradata.jdbc.jdbc_4.io.TDPacketStream.readStream(TDPacketStream.java:818)\\n               at com.teradata.jdbc.jdbc_4.io.TDPacketStream.readStream(TDPacketStream.java:794)\\n               at com.teradata.jdbc.jdbc.GenericLogOffRspState.action(GenericLogOffRspState.java:66)\\n               at com.teradata.jdbc.jdbc.GenericLogoffController.run(GenericLogoffController.java:43)\\n               - locked &lt;..&gt; (a com.teradata.jdbc.jdbc_4.io.TDNetworkIOIF)\\n               at com.teradata.jdbc.jdbc_4.TDSession.close(TDSession.java:476)\\n               at com.teradata.jdbc.jdbc_3.ifjdbc_4.TeraLocalConnection.close(TeraLocalConnection.java:259)\\n...\\n</code></pre>\\n\\n<p>I'm using the JDBC Teradata driver version 13.10.00.10.</p>\\n\\n<p>Any idea why it happens?\\nThis issue is very painful for us and any help will be appreciated.</p>\\n\\n<p>Thanks!</p>\\n-Threads hanging permanently on JDBC Teradata requests-<java><jdbc><teradata>\",\n",
       " \"<p>I have been building cubes for the last six month and was doing well until I was approached with a new task. This new task was to setup a cube that uses Teradata as it's data source so I get to building the cube and I am able to select a data source and test connection that goes fine. I am able to build a data source view selecting tables from within our teradata infrastructure. I am able to explore the data at the datasource level. So I build the cube and select my measures and build a dimension to go with this cube. so when I go to process this cube I get and Idbconnection error saying that the teradata client has thrown an exception. Keep in mind I am able to right click on the tables in the data source view and explore the data. If anyone has any insight on what I may be doing wrong and if any other information is needed let me know.</p>\\n-Connecting SSAS to Teradata-<sql-server-2008-r2><ssas><teradata><ssas-2008>\",\n",
       " '<p>I wrote a plugin for eclipse ( <a href=\"http://tech.knime.org/node/10043\" rel=\"nofollow\">knime</a> ). But the jar generated by eclipse doesn\\'t contain any class:</p>\\n\\n<pre><code>jar tvf plugins/fr.inserm.umr915.knime4ngs.nodes_1.0.0.jar\\n     0 Thu May 19 08:45:26 CEST 2011 META-INF/\\n  2543 Thu May 19 08:45:24 CEST 2011 META-INF/MANIFEST.MF\\n  7941 Thu May 19 08:45:24 CEST 2011 plugin.xml\\n</code></pre>\\n\\n<p>here is the content of my manifest:</p>\\n\\n<pre><code>Manifest-Version: 1.0\\nBundle-ManifestVersion: 2\\nBundle-Name: Node extension for KNIME Workbench\\nBundle-SymbolicName: fr.inserm.umr915.knime4ngs.nodes;singleton:=true\\nBundle-Version: 1.0.0\\nBundle-Vendor: Pierre\\nRequire-Bundle: org.eclipse.core.runtime,\\n org.knime.workbench.core,\\n org.knime.workbench.repository,\\n org.knime.base\\nBundle-RequiredExecutionEnvironment: JavaSE-1.6\\nExport-Package: fr.inserm.umr915.knime4ngs.corelib.bio,\\n fr.inserm.umr915.knime4ngs.corelib.knime,\\n fr.inserm.umr915.knime4ngs.corelib.util,\\n fr.inserm.umr915.knime4ngs.nodes.bam.view,\\n fr.inserm.umr915.knime4ngs.nodes.sql.query;\\n  uses:=\"org.knime.core.node,\\n   org.eclipse.core.runtime,\\n   org.osgi.framework,\\n   org.knime.core.data,\\n   org.knime.core.node.defaultnodesettings,\\n   fr.inserm.umr915.knime4ngs.corelib.knime\",\\n fr.inserm.umr915.knime4ngs.nodes.unix.echo;\\n (... other packages here ...)\\n  uses:=\"fr.inserm.umr915.knime4ngs.nodes,\\n   org.knime.core.node,\\n   org.eclipse.core.runtime,\\n   org.osgi.framework,\\n   org.knime.core.node.defaultnodesettings\"\\n</code></pre>\\n\\n<p>ay help ?\\nThanks</p>\\n\\n<p><strong>EDIT</strong>:\\nand here is my build.properties:</p>\\n\\n<pre><code>bin.includes = plugin.xml,\\\\\\n               META-INF/\\n</code></pre>\\n-Eclipse exports an empty plugin-<eclipse><eclipse-plugin><export><knime>',\n",
       " '<p>I\\'m trying to execute Teradata stored procedure from SAS ,but am failed to find the correct syntax,\\nhere is example of what i tried: </p>\\n\\n<p>libname tbconn teradata server=\"10.11.18.15\" database=\"yy\" user=x pw=xx;\\nexecute tbconn.ProcedureName(date \\'2011-03-31\\');</p>\\n\\n<p>and i also tried to use call command instead of excecute but it didnt work also.\\nany idea people.</p>\\n-Executing teradata stored procedure from SAS-<sas><teradata>',\n",
       " '<p>I\\'m back with a question. I\\'m playing with Rapid Miner for automatic text classification and cant get it work. I\\'m getting an error that says, \"no example set in the example, offending operator Performance \". Any idea what that is referring to ? </p>\\n-Rapid Miner 101-<java><rapidminer>',\n",
       " \"<p>I'm working with a SSH server, to run my Galaxy platform. Without using SSH, I can use my Galaxy website, so I think that Galaxy runs on deamon. Now I want to update my Galaxy files, and therefore I need to stop the Galaxy server and restart him. </p>\\n\\n<p>My question is, how can I stop the Galaxy server, make an update en restart this server.</p>\\n-Restart Galaxy in SSH-<ssh><restart><galaxy>\",\n",
       " '<p>How do I copy data from multiple tables within one database to another database residing on a different server?</p>\\n\\n<p>Is this possible through a BTEQ Script in Teradata?</p>\\n\\n<p>If so, provide a sample.</p>\\n\\n<p>If not, are there other options to do this other than using a flat-file?</p>\\n-Bteq Scripts to copy data between two Teradata servers-<sql><teradata>',\n",
       " '<p>I am new to Teradata. Can anyone tell me How exactly the AMPs going to helpful in creation of any table in Teradata.\\nLets have a scenario.\\nI have a Teradata database with 4 AMPs. I learned that AMPs will usefull when we inserting the data into a table. Depending on the indexes it will distribute the data with the help of respected AMPs. But while creating the table, the command needs to execute through AMPs only. So i want to know which AMP will be used at that time??</p>\\n-Use of AMPs in create table command in Teradata-<teradata>',\n",
       " '<p>I have a question about the use of gsub. The rownames of my data, have the same partial names. See below: </p>\\n\\n<pre><code>&gt; rownames(test)\\n[1] \"U2OS.EV.2.7.9\"   \"U2OS.PIM.2.7.9\"  \"U2OS.WDR.2.7.9\"  \"U2OS.MYC.2.7.9\"\\n[5] \"U2OS.OBX.2.7.9\"  \"U2OS.EV.18.6.9\"  \"U2O2.PIM.18.6.9\" \"U2OS.WDR.18.6.9\"\\n[9] \"U2OS.MYC.18.6.9\" \"U2OS.OBX.18.6.9\" \"X1.U2OS...OBX\"   \"X2.U2OS...MYC\"\\n[13] \"X3.U2OS...WDR82\" \"X4.U2OS...PIM\"   \"X5.U2OS...EV\"    \"exp1.U2OS.EV\"\\n[17] \"exp1.U2OS.MYC\"   \"EXP1.U20S..PIM1\" \"EXP1.U2OS.WDR82\" \"EXP1.U20S.OBX\"\\n[21] \"EXP2.U2OS.EV\"    \"EXP2.U2OS.MYC\"   \"EXP2.U2OS.PIM1\"  \"EXP2.U2OS.WDR82\"\\n[25] \"EXP2.U2OS.OBX\"\\n</code></pre>\\n\\n<p>In my previous question, I asked if there is a way to get the same names for the same partial names. See this question: <a href=\"https://stackoverflow.com/questions/5191429/replacing-rownames-of-data-frame-by-a-sub-string\">Replacing rownames of data frame by a sub-string</a> </p>\\n\\n<p>The answer is a very nice solution. The function gsub is used in this way:  </p>\\n\\n<pre><code> transfecties = gsub(\".*(MYC|EV|PIM|WDR|OBX).*\", \"\\\\\\\\1\", rownames(test)\\n</code></pre>\\n\\n<p>Now, I have another problem, the program I run with R (<a href=\"http://main.g2.bx.psu.edu/\" rel=\"nofollow noreferrer\">Galaxy</a>) doesn\\'t recognize the | characters. My question is, is there another way to get to the same solution without using this |?</p>\\n\\n<p>Thanks!  </p>\\n-R: Replacing rownames of data frame by a substring[2]-<r><dataframe><gsub><galaxy>',\n",
       " '<p>I am trying to execute this query:</p>\\n\\n<pre><code>INSERT INTO SEN.Interval_Day_Minute_Table VALUES(\\'NegativeIntervalDay\\', CAST(INTERVAL -\\'200 5\\' DAY TO HOUR AS INTERVAL DAY (4) TO MINUTE))\\n</code></pre>\\n\\n<p>into a teradata database using ODBCtest, and I get the following error:</p>\\n\\n<pre><code>SQLExecDirect:\\n                In:             Statementhandle = 0x000000000053C270, StatementText = \"INSERT INTO SEN.Interval_Day_Minute_Table VALUES(\\'...\", Statementlength = 142\\n                Return: SQL_ERROR=-1\\n                stmt:       szSqlState = \"37000\", *pfNativeError = -3706,                       szErrorMsg = \"[Teradata][ODBC Teradata Driver][Teradata Database] Syntax error: Invalid INTERVAL Literal. \"\\n</code></pre>\\n\\n<p>The table was created with:</p>\\n\\n<pre><code>CREATE TABLE SEN.Interval_Day_Minute_Table(\\nKeyColumn CHAR (255) CHARACTER SET UNICODE,\\nColumn1 INTERVAL DAY (4) TO MINUTE)\\n</code></pre>\\n\\n<p>I\\'m new to Teradata, but as far as I can tell, I\\'m using the correct syntax.</p>\\n-problem with inserting a casted interval literal into a teradata 13.0 database using ODBCtest-<sql><odbc><teradata>',\n",
       " '<p>Has anyone had any success with this? There aren\\'t a great deal of references online and I\\'ve exhausted every relevant result on Google. Here\\'s my script:</p>\\n\\n<pre><code>#!/usr/bin/perl\\n\\nuse DBI;\\nuse DBD::ODBC;\\n\\n$user = \"user\";\\n$pw = \"pw\";\\n$ip = \"192.168.1.0\"\\n\\n#DBI-&gt;trace(DBD::ODBC-&gt;parse_trace_flags(\\'odbconnection\\'));\\n\\n#my $connect_attrs = { PrintError =&gt; 0, RaiseError =&gt; 1, AutoCommit =&gt; 1 };\\n\\nmy $dbh = DBI-&gt;connect(\"dbi:ODBC:$ip\", $user, $pw);\\n</code></pre>\\n\\n<p>The error message:</p>\\n\\n<pre><code>DBI connect(\\'192.168.1.0\\',\\'user\\',...) failed: (no error string) at ./teradata.pl line 13\\n</code></pre>\\n\\n<p>The two lines that are commented out are leftover from my previous fruitless attempts to connect to the DB.</p>\\n\\n<p><strong>UPDATE</strong>: Here are the previous efforts I made with the DBD module.</p>\\n\\n<pre><code>#!/usr/bin/perl\\n\\nuse DBI;\\n\\n$user = \"xxxx\";\\n$pw = \"xxxx\";\\n\\nmy $dbh = DBI-&gt;connect(\"dbi:Teradata:tdsn\", $user, $pw);\\n</code></pre>\\n\\n<p><strong>Error:</strong> </p>\\n\\n<pre><code>DBI connect(\\'tdsn\\',\\'xxxx\\',...) failed: Unable to get host address. at ./teradata.pl line 12\\n</code></pre>\\n\\n<hr>\\n\\n<p><strong>Second Attempt:</strong></p>\\n\\n<pre><code>#!/usr/bin/perl\\n\\nuse DBI;\\n\\n$user = \"xxxx\";\\n$pw = \"xxxx\";\\n\\nmy $dbh = DBI-&gt;connect(\"dbi:Teradata:192.168.1.0\", $user, $pw);\\n</code></pre>\\n\\n<p><strong>Error:</strong></p>\\n\\n<pre><code>DBI connect(\\'192.168.1.0\\',\\'xxxx\\',...) failed: Deprecated logons are not allowed by administrator.  Upgrade client software to latest version. at ./teradata.pl line 12\\n</code></pre>\\n\\n<hr>\\n\\n<p><strong>Third...</strong></p>\\n\\n<pre><code>#!/usr/bin/perl\\n\\nuse DBI;\\nuse DBD::ODBC;\\n\\n$user = \"xxxx\";\\n$pw = \"xxxx\";\\n\\nmy $dbh = DBI-&gt;connect(\"dbi:ODBC:tdsn\", $user, $pw);\\n</code></pre>\\n\\n<p><strong>.odbc.ini</strong></p>\\n\\n<pre><code>[ODBC]\\nInstallDir              = /usr/odbc\\nTrace           = 0\\nTraceDll                = /usr/odbc/lib/odbctrac.so\\nTraceFile               = /home/xxxx/odbctrace.log\\nTraceAutoStop           = 0\\n\\n[ODBC Data Sources]\\ndefault         = tdata.so\\ntestdsn         = tdata.so\\n\\n[default]\\nDriver          = /usr/odbc/drivers/tdata.so\\nDescription             = Default DSN is Teradata 5100\\nDBCName         = **ip_addr**\\nLastUser                = DLPStats\\nUsername                = xxxx\\nPassword                = xxxx\\nDatabase                = MSS_TEMP\\nDefaultDatabase         = MSS_TEMP\\n\\n[tdsn]\\nDriver=/usr/odbc/drivers/tdata.so\\nDescription=Teradata running Teradata V1R5.2\\nDBCName=**ip_addr**\\nLastUser=\\nUsername=xxxx\\nPassword=xxxx\\nDatabase=\\nDefaultDatabase=\\n</code></pre>\\n\\n<p><strong>Error:</strong></p>\\n\\n<pre><code>DBI connect(\\'tdsn\\',\\'xxxx\\',...) failed: (no error string) at ./teradata.pl line 13\\n</code></pre>\\n\\n<hr>\\n\\n<p><strong>odbcinst.ini</strong></p>\\n\\n<pre><code>[ODBC DRIVERS]\\nTeradata=Installed\\n\\n[Teradata]\\nDriver=/usr/odbc/drivers/tdata.so\\nAPILevel=CORE\\nConnectFunctions=YYY\\nDriverODBCVer=3.51\\nSQLLevel=1\\n</code></pre>\\n-Connecting to Teradata via Perl-<sql><perl><odbc><teradata><dbd>',\n",
       " \"<p>I have taken a good look at the Teradata Syntax reference to no avail</p>\\n\\n<p>I have some rows with numbers:</p>\\n\\n<pre><code>ID\\nMickey\\nLaura9\\nLarry59N\\n</code></pre>\\n\\n<p><strong>How do I take away the integers from my rows?</strong> </p>\\n\\n<p>I understand that SUBSTR(id, 0, index(id, '%FORMAT%')) would work, but I don't know what could I enter in the %FORMAT% area to just find integers.</p>\\n-Teradata String Manipulation-<string><teradata>\",\n",
       " \"<p>I keep having the same issue revolving a particular query which has the following column:</p>\\n\\n<pre><code>CAST((COALESCE(price, 0.000000)) AS DECIMAL(18,6)) * quantity amount (TITLE 'Amount')\\n</code></pre>\\n\\n<p>If I don't mention it in the group by it spews error 3504, if I do it spews back error 3627.</p>\\n\\n<p>Any idea on how to properly structure this part of the query?</p>\\n\\n<p><em><strong>edit: if you need more info, please ask away :)</em></strong></p>\\n-Teradata 3504/3627 Issue-<sql><group-by><teradata>\",\n",
       " \"<p>I have a table in Teradata that I suspect was changed.  Isn't there a quick sql command to get that info as in SQL Server?</p>\\n-How can I find out the date of the last change on a table in Teradata?-<sql><teradata>\",\n",
       " \"<p>Does anyone know how to prepare a string in SAS for insert into external database(teradata for ex.)\\nI mean if there is a function like php's addslashes or something similar?</p>\\n-How to prepare string in SAS to insert into teradata-<sql><string><escaping><quotes><sas>\",\n",
       " '<p>How do I get the name of the class I am currently in?</p>\\n\\n<p>Example:</p>\\n\\n<pre><code>def get_input(class_name):\\n    [do things]\\n    return class_name_result\\n\\nclass foo():\\n    input = get_input([class name goes here])\\n</code></pre>\\n\\n<p>Due to the nature of the program I am interfacing with (vistrails), I cannot use <code>__init__()</code> to initialize <code>input</code>.</p>\\n-Get name of current class?-<python><class>',\n",
       " '<p>I have a collection of short messages classified as positive or negative which is saved in an ARFF file genereated in WEKA. I would like to move this data to RapidMiner for classification and processing purposes. </p>\\n\\n<p>Being a complete newbie in RapidMiner does anyone have examples on how to build a classifier for these messages. The messages are in one file in the format:</p>\\n\\n<pre><code>\"MSG 1 TEXT\", categoryX\\n\"MSG 2 TXT\", categoryX\\n</code></pre>\\n\\n<p>Thanks!</p>\\n-RapidMiner Sentiment Analysis -<string><machine-learning><weka><text-mining><rapidminer>',\n",
       " '<p>I have two tables like this</p>\\n\\n<p><img src=\"https://i.stack.imgur.com/XuKld.png\" alt=\"enter image description here\"></p>\\n\\n<p>I would like to insert from Table1 to Table2 here. This is how I want it.</p>\\n\\n<p>Take MOU = 10. It has num1 and hour1 in the same row.\\nI would like to insert it into the cell that is at the same row as that of num1 and same column as that of hour1.</p>\\n\\n<p>How could I do that?</p>\\n\\n<p>Disclaimer: I am not offering any code here because I am unsure of how to write this query. I sure do know to write a simple update. I am a teracota newbie.</p>\\n-Update table1 based on value from table 2 in teradata-<sql><data-warehouse><teradata>',\n",
       " '<p>I am trying to fetch a huge set of records from Teradata using JDBC. And I need to break this set into parts for which I\\'m using \"Top N\" clause in select.\\nBut I dont know how to set the \"Offset\" like how we do in <strong>MySQL</strong> -</p>\\n\\n<pre><code>   SELECT * FROM tbl LIMIT 5,10\\n</code></pre>\\n\\n<p>so that next select statement would fetch me the records from (N+1)th position.</p>\\n-Teradata - limiting the results using TOP-<sql><teradata>',\n",
       " \"<p>Has anyone had any experience with MonetDB? Currently, I have a MySQL database that is growing too large, and queries are getting too slow. According to column-oriented paradigm, insertions will be slower (which I don't mind at all), but data retrieval becomes very fast. Do I stand a chance of getting more data retrieval performance just by switching to MonetDB? Is it MonetDB mature enough?</p>\\n-Is it worth trying MonetDB?-<database><column-oriented><monetdb>\",\n",
       " \"<p>I am trying to tune a query in Teradata. It's pretty huge, so I am giving below the outline:</p>\\n\\n<pre><code>SEL column_1, column_2......column_20, sum(column_21), sum(column_22),.....sum(column_30)\\nfrom table_a a\\ninner join table_b b\\non conditions...\\ngroup by column_1, ...,column_20;\\n</code></pre>\\n\\n<p>I am trying to tune this. It's hitting a performance roadblock in the group by. The tables A and B are huge (more than 2 billion records).</p>\\n\\n<p>I tried the following options, but none of them improved the performance:</p>\\n\\n<p>1) Collected all necessary stats</p>\\n\\n<p>2) Created a JI on the columns from table A and B</p>\\n\\n<p>3) Created an AJI on the columns and the summations from table A and B</p>\\n\\n<p>4) Created a SI on each of the tables for the columns involved in group by.</p>\\n\\n<p>Can someone suggest how to proceed further?</p>\\n-Tuning in Teradata (group by)-<sql><database><performance><teradata>\",\n",
       " '<p>There is this one table which contains the amounts and states that I need. However, this table contains a year information but I want month. For example, in the table it shows information for Kentucky for 2011..and thats it. For California it shows about 5 different years. But I need it to repeat by month.</p>\\n\\n<p>So if in 2011 Kentucky had 12 total, then I need a query that shows 12 for January, February, May....repeatedly</p>\\n\\n<p>Right now I get this output with a dumb query I have:</p>\\n\\n<pre><code>Kentucky  12   January\\nCalifornia 800 January\\n</code></pre>\\n\\n<p>This is done easily by grouping by State, Quantity and Month</p>\\n\\n<p>I want to make sure that no matter what the Quantity is, each State has ALL months</p>\\n\\n<pre><code>Kentucky  12   January\\nKentucky  12   February\\nKentucky  12   May\\nCalifornia 800 January\\nCalifornia 800 February\\nCalifornia 800 May\\n</code></pre>\\n\\n<p>Any idea on how to do this with Teradata SQL?</p>\\n-Teradata Default List-<sql><teradata>',\n",
       " '<p>I am looking for a off the shelf workflow engine to be used in my Java based web application. Following are my initial requirements -</p>\\n\\n<ol>\\n<li>The engine should have a nice UI to create/manage workflows.</li>\\n<li>Should work with Oracle database</li>\\n<li>Provides java api or web service api to interact with workflow from my application so that I can build logic on the workflow.</li>\\n<li>Ability to define custom business rules.</li>\\n</ol>\\n\\n<p>As of now I am looking at JBoss JBPM and Drools together. Do let me know if you have experience of this or other contenders which I should consider for evaluation?</p>\\n-Which Java based workflow engine should I use?-<java><workflow>',\n",
       " '<p>I am learning the K-medoids algorithm so I am sorry if I ask inappropriate questions. As I know,the K-medoids algorithm implements a K-means clustering but use actual data points to be centroid instead of mathematical calculated means.</p>\\n\\n<p>As I googled online, I found a lot of k-means tools such as GenePattern, geWengh,...etc but not the k-medoids ones. Some nice friends has showed me that at Matlab, there is also one written by some user. However, I am afraid that personal implemented tool may still have some bugs or limitations. Thus, I am wondering if there is some widely used reliable open source software/tools  which uses actual data points as centroids to cluster. I need to find out the information about actual centroids so only returning clustering results is not enough. I prefer website online ones but if this is not the case, I am ok to install it to my local machine. Thank you very much,</p>\\n-Where to find a reliable K-medoid(Not k-means) open source software/tool?-<open-source><cluster-analysis><k-means>',\n",
       " \"<p>I have a line of code in Oracle and I had to convert it into Teradata. \\nThe Oracle query is</p>\\n\\n<pre><code> /* add to avoid invalid number due to junk in column */\\n\\nAND regexp_instr(table.column, ''[^[:digit:]]'', 1, 1)  = 0 \\n</code></pre>\\n\\n<p>The code I have written in Teradata </p>\\n\\n<pre><code>AND (CASE WHEN (POSITION('' '' IN TRIM(table.column)) &gt; 0) OR (UPPER(TRIM(table.column))              \\n        (CASESPECIFIC) &lt;&gt; LOWER(TRIM(table.column)) (CASESPECIFIC)) \\n          THEN 1 ELSE 0 end ) = 0\\n</code></pre>\\n\\n<p>The column is defined as a <code>VARCHAR(20)</code> but I only want to select rows where the data is all numeric.  I cannot verify the Teradata query as it is a very long-running query and I don't have access to create tables or rather I can not verify the out put on the database I have. I some how tried and it looks like it works but I once wanted to verify the syntax and my understanding of REGEXP_INSTR.</p>\\n-Can somebody verify this for me in sql or teradata-<sql><regex><oracle10g><teradata><plsqldeveloper>\",\n",
       " '<p>Im using monetdb and i have two databases on it \"mydb\" and \"test\".</p>\\n\\n<p>I want to get a sub-set of values from \\'mydb\\' into \\'test\\'.</p>\\n\\n<p>My code: </p>\\n\\n<pre><code> insert into test.result \\nselect sum(chargfeeprepaid) from mydb.data where callingpartyno = 628388881507 \\nunion \\nselect sum(chargefeeprepaid) from mydb.sms where callingpartyno = 628388881507;\\n</code></pre>\\n\\n<p>This works fine in MySQL...but in Monetdb i get the error: INSERT INTO: no such scheme \\'test\\'.</p>\\n\\n<p>Where did i go wrong and what is the correct syntax to do this in monetdb?</p>\\n\\n<p>Greetings Seleen</p>\\n-Query two databases in Monetdb-<mysql><monetdb>',\n",
       " \"<p>I'm having great difficulty solving this seemingly easy task:</p>\\n\\n<p>Purpose:\\nCreate a query that eliminates the middle Initial</p>\\n\\n<p>Example</p>\\n\\n<pre><code>Name\\nSmith, John A\\nJane, Mary S\\n</code></pre>\\n\\n<p>I would like an output such as this:</p>\\n\\n<pre><code>Name\\nSmith, John\\nJane, Mary\\n</code></pre>\\n\\n<p>Any tips on how to do this with Teradata SQL</p>\\n\\n<p>I believe I solved the issue, albeit in a very poor way:</p>\\n\\n<pre><code>SELECT SUBSTR('SMITH, JOHN A', 0, (POSITION(' ' IN 'SMITH, JOHN A') + (POSITION(' ' IN SUBSTR('SMITH, JOHN A',(POSITION(' ' IN 'SMITH, JOHN A'))+ 1,50)))))\\n</code></pre>\\n-Teradata String Manipulation (Second Space)-<sql><string><teradata>\",\n",
       " \"<p>I have a half dozen views in SQL Server that I need to replicate in Teradata, but I haven't been able to find the TD equivalent of the SQL metadata tables. I'd like to replicate the following functionality (which I assume is fairly self-explainatory):</p>\\n\\n<pre><code>select table_name, column_id ordinal_position, column_name,\\n   data_type, char_length char_max_length, \\n   data_precision numeric_precision, data_scale numeric_scale\\nfrom user_tab_columns\\n\\nselect name as FUNCTION_NAME\\nfrom sys.objects\\nwhere type_desc='SQL_SCALAR_FUNCTION'\\n\\nselect TABLE_NAME as VIEW_NAME\\nfrom INFORMATION_SCHEMA.VIEWS\\n</code></pre>\\n\\n<p>I'd also like to know if there are any usable Teradata references online; everything I run across seems to be advertising rather than practical information.</p>\\n-Retrieving column and other metadata information in Teradata-<sql><teradata>\",\n",
       " \"<p>I'm evaluating platform for build dashboard using Java technology only and I want to know the best suitable to this.</p>\\n\\n<p>I'm looking for solution in these categories:</p>\\n\\n<ul>\\n<li>Easy to develop, programming, extend and integrate with databases MySQL and DB2.</li>\\n<li>Good layout features, customization, chart graphs and themes support</li>\\n<li>Complete documentation and tutorials dedicated for dashboard apps.</li>\\n<li>A solution with a DRY way to apply observer pattern out of the box (or similar approach) for real time sync and visualize information between source (database table) and target (graph or table) in the the view instantly when data change in the source.</li>\\n</ul>\\n\\n<p>I don't want in principle to develop all features and deal with all issues related with dashboard interface, but I'm considering to implement a J2EE application by my self if If I don't see good resource or complete tutorial for this.    </p>\\n-Suitable platform for Dashboard: Pentaho, Liferay, RapidMiner or J2EE (JSF, EJB)-<jakarta-ee><liferay><dashboard><pentaho><rapidminer>\",\n",
       " \"<p>what is the equivalent of Oracle's <code>CURSOR%NOTFOUND</code> found in Teradata?</p>\\n\\n<p>If not, then how can I translate below code to run in Teradata.</p>\\n\\n<pre><code>OPEN OPEN_CUR1;\\n   LOOP\\n      FETCH OPEN_CUR1 INTO ... some variables ...;\\n      EXIT WHEN OPEN_CUR1%NOTFOUND;\\nCLOSE OPEN_CUR1;\\n</code></pre>\\n\\n<p>Thanks</p>\\n-Oracle/PL SQL curosr%notfound equivalent in Teradata-<oracle><stored-procedures><plsql><cursor><teradata>\",\n",
       " \"<p>I have a column which is defined as varchar(19). So it can have alpha-numeric values.I have cast it to integer. Simple casting will give overflow exception and if I am tring to format it as Z(9) or '999999999'  it shows error saying column has combination of  numeric, character  and GRAPHIC values</p>\\n\\n<p>I have another casting too for which the column is defined as decimal(13,3) and I need to convert it to integer..I am clue less here too.</p>\\n\\n<p>Any ideas guys??</p>\\n-cast varchar to integer in teradata-<casting><integer><varchar><teradata>\",\n",
       " '<p>I have been working ot see the equivalent function for Oracle lead and lag function.</p>\\n\\n<p>The oracle lead would look like</p>\\n\\n<pre><code>LEAD(col1.date,1,ADD_MONTHS(col1.DATE,12)) \\nOVER(Partition By tab.a,tab.b,tab.c Order By tab.a)-1 END_DATE\\n\\nLAG(col1.DATE + 7,1,col1.DATE-1) \\nOVER(partition by tab.a,tab.b Order By tab.b) LAG_DATE\\n</code></pre>\\n\\n<p>Any better idea</p>\\n-Teradata equivalent for lead and lag function of oracle-<sql><oracle><lag><teradata><lead>',\n",
       " \"<p>I was trying to convert the connect by level function of oracle to teradata. I have seen many examples over the net but this particular one is different,  </p>\\n\\n<pre><code> (SELECT \\n     CASE LEVEL \\n        WHEN 1 THEN 'MB'\\n        WHEN 2 THEN 'SB'\\n        ELSE 'TOTAL'\\n     END AS DRUG_SOURCE\\n FROM\\n    DUAL \\n CONNECT BY LEVEL &lt;= 3) RW  \\n</code></pre>\\n\\n<p>Please let me know if you guys have any idea.</p>\\n-How to convert connect by level in teradata-<sql><oracle><case><teradata><connect-by>\",\n",
       " \"<p>Below is the content of my script.bat :</p>\\n\\n<pre><code>@echo off\\n\\ncd C:\\\\Program Files\\\\Teradata\\\\Client\\\\13.0\\\\bin\\n\\nbteq .LOGON server/username,password;\\n\\nselect date;\\n\\n.LOGOFF\\n\\n@echo off goto end\\n\\n:end @echo exit\\n</code></pre>\\n\\n<p>I have no problem with the logon, but it seems that bteq can't read my query statement:</p>\\n\\n<blockquote>\\n  <p>select date;</p>\\n</blockquote>\\n\\n<p>It keeps prompting for input. Can anyone help me to get bteq to read and execute the query statement?</p>\\n\\n<p>I've tried the solutions online about input and output file:</p>\\n\\n<pre><code>bteq &lt;myscript.txt&gt; mylog.log\\n</code></pre>\\n\\n<p>but it didn't work either.</p>\\n-How to write batch (*.bat) script to execute Teradata query using BTEQ?-<sql><batch-file><teradata>\",\n",
       " \"<p>I'm trying to connect to Teradata in SAS. I set up an teradata ODBC on the machine. The assumption currently for me is that using ODBC is the only way for me to access the database. And here is the syntax of my connection command:</p>\\n\\n<p>Libname Teradata ODBC dsn = 'dsnname' uid = 'uid' pwd = 'pwd';</p>\\n\\n<p>results:\\nError: The ODBC engine cannot be found.\\nError: Error in the LIBNAME statement.</p>\\n\\n<p>It keeps saying that the ODBC engine cannot be found. I'm really confused now. Is there anything wrong with the command? Or I have to do something else outside SAS?</p>\\n\\n<p>I check the licence\\nProc Setinit;</p>\\n\\n<p>result:\\nSAS/ACCESS Interface to Teradata                  <strong><em>*</em>*</strong> the date shows not expired.</p>\\n\\n<p>Could anyone give me some idea. Thank you very much!</p>\\n-SAS connection to Teradata Database using Teradata ODBC-<odbc><database-connection><sas><teradata>\",\n",
       " \"<p>I have data in LISP form and I need to process them in RapidMiner. I am new to LISP and to RapidMiner aswell. RapidMiner doesn't accept the LISP (I guess it's because it is programming language) so I probably need somehow to convert LISP form to CSV or something like that. Little example of code:</p>\\n\\n<pre><code>(def-instance Adelphi\\n   (state newyork)\\n   (control private)\\n   (no-of-students thous:5-10)\\n   ...)\\n(def-instance Arizona-State\\n   (state arizona)\\n   (control state)\\n   (no-of-students thous:20+)\\n   ...)\\n(def-instance Boston-College\\n   (state massachusetts)\\n   (location suburban)\\n   (control private:roman-catholic)\\n   (no-of-students thous:5-10)\\n   ...)\\n</code></pre>\\n\\n<p>I would be really grateful for any advice.</p>\\n-Import LISP data to RapidMiner (CSV,...)-<csv><lisp><rapidminer>\",\n",
       " '<p>Please could anyone point me in the right direction on how to design/build a web service client that will consume terabytes of data and perform some computation on the retrieved data?</p>\\n\\n<p>I inherited a project at my new job. The project has been designed and has been started by the group a few weeks before I joined the team.\\nThe project is about retrieving data from several web services (soap &amp; rest) and performing some computation on the data before storing in database, displaying to user and generating reports.</p>\\n\\n<p>The process of getting the data involves pulling some data from web service A, B, C and using the response to make another request to web service X, Y&amp;Z. (we don’t have control over the web service producers).\\nThe current implementation is very slow most times we run out of memory when trying to do some computation on the retrieved data. The data is in terabytes or more.\\nThe current implementation uses maven/spring.</p>\\n\\n<p>I am at the point of drawing up a new design for this project (introducing a bit of caching etc) but I would need some suggestions from anyone who has encountered this kind of problem before.</p>\\n\\n<p>Aside from the obvious, are there any special tricks or approach to this?\\nI know this might sound like a stupid question to some people, but any pointers would help. </p>\\n-How to consume terabytes of data using a Java RESTful client-<performance><web-services><spring><maven><rest-client>',\n",
       " \"<p>I need to add an identity column to an existing table with this SQL:</p>\\n\\n<pre><code>alter table app.employee \\nadd ID INTEGER GENERATED BY DEFAULT AS IDENTITY (START WITH 1 INCREMENT BY 1 MINVALUE 0 MAXVALUE 100000000 NO CYCLE)\\n</code></pre>\\n\\n<p>I can create new tables with an identity column just fine, but the above script gives me the following error: </p>\\n\\n<blockquote>\\n  <p><code>ALTER TABLE Failed. 3706: Syntaxt error: Cannot add new Identity\\n  Column option</code></p>\\n</blockquote>\\n\\n<p>Teradata database is severely lacking in online support and I've only come across one option which is to basically create a copy of the table with the identity column and do a mass insert from the old table to the new one and change all references to the new table.  I find it difficult to believe that this is the only possible way to do this.</p>\\n\\n<p>What are my options here?</p>\\n-Teradata: How can you add an identity column to an existing table?-<alter-table><teradata><identity-column>\",\n",
       " \"<p>I can't seem to find a good resource on the internet that explains the <code>check option</code> that's used when adding a foreign key constraint.  I've seen it as <code>with check option</code> and <code>with no check option</code>.</p>\\n-Teradata: What is the purposed of CHECK OPTION when adding a foreign key constraint?-<foreign-keys><constraints><teradata>\",\n",
       " '<p>I have generated a query in the excel by using macros now \"<strong>i want to import these queries into teradata and compile it and display the result Automatically</strong>\".Can anyone help me with this?</p>\\n-import query which is generated in excel into teradata-<sql><excel><teradata><vba>',\n",
       " '<p>I\\'m developing a Monte Carlo simulation software package that involves multiple physics and simulators. I need to do online analysis, track of the dependency of derived data on raw data, and perform queries like \"give me the waveforms for temperature>400 and position near (x0,y0)\". So the in-memory data model is rather complicated.</p>\\n\\n<p>The application is written in Python, with each simulation result modeled as a Python object.\\nIn every hour it produces ~100 results (objects). Most objects have heavy data (several MB of binary numeric array), as well as some light data (temperature, position etc). The total data generate rate is several GB per hour.</p>\\n\\n<p>I need some data persistency solution, and an easy-to-use query API.\\nI\\'ve already decided to store the heavy data (numeric array) in HDF5 storage(s).\\nI\\'m considering using MongoDB as for object persistency (light data only), and for indexing the heavy data in HDF5.\\nObject persistency with MongoDB is straightforward, and the query interface looks sufficiently powerful.</p>\\n\\n<p>I am aware of the sqlalchemy+sqlite option. However, streaming the heavy data to HDF5 does not seem naturally supported in SqlAlchemy, and a fixed schema is cumbersome.</p>\\n\\n<p>I am aware of this post(\\n<a href=\"https://stackoverflow.com/questions/1686869/searching-a-hdf5-dataset\">Searching a HDF5 dataset</a>), but the \"index table\" itself needs some in-memory indices for fast query.</p>\\n\\n<p>I wonder if there is any alternative solutions I should look at before I jump in? Or is there any problem I\\'ve overlooked in my plan?</p>\\n\\n<p>TIA.</p>\\n-Data persistency of scientific simulation data, Mongodb + HDF5?-<python><mongodb><orm><scientific-computing><hdf5>',\n",
       " '<p>can you extract data from teradata on ibm z/os as an XML document?\\ni have searched and found Teradata XML services on windows, unix, etc but it doesnt appear to be available on IBM Mainframe.\\nare there any other options to extract data from Teradata as a complete xml document?</p>\\n-teradata xml services on IBM z/os-<xml><teradata><zos>',\n",
       " '<p>I\\'ve recently discovered RapidMiner, and I\\'m very excited about it\\'s capabilities. However I\\'m still unsure if the program can help me with my specific needs. I want the program to scrape xpath matches from an URL list I\\'ve generated with another program. (it  has more options then the \\'crawl web\\' operator in RapidMiner) </p>\\n\\n<p>I\\'ve seen the following tutorials from Neil Mcguigan: <a href=\"http://vancouverdata.blogspot.com/2011/04/web-scraping-rapidminer-xpath-web.html\" rel=\"nofollow\">http://vancouverdata.blogspot.com/2011/04/web-scraping-rapidminer-xpath-web.html</a>. But the websites I try to scrape have thousands of pages, and I don\\'t want to store them all on my pc. And the web crawler simply lacks critical features so I\\'m unable to use it for my purposes. Is there a way I can just make it read the URLS, and scrape the xpath\\'s from each of those URLS?</p>\\n\\n<p>I\\'ve also looked at other tools for extracting html from pages, but I\\'ve been unable to figure out how they work (or even install) since I\\'m not a programmer. Rapidminer on the other hand is easy to install, the operator descriptions make sense but I\\'ve been unable to connect them in the right order. </p>\\n\\n<p>I need to have some input to keep the motivation going. I would like to know what operator I could use instead of \\'process documents from files.\\' I\\'ve looked at \\'process documents from web\\' but it doesn\\'t have an input, and it still needs to crawl.  Any help is much appreciated.</p>\\n\\n<p>Looking forward to your replies.</p>\\n-Can rapidminer extract xpaths from a list of URLS, instead of first saving the HTML pages?-<xpath><screen-scraping><web-scraping><data-mining><rapidminer>',\n",
       " '<p>I have a file with a list of string (one cloumn).\\nFile example</p>\\n\\n<pre><code>sdfsdfsdf\\nhfhfhfghf\\ndfgdggdfg\\npookokkoo\\n</code></pre>\\n\\n<p>base on the documentation on monetdb web site, I have to create a BAT file. </p>\\n\\n<p>How do I convert my file with strings into a BAT file ready to be imported in monetdb?\\nHow do I do this from Java?</p>\\n\\n<p>Thanks,</p>\\n\\n<p>monetdb site doc\\n<a href=\"http://www.monetdb.org/Documentation/Cookbooks/SQLrecipies/BinaryBulkLoad\" rel=\"nofollow\">http://www.monetdb.org/Documentation/Cookbooks/SQLrecipies/BinaryBulkLoad</a></p>\\n-how to create a BAT file to be used in monetdb bulk load from a Java program-<monetdb>',\n",
       " \"<p>I need to import a large CSV file into MonetDB and I'm wondering if it would be possible to split the file in two and run two scripts like:</p>\\n\\n<pre><code>mclient -u monetdb -d mydb &lt; import1.sql\\nmclient -u monetdb -d mydb &lt; import2.sql\\n</code></pre>\\n\\n<p>where </p>\\n\\n<ul>\\n<li>import1.sql issues a SQL <code>copy</code> instruction using file1.csv, and</li>\\n<li>import2.sql issues a SQL <code>copy</code> instruction using file2.csv </li>\\n</ul>\\n\\n<p>Would this be faster? Would this peform fine?</p>\\n\\n<p>Thanks</p>\\n-How would MonetDB perform with multiple sql copy operations running at the same time?-<monetdb>\",\n",
       " '<p>I am checking the contents of <code>y.cap_ts</code> and if there is a value (non null) then replace it with <code>current_date</code> or else leave it as null. but I get data type mismatch in then/else error.</p>\\n\\n<p>Here cap_ts is date data type.  </p>\\n\\n<p>Can any one suggest any better work around?</p>\\n\\n<pre><code>SET  \\ncap_ts  = CASE WHEN y.cap_ts IS NULL AND y.rwrd &gt; 50  \\n                THEN current_date  \\n                ELSE NULL END  \\n</code></pre>\\n\\n<p>I am currently working in teradata.</p>\\n-Datatype mismatch in then/else in teradata-<sql><case><teradata>',\n",
       " \"<p>I've tried a few examples I've seen on the internet, but I can't seem to figure them out.  This is a Teradata database.</p>\\n\\n<p>I have TableA that has CustomerId and DepartmentId.</p>\\n\\n<p>I have TableB that also CustomerId and DepartmentId.</p>\\n\\n<p>I know this structure is not practical, but this is a highly non-normalized database that we took over from an offshore development team and we have to work with what we have.</p>\\n\\n<p>What I want to do is join TableA and TableB on the CustomerId then set the DepartmentId of TableB to what's in TableA.  I would greatly appreciate the proper syntax.</p>\\n-Teradata update join syntax-<sql><join><teradata>\",\n",
       " \"<p>Suppose I have a table of week_nbr and cust_id.</p>\\n\\n<p>Suppose I want a sample of 500 customers from each week.</p>\\n\\n<p>The dumb way is to do this for each week:</p>\\n\\n<pre><code>select cust_id\\nfrom week_cust\\nsample randomized allocation 500\\nwhere week_nbr=1\\n</code></pre>\\n\\n<p>What's the smart way?  I.e., is there a way to make the following concept work?</p>\\n\\n<pre><code>select week_nbr\\n      ,random sample of 500 cust_id in this week\\nfrom week_cust\\n</code></pre>\\n\\n<p>This is on Teradata 12.</p>\\n\\n<p>Best, and thanks in advance.</p>\\n-sample of data within groups in Teradata-<sql><random><teradata>\",\n",
       " \"<p>Would a query run faster in a column defined as boolean or as varchar(1) in monetdb ?</p>\\n\\n<p>queries would look like:</p>\\n\\n<p>with varchar(1)</p>\\n\\n<p><code>select * from many_many_rows where has_fancy_value = 'T'</code></p>\\n\\n<p>with boolean </p>\\n\\n<p><code>select * from many_many_rows where has_fancy_value = true</code></p>\\n\\n<p>Is there any difference performance wise</p>\\n-would monetdb work faster with boolean or varchar(1) data types?-<monetdb>\",\n",
       " \"<p>I have a huge dataset of book titles and their authors and would like to find which authors are more likely to work with each other! I'm trying to figure it out by the association rules and fp-growth modules in rapidminer but it's not working! I think because the word vector is very large and the cooperation between the scientists would not be a big number in comparison with the whole matrix! </p>\\n\\n<p>Could you please tell me how can I solve this problem or, if there is a similar free software that I can use for this purpose.</p>\\n\\n<p>Thanks,</p>\\n-How can I use RapidMiner to detect very weak associations in a large dataset?-<associations><design-patterns><rapidminer>\",\n",
       " \"<p>I'm wondering if we can use the text processing plugin of RapidMiner to create co-authorship networks like ones we can get from Pajek. If yes, please tell me how.</p>\\n\\n<p>Thanks, </p>\\n-Can we use RapidMiner Text Processing plugin for drawing co-authorship networks?-<rapidminer>\",\n",
       " \"<p>A third party application I'm using (Knime) hangs when using Java for Mac 1.6.0_29 (Java for OS X Lion Update 1). In the user forums of that app, it is recommended to use version 1.6.0_26. </p>\\n\\n<p>Sadly, I got a fresh install, so there are no previous Java versions installed in my computer to downgrade to.</p>\\n\\n<p>I've been looking for a while and still cannot find out how to install a previous version of Java for Mac OS X Lion.</p>\\n\\n<p>Any ideas?</p>\\n-Java 1.6.0_26 on Mac OS X Lion-<java><macos><osx-lion><knime>\",\n",
       " '<p>I am attempting to learn to use RapidMiner, and my boss wants me to perform a market basket analysis on a set of data. But when I use the given template, I get the following error:  </p>\\n\\n<p>Regular Attributes must be of type binomial.<br>\\nThis is given withing the FP-Growth operator.  </p>\\n\\n<p>I have a customerID (only numbers), a productName(Letters) and a Product Quantity (numbers) column.  </p>\\n\\n<p>As I am a newbie with RM, I have no idea what is wrong.  </p>\\n\\n<p>Any input would be greatly appreciated.<br>\\nThank you in advance.</p>\\n-RapidMiner error: Regular Attributes must be of type binomial. Market Basket Analysis-<data-mining><rapidminer>',\n",
       " '<p>I have been given 2 data sets and want to perform cluster analysis for the sets using KNIME.</p>\\n\\n<p>Once I have completed the clustering, I wish to carry out a performance comparison of 2 different clustering algorithms. </p>\\n\\n<p>With regard to performance analysis of clustering algorithms, would this be a measure of time (algorithm time complexity and the time taken to perform the clustering of the data etc) or the validity of the output of the clusters? (or both)</p>\\n\\n<p>Is there any other angle one look at to identify the performance (or lack of) for a clustering algorithm?</p>\\n\\n<p>Many thanks in advance,</p>\\n\\n<ul>\\n<li>T</li>\\n</ul>\\n-Performance Analysis of Clustering Algorithms-<machine-learning><data-mining><cluster-analysis><knime>',\n",
       " \"<p>I'm trying to remove all white space characters (tab, cr, lf) and the best expression I've found (that actually works) is:</p>\\n\\n<pre><code>syslib.oreplace(\\n syslib.oreplace(\\n  syslib.oreplace(my_string,X'0a',''),X'0d',''),X'09','')\\n</code></pre>\\n\\n<p>is there anything more elegant? can't I remove all of them in a single oreplace call?</p>\\n-using teradata syslib.oreplace to remove all whitespace characters-<sql><teradata>\",\n",
       " \"<p>I am building system to analyze large quantities of financial data regarding securities trading prices. A large challenge in this is determining what storage method to use for the data given that the data will be in the 10's of terrabytes. There will be many queries on the data such as taking averages, calculating standard deviations, and sums filtered by multiple columns such as price, time, volume, etc. Join statements aren't a requisite, but would be nice to have.</p>\\n\\n<p>Right now, I am looking at infobright community edition, monetdb, and greenplum community edition for evaluation purposes. They seem great so far, but for more advanced features, some of each are required are not available in some of these editions (using multiple servers, insert/update statements, etc).</p>\\n\\n<p>What solutions would you use for this situation, and benefits does it provide over the alternatives? Being cost effective is a major plus. If I must pay for a data warehousing solution I will, but I would much rather avoid it and take the open-source/community edition route if possible.</p>\\n-Data storage for financial analysis-<database><data-warehouse><greenplum><infobright><monetdb>\",\n",
       " '<p>I created a table then added a foreign key reference to its primary key in another table.  I need to drop the new table to recreate it with additional columns (I do not want to add the new columns to it).  When I attempt to delete it, it tells me I cannot deleted a referenced table.  So I try to drop the foreign key column from the other table and it tells me that the foreign key column cannot be dropped.  This leaves me with removing the foreign key itself first, but I don\\'t know the name of it.  I came upon this link:</p>\\n\\n<p><a href=\"http://forums.teradata.com/forum/database/how-to-drop-a-constraint-without-knowing-its-name\" rel=\"nofollow\">http://forums.teradata.com/forum/database/how-to-drop-a-constraint-without-knowing-its-name</a></p>\\n\\n<p>...but it is no help.  I can\\'t seem to locate the name of this foreign key anywhere.  Any help on how to drop this foreign key?</p>\\n-Teradata: How can I drop a foreign key constraint from a table?-<foreign-keys><teradata>',\n",
       " \"<p>Is it possible to use Teradata Macros directly from Entity Framework? I tried the model designer and guessed they would show up under stored procedures which they don't. Does the Teradata .Net provider support macros at all?</p>\\n\\n<p>Or, do I have to manually write the SQL sentences to execute the macros?</p>\\n-Using Teradata macros from Entity Framework?-<c#><.net><entity-framework><entity-framework-4><teradata>\",\n",
       " '<p>I have a programming program written in fortran installed. It makes use of packages such as Lapack (Linear algebra package) that have to have root privileges to install. I have access to a cluster in my university which does not have the required packages installed. Can I compile this program on my computer but run the executable .o file in the cluster? </p>\\n\\n<p>I also have other computers connected to the network. Can you point me to a source/tutorial so that I can use those other computers to shorten my program execution time. If I ask my friends to leave their computers on when they are away, how do I use them?</p>\\n-running your programe on a cluster in ubuntu-<ubuntu><parallel-processing><fortran><cluster-computing><hpc>',\n",
       " '<p>I am very interested in using monetdb as a datamart, holding some huge data tables for querying and reporting</p>\\n\\n<p>However, after some searching, I am unable to find any online posts / blogs regarding their use of Monetdb in any kind of production capacity.</p>\\n\\n<p>Also, there seems to be little or next to no activity online regarding Monetdb.</p>\\n\\n<p>Is this a bad sign for the future of Monetdb ?</p>\\n-monetdb - anyone uses it in production?-<monetdb>',\n",
       " \"<p>Script is as follows:</p>\\n\\n<pre><code>SELECT\\n   CAST(SUBSTR(TRIM(ABU_PTY_CD),4,2) AS CHAR(2)) ABU_PTY_CD,\\n   ACCTG_PRD_YR_CD,\\n   ACCTG_PRD_MO_CD ,\\n\\n   CASE \\n       WHEN (OUDS_FAC_CD &gt; ' ' ) THEN CAST (SUBSTR(OUDS_FAC_CD, 1, 5) AS CHAR(5))\\n       ELSE CAST (COALESCE(CC_PTY_CD, '') AS CHAR(5)) \\n   END OUDS_FAC_CD,\\n\\n   CASE \\n       WHEN GL_ACCT_NUM &gt; ' ' THEN CAST (SUBSTR(GL_ACCT_NUM, 1, 12) AS CHAR(12)) \\n       ELSE CAST (SAP_ACCT_NUM AS CHAR(12)) \\n   END ACCT_NUM,\\n\\n   CASE\\n       WHEN LE_PTY_CD &gt; ' ' THEN CAST (LE_PTY_CD AS CHAR(4) )\\n       ELSE CAST (SUBSTR( COMP_PTY_CD, 1, 4) AS CHAR(4) )\\n   END COMP_PTY_CD,\\n\\n   JE_LWR_TIER_RPTG_NUM, \\n   LT2_NM, \\n\\n   CASE\\n       WHEN PRD_VAR_CD &gt; ' ' THEN CAST (PRD_VAR_CD AS CHAR(3))\\n       ELSE CAST (COALESCE(FA_PTY_CD, '') AS CHAR(3))  \\n   END FA_PTY_CD,\\n\\n   JE_LN_GLBL_CCY_CD, \\n   JE_LN_LC_CD,\\n\\n   CASE \\n       WHEN GL_ACCT_LVL_NUM &gt; ' ' THEN CAST(SUBSTR(GL_ACCT_LVL_NUM,1,10)  AS CHAR(11) ) \\n       ELSE CAST(SUBSTR(FNCL_STMT_LN_NUM,1,11) AS CHAR(11) )\\n   END ACCT_LVL_NUM,\\n\\n   SUM (JE_LN_GLBL_AMT) JE_LN_GLBL_AMT, \\n   DDU_FAC_CD, \\n   JE_HDR_DESC, \\n   JE_HDR_NUM, \\n   JE_GRP_NUM, \\n   SOURCE_SYSTEM_ID\\nFROM   EDW_BI_SL_M1.BFV \\nWHERE  CPC_PTY_CD = '00000000SX'\\n       AND TO_CHAR(RPTG_ACCTG_DT, 'YYYY')  &gt;= TO_CHAR(ADD_MONTHS((CURRENT_DATE),-1),'YYYY')\\n       AND JE_VERS_NUM in ('200', '001') \\n       AND SRC_LDGR_CD &lt;&gt; 'LG'\\n       AND ACCTG_PRD_MO_CD &lt; 13\\nGROUP BY ABU_PTY_CD, ACCTG_PRD_YR_CD, ACCTG_PRD_MO_CD, OUDS_FAC_CD, ACCT_NUM, COMP_PTY_CD, JE_LWR_TIER_RPTG_NUM, \\nLT2_NM, FA_PTY_CD, JE_LN_GLBL_CCY_CD, JE_LN_LC_CD, ACCT_LVL_NUM, JE_LN_GLBL_AMT, DDU_FAC_CD, JE_HDR_DESC, JE_HDR_NUM, JE_GRP_NUM, \\nSOURCE_SYSTEM_ID;\\n</code></pre>\\n\\n<p><strong>Error message:</strong></p>\\n\\n<p>3504: selected non-aggregate values must be part of the associated group</p>\\n-could not figure out the error in this teradata script-<sql><oracle><teradata>\",\n",
       " '<p>In Teradata DB I have source table</p>\\n\\n<pre><code>create set table SRC_TABLE (\\n    Some_Id varchar(2O) not null\\n);\\n</code></pre>\\n\\n<p>This table is loaded with data from external system. I have target table</p>\\n\\n<pre><code>create set table DST_TABLE (\\n    Some_Id decimal(4,0) not null\\n);\\n</code></pre>\\n\\n<p>I need to copy rows from SRC_TABLE to DST_TABLE safely. There is a contract in place that external system will provide only values convertible to DECIMAL(4). However, is there any safe way how to select rows in SRC_TABLE which are not compliant with contract and may cause  typecasting failure?</p>\\n\\n<p>Update: I cannot use UDF functions due to restrictions in environment I am working in.</p>\\n-Safe casting VARCHAR to DECIMAL in Teradata-<casting><decimal><varchar><teradata><isnumeric>',\n",
       " \"<p>I am designing a table in Teradata with about 30 columns.  These columns are going to need to store several time-interval-style values such as Daily, Monthly, Weekly, etc.  It is bad design to store the actual string values in the table since this would be an attrocious repeat of data.  Instead, what I want to do is create a primitive lookup table.  This table would hold Daily, Monthly, Weekly and would use Teradata's identity column to derive the primary key.  This primary key would then be stored in the table I am creating as foreign keys.</p>\\n\\n<p>This would work fine for my application since all I need to know is the primitive key value as I populate my web form's dropdown lists.  However, other applications we use will need to either run reports or receive this data through feeds.  Therefore, a view will need to be created that joins this table out to the primitives table so that it can actually return Daily, Monthly, and Weekly.</p>\\n\\n<p>My concern is performance.  I've never created a table with such a large amount of foreign key fields and am fairly new to Teradata.  Before I go on the long road of figuring this all out the hard way, I'd like any advice I can get on the best way to achieve my goal.</p>\\n\\n<p><strong>Edit:</strong> I suppose I should add that this lookup table would be a mishmash of unrelated primitives.  It would contain group of values relating to time intervals as already mentioned above, but also time frames such as 24x7 and 8x5.  The table would be designed like this:</p>\\n\\n<pre>ID  Type         Value\\n--- ------------ ------------\\n1   Interval     Daily\\n2   Interval     Monthly\\n3   Interval     Weekly\\n4   TimeFrame    24x7\\n5   TimeFrame    8x5</pre>\\n\\n<p><strong>Edit Part 2</strong>: Added a new tag to get more exposure to this question.</p>\\n-Teradata: How to design table to be normalized with many foreign key columns?-<performance><foreign-keys><primary-key><primitive><teradata>\",\n",
       " \"<p>In Teradata, the way I've been doing backups for tables is like this:</p>\\n\\n<p><code>create table xxx_bak as xxx with data</code></p>\\n\\n<p>Works great, but I have just discovered that this doesn't work for tables with identity columns.</p>\\n\\n<p>I need a backup method that can duplicate a table with its data intact so that I can roll it back in case I mess up some data.</p>\\n-Teradata: How to back up a table that uses an identity column?-<sql><backup><teradata><identity-column>\",\n",
       " \"<p>In rapidminer what is the meaning of:</p>\\n\\n<pre><code>    - training cycles\\n    - learning rate\\n    - momentum\\n</code></pre>\\n\\n<p>I'm trying to work with rapidminer but i cant understand this notions. Any help would be apreciated, as newbly as possible.</p>\\n-rapidminer some concepts-<rapidminer>\",\n",
       " '<p>Hi am new to Teradata and am stuck with a problem</p>\\n\\n<p>There is an ID table which stores an Unique ID given to each person</p>\\n\\n<pre><code>CREATE TABLE IDS(\\nID VARCHAR(8),\\nUPDATED_DATE DATE)\\n</code></pre>\\n\\n<p>Then we have a name and address table which do not have any primary keys that stores demographic information for the IDS</p>\\n\\n<pre><code>CREATE TABLE NAMES(\\nID VARCHAR(8),\\nNAME VARCHAR(50))\\nCREATE TABLE ADRRESSES(\\nID VARCHAR(8)\\nADDRESS VARCHAR(200))\\n</code></pre>\\n\\n<p>Now each ID can have multiple name and IDS. However for names and address I want to use the ones that are have more counts. If two names have the same COUNT I just want the First row</p>\\n\\n<p>ID              NAME                COUNT</p>\\n\\n<p>1234    John Smith  6</p>\\n\\n<p>1234    Johnnie Smith   6</p>\\n\\n<p>1234    J Smith     2</p>\\n\\n<p>In the above example I want the name John Smith. Here is the left Join I am performing since an ID may not have a name or address. Here is what I am trying</p>\\n\\n<pre><code>SELECT * FROM\\n(SELECT ID as V_ID from IDS) a\\nLEFT JOIN\\n(SELECT ID, NAME, COUNT(*) AS COUNTER,(RANK() OVER(ORDER BY COUNTER DESC)) AS RNK\\nFROM NAMES \\nGROUP BY ID)b\\nON a.ID = b.ID\\nAND b.RNK = 1            -- Should give me only the first row\\nLEFT JOIN\\n(SELECT ID, ADDRESS, COUNT(*) AS COUNTER, (RANK() OVER (ORDER BY COUNTER DESC) ) AS RNK\\nFROM ADDRESSES\\nGROUP BY ID) c\\nON c.ID = a.ID\\nAnd c.RNK = 1\\n</code></pre>\\n\\n<p>However this is not getting me the desired result. I tried using ROW NUMBER instead of RANK also but still no results. How should I write this query in TERDATA?</p>\\n-Teradata Rank Over Query (Getting one row to left join)-<sql><left-join><teradata>',\n",
       " '<p>How do I covert epoch time value into a timestamp(6) in teradata ?</p>\\n\\n<p>Lets take 1336987231051 as the example epoch time ( note this is in milliseconds, where as epoch time is in seconds), I did something like this</p>\\n\\n<pre><code>// miliseconds epoch time \\nselect cast(1336987231051 as timestamp(6))\\n\\n// seconds epoch time \\nselect cast((1336987231051/1000) as timestamp(6))\\n</code></pre>\\n\\n<p>and I get this error message for both of the above select statements : </p>\\n\\n<pre><code>[Error] Script lines: 12-12 ------------------------\\n[Teradata Database] [TeraJDBC 13.10.00.31] [Error 5407] [SQLState HY000] Invalid operation for DateTime or Interval. \\n</code></pre>\\n\\n<p>I verified in <a href=\"http://www.epochconverter.com/\" rel=\"nofollow\">http://www.epochconverter.com/</a> that 1336987231051 is valid epoch time.</p>\\n\\n<p>What is the right sql for this in teradata ?</p>\\n-Coverting epoch time value into a timestamp(6) in teradata-<sql><teradata>',\n",
       " '<p>We are using Hibernate to insert data from our JAVA app to Teradata database.\\nNow, there is a date column in table,so we just pass the new Data() in that and trying to save it. But it is giving us the below error:</p>\\n\\n<pre><code>Caused by: com.teradata.jdbc.jdbc_4.util.JDBCException: [Teradata Database] [TeraJDBC 14.00.00.13] [Error 5404] [SQLState HY000] Datetime field overflow.\\n    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDatabaseSQLException(ErrorFactory.java:307)\\n    at com.teradata.jdbc.jdbc_4.statemachine.ReceiveInitSubState.action(ReceiveInitSubState.java:102)\\n    at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.subStateMachine(StatementReceiveState.java:302)\\n</code></pre>\\n\\n<p>For reference:</p>\\n\\n<p>My enitiy class:</p>\\n\\n<pre><code>@Temporal(TemporalType.TIMESTAMP)\\n    @Column(name = \"EDW_CREATE_DTM\")    \\n    public Date getEdwCreateDtm() {\\n        return edwCreateDtm;\\n    }\\n</code></pre>\\n\\n<p>In my service class... setting the value before insert:</p>\\n\\n<pre><code>someObject.setEdwCreateDtm(new Date());\\n</code></pre>\\n\\n<p>Is there any specific way for inserting date to teradata ??</p>\\n\\n<p>Any pointers on this would really help.</p>\\n-Teradata DB date insertion error using Hibernate-<hibernate><date><insert><teradata>',\n",
       " '<p>I have loaded 150 million records into MonetDB. All data inserted into a single table. The table does not have any constraint(ex. <code>UNIQUE</code>, ..). I have not created any index myself. The original source CSV file is about 7.2 GB and after importing database is about 8GB. I ran a <code>COUNT(*)</code> with <code>WHERE</code>, it returned in 12 seconds. according to documentation:</p>\\n\\n<blockquote>\\n  <p>The index statements in the SQL standard are recognized, but their\\n  implementation is different from competitive products. MonetDB/SQL\\n  interprets these statements as an advice and often freely neglects it,\\n  relying on its own decision to create and maintain indexes for fast\\n  access.</p>\\n</blockquote>\\n\\n<p>Now How can know that MonetDB has created index itself? I used <code>EXPLAIN</code> but I didn\\'t understand the output:\\nThis is the actual query:</p>\\n\\n<pre><code>EXPLAIN SELECT COUNT(*) FROM vbvdata WHERE vbvdata_speed &gt; 80 AND vbvdata_lane_id = 2;\\n</code></pre>\\n\\n<p>and this is the <code>EXPLAIN</code> output:</p>\\n\\n<pre><code>+--------------------------------------------------------------------------------+\\n| mal                                                                            |\\n+================================================================================+\\n| function user.s11_1{autoCommit=true}(A0:bte,A1:bte):void;                      |\\n|     X_4 := sql.mvc();                                                          |\\n|     X_46:bat[:oid,:bte]  := sql.bind(X_4,\"sys\",\"vbvdata\",\"vbvdata_speed\",0);   |\\n|     X_38:bat[:oid,:bte]  := sql.bind(X_4,\"sys\",\"vbvdata\",\"vbvdata_speed\",2);   |\\n|     X_48 := algebra.kdifference(X_46,X_38);                                    |\\n|     X_49 := algebra.kunion(X_48,X_38);                                         |\\n|     X_32:bat[:oid,:bte]  := sql.bind(X_4,\"sys\",\"vbvdata\",\"vbvdata_speed\",1);   |\\n|     X_50 := algebra.kunion(X_49,X_32);                                         |\\n|     X_18:bat[:oid,:oid]  := sql.bind_dbat(X_4,\"sys\",\"vbvdata\",1);              |\\n|     X_19 := bat.reverse(X_18);                                                 |\\n|     X_51 := algebra.kdifference(X_50,X_19);                                    |\\n|     X_25:bat[:oid,:bte]  := sql.bind(X_4,\"sys\",\"vbvdata\",\"vbvdata_lane_id\",0); |\\n|     X_27 := algebra.uselect(X_25,A1);                                          |\\n|     X_23:bat[:oid,:bte]  := sql.bind(X_4,\"sys\",\"vbvdata\",\"vbvdata_lane_id\",2); |\\n|     X_28 := algebra.kdifference(X_27,X_23);                                    |\\n|     X_24 := algebra.uselect(X_23,A1);                                          |\\n|     X_29 := algebra.kunion(X_28,X_24);                                         |\\n|     X_21:bat[:oid,:bte]  := sql.bind(X_4,\"sys\",\"vbvdata\",\"vbvdata_lane_id\",1); |\\n|     X_22 := algebra.uselect(X_21,A1);                                          |\\n|     X_30 := algebra.kunion(X_29,X_22);                                         |\\n|     X_31 := algebra.kdifference(X_30,X_19);                                    |\\n|     X_52 := algebra.semijoin(X_51,X_31);                                       |\\n|     X_53 := algebra.thetauselect(X_52,A0,\"&gt;\");                                 |\\n|     X_55 := algebra.kdifference(X_53,X_38);                                    |\\n|     X_41 := algebra.semijoin(X_38,X_31);                                       |\\n|     X_42 := algebra.thetauselect(X_41,A0,\"&gt;\");                                 |\\n|     X_56 := algebra.kunion(X_55,X_42);                                         |\\n|     X_35 := algebra.semijoin(X_32,X_31);                                       |\\n|     X_36 := algebra.thetauselect(X_35,A0,\"&gt;\");                                 |\\n|     X_57 := algebra.kunion(X_56,X_36);                                         |\\n|     X_58 := algebra.kdifference(X_57,X_19);                                    |\\n|     X_59 := algebra.markT(X_58,0@0:oid);                                       |\\n|     X_60 := bat.reverse(X_59);                                                 |\\n|     X_12:bat[:oid,:lng]  := sql.bind(X_4,\"sys\",\"vbvdata\",\"vbvdata_id\",0);      |\\n|     X_10:bat[:oid,:lng]  := sql.bind(X_4,\"sys\",\"vbvdata\",\"vbvdata_id\",2);      |\\n|     X_14 := algebra.kdifference(X_12,X_10);                                    |\\n|     X_15 := algebra.kunion(X_14,X_10);                                         |\\n|     X_6:bat[:oid,:lng]  := sql.bind(X_4,\"sys\",\"vbvdata\",\"vbvdata_id\",1);       |\\n|     X_16 := algebra.kunion(X_15,X_6);                                          |\\n|     X_61 := algebra.leftjoin(X_60,X_16);                                       |\\n|     X_62 := aggr.count(X_61);                                                  |\\n|     sql.exportValue(1,\"sys.vbvdata\",\"L1\":str,\"wrd\",64,0,6,X_62,\"\");            |\\n| end s11_1;                                                                     |\\n| # optimizer.mitosis()                                                          |\\n| # optimizer.dataflow()                                                         |\\n+--------------------------------------------------------------------------------+\\n</code></pre>\\n\\n<p>Can anyone help?</p>\\n-How could we know that monetDB is using Index?-<indexing><explain><monetdb>',\n",
       " '<p>I am working on migration project of Oracle to Teradata.</p>\\n\\n<p>The tables have been migrated using datastage jobs.</p>\\n\\n<p>How do I migrate Oracle Views to Teradata?</p>\\n\\n<p>Direct script copying is not working due to SQL statements difference of both databases </p>\\n\\n<p>Please help?</p>\\n-How to migrate Oracle View to Teradata-<oracle><migration><view><etl><teradata>',\n",
       " '<p>I read <code>proc/&lt;pid&gt;/io</code> to measure the IO-activity of SQL-queries, where <code>&lt;pid&gt;</code> is the PID of the database server. I read the values before and after each query to compute the difference and get the number of bytes the request caused to be read and/or written.</p>\\n\\n<p>As far as I know the field <code>READ_BYTES</code> counts actual disk-IO, while <code>RCHAR</code> includes more, like reads that could be satisfied by the linux page cache (see <a href=\"https://stackoverflow.com/questions/3633286/understanding-the-counters-in-proc-pid-io\">Understanding the counters in /proc/[pid]/io</a>  for clarification).\\nThis leads to the assumption, that <code>RCHAR</code> should come up with a value equal or greater than <code>READ_BYTES</code>, but my results contradict this assumption.</p>\\n\\n<p>I could imagine some minor block or page overhead for results I get for Infobright ICE (values are MB):</p>\\n\\n<pre><code>        Query        RCHAR   READ_BYTES\\ntpch_q01.sql|    34.44180|    34.89453|\\ntpch_q02.sql|     2.89191|     3.64453|\\ntpch_q03.sql|    32.58994|    33.19531|\\ntpch_q04.sql|    17.78325|    18.27344|\\n</code></pre>\\n\\n<p>But I completely fail to understand the IO-counters for MonetDB (values are MB):</p>\\n\\n<pre><code>        Query        RCHAR   READ_BYTES\\ntpch_q01.sql|     0.07501|   220.58203|\\ntpch_q02.sql|     1.37840|    18.16016|\\ntpch_q03.sql|     0.08272|   162.38281|\\ntpch_q04.sql|     0.06604|    83.25391|\\n</code></pre>\\n\\n<p>Am I wrong with the assumption that <code>RCHAR</code> includes <code>READ_BYTES</code>? Is there a way to trick out the kernels counters, that MonetDB could use? What is going on here?</p>\\n\\n<p>I might add, that I clear the page cache and restart the database-server before each query.\\nI\\'m on Ubuntu 11.10, running kernel 3.0.0-15-generic.</p>\\n-Does RCHAR include READ_BYTES (proc/<pid>/io)?-<linux><io><linux-kernel><procfs><monetdb>',\n",
       " \"<p>I've got an implementation in RapidMiner that classifies questions according to Bloom's taxonomy. I need to consume the data produced by a web application developed in PHP and show the results in the interface of the web application. </p>\\n\\n<p>So I'm wondering if it is possible that the application in PHP communicates with RapidMiner to process the data and show the results provided by RapidMiner in the interface.</p>\\n\\n<p>I know RapidMiner is implemented in Java, and there is the option of using Java bridge to comunicate PHP and JAVA, but I'm not sure if that is a solution to this.</p>\\n-How integrate RapidMiner in a PHP web application?-<java><php><rapidminer>\",\n",
       " '<p>I have MonetDB database with table containing user table:</p>\\n\\n<pre><code>CREATE TABLE user\\n(\\n    birth_date TIMESTAMP NOT NULL\\n);\\n</code></pre>\\n\\n<p><code>birth_date</code> is saved in GMT without DST. (This is the default behavior of MonetDB). So I should change the <code>TimeZone</code> in my application. Here is my code:</p>\\n\\n<pre><code>Class.forName(\"nl.cwi.monetdb.jdbc.MonetDriver\");\\nConnection con = DriverManager.getConnection(\"jdbc:monetdb://localhost/online\", \"monetdb\", \"monetdb\");      \\nStatement st = con.createStatement();\\nResultSet rs;\\n\\nrs = st.executeQuery(\"SELECT * FROM user\");\\nwhile (rs.next()) {\\n    Calendar c = Calendar.getInstance(TimeZone.getTimeZone(\"Asia/Tehran\"));\\n    c.setTime(rs.getTimestamp(\"birth_date\"));\\n    System.out.println(c.get(Calendar.YEAR) + \"-\" + c.get(Calendar.MONTH) + \"-\" + c.get(Calendar.DAY_OF_MONTH) + \" \" + c.get(Calendar.HOUR_OF_DAY) + \":\" + c.get(Calendar.MINUTE) + \":\" + c.get(Calendar.SECOND));\\n}\\n</code></pre>\\n\\n<p>But this code print the same <code>TIMESTAMP</code> in the database. Is this the wrong way to convert <code>TimeZone</code>? I\\'m using MonetDB version 11.9.5-20120516 on Debian 6 with openjdk 6. Here is the <code>monetdbd getall /home/dbfarm</code>:</p>\\n\\n<pre><code>dbfarm           /home/dbfarm/\\nstatus           monetdbd[4187] 1.6 (Apr2012-SP1) is serving this dbfarm\\nmserver          /usr/bin/mserver5\\nlogfile          /home/dbfarm//merovingian.log\\npidfile          /home/dbfarm//merovingian.pid\\nsockdir          /tmp\\nport             50000\\nexittimeout      60\\nforward          proxy\\ndiscovery        yes\\ndiscoveryttl     600\\ncontrol          yes\\npassphrase       {SHA512}ba3253876aed6bc22d4a6ff53d8406c6ad864195ed144ab5c87621b6c233b548baeae6956df346ec8c17f5ea10f35ee3cbc514797ed7ddd3145464e2a0bab413\\nmapisock         /tmp/.s.monetdb.50000\\ncontrolsock      /tmp/.s.merovingian.50000\\n</code></pre>\\n-Changing time zone after getting GMT time from database - MonetDB-<java><jdbc><timezone><timestamp><monetdb>',\n",
       " '<p>Does Teradata have its JDBC driver uploaded as a jar referenced from a maven file?</p>\\n-Maven dependency for Teradata JDBC driver-<maven><jdbc><dependency-management><teradata>',\n",
       " \"<p>MonetDB seems to support a fairly comprehensive set of <strong>system catalog views</strong> in order to discover the schema structure of the database.  Unfortunately, I can't seem to find a SQL query that will obtain the <strong>set of columns for a given key or index</strong>.  Here are the system tables/views are reported by the Tables table:</p>\\n\\n<p>schemas\\ntypes\\nfunctions\\nargs\\nsequences\\ndependencies\\nconnections\\n_tables\\n_columns\\nkeys\\nidxs\\ntriggers\\nobjects\\ntables\\ncolumns\\ndb_user_info\\nusers\\nuser_role\\nauths\\nprivileges\\nqueryhistory\\ncallhistory\\nquerylog\\nsystemfunctions</p>\\n\\n<p>I tried dependencies, but the IDs don't seem to match up.  BTW, I did try looking in the source code, but I haven't yet found where the system views are created and maintained.</p>\\n-How to discover the columns for a given index or key in MonetDB-<sql><monetdb>\",\n",
       " '<p>I am Clustering bunch of words with k-means algorithm in RapidMiner 5.2\\nI am converting nominal to numerical before the clustering. However, to really view my clustering, i need to view numbers back as word. How can i convert it back?</p>\\n-RapidMiner 5.2 Converting Numeric to Nominal and Reverse-<rapidminer>',\n",
       " '<p>We are currently upgrading our Teradata clients from v12 to v13. For that, the old installations of Teradata 12 were uninstalled from the system and TTU13 was installed. </p>\\n\\n<p>After installation when I try to add a ODBC connection using the new Teradata driver, it gives me the following error.</p>\\n\\n<blockquote>\\n  <p>Unknown error occurred in terasso library</p>\\n</blockquote>\\n\\n<p>Any help would be highly appreciated.</p>\\n-Error after upgrading from Teradata 12 to Teradata 13....terasso.dll-<odbc><teradata>',\n",
       " '<p>Found a similar question but without an answer that worked succuessfully.</p>\\n<p>I need to select a sample of 50 of each status type within a single table.</p>\\n<h3>TABLE1</h3>\\n<pre><code>MEMBER  STATUS\\n1234       A\\n1324       A\\n3424       R\\n3432       S\\n3232       R\\n2783       A\\n2413       S\\n4144       R\\n2387       S\\n</code></pre>\\n<p>I tried:</p>\\n<blockquote>\\n<p>SEL Member, status\\nFROM TABLE1 Qualify Row_Number ( ) OVER (PARTITION\\nBY status ORDER BY random (1,10000)) &lt;=50</p>\\n</blockquote>\\n<p>As suggested in the previous question/answer but Teradata does not like RANDOM in an Aggregate or Ordered Analytical Function.</p>\\n-Sample of data within groups - Teradata-<sql><teradata><sample-data>',\n",
       " \"<p>Can Teradata SQL Query do Auto-Increment?</p>\\n\\n<p>I'm looking for something similar to</p>\\n\\n<pre><code>SELECT\\n  Date (Auto-Increment by 1 over Column 2),\\n  Column 2,\\n  Column 3\\nFROM Fake_Table\\nGROUP BY 1,2,3\\n</code></pre>\\n\\n<p>And get something such as</p>\\n\\n<pre><code>Date        Column 2    Column 3\\n2012-06-11  A           A\\n2012-06-11  A           B\\n2012-06-11  A           C\\n2012-06-12  B           A\\n2012-06-13  C           B\\n</code></pre>\\n\\n<p>Is this possible?</p>\\n-Teradata Auto-Increment Query-<sql><teradata>\",\n",
       " \"<p>Sorry if the title is unclear. Basically I'm trying to select certain records from multiple tables then update a certain column value for the returned records.</p>\\n\\n<p>T-SQL Implementation</p>\\n\\n<pre><code>    UPDATE \\n        CUSTOMERS\\n    SET\\n        LIKES_US = 'Y'\\n    FROM\\n        RESTAURANT REST INNER JOIN CUSTOMERS CUST ON REST.LINK_ID = CUST.LINK_ID\\n        WHERE\\n        REST.REST_TYPE = 'Diner' AND CUST.LIKES_US IS NULL\\n</code></pre>\\n\\n<p>Oracle</p>\\n\\n<pre><code>    UPDATE \\n       (SELECT CUST.LIKES_US\\n        FROM CUSTOMERS CUST INNER JOIN RESTAURANT REST ON CUST.LINK_ID=REST.LINK_ID\\n        WHERE REST.REST_TYPE = 'Diner' AND CUST.LIKES_US IS NULL) NEW_CUST\\n    SET\\n        NEW_CUST.LIKES_US = 'Y';\\n</code></pre>\\n\\n<p>I am tried doing the same thing in Teradata as I did in Oracle but I get the following error:</p>\\n\\n<pre><code>Executed as Single statement.  Failed [3707 : 42000] Syntax error, expected something like a name or a Unicode delimited identifier or an 'UDFCALLNAME' keyword between the 'UPDATE' keyword and '('. \\nElapsed time = 00:00:00.003 \\n\\nSTATEMENT 1: Unknown failed. \\n</code></pre>\\n\\n<p>I looked online for the solution but had no luck.</p>\\n-Teradata Update Table from Select Statement-<sql><teradata>\",\n",
       " '<p>I have an application which loads the data in XML to Oracle tables. Now we are moving to teradata. Using Java a DAO is populated and that DAO calls oracle stored procedure after mapping DTO a sql struct object. If I want to do the same for Teradata, there is no such object as sql struct object which I can load through Java. Is there anyway I can use some method to interact with Teradata directly from DTO.</p>\\n\\n<p>Is there any way to populate an XML directly to teradata without loading to any landing table?</p>\\n-What is the Sql struct equivalent to load teradata tables-<java><teradata>',\n",
       " '<p>Hope somebody could help me .. I\\'m trying to install the mzmatch.R package for metabolomics, following the instructions here: <a href=\"http://mzmatch.sourceforge.net/tutorial.mzmatch.r.php\" rel=\"nofollow\">http://mzmatch.sourceforge.net/tutorial.mzmatch.r.php</a></p>\\n\\n<p>I\\'m completely new to R so this is my first time doing this sort of thing. First, I downloaded the latest version of R and installed it on Mac OSX 10.7. This is the version of R that I have running:\\nR 2.15.0 GUI 1.51 Leopard build 64-bit (6148)</p>\\n\\n<p>Then I launched R64.app and typed the following commands (as specified in the instructions in the link above) to install the package and all its dependencies.</p>\\n\\n<pre><code>&gt; source(\"http://bioconductor.org/biocLite.R\")\\n&gt; biocLite(c(\"faahKO\", \"xcms\", \"multtest\"))\\n&gt; install.packages(c(\"rJava\",\"XML\",\"snow\",\"caTools\",\"bitops\",\"ptw\"))\\n&gt; source (\"http://puma.ibls.gla.ac.uk/mzmatch.R/install_mzmatch.R\")\\n</code></pre>\\n\\n<p>The last step will always fail with the following message:</p>\\n\\n<pre><code>trying URL \\'http://puma.ibls.gla.ac.uk/mzmatch.R/mzmatch.R.tar.gz\\'\\nContent type \\'application/x-gzip\\' length 104535 bytes (102 Kb)\\nopened URL\\n==================================================\\ndownloaded 102 Kb\\n\\n* installing *source* package ‘mzmatch.R’ ...\\n** R\\n** preparing package for lazy loading\\nError : .onLoad failed in loadNamespace() for \\'mzR\\', details:\\n  call: value[[3L]](cond)\\n  error: failed to load module Ramp from package mzR\\ncould not find function \"errorOccured\"\\nError : package ‘mzR’ could not be loaded\\nERROR: lazy loading failed for package ‘mzmatch.R’\\n* removing ‘/Library/Frameworks/R.framework/Versions/2.15/Resources/library/mzmatch.R’\\nWarning message:\\nIn install.packages(pkgs = \"mzmatch.R.tar.gz\", repos = NULL, type = \"source\") :\\n  installation of package ‘mzmatch.R.tar.gz’ had non-zero exit status\\n</code></pre>\\n\\n<p>I thought this was because the library \"mzR\" cannot be loaded, so I tried:</p>\\n\\n<pre><code>&gt; biocLite(\"mzR\")\\n&gt; library(\"mzR\")\\n</code></pre>\\n\\n<p>and sure enough, the same error appeared:</p>\\n\\n<pre><code>Loading required package: Rcpp\\nError : .onLoad failed in loadNamespace() for \\'mzR\\', details:\\n  call: value[[3L]](cond)\\n  error: failed to load module Ramp from package mzR\\ncould not find function \"errorOccured\"\\nError: package/namespace load failed for ‘mzR’    \\n</code></pre>\\n\\n<p>I\\'m quite lost now, and not sure what to do at all. Thanks for reading !</p>\\n-R bioconductor mzR library load error-<r><bioconductor>',\n",
       " \"<p>I've seen other people reporting the <code>System.Net.Sockets.SocketException</code> exception, but they largely involve web services.  We're not calling any web services.  In fact, this ASP.NET app is a single-tier app with the UI and data layer contained in a single assembly.  To retrieve data, we manually open a connection, create a command, and execute a reader or use a data adapter to fill a data table before delivering it to the page.</p>\\n\\n<p>A few times per month when trying to execute a command, we get <code>System.Net.Sockets.SocketException</code>.  I have no idea what could be causing this.  As stated in the subject, we're accessing Teradata database, so we use TdConnection, TdCommand, TdDataAdapter.</p>\\n\\n<pre><code>cmdSolutionName = New TdCommand(sSql, Con)\\ndaSolutionName.SelectCommand = cmdSolutionName\\ndaSolutionName.Fill(tmpTable) 'Exception is thrown here\\n</code></pre>\\n\\n<p>Does anyone know what could cause this exception when working directly with a database connection and not a web service?</p>\\n-System.Net.Sockets.SocketException when trying to fill DataTable with TdDataAdapter.Fill() (Teradata DataAdapter)-<.net><exception><command><teradata><dataadapter>\",\n",
       " '<p>I am using PHP 5.3.3 on RHEL 5.  I have installed all the necessary drivers and add-ons for ODBC connections to Teradata.  I wrote a simple script to test it:</p>\\n\\n<pre class=\"lang-php prettyprint-override\"><code>$con_string = \"DRIVER={Teradata};DBCName=**HOST**; DATABASE=**database**;\";\\n$con = odbc_pconnect( $con_string , \"user\", \"pass\" );\\nvar_dump($con);\\necho \\'here\\';\\n$res = odbc_exec($con, \"SELECT id FROM database.table\");\\necho \\'there\\';\\nvar_dump($res);\\n</code></pre>\\n\\n<p>The output is as follows:</p>\\n\\n<blockquote>\\n  <p>resource(5) of type (odbc link persistent)</p>\\n  \\n  <p>hereSegmentation fault</p>\\n</blockquote>\\n\\n<hr>\\n\\n<p>Obviously the connection works.  I have also tried other commands, some work, some don\\'t.  <code>odbc_tables()</code> and <code>odbc_procedures()</code> return the correct information without problem.  However, anything related to a SQL query gets a seg fault.  I have not been able to get trace/debug to turn on for ODBC.</p>\\n\\n<p>Any help would be greatly appreciated!</p>\\n-PHP odbc_exec Segmentation Fault after successful odbc_connect to Teradata 14.0-<php><odbc><teradata>',\n",
       " '<p>In RapidMiner, I\\'m trying to pull data from div class \"description\". When I test the syntax using Google XPath import XML, it returns the appropriate data. When I try the following in RapidMiner, I get blank results:</p>\\n\\n<pre><code>//h:div[@id=\\'description\\']/text()\\n</code></pre>\\n\\n<p>This works:</p>\\n\\n<pre><code>//h:div[@id=\\'description\\']\\n</code></pre>\\n\\n<p>but I get all of the HTML markup data with it. </p>\\n\\n<p>What do I need to add or remove?</p>\\n\\n<p>On the same document, I was able to get this to return expected results:</p>\\n\\n<pre><code>//h:label[@id=\\'desc\\']/text()\\n</code></pre>\\n\\n<p>XML:</p>\\n\\n<pre><code>div id=\"description\" class=\"longdesc\" style=\" padding-top: 0px; display: block;\" xmlns:IDB=\"urn:ItemDataBinding\"&gt;Description Text Here./div\\n</code></pre>\\n-RapidMiner xpath text extraction-<xpath><rapidminer>',\n",
       " '<p>There is very little information on <code>dynamic parallelism</code> of Kepler, from the description of this new technology, does it mean the issue of thread control flow divergence in the same warp is solved? </p>\\n\\n<p>It allows <code>recursion</code> and lunching kernel from device code, does it mean that control path in different thread can be executed simultaneously?</p>\\n-Kepler CUDA dynamic parallelism and thread divergence-<cuda><kepler><dynamic-parallelism>',\n",
       " '<p>I am trying to use odbc to connect to monetDB in node.js. However, I keep getting the following error:</p>\\n\\n<pre><code>The driver reported the following diagnostics whilst running SQLDriverConnect\\n\\nIM002:1:0:[unixODBC][Driver Manager]Data source name not found, and no default driver      specified\\nDEBUG: []\\n</code></pre>\\n\\n<p>Here is the source code:</p>\\n\\n<pre><code>var sys  = require(\"util\");\\nvar odbc = require(\"/home/vash/node_modules/odbc/odbc.js\");\\n\\nvar db = new odbc.Database();\\ndb.open(\"Driver={ODBC for MonetDB};Setup=/usr/lib/libMonetODBCs.so;UsageCount= 1;SERVER=vash-G75VW;USER=monetdb;PASSWORD=monetdb;PORT=5000;DATABASE=demo\", function(err)\\n{\\n    db.query(\"select * from demo\", function(err, rows, moreResultSets)\\n    {\\n        sys.debug(sys.inspect(rows));\\n        db.close(function(){});\\n    });\\n});\\n</code></pre>\\n\\n<p>Using the command \"isql -v monetdb\", I can connect to monetDB using ODBC with the following odbc.ini and odbcinst.ini contents:</p>\\n\\n<pre><code>[monetdb]\\nDescription             = \"ODBC for MonetDB\"\\nDriver          = /usr/lib/libMonetODBC.so\\nSetup           = /usr/lib/libMonetODBCs.so\\nUsageCount              = 1\\nSERVER = vash-G75VW\\nUSER = monetdb\\nPASSWORD = monetdb\\nPORT = 5000 \\nDATABASE = demo\\n</code></pre>\\n\\n<p>Since I connect using isql, I am guessing the problem lies in the line of code:</p>\\n\\n<pre><code>\"Driver={ODBC for MonetDB};Setup=/usr/lib/libMonetODBCs.so;UsageCount= 1;SERVER=vash-G75VW;USER=monetdb;PASSWORD=monetdb;PORT=5000;DATABASE=demo\"\\n</code></pre>\\n\\n<p>Any ideas what\\'s wrong, and do I have the connection string above set properly?</p>\\n-Node.js and ODBC for MonetDB-<node.js><odbc><monetdb>',\n",
       " '<p>I\\'d like to do something like this:</p>\\n\\n<p><code>Dim pId As TdParameter = New TdParameter With {.ParameterName = \"p_id\", .SourceColumn = \"id\", .TdType = TdType.Integer, .Direction = ParameterDirection.Output}</code></p>\\n\\n<p>And then go like:</p>\\n\\n<p><code>cmd.Parameters.Add(pId);</code></p>\\n\\n<p><code>cmd.CommandText = \"insert into employee (id, name, age) values (?, \\'bob\\', 34)\"</code></p>\\n\\n<p><code>cmd.ExecuteNonQuery();</code></p>\\n\\n<p>And then have <code>pId.Value</code> return the value of the new record\\'s Id column (which is an identity column).  If I could do that, that would be great.</p>\\n-Teradata .NET Paramaters: Can I use Output parameters with SQL (ie, not using a stored procedure)?-<.net><sql><parameters><teradata>',\n",
       " \"<p>I'm looking for a way to identify the default permissions granted to a user in a particular database.  I have tried this:</p>\\n\\n<pre><code>select *\\nfrom dbc.allrights\\nwhere username='user-id'\\n  and databasename='database-name'\\n</code></pre>\\n\\n<p>There are two problems with the above;  first, as written, the query returns one row for each permisison for each table owned by the user-id and it includes permissions that were specifically granted.  Second, if the user-d has not created any tables at all, no rows are returned.</p>\\n\\n<p>I'm hoping there is be another DBC view that contains the default permissions for a user and for a database.  </p>\\n-How to identify all rights for a Teradata user?-<teradata>\",\n",
       " \"<p>I need the php_monetdb.dll for php 5.4. I've found that by building the monetDB with the HAVE_PHP=1 option on windows will create the dll, but the build procedure fails at numerous points. I am using the monetdb source from sourceforge which is a little outdated one. The newer version has problems in the .msc files itself so proceeding to the build itself is a pain. The older version that I am trying on is atleast passing the nmake step but failing on the nmake install step. The reasons vary from component to component. One states that the devenv  /build does not exist. For this I copied the vcexpress and renamed it to devenv. I am not sure if this is right but when I run it from the command prompt it seems to work ok with a few warning alerts shown. During other make installs, the header files seem to be missing. Could anyone please help me with this. Also please let me know if there is any other alternative for this approach of getting php_monetdb.dll.</p>\\n\\n<p>P.S: I am using windows 7, MSVC 10 express, PHP 5.4.4</p>\\n-php_monetdb.dll-<php><windows><build><monetdb>\",\n",
       " '<p>While executing the following code it gives a file not found error at a:28 (in comments). Is it because of directory is not refreshed, or the file not created by subprocess before executing line at a:28?</p>\\n\\n<pre><code>File outputFile = new File(\"RunstatsCmd.db2\");\\nFileWriter out = new FileWriter(outputFile);\\n\\nString cmd = \"DB2CMD;DB2;\"+\" EXPORT TO \"+ \"\\\\\"C:\\\\\\\\file.xml\\\\\"\" +\" OF IXF MESSAGES \"+\"\\\\\"C:\\\\\\\\msg.txt\\\\\"\"+\" SELECT * FROM OLTP.ACCOUNT_DETAILS\";\\n\\nout.write(\"CONNECT TO OLTPDB;\\\\n\");\\nout.write(cmd + \";\\\\n\");\\nout.write(\"CONNECT RESET;\\\\n\");\\n\\nout.close();\\n\\nSystem.out.println(\"before creating connection....\");\\nClass.forName (\"com.ibm.db2.jcc.DB2Driver\").newInstance ();\\nProcess p = Runtime.getRuntime().exec(\"db2 -vtf RunstatsCmd.db2\");\\n\\n// open streams for the process\\'s input and error                                       \\nBufferedReader stdInput = new BufferedReader(new \\n                                      InputStreamReader(p.getInputStream()));\\nBufferedReader stdError = new BufferedReader(new\\n                                      InputStreamReader(p.getErrorStream()));\\nString s;\\n\\n// read the output from the command and set the output variable with \\n// the value\\nwhile ((s = stdInput.readLine()) != null)\\n{\\n    System.out.println(s);\\n}\\n\\n// read any errors from the attempted command and set the error  \\n// variable with the value\\nwhile ((s = stdError.readLine()) != null) \\n{\\n    System.out.println(s);\\n}\\n\\n// destroy the process created \\n\\n\\n// delete the temporary file created\\noutputFile.deleteOnExit(); \\n\\n\\nSystem.out.println(\"query executed...\");   \\np.waitFor();        \\n//a:28////////////////////////                  \\nFileInputStream fis=new FileInputStream(\"C:\\\\\\\\file.xml\");\\nint i;\\nwhile((i=fis.read())!=-1){\\n    System.out.println((char)i);\\n}\\n\\n}catch(Exception e){\\n    e.printStackTrace();\\n}\\n</code></pre>\\n-java process: file not found-<java><file>',\n",
       " '<p>in oracle we have </p>\\n\\n<p>decalre \\nv_data number ;</p>\\n\\n<p>begin \\n select max(deptno) into v_data from dept;</p>\\n\\n<p>end;</p>\\n\\n<p>do we have this type of advatntage of in terdata like selecting the data from table into a variable</p>\\n\\n<p>can u give the equivalent code in terdata</p>\\n\\n<p>Thanks</p>\\n-teradata : select data into variable-<oracle11g><teradata>',\n",
       " '<p>The web seems a bit short on working examples for something that should be quite common. A plain Jane example of \"Get me some records\". This is my first ever Stored proc and all I want is to see some records. Why is that so flipping hard? ;-) I figure that if I can get one example that works, I can tune it from there into something I can really use. This is taken from another example I found on the web. Doesn\\'t compile because CURSOR declaration is a syntax error of some sort. </p>\\n\\n<pre><code>CREATE PROCEDURE \"SCHEMA\".\"GETRESULTSET (\\n    IN \"p1\" VARCHAR(30))\\nDYNAMIC RESULT SETS 1\\nBEGIN       \\nDECLARE CURSOR cur1 WITH RETURN ONLY TO CLIENT FOR \\n SELECT partitioninfo FROM SCHEMA.SessionInfo where username = p1;    \\n  OPEN cur1;    \\nEND;\\n</code></pre>\\n\\n<p>Anyway, sure could use a clue. I saw an example where the CURSOR was declared separately from the SQL but then there wasn\\'t an example that showed how to get the variable into the SQL when it was declared as a VARCHAR. The example I am working off of was pretty old but it\\'s the best I could find. </p>\\n-Simple teradata stored procedure-<stored-procedures><cursor><resultset><teradata>',\n",
       " '<p>I created a table using the below definition for a Teradata identity column:</p>\\n\\n<pre>\\n      ID INTEGER GENERATED BY DEFAULT AS IDENTITY\\n           (START WITH 1 \\n            INCREMENT BY 1 \\n            MINVALUE 0 \\n            MAXVALUE 100000000 \\n            NO CYCLE),\\n----\\n      UNIQUE PRIMARY INDEX ( ID )\\n</pre>\\n\\n<p>For several months, the ID column has been working properly, automatically generating a unique value for the column.  Over the past month, however, ELMAH has been <em>intermittently</em> reporting the following exception from our .NET 4.0 ASP.NET app:</p>\\n\\n<p><code>Teradata.Client.Provider.TdException: [Teradata Database] [2801] Duplicate unique prime key error in DATABASENAME.TABLENAME.</code></p>\\n\\n<p>I was able to replicate it by opening SQL Assistant and inserting a bunch of records into the table with raw SQL.  As expected, most of the time it would insert successfully, but other times it would throw the above exception.</p>\\n\\n<p>It appears that this error is occuring because Teradata is trying to generate a value for this column that it has previously generated.</p>\\n\\n<p>Does anyone have any idea how to get to the bottom of what\\'s happening?  At the very least, I\\'d like some way to debug the issue a bit deeper.</p>\\n-Teradata identity column and \"Duplicate unique prime key error in dbname.tablename\"-<unique><identity><teradata>',\n",
       " \"<p>In Oracle, I used to use sequences to generate value for a table's unique identifier.  In a stored procedure, I'd call sequencename.nextval and assign that value to a variable.  After that, I'd use that variable for the procedure's insert statement and the procedure's out param so I could deliver the newly-generated ID to the .NET client.</p>\\n\\n<p>I'd like to do the same thing with Teradata, but I am thinking the only way to accomplish this is to create a table that holds a value that is sequentially incremented.  Ideally, however, I'd really like to be able to acquire the value that will be used for an identity column's next value without actually creating a new record in the database.</p>\\n-Teradata: Is it possible to generate an identity column value without creating a record?-<teradata><identity-column>\",\n",
       " '<p>I am newbie to teradata.</p>\\n\\n<p>I need to delete a row once the case condition is satisfied.</p>\\n\\n<p>Eg: case condition true delete the selected row.</p>\\n-Can I use delete clause in Case Clause in teradata-<sql><teradata><sql-delete><delete-row>',\n",
       " \"<p>I wonder if we can a reduce just a little bit the effort around packages\\nunder linux/unix OS environments and software installations.\\nIt is my stance that there is too much redundant effort about $subject.</p>\\n<p>I have been pondering about ways to connect build systems of $subject\\nwith some next &quot;stage build tools&quot;, like: easybuild (1) &amp; openbuildservice (2);\\nread below for more details.</p>\\n<p>To be more specific, I was able last week to take pkgsrc's repository,\\nprocess the Makefiles via a tiny &quot;pkg2eb&quot; script and produce *.eb files\\nfor easybuild, then fed many parallel gcc compilations with them.\\nThat &quot;blindly-driven process&quot; ended up in &gt;600 successful builds,\\nie. these were packages that simply needed 'wget/configure/make/make install';\\nIt's not bad for a first run, just wonder if it can be done any better.</p>\\n<p>So:</p>\\n<p>According to your experience, which OS has the cleanest/leanest\\npkgsrc/port structure to be sourced &amp; fed to other external tools?\\nThis is NOT the same as which has the most available packages!</p>\\n<p>Have you heard of any similar efforts trying to massively produce\\npackages from eg. a common source list in a structured manner?\\n(I mean, in a transferable way across different build systems)</p>\\n-pkgsrc, ports, portage, macports etc-<linux><package>\",\n",
       " '<p>I am brand new to teradata. I wrote a fastexport script to take some data from a db and export it to some excel files.\\nThe script is all good. I just can\\'t run it!</p>\\n\\n<p>I am running the cmd <code>fexp &lt; C:\\\\Documents\\\\ScriptName</code> (that is not the actual path)</p>\\n\\n<p>Where ScriptName is a .txt file containing the script.</p>\\n\\n<p>I get the error:</p>\\n\\n<blockquote>\\n  <p>\"The system cannot find the file specified\"</p>\\n</blockquote>\\n\\n<p>I have tried changing the location of the file and such but always get the same error.</p>\\n\\n<p>What am I missing here?</p>\\n-Teradata FastExport Script - cannot get it to run - The system cannot find the file specified-<database><teradata>',\n",
       " '<p>@@\\nI am using cursors in Teradata and I need some help related to fetch_status</p>\\n\\n<p>below are the SQL Server FETCH_STATUS values</p>\\n\\n<p>Return value Description\\n0 FETCH statement was successful.\\n-1 FETCH statement failed or the row was beyond the result set.\\n-2 Row fetched is missing.</p>\\n\\n<p>Can you tell me the equivalent values for the FETCH_STATUS in Teradata</p>\\n\\n<p>?</p>\\n-Fetch_status in Teradata-<sql><sql-server><stored-procedures><teradata>',\n",
       " '<p>I need to pass data to Neural Net operator. Some attributes are binomial and Neural Net does not accept binomial data type,  however it accepts integer. \\nThere is no Binomial to Integer (Numerical) type conversion operator. So how can I convert this ?</p>\\n\\n<p>Thanks for an answers</p>\\n-RapidMiner - binomial to integer conversion-<binary><integer><numerical><rapidminer>',\n",
       " '<p>There are several date/time fields in this view and I can never wrap my head around which column to order by (and which secondary column) in order to retrieve a list of SQL statements in the order in which they were executed on the server.</p>\\n-Teradata: What field do I order by when querying dbc.DBQLogTbl to return SQL in chronological order?-<teradata>',\n",
       " '<p>I have a fastload script that is giving me problems. I am new to teradata and don\\'t know how the date should be properly formatted. the date column is currently formatted as :\"12/31/2011\" in excel (csv). </p>\\n\\n<p>My script is as follows: </p>\\n\\n<pre><code>BEGIN LOADING \"table\"\\n    ERRORFILES \"errors1\", \"errors2\";\\n\\n\\nSET RECORD VARTEXT \",\";\\n\\n\\nDEFINE PERD_END_RPT_DT   (VARCHAR(20)),  &lt;---- date column returning the error\\n       RPT_PERD_TYPE_CD  (VARCHAR(20)),\\n       PERD_NM       (VARCHAR(30)),\\n       QTR_NUM           (VARCHAR(2)),\\n       YEAR_NUM          (VARCHAR(4)),\\n       RPT_PERD_TYPE_NM  (VARCHAR(10)),\\n       DATA_VLDTN_IND    (VARCHAR(1)),\\n       EDW_PUBLN_ID      (VARCHAR(18)) \\n\\nFILE=C:\\\\pathtofile\\\\file.csv;\\n\\nINSERT INTO \"table\" ( PERD_END_RPT_DT, RPT_PERD_TYPE_CD, PERD_NM, QTR_NUM, YEAR_NUM, RPT_PERD_TYPE_NM,\\n                          DATA_VLDTN_IND, EDW_PUBLN_ID )\\n    VALUES ( :PERD_END_RPT_DT, :RPT_PERD_TYPE_CD, :PERD_NM, :QTR_NUM, :YEAR_NUM, :RPT_PERD_TYPE_NM,\\n                          :DATA_VLDTN_IND, :EDW_PUBLN_ID ) ;\\n\\nEND LOADING ;\\nLOGOFF ;\\n</code></pre>\\n\\n<p>So how should the date be formatted and how can I modify my script to make it work? Thanks for your help!</p>\\n-Teradata Fastload Script Issue - Date not properly formatted error 2665-<sql><datetime><teradata>',\n",
       " '<p>Lately, we\\'ve been getting hit with the error <code>Transaction ABORTed due to Deadlock</code> when we go to update a record in one of our tables.  Something is placing a lock on this table and it\\'s not being released, but I have spent literal work days trying to track it down and it eludes me still.</p>\\n\\n<p>While the error is random, I do know what repetive cycle of steps I need to repeat in order to eventually trigger it.  However, I\\'ve queried dbc.DBQLogTbl and looked at all the SQL that\\'s been executed 2 minutes before and after the error occurs and nothing appears to be selecting from any table without an access lock.  Furthermore, after the error occurs, I will hit F5 repost the web form back to the server to repeat the exact same set of updates and it will work.</p>\\n\\n<p>My hunch is that some process outside our ASP.NET application is locking the table since I\\'ve checked all the SQL that\\'s being executed by our application.  I think there has to be a way to find out what specific SQL has been executed that has placed locks on a table.</p>\\n\\n<p><strong>8/9/2012 Additional info:</strong>  All of the following is occuring in this order within the same transaction based on what I am seeing when querying <code>dbc.DBQLogTbl</code> and ordering by <code>firststeptime</code>:</p>\\n\\n<ul>\\n<li>update employee table</li>\\n<li>locking row for access select * from employeesecurity where empid = X (do this to get current employee record to see if anything has changed)</li>\\n<li>if there is a change, update aforementioned employeesecurity record</li>\\n<li>update employeeconfig table (deadlock error always occurs here)</li>\\n</ul>\\n\\n<p>I didn\\'t mention this before, but the deadlock error is occurring on a table that I am not selecting from at all.  When the page loads, I do read from an employeeconfig view, but <code>locking row for access</code> is specified in the view.</p>\\n\\n<p>Answer to Rob\\'s 4 questions:</p>\\n\\n<ul>\\n<li>It\\'s only one transaction.</li>\\n<li>As I stated in my latest update, the table that\\'s locked is not even the table that\\'s being selected from.</li>\\n<li>All queries use <code>locking row for access</code></li>\\n<li>We are selecting from the view <code>employeeconfig</code>.  This view selects from the <code>employeeconfig</code> table using <code>locking row for access</code>.  We do not use <code>locking row for access</code> when querying the view itself.</li>\\n</ul>\\n\\n<p>As far as handling the deadlock, I\\'d rather not have the code just try to resubmit it as this seems like a problem that needs to be fixed.  As you said, Rob, it\\'s possible that my access to <code>dbc.DBQLogTbl</code> is limited, so perhaps I just cannot see everything that is occurring.  I\\'ve been in contact with the DBA and will follow up again today.</p>\\n-Teradata: How do you debug \"Transaction ABORTed due to Deadlock\" errors?-<deadlock><teradata>',\n",
       " '<p>I am trying to convert the following query from SQL server based code to TERADATA code.</p>\\n\\n<pre><code>INSERT INTO #Keyword(Keyword,OmnitureHitsID,Hit_Time_GMT)    \\n            SELECT  Search, @PrvRowIdentity,Hit_Time_GMT     \\n            FROM    \\n            (SELECT     \\n                Evar02_Search     \\n            , Hit_Time_GMT     \\n            , Evar11_End_Keyword     \\n            , Evar14_End_SrchTrmPassed     \\n            , Post_Evar02     \\n            , Post_Evar11     \\n            , Post_Evar14     \\n            FROM #MaINTable WHERE ID = @i    \\n            AND Visid_High =@Visid_High ) p    \\n            UNPIVOT     \\n            (SEARCH FOR SearchKeyword IN     \\n                (Evar02_Search     \\n                , Evar11_End_Keyword     \\n                , Evar14_End_SrchTrmPassed     \\n                , Post_Evar02     \\n                , Post_Evar11     \\n                , Post_Evar14 )    \\n            ) AS unpvt;\\n</code></pre>\\n\\n<p>Can anyone tell me how to convert the part after UNPIVOT. Actually I am not sure what the <code>SEARCH FOR SearchKeyword IN</code> part does.</p>\\n\\n<p>Any help is appreciated :)</p>\\n-Teradata Equivalent for UNPIVOT and SEARCH FOR from SQL SERVER-<sql><sql-server><database><teradata><unpivot>',\n",
       " '<p>I have feed-forward neural network with six inputs, 1 hidden layer and two output nodes (1; 0). This NN is learned by 0;1 values. \\nWhen applying model, there are created variables confidence(0) and confidence(1), where sum of this two numbers for each row is 1. \\nMy question is: what do these two numbers (confidence(0) and confidence(1)) exactly mean? Are these two numbers probabilities?</p>\\n\\n<p>Thanks for answers</p>\\n-Rapidminer - neural net operator - output confidence-<neural-network><rapidminer>',\n",
       " '<p>When doing a select of all columns from a table consisting of 86 columns in SQLA, I always get the error <code>Row size or Sort Key size overflow</code>.  The only way to avoid this error is to trim down the number of columns in the select, but this is an unconventional solution.  There has to be a way to select all columns from this table in one select statement.</p>\\n\\n<p><strong>Bounty</strong></p>\\n\\n<p>I am adding this bounty because I cannot hack my way past this issue any longer.  There has to be a solution to this.  Right now, I am selecting from a table with Unicode columns.  I am assuming this is causing the row size to exceed capacity.  When I remove <code>Session Character Set=UTF8</code> from my connection string, I get the error of <code>The string contains an untranslatable character</code>.  I am using NET data provider 14.0.0.1.  Is there a way to increase the size?</p>\\n\\n<p><strong>Update</strong></p>\\n\\n<p>Rob, you never cease to impress!  You suggestion of using UTF16 works.  It even works in SQLA after I update my ODBC config.  I think my problem all along is my lack of understanding of ASCII, Latin, UTF8, and UTF16.</p>\\n\\n<p>We also have an 80-column table that consists of all Latin columns, a few of which are `varchar(1000)\\'.  I get the same error in SQLA when selecting from it in UTF8 and UTF16, but I can select from it just fine after updating my character set to ASCII or Latin mode in my ODBC config.</p>\\n\\n<p>Rob, can you provide insight as to what\\'s happening here?  My theory is that, because it\\'s in the Latin set, using UTF8 or UTF16 causes a conversion to a larger set of bytes which results in the error, especially for the <code>varchar(1000)</code>\\'s.  If I use Latin as my session character set, no conversion is done and I get the string in its native encoding.  As for the issue in question, UTF8 fails because the encoding cannot be \"downgraded\"? </p>\\n\\n<p>Per request, here is the DDL of the table in question:</p>\\n\\n<pre><code>CREATE MULTISET TABLE mydb.mytable ,NO FALLBACK ,\\n     NO BEFORE JOURNAL,\\n     NO AFTER JOURNAL,\\n     CHECKSUM = DEFAULT,\\n     DEFAULT MERGEBLOCKRATIO\\n     (\\n      FIELD1 VARCHAR(214) CHARACTER SET LATIN CASESPECIFIC NOT NULL,\\n      FIELD2 VARCHAR(30) CHARACTER SET UNICODE CASESPECIFIC,\\n      FIELD3 VARCHAR(60) CHARACTER SET UNICODE CASESPECIFIC NOT NULL,\\n      FIELD4 VARCHAR(4000) CHARACTER SET UNICODE CASESPECIFIC,\\n      FIELD5 VARCHAR(900) CHARACTER SET UNICODE CASESPECIFIC,\\n      FIELD6 VARCHAR(900) CHARACTER SET UNICODE CASESPECIFIC,\\n      FIELD7 VARCHAR(900) CHARACTER SET UNICODE CASESPECIFIC,\\n      FIELD8 VARCHAR(900) CHARACTER SET UNICODE CASESPECIFIC,\\n      FIELD9 VARCHAR(900) CHARACTER SET UNICODE CASESPECIFIC,\\n      FIELD10 VARCHAR(900) CHARACTER SET UNICODE CASESPECIFIC,\\n      FIELD11 VARCHAR(3600) CHARACTER SET UNICODE CASESPECIFIC,\\n      FIELD12 VARCHAR(3600) CHARACTER SET UNICODE CASESPECIFIC,\\n      FIELD13 VARCHAR(3600) CHARACTER SET UNICODE CASESPECIFIC,\\n      FIELD14 VARCHAR(3600) CHARACTER SET UNICODE CASESPECIFIC)\\nPRIMARY INDEX ( FIELD1 );\\n</code></pre>\\n-Teradata SQLA: Row size or Sort Key size overflow-<teradata>',\n",
       " \"<p>I made this mload script and it gets stuck at LOGON. I have access to the db and similar logons work fine in fastload scripts.</p>\\n\\n<p>Here is the script:</p>\\n\\n<pre><code>.LOGTABLE UD821.FRC_RPT_PERD_Log;\\n.LOGON oneview/uid,pw;\\n\\n\\n\\n.BEGIN IMPORT MLOAD TABLES UD821.FRC_RPT_PERD SESSIONS 5;\\n\\n\\n\\n\\n.LAYOUT InputFile_layout;\\n\\n       .field PERD_END_RPT_DT   (VARCHAR(20));\\n       .field RPT_PERD_TYPE_CD  (VARCHAR(20));\\n       .field PERD_NM        (VARCHAR(30));\\n       .field QTR_NUM           (VARCHAR(2));\\n       .field YEAR_NUM          (VARCHAR(4));\\n       .field RPT_PERD_TYPE_NM  (VARCHAR(10));\\n       .field DATA_VLDTN_IND    (VARCHAR(1));\\n       .fieldEDW_PUBLN_ID      (VARCHAR(18));\\n\\n.DML LABEL FRC_RPT_PERD_INSERT;\\n\\n\\n.INSERT INTO UD821.FRC_RPT_PERD ( PERD_END_RPT_DT, RPT_PERD_TYPE_CD, PERD_NM, QTR_NUM, YEAR_NUM, RPT_PERD_TYPE_NM,\\n                          DATA_VLDTN_IND, EDW_PUBLN_ID )\\n    VALUES ( \\n\\nCASE WHEN Substr(PERD_END_RPT_DT,3,1) = '/'\\nAND Substr(PERD_END_RPT_DT,6,1) = '/'\\nTHEN Substr(PERD_END_RPT_DT,7,4)||'-'||Substr(PERD_END_RPT_DT,1,2)||'-' ||Substr(PERD_END_RPT_DT,4,2)\\n\\nWHEN Substr(PERD_END_RPT_DT,2,1) = '/'\\nAND Substr(PERD_END_RPT_DT,5,1) = '/'\\nTHEN Substr(PERD_END_RPT_DT,6,4)||'-0'||Substr(PERD_END_RPT_DT,1,1)||'-' ||Substr(PERD_END_RPT_DT,3,2)\\n\\nWHEN Substr(PERD_END_RPT_DT,2,1) = '/'\\nAND Substr(PERD_END_RPT_DT,4,1) = '/'\\nTHEN Substr(PERD_END_RPT_DT,5,4)||'-0'||Substr(PERD_END_RPT_DT,1,1)||'-0' ||Substr(PERD_END_RPT_DT,3,1)\\n\\nWHEN Substr(PERD_END_RPT_DT,3,1) = '/'\\nAND Substr(PERD_END_RPT_DT,5,1) = '/'\\n\\nTHEN Substr(PERD_END_RPT_DT,6,4)||'-'||Substr(PERD_END_RPT_DT,1,2)||'-0' ||Substr(PERD_END_RPT_DT,4,1)\\nELSE PERD_END_RPT_DT\\nEND (Date,Format 'yyyy-mm-dd'),, :RPT_PERD_TYPE_CD, :PERD_NM, :QTR_NUM, :YEAR_NUM, :RPT_PERD_TYPE_NM,\\n                          :DATA_VLDTN_IND, :EDW_PUBLN_ID ) ;\\n\\n.import infile C:\\\\Documents and Settings\\\\bkp343\\\\Desktop\\\\FRC_RPT_PERD.csv\\n format vartext  ','\\n display errors\\n nostop\\n layout InputFile_Layout\\n apply FRC_RPT_PERD_INSERT;\\n\\n.end mload;\\n .logoff;\\n</code></pre>\\n\\n<p>Is there something obviously wrong here? It just hangs at 0002 LOGON.</p>\\n-Teradata Multiload Script stuck at LOGON-<sql><date><teradata>\",\n",
       " \"<p>Hi Teradata colleagues!</p>\\n\\n<p>I have to do some modifications in production:\\n1) Modify existing collumns\\n2) Add some new collumns\\n3) Drop and re-create primary index</p>\\n\\n<p>Can anyone help me in 1) case - modifying column segment with addition of compress part:</p>\\n\\n<p>segment CHAR(10) CHARACTER SET LATIN NOT CASESPECIFIC COMPRESS ('AM        ','CAM       ','KAM       ','KO        ','PSHC      ','RES       ','SBS       ')</p>\\n\\n<p>How to add this column:\\npot_klasse CHAR(1) CHARACTER SET LATIN NOT CASESPECIFIC COMPRESS ('B','U')</p>\\n\\n<p>How to make new index? (Do I have to drop it first and then make a new one?)</p>\\n-Modifying table in Teradata-<teradata>\",\n",
       " '<p>Please help me through this</p>\\n\\n<pre><code>sel a.col1,a.co2,a.col3,.........b.col1,b.col2..,c.col1,c.col2\\nfrom table1 as a inner join table2 as b on a.col1 =b.col1\\ninner join table3 as c on a.col1 = b.col1\\nwhere col1 = xxxxx;\\n</code></pre>\\n\\n<p>Now i need join one more table table4. As table4 dont have col1 as primary index in it I need to join this to another table which has Primary key.</p>\\n\\n<p>The below is the different query which i need inculde this in to the above sel statement.</p>\\n\\n<pre><code>Sel xx.col1,yy.aaa,yy.bbb,zz.ccc,zz.ddd,zz.eee\\nfrom tablea as xx, tableb as yy, table4 as zz\\nwhere xx.col1 = yy.bbb and yy.aaa = zz.ccc\\n</code></pre>\\n\\n<p>Primary indexs :</p>\\n\\n<ul>\\n<li>col1 for table1,table2,table3,tablexx</li>\\n<li>aaa for tableb</li>\\n<li>ccc for table4</li>\\n</ul>\\n\\n<p>Thanks in advance</p>\\n-Joining on multiple tables in teradata-<sql><database><inner-join><teradata>',\n",
       " '<p>When I run my R script, it gives me an error:</p>\\n\\n<pre><code>Error in list.files(lib, pattern = paste0(\"^\", pkg, \"$\"), full.names = TRUE) : \\n  invalid \\'pattern\\' regular expression\\n</code></pre>\\n\\n<p>What does this error mean?\\nthe link to the script is: <a href=\"http://mzmatch.sourceforge.net/metabolomics/Processing_Code.R\" rel=\"nofollow\">http://mzmatch.sourceforge.net/metabolomics/Processing_Code.R</a></p>\\n\\n<p>But I changed few lines at the beginning:</p>\\n\\n<pre><code>library (\"D:\\\\\\\\java projects\\\\\\\\RScriptRunning\\\\\\\\R\\\\\\\\win-library\\\\\\\\2.15\\\\\\\\mzmatch.R\")\\nmzmatch.init (6000)\\nsetwd (\"D:\\\\\\\\R_Script\\\\\\\\raw\")\\n</code></pre>\\n-Error when running R script-<r>',\n",
       " \"<p>IS it possible to copy status message in teradata?\\n(I don't want to type them everytime, for example when I try to google the error message)</p>\\n-Copy Status Message in Teradata-<teradata>\",\n",
       " '<p>How can I reverse a string using Teradata Stored Procedure Language (SPL) ? The requirement was to replicate the SQL-Server replace() function. This could be achieved by writing a corresponding UDF in Teradata but I want to achieve the same using a Procedure.</p>\\n\\n<p>The procedure could be then used to initialize the variable before its use in the actual statement having reverse().</p>\\n-Reversing a string using Teradata SPL-<sql><string><stored-procedures><reverse><teradata>',\n",
       " \"<p>I´ve created one new column in Teradata, and then I have moved the data from another colum into this new colmn. After that, I've deleted the old column.\\nWhat I would like to do now is to move the new column n the same place in the table where the old column was.\\nI would like to ask you - is that possible or not?</p>\\n-Moving columns in Teradata-<sql><ddl><teradata>\",\n",
       " \"<p>I am using Teradata's SQL Assistant to load data into a table. File has matching columns with same layout. Any ideas of why this would complete the import but not insert any data into the table?</p>\\n\\n<p>Here is the SQL for the table and for the insert...</p>\\n\\n<pre><code>CREATE MULTISET TABLE DLAB_MARKETING.ProfileAttributes,\\nNO FALLBACK,\\nNO BEFORE JOURNAL,\\n     NO AFTER JOURNAL,\\n     CHECKSUM = DEFAULT,\\n     DEFAULT MERGEBLOCKRATIO\\n     (\\n      FullName VARCHAR(50) CHARACTER SET LATIN NOT CASESPECIFIC NOT NULL,\\n      EmailAddress VARCHAR(100) CHARACTER SET LATIN NOT CASESPECIFIC NOT NULL,\\n      FirstName VARCHAR(100) CHARACTER SET LATIN NOT CASESPECIFIC,\\n      LastName VARCHAR(100) CHARACTER SET LATIN NOT CASESPECIFIC,\\n      ZipCode INTEGER,\\n      StoreNumber INTEGER,\\n      StoreAddress VARCHAR(100) CHARACTER SET LATIN NOT CASESPECIFIC,\\n      StoreCity VARCHAR(100) CHARACTER SET LATIN NOT CASESPECIFIC,\\n      StoreState CHAR(2) CHARACTER SET LATIN NOT CASESPECIFIC,\\n      StoreZip INTEGER,\\n      State VARCHAR(25) CHARACTER SET LATIN NOT CASESPECIFIC NOT NULL,\\n      Source_ VARCHAR(75) CHARACTER SET LATIN NOT CASESPECIFIC NOT NULL,\\n      PhoneNumber INTEGER,\\n      Address1 VARCHAR(150) CHARACTER SET LATIN NOT CASESPECIFIC,\\n      Address2 VARCHAR(150) CHARACTER SET LATIN NOT CASESPECIFIC,\\n      City VARCHAR(100) CHARACTER SET LATIN NOT CASESPECIFIC,\\n      Gender CHAR(1) CHARACTER SET LATIN NOT CASESPECIFIC,\\n      MobilePhone INTEGER,\\n      CR2StoreIntersection INTEGER,\\n      CR2StoreCity VARCHAR(100) CHARACTER SET LATIN NOT CASESPECIFIC,\\n      Pet INTEGER,\\n      ShoppingFrequency INTEGER,\\n      AgeToddler INTEGER,\\n      AgeChildren6to17 INTEGER,\\n      Age25To34 INTEGER,\\n      Age18To24 INTEGER,\\n      Age35To44 INTEGER,\\n      Age45To54 INTEGER,\\n      AdultApparel INTEGER,\\n      Age55To64 INTEGER,\\n      Age65orGreater INTEGER,\\n      Dog INTEGER,\\n      Cat INTEGER,\\n      Groceries INTEGER,\\n      Snacks INTEGER,\\n      Cleaning INTEGER,\\n      Q4NoAnswer INTEGER,\\n      AgeInfant INTEGER,\\n      HBA INTEGER,\\n      Baby INTEGER,\\n      ChildrenClothing INTEGER,\\n      Housewares INTEGER,\\n      Party INTEGER,\\n      Seasonal INTEGER,\\n      Toys INTEGER,\\n      SchoolSupplies INTEGER,\\n      Cellular INTEGER,\\n      General INTEGER,\\n      Ad INTEGER,\\n      Survey INTEGER,\\n      Coupon INTEGER,\\n      Contest INTEGER,\\n      White INTEGER,\\n      AA INTEGER,\\n      AmericanIndian INTEGER,\\n      Asian INTEGER,\\n      Hawaiian INTEGER,\\n      Hispanic INTEGER,\\n      Other INTEGER,\\n      Q6NoAnswer INTEGER,\\n      CR2StoreNumber INTEGER,\\n      HTMLEmails VARCHAR(100) CHARACTER SET LATIN NOT CASESPECIFIC,\\n      SubscriberKey VARCHAR(125) CHARACTER SET LATIN NOT CASESPECIFIC,\\n      Status VARCHAR(50) CHARACTER SET LATIN NOT CASESPECIFIC NOT NULL)\\nPRIMARY INDEX ( EmailAddress );\\n\\nINSERT  INTO DLAB_MARKETING.ProfileAttributes\\n(FullName, EmailAddress, FirstName, LastName, ZipCode, StoreNumber,\\n        StoreAddress, StoreCity, StoreState, StoreZip, State, Source_,\\n        PhoneNumber, Address1, Address2, City, Gender, MobilePhone, CR2StoreIntersection,\\n        CR2StoreCity, Pet, ShoppingFrequency, AgeToddler, AgeChildren6to17,\\n        Age25To34, Age18To24, Age35To44, Age45To54, AdultApparel, Age55To64,\\n        Age65orGreater, Dog, Cat, Groceries, Snacks, Cleaning, Q4NoAnswer,\\n        AgeInfant, HBA, Baby, ChildrenClothing, Housewares, Party, Seasonal,\\n        Toys, SchoolSupplies, Cellular, General, Ad, Survey, Coupon,\\n        Contest, White, AA, AmericanIndian, Asian, Hawaiian, Hispanic,\\n        Other, Q6NoAnswer, CR2StoreNumber, HTMLEmails, SubscriberKey,\\n        Status)\\n\\nVALUES  \\n(?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?,\\n        ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?,\\n        ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?,\\n        ?)\\n</code></pre>\\n-Teradata import error - import complete but no data inserted into columns-<sql><teradata>\",\n",
       " '<p>I\\'m getting the error \"Selected non aggregate values must be part of the associated group\"</p>\\n\\n<p>I started getting this error when I added the line \" ,current_date - pmnt_rlse_dt as \"Days Since Paid\"</p>\\n\\n<p>I know it has something to do with the group by but I haven\\'t been able to figure it out yet.</p>\\n\\n<pre><code>    select ee.alias_last_nm || \\', \\' || ee.alias_first_nm as \"Custodian\"\\n   ,clm.i_sys_clm\\n   ,clm.c_clm as \"Claim Number\"\\n   ,clm.i_pol as \"Policy Number\"\\n   ,clm.n_lst as \"Last Name\"\\n   ,clm.n_fst as \"First Name\"\\n   ,clm.c_sta_clm as \"Status\"\\n   ,mx_dt as \"Last Payment Date\"\\n\\nfrom ltc_p.vltc_clm_mo clm\\n\\nleft join (select indiv_sorce_syst_cd\\n            ,sorce_upc_indiv_id\\n            ,alias_last_nm\\n            ,alias_first_nm\\nfrom edw_p.upc_indiv_alias_v2\\nwhere row_end_dt = \\'9999-12-31\\' and\\nindiv_sorce_syst_cd = \\'ORG\\') ee\\non clm.c_ams_clm = ee.sorce_upc_indiv_id\\n\\ninner join(select sorce_claim_id\\n            ,max(pmnt_rlse_dt) as mx_dt\\n            ,current_date - pmnt_rlse_dt as \"Days Since Paid\"\\nfrom ltc_p.claim_pmnt pp\\n\\nhaving  max(pmnt_rlse_dt) is not null) as d\\non clm.i_sys_clm = d.sorce_claim_id\\n\\nwhere clm.c_sta_clm = \\'AC\\'\\norder by 1,2\\n</code></pre>\\n-issue with grouping (Teradata SQL)-<sql><teradata>',\n",
       " \"<p>So I have a query that requires a bunch of CASE statements in the SELECT. This was not the orginal design but part of a compromise.</p>\\n\\n<p>So the query looks something like this:</p>\\n\\n<pre><code>SELECT\\n  CONT.TABLE.FINC_ACCT_NM,\\n  CONT.TABLE.FINC_ACCT_ID,\\n  CONT.TABLE.CURR_END_OF_PERD_ACTL_VAL,\\n  CONT.TABLE.PREV_END_OF_PERD_ACTL_VAL,\\n  CONT.TABLE.VARNC_PLAN_VAL,\\n  CONT.TABLE.OUTLOOK_BDGT_PLAN_VAL,\\n  CONT.TABLE.PERD_END_RPT_DT,\\n  CONT.TABLE.PLAN_VERS_NM,\\n  CONT.TABLE.FRMT_ACTL_CD,\\n  CONT.TABLE.FRMT_PLAN_CD,\\n  CONT.TABLE.RPT_PERD_TYPE_CD,\\n  CASE \\n                WHEN ( CONT.TABLE.FINC_ACCT_ID )=           'XXXX'        and ( CONT.TABLE.BAL_TYPE_CD ) =             'EOP'      then      '  Net Interest Income'  \\n                WHEN ( CONT.TABLE.FINC_ACCT_ID )=           'XXXX'        and ( CONT.TABLE.BAL_TYPE_CD ) =             'EOP'      then      '  Non Interest Income'\\n                WHEN ( CONT.TABLE.FINC_ACCT_ID )=           'XXXX'        and ( CONT.TABLE.BAL_TYPE_CD ) =             'EOP'      then      'Non-Interest Expense'\\n                WHEN ( CONT.TABLE.FINC_ACCT_ID )=           'XXXX'        and ( CONT.TABLE.BAL_TYPE_CD ) =             'EOP'      then      '  Total Marketing Expense'\\n                WHEN ( CONT.TABLE.FINC_ACCT_ID )=           'XXXX'        and ( CONT.TABLE.BAL_TYPE_CD ) =             'EOP'      then      '  Total Operating Expense'\\n                WHEN ( CONT.TABLE.FINC_ACCT_ID )=           'XXXX'        and ( CONT.TABLE.BAL_TYPE_CD ) =             'EOP'      then      'Pre-Provision Earnings (before tax)'\\n                WHEN ( CONT.TABLE.FINC_ACCT_ID )=           'XXXX'        and ( CONT.TABLE.BAL_TYPE_CD ) =             'EOP'      then      '  Net Charge-offs'\\n                WHEN ( CONT.TABLE.FINC_ACCT_ID )=           'XXXX'        and ( CONT.TABLE.BAL_TYPE_CD ) =             'EOP'      then      '  Other'\\n                WHEN ( CONT.TABLE.FINC_ACCT_ID )=           'XXXX'        and ( CONT.TABLE.BAL_TYPE_CD ) =             'EOP'      then      '  Allowance Build (Release)'\\n                WHEN ( CONT.TABLE.FINC_ACCT_ID )=           'XXXX'        and ( CONT.TABLE.BAL_TYPE_CD ) =             'EOP'      then      'Provision Expense'\\n                WHEN ( CONT.TABLE.FINC_ACCT_ID )=           'XXXX'        and ( CONT.TABLE.BAL_TYPE_CD ) =             'EOP'      then      'Pretax Income'\\n                WHEN ( CONT.TABLE.FINC_ACCT_ID )=           'XXXX'        and ( CONT.TABLE.BAL_TYPE_CD ) =             'EOP'      then      'Tax Expense'\\n                WHEN ( CONT.TABLE.FINC_ACCT_ID )=           'XXXX'        and ( CONT.TABLE.BAL_TYPE_CD ) =             'EOP'      then      'NIAT'\\n                WHEN ( CONT.TABLE.FINC_ACCT_ID )=           'XXXX'        and ( CONT.TABLE.BAL_TYPE_CD ) =             'EOP'      then      'EPS'\\n                WHEN ( CONT.TABLE.FINC_ACCT_ID )=           'XXXX'        and ( CONT.TABLE.BAL_TYPE_CD ) =             'EOP'      then      'Ending Loans - HFI'\\n                WHEN ( CONT.TABLE.FINC_ACCT_ID )=           'XXXX'        and ( CONT.TABLE.BAL_TYPE_CD ) =             'avg'       then      'Average Loans - HFI'\\n                WHEN ( CONT.TABLE.FINC_ACCT_ID )=           'XXXX'        and ( CONT.TABLE.BAL_TYPE_CD ) =             'avg'       then      'Average Earning Assets'\\n                WHEN ( CONT.TABLE.FINC_ACCT_ID )=           'XXXX'        and ( CONT.TABLE.BAL_TYPE_CD ) =             'EOP'      then      'Ending Deposits'\\n                WHEN ( CONT.TABLE.FINC_ACCT_ID )=           'XXXX'        and ( CONT.TABLE.BAL_TYPE_CD ) =             'avg'       then      'Average Deposits'\\n                WHEN ( CONT.TABLE.FINC_ACCT_ID )=           'XXXX'        and ( CONT.TABLE.BAL_TYPE_CD ) =             'EOP'      then      'NIM on Loans'\\n                WHEN ( CONT.TABLE.FINC_ACCT_ID )=           'XXXX'        and ( CONT.TABLE.BAL_TYPE_CD ) =             'EOP'      then      'Revenue Margin'\\n                WHEN ( CONT.TABLE.FINC_ACCT_ID )=           'AC579'        and ( CONT.TABLE.BAL_TYPE_CD ) =             'EOP'      then      'Charge off rate'\\n                WHEN ( CONT.TABLE.FINC_ACCT_ID )=           'XXXX'        and ( CONT.TABLE.BAL_TYPE_CD ) =             'EOP'      then      'Efficiency ratio'\\n                WHEN ( CONT.TABLE.FINC_ACCT_ID )=           'XXXX'        and ( CONT.TABLE.BAL_TYPE_CD ) =             'EOP'      then      'ROA'\\n                WHEN ( CONT.TABLE.FINC_ACCT_ID )=           'XXXX'        and ( CONT.TABLE.BAL_TYPE_CD ) =             'EOP'      then      'ROE'\\n                WHEN ( CONT.TABLE.FINC_ACCT_ID )=           'XXXX'        and ( CONT.TABLE.BAL_TYPE_CD ) =             'EOP'      then      'Return on Allocated Capital (ROAC)'\\n\\n\\n\\n  ELSE ( CONT.TABLE.FINC_ACCT_NM ) end\\nFROM\\n  CONT.TABLE\\nWHERE\\n  (\\n   (\\n    ( ( CONT.TABLE.PERD_END_RPT_DT ) = (\\n\\nSELECT Max(Perd_END_RPT_DT) \\nFROM CONT.TABLE\\nWhere VERS_NM='Actual'\\n   AND RPT_PERD_TYPE_CD = 'Q'\\n   AND DATA_VLDTN_IND='Y'\\n)\\n   AND RPT_PERD_TYPE_CD = 'Q'\\n  AND DATA_VLDTN_IND='Y'  )\\n    OR\\n    ( ( CONT.TABLE.PERD_END_RPT_DT ) = (\\n\\nSELECT Max(Perd_END_RPT_DT) \\nFROM CONT.TABLE\\nWhere VERS_NM='Actual'\\n   AND RPT_PERD_TYPE_CD = 'M'\\n   AND DATA_VLDTN_IND='Y'\\n) \\n\\n  AND RPT_PERD_TYPE_CD = 'M'\\n  AND DATA_VLDTN_IND='Y'  )\\n   )\\n   AND\\n   ( ( CONT.TABLE.DATA_VLDTN_IND )='Y'  )\\n   AND\\n   ( ( CONT.TABLE.FINC_ACCT_ID )IN ('AC0006470','AC8000199','AC8002145','AC0006586','AC8000094')  AND ( CONT.TABLE.DEPT_ID )='OR80637'  )\\n  )\\n</code></pre>\\n\\n<p>My question is what affect would changing all those CASE statements to direct column references have on performance.</p>\\n\\n<p>In other words: If I changed every CASE statement to just a column name and removed all CASE statements from the query would there be a large impact on performance and why?</p>\\n\\n<p>I am testing this out so I can figure out if performance is affected but I am just as interested in the details of WHY? (Technical details of why)</p>\\n\\n<p>Thanks for your help!</p>\\n-SQL - What is the performance impact of having multiple CASE statements in SELECT - Teradata-<sql><performance><teradata>\",\n",
       " \"<p><strong>Situation:</strong></p>\\n\\n<p><code>varchar(20)</code> seems to <strong>truncate silently</strong> in Teradata and <strong>not</strong> to expand or complain when encountering strings larger than 20 characters long... This is a bit of a surprise as I expected either automatic expansion of the column to fit larger strings, say 30 characters, OR for an error to be thrown if a larger string were encountered.  Silent truncation seems to get me the worst of all worlds...</p>\\n\\n<p><strong>Complication:</strong></p>\\n\\n<p>For my application (prototype analytics design) I don't know in advance how large will be the data I will be ingesting over the course of a few weeks.  That seems to rule out using varchar(N), except for max</p>\\n\\n<p><strong>Questions:</strong></p>\\n\\n<p>So now I have a few choices, and am looking for some guidance:</p>\\n\\n<p>Q1. User error?  Am I misunderstanding a key concept about <code>varchar(N)</code>?</p>\\n\\n<p>If this is in fact how Teradata handles <code>varchar</code> fields, then </p>\\n\\n<p>Q2. why would anyone specify anything less than <code>varchar(max)</code> especially when it is not clear in advance how many characters might need to be stored in the field.</p>\\n\\n<p>Q3. Is there a different data type that permits flexible sizing of the string -- i.e. a true <em>variable</em> length character string?</p>\\n\\n<p>If I recall, other SQL dialects implement <code>varchar(n)</code> as a recommended initial size for the string but allow it to expand as needed to fit the maximum length of the data strings thrown in.  Is there a similar data type in Teradata?</p>\\n\\n<p>(Note: since I'm prototyping the tables, I am less concerned about performance efficiency at this point; more concerned about quick but safe designs that allow the prototype to progress.)</p>\\n-SQL prototype design:  facing silent truncation of data using varchar(N) -- any better alternatives?  (Teradata)-<sql><database-design><varchar><teradata>\",\n",
       " \"<p>I'm writing some code that wraps various content into columns of text (and images, videos, etc).  The code works fine, but due to the algorithm I'm using it's rather slow, specifically this general logic:</p>\\n\\n<ul>\\n<li>add something (text for this example) to a column</li>\\n<li>check to see if column.scrollHeight > column.offsetHeight (this requires a DOM reflow)\\n<ul>\\n<li>if yes, start to binary split the text until it's shorter</li>\\n</ul></li>\\n</ul>\\n\\n<p>Basically my issue is that I'm adding an unknown amount of text to a column, so after each chunk of text I check the column's scroll height which requires the browser to actively reflow the DOM in order to give me the correct scrollHeight.  So I have 50-100 or more reflows in order to properly lay everything out.</p>\\n\\n<p>Any general ideas on how to avoid most of these?</p>\\n-Looking for ideas to quickly flow content-<javascript>\",\n",
       " '<p>we are trying to install Kepler 2.3 workflow on a Red Hat machine (its a virtual machine) having Java 1.6 (64 bit). But we are getting following error</p>\\n\\n<p>*[null] Error occurred during initialization of VM\\n*[null] Too small initial heap for new size specified</p>\\n\\n<p>Any idea what is wrong? If it helps, I have installed it on my local machine and it is working fine.\\nThanks  </p>\\n-Kepler 2.3 installation giving error-<java><linux>',\n",
       " '<p>I am working in Teradata with some descriptive data that needs to be transformed from a gerneric varchar(60) into the different field lengths based on the type of data element and the attribute value. So I need to take whatever is in the Varchar(60) and based on field \\'ABCD\\' act on field \\'XYZ\\'. In this case XYZ is a varchar(3). To do this I am using CASE logic within my select. What I want to do is </p>\\n\\n<p>eliminate all occurances of non alphabet/numeric data. All I want left are upper case Alpha chars and numbers.\\nIn this case \"Where abcd = \\'GROUP\\' then xyz should come out as a \\'000\\', \\'002\\', \\'A\\', \\'C\\'\\neliminate extra padding\\nShift everything Right</p>\\n\\n<pre><code>        abcd    xyz\\n1   GROUP   NULL\\n2   GROUP   $\\n3   GROUP   000000000000000000000000000000000000000000000000000000000000\\n4   GROUP   000000000000000000000000000000000000000000000000000000000002\\n5   GROUP   A\\n6   GROUP   C\\n7   GROUP   r\\n</code></pre>\\n\\n<p>To do this I have tried TRIM and SUBSTR amongst several other things that did not work. I have pasted what I have working now, but I am not reliably working through the data within the select. I am really looking for some options on how to better work with strings in Teradata. I have been working out of the \"SQL Functions, Operators, Expressions and Predicates\" online PDF. Is there a better reference. We are on TD 13</p>\\n\\n<pre><code>SELECT abcd\\n        , CASE \\n            -- xxxxxxxxxxxxxxxxxxxxxxxxxxxxx\\n            WHEN abcd= \\'GROUP\\'\\n                THEN(\\n                    CASE\\n                        WHEN SUBSTR(tx.abcd,60, 4) = 0\\n                            THEN (\\n                                    SUBSTR(tx.abcd,60, 3)\\n                                        )\\n                            ELSE\\n                                TRIM (TRAILING FROM tx.abcd)\\n                        END\\n                        )\\n            END AS abcd\\n\\nFROM db.descr tx\\nWHERE tx.abcd IS IN ( \\'GROUP\\')\\n</code></pre>\\n\\n<p>The end result should look like this </p>\\n\\n<pre><code>    abcd    xyz\\n1   GROUP   000\\n2   GROUP   002\\n3   GROUP     A\\n4   GROUP     C\\n</code></pre>\\n\\n<p>I will have to deal with approx 60 different \"abcd\" types, but they should all conform to the type of data I am currently seeing.. ie.. mixed case, non numeric, non alphabet, padded, etc..</p>\\n\\n<p>I know there is a better way, but I have come in several circles trying to figure this out over the weekend and need a little push in the right direction.</p>\\n\\n<p>Thanks in advance,\\nPat</p>\\n-substring and trim in Teradata-<sql><string><trim><teradata>',\n",
       " '<p>I am searching a large database for a list of words of varying length between 5-7 characters. \\nSo far, I have:</p>\\n\\n<pre><code>Select *\\n  from sometable\\n Where upper(\"Description\") like any (\"%ABC_123%\", \"%ABC_124%\", \"%DE_25%\")\\n</code></pre>\\n\\n<p>I would like to also return the word that was found in the query but am stuck on how to do this without duplicating the list of words in a subtr function. </p>\\n\\n<p>There is probably a much better way of doing this and I\\'d appreciate some direction.</p>\\n-Need to Search a Text Field in Teradata SQL against a list of words and return that word-<sql><search><text><full-text-search><teradata>',\n",
       " '<p>I want to calculate the time difference between 2 time intervals like as shown in the code</p>\\n\\n<pre><code>replace procedure database.table()               \\nbegin  \\ndeclare v_StartTime Timestamp;  \\ndecalre v_EndTime Timestamp;        \\ndecalre v_TimeTaken Varchar(20);         \\n\\nset v_StartTime = current_timestamp;       \\n/* set of queries...........          \\n...................                \\n.....................             \\n\\n*/       \\nset v_EndTime =current_timestamp;\\n\\nset v_timeTaken = v_EndTime - v_StartTime ;\\n\\n/* I want the time difference in secs or mins*/\\n\\nEnd;\\n</code></pre>\\n\\n<p>Is there any other better way to get the time intervals in mins or secs</p>\\n-teradata : calculating diffreence between 2 timeintervals-<sql><stored-procedures><teradata>',\n",
       " '<p>Rapidminer has a SVM module based on libsvm. How can I know which version of libsvm it uses?</p>\\n\\n<p>I tested the SVM classifier against the same data set using both libsvm module in Rapidminer and Libsvm itself, and the resulting prediction score are different even they use the same parameter setting.</p>\\n-Libsvm module in Rapidminer-<svm><libsvm><rapidminer>',\n",
       " '<p>Is there a way to do this without having to fire up an instance? </p>\\n-Import CSV using monetdb from daemon-<csv><monetdb>',\n",
       " '<p>So I\\'m working on getting MonetDB hooked up with JDBC but am having issues even with the basic tutorial. I\\'m probably doing something wrong here and maybe someone could point me in the right direction (that would be greatly appreciated!). </p>\\n\\n<p>So the commands I\\'m running are:</p>\\n\\n<pre><code>monetdbd start /dbfarm\\nmonetdb create test1\\nmonetdb release test1\\njava -jar /jars/jdbcclient.jar -u monetdb -d test1\\n\\npassword:\\nDatabase connect failed: no such database \\'test1\\', please create first\\n</code></pre>\\n\\n<p>--</p>\\n\\n<p>This seems odd to me because I can access test1 via mclient -u monetdb -d test1.</p>\\n\\n<p>TIA</p>\\n-MonetDB + JDBC \"no such database\"-<jdbc><monetdb>',\n",
       " '<p>I tried to drop a table in Teradata database with C# if the table exist. </p>\\n\\n<pre><code>cmd.CommandText = string.Format(\"IF EXISTS\\n(SELECT * FROM sysobjects WHERE type = \\'U\\' AND name = \\'{0}\\')  \\n     BEGIN DROP TABLE \\'{0}\\' END\", Customer.TableName);  \\ncmd.ExecuteNonQuery();\\n</code></pre>\\n\\n<p>But the above always failed with :   </p>\\n\\n<blockquote>\\n  <p>{\"[Teradata Database] [3706] Syntax error: expected something between the beginning of the request and the \\'IF\\' keyword.\"}</p>\\n</blockquote>\\n\\n<p>Second code i tried, the code below works !!! </p>\\n\\n<pre><code>cmd.CommandText = \"select count (*) from Customer.TableName\";\\n                    reader = cmd.ExecuteReader();\\n\\n                    if (reader.FieldCount &gt; 0)\\n                    {\\n                        reader.Close();\\n                        cmd.CommandText = \"Drop table Customer.TableName\";\\n                        reader = cmd.ExecuteReader();\\n                    }\\n</code></pre>\\n\\n<p>However, it works only when got table exist. If the table Customer.TableName does not exist, then it will failed when undergo this </p>\\n\\n<pre><code>\"select count (*) from Customer.TableName\";\\n                        reader = cmd.ExecuteReader();\\n</code></pre>\\n-Drop table in teradata-<c#><teradata>',\n",
       " \"<p>Does anyone know how to read a view from a database into rapidminer?  I have a connection set up to a Microsoft SQL server, and I'm using the Read Database operator, but it only shows a list of tables, not views.  The has been set up, and I can see it using Access.  The view is being worked on while I'm working in rapidminer so I need a live link to it, rather than re-creating the query in rapidminer.  Is there something I'm missing, or does rapidminer just not support views?</p>\\n-How to read a database view in rapidminer-<database><view><rapidminer>\",\n",
       " \"<p>I haven't had much experience with machine learning or clustering, so I'm at a bit of a loss as to how to approach this problem.  My data of interest consists of 4 columns, one of which is just an id.  The other 3 contain numerical data, values >= 0.  The clustering I need is actually quite straightforward, and I could do it by hand, but it will get less clear later on so I want to start out with the right sort of process.  I need 6 clusters, which depend on the 3 columns (call them A, B and C) as follows:</p>\\n\\n<pre><code>A    B    C        Cluster\\n---- ---- -------- -------\\n0    0    0        0\\n0    0    &gt;0       1\\n0    &gt;0   &lt;=B      2\\n0    &gt;0   &gt;B       3\\n&gt;0   any  &lt;=(A+B)  4\\n&gt;0   any  &gt;(A+B)   5\\n</code></pre>\\n\\n<p>At this stage, these clusters will give an insight to the data to inform further analysis.</p>\\n\\n<p>Since I'm quite new to this, I haven't yet learned enough about the various algorithms which do clustering, so I don't really know where to start.  Could anyone suggest an appropriate model to use, or a few that I can research.</p>\\n-Designing a clustering process using RapidMiner-<classification><cluster-analysis><data-mining><decision-tree><rapidminer>\",\n",
       " '<p>I encountered a strange problem when I execute <code>select * from dbc.columnsX</code> on teradata 13.10. It is very slow; but on teradata 12.0 it is very fast. For teradata 13.10, does the dbc user lack some rights or there are some other reasons lead to this phenomena.</p>\\n\\n<p>I know columnsX is X view.</p>\\n-Why is executing \"select * from dbc.columnsX\" very slow for teradata 13.10-<teradata>',\n",
       " '<p>I\\'m working in an HPC environment which uses the <a href=\"http://modules.sourceforge.net/\" rel=\"nofollow\">module system</a> for software-version management. I\\'m using python for the automation of the installation of a certain software which requires some dependencies.</p>\\n\\n<p>Do you know if there exists any python package/module to manage the module system? (list available modules, load/unload modules, read modulefiles, etc). I could\\'n find anything similar. </p>\\n-Python package to manage module system in HPCs-<python><module><environment><hpc>',\n",
       " '<p>Trying to work with a JDBC connection to Teradata.  I\\'ve loaded the tdgssconfig.jar and terajdbc4.jar file and adding them to the classpath with javac when I compile in Linux.  But I still get a ClassnotFoundException when trying to compile. </p>\\n\\n<p>I\\'ve not worked with java in a while, but I\\'ve scoured the net and it looks like it should work.</p>\\n\\n<p>Simple Code:</p>\\n\\n<pre><code>import java.sql.*;\\nclass TDtest {\\n    public static void main(String[] args) {    \\n        System.out.println(classpath);\\n\\n        Class.forName(\"com.teradata.jdbc.TeraDriver\");\\n    }\\n}\\n</code></pre>\\n\\n<p>*.jars are definitely there:</p>\\n\\n<pre><code>[user1@box java]# ls -l /home/user1/test/java/libs/*\\n-rwxrwxrwx 1 user1 user1 2405 Oct 26 12:00 /home/user1/test/java/libs/tdgssconfig.jar\\n-rwxrwxrwx 1 user1 user1 873860 Oct 26 12:00 /home/user1/test/java/libs/terajdbc4.jar\\n</code></pre>\\n\\n<p>verbose error log - it looks like the classpath is right to me:</p>\\n\\n<pre><code>javac -verbose -cp \".:/home/user1/test/java/libs/tdgssconfig.jar:/home/user1/test/java/libs/terajdbc4.jar\" TDtest.java\\n[parsing started TDtest.java]\\n[parsing completed 21ms]\\n[search path for source files: .,/home/user1/test/java/libs/tdgssconfig.jar,/home/user1/test/java/libs/terajdbc4.jar]\\n[search path for class files: /usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/resources.jar,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/rt.jar,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/sunrsasign.jar,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/jsse.jar,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/jce.jar,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/charsets.jar,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/classes,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/ext/dnsns.jar,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/ext/sunpkcs11.jar,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/ext/localedata.jar,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/ext/gnome-java-bridge.jar,/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64/jre/lib/ext/sunjce_provider.jar,.,/home/user1/test/java/libs/tdgssconfig.jar,/home/user1/test/java/libs/terajdbc4.jar]\\n[loading java/lang/Object.class(java/lang:Object.class)]\\n[loading java/lang/String.class(java/lang:String.class)]\\n[checking TDtest]\\n[loading java/lang/Class.class(java/lang:Class.class)]\\n[loading java/lang/Error.class(java/lang:Error.class)]\\n[loading java/lang/ClassNotFoundException.class(java/lang:ClassNotFoundException.class)]\\n[loading java/lang/Exception.class(java/lang:Exception.class)]\\n[loading java/lang/Throwable.class(java/lang:Throwable.class)]\\n[loading java/lang/RuntimeException.class(java/lang:RuntimeException.class)]\\nTDtest.java:4: unreported exception java.lang.ClassNotFoundException; must be caught or declared to be thrown\\n        Class.forName(\"com.teradata.jdbc.TeraDriver\");\\n</code></pre>\\n\\n<p>I\\'ve tried un-jar\\'ing the jdbc jar\\'s and they definitely have com/teradata/jdbc/TeraDriver.class in them.</p>\\n\\n<p>I\\'m at a loss.  Any idea what I\\'m doing wrong?</p>\\n-Teradata and JDBC driver - classnotfoundexception ...but its there?-<java><linux><classpath><teradata>',\n",
       " '<p>I have a set of data with 14 regular attributes. I am trying to create the best decison tree in <strong>rapidminer</strong> from this training data so that I can use this tree on scoring data. </p>\\n\\n<p>However I am not sure what paramaters to use for the decision tree (eg: criterion, minimal gain, confidence, etc)? \\nI am also unsure of which (if at all) other operators I could/should apply to my model?</p>\\n\\n<p>Could anyone provide me with some general tips about what would work best?</p>\\n\\n<p>The data I have is to try and determine whether someone opening a new bank account, will they have a good credit standing. I have information such as Credit standing, account type, history, employment, gender, job, etc.</p>\\n\\n<p>Thank you.</p>\\n-What parameters are best for a Decision Tree in Rapidminer-<statistics><decision-tree><data-analysis><rapidminer>',\n",
       " '<p>I am building out a Staging layer for Master Data and have the following situation and current solution. My Transaction data has nothing but Master Data id\\'s or \"codes\" and no text descriptions. Most all my text descriptions are delivered in a single generic table which has all the possible \"codes\" and their subsequent text values. In moving this data into the Staging layer I need to trim the codes from the VARCHAR(60) into their actual field length based on their original field length.</p>\\n\\n<p>in short, Master Data codes are moved from the source environment where they are correctly cast into this Landing environmental and a VARCHAR(60) where they get padded. I need to trim according to field length as I move into Staging.</p>\\n\\n<p>I am using the following SQL, but have wrapped it in CASE logic to manage the different possible field lengths I will see. I have to therefore reproduce this bit of logic for each different MD code I deal with. I am looking for a way to tweak this so that it handles all field lengths in one bit of logic. </p>\\n\\n<pre><code>SELECT ATTR_NM\\n    , CASE \\n        WHEN ATTR_NM = \\'DOC_TYP\\'\\n            THEN(\\n                CASE WHEN CHARACTER_LENGTH (TRIM(BOTH FROM tx.MSTR_DATA_ATTR_CD)) &gt; 1\\n                    THEN SUBSTRING( tx.MSTR_DATA_ATTR_CD FROM CHARACTER_LENGTH(TRIM(BOTH FROM tx.MSTR_DATA_ATTR_CD))-1)\\n                    ELSE tx.MSTR_DATA_ATTR_CD\\n                END\\n                )\\n    WHEN SAP_ATTR_NM = \\'ACCT_GRP\\'\\n        THEN(\\n            CASE WHEN CHARACTER_LENGTH (TRIM(BOTH FROM tx.MSTR_DATA_ATTR_CD)) &gt; 4\\n                THEN SUBSTRING( tx.MSTR_DATA_ATTR_CD FROM CHARACTER_LENGTH(TRIM(BOTH FROM tx.MSTR_DATA_ATTR_CD))-2)\\n                ELSE tx.MSTR_DATA_ATTR_CD\\n            END\\n            )\\n        END AS MSTR_DATA_ATTR_CD\\n    , LANG_ISO_CD AS LANG_ISO_CODE\\n    , tx.LANG AS LANG\\n    , tx.END_DT AS END_DT\\n    , tx.FRM_DT AS FRM_DT\\nFROM MASTER_DATA_DESC_BASE_VOL tx;\\n</code></pre>\\n\\n<p>I can\\'t just Trim leading 0\\'s since I can have a code with a leading 0. I need to know the field length and then apply that numeric value with the Trim function call. Or someone has a better way altogether. I am a longtime programmer consultant, but somewhat new to Teradata.</p>\\n\\n<p>Also, this is working and currently handles more than 20 different cases and runs rather efficiently. I just know there is a better way to do what I need and I will also have to continue to add more and more Cases.</p>\\n-How to re-design my current SQL Case Logic implementation in Teradata 13.0-<sql><database-design><case><teradata>',\n",
       " '<p>I have a table abc which have many records with columns col1,col2,col3,</p>\\n\\n<pre><code>dept         | name |  marks |\\n\\nscience         abc      50\\nscience         cvv      21\\nscience         cvv      22  \\nmaths           def      60\\nmaths           abc      21\\nmaths           def      62\\nmaths           ddd      90\\n</code></pre>\\n\\n<p>I need to order by dept and name with ranking as ddd- 1, cvv - 2, abc -3, else 4 then need to find out maximum mark of an individual. Expected result is   </p>\\n\\n<pre><code>dept         | name |  marks |\\n\\nscience         cvv      22\\nscience         abc      50\\nmaths           ddd      90\\nmaths           abc      21\\nmaths           def      62\\n</code></pre>\\n\\n<p>. How may I do it.?            </p>\\n-SQL Teradata query-<sql><sql-order-by><teradata>',\n",
       " '<p>I have a list of text data from which I want to extract certain portions.  I am currently using a regular expression to extract the data I want, but it\\'s starting to get very complicated because each record is slightly different.  Is there a way to use Rapidminer to \"learn\" a regular expression based on some typical examples?</p>\\n\\n<p>For example, for each of the following records I want to extract the text <code>24</code> and <code>18</code> into two new attributes:</p>\\n\\n<pre><code>word 24 on line 18\\nWrd 24 of Ln 18\\nLine 18, Word 24\\nWord 24 comes after word 22 on line 18 (not line 19)\\n</code></pre>\\n\\n<p>I have watched all the text processing videos, but none of them show how to do this sort of thing, and I don\\'t really know where to start.  Can anyone suggest a way of doing this other than manually creating regular expressions?</p>\\n-Extracting text information using rapidminer-<text><full-text-search><text-processing><rapidminer>',\n",
       " '<p>What is the TDPID when i connect SAS to teradata installed on my machine\\ni am using the following code to connect SAS to teradata:\\nenter code here</p>\\n\\n<pre><code>proc sql; \\noptions symbolgen;\\nCONNECT TO Teradata as tdm \\n( \\nschema=127.0.0.1 \\nuser=&amp;tdusr. \\npassword=&amp;tdpwd. \\ndefer=yes \\ntdpid=127.0.0.1 \\nmode = teradata \\n); \\n</code></pre>\\n\\n<p>and i m getting this error:\\nERROR: The TERADATA engine cannot be found.\\nERROR: A Connection to the Teradata DBMS is not currently supported, or is not installed at your site.</p>\\n\\n<p>and the teradata is installed :-)</p>\\n-What is the TDPID when i connect SAS to teradata installed on my machine-<sas><teradata>',\n",
       " \"<p>I have  a table with transaction, date and their respective sales value. I need to calculate  sum of Sales of all the distinct transactions on all Saturdays between date x and y. Teradata doesn't have a datename, datepart function. How can I do this?</p>\\n-Calculating sales for a particular weekday in a given period in teradata-<sql><teradata>\",\n",
       " '<p>Im learning how to use Rapidminer for a project. Im stuck at some point. I have a dataset as follows: There are countries. For each country Im keeping track of some values (medals lets say) for years 1990-2012. As an example: </p>\\n\\n<ul>\\n<li><p>Country       Year   Gold   Silver    Bronze</p>\\n\\n<hr>\\n\\n<pre><code>USA      1990    10      5         7\\n.....\\nUSA      2012    12      3         8\\nSpain    1990    8       12        9\\n...\\nSpain    1992    7       ?         8\\n....\\nSpain    2012    4       11        12\\n...GOES ON...\\n</code></pre></li>\\n</ul>\\n\\n<p>What I want to do is to replace the missing values. For example Spain has a missing value in 1992 for Silver Medals. I want to find the average for Silver data available for Spain and replace the missing value with that. How can I do this? If the present modules in Rapidminer not able to do this, is there some kind of macro etc? I can also use Excel to preprocess the data (but how)???.</p>\\n-Rapidminer / Excel Missing Value Replacement-<excel><missing-data><rapidminer>',\n",
       " '<p>I want to filter my results (document occurrence) by the most frequent (the 10 most popular them). How do I do that?</p>\\n\\n<p><img src=\"https://i.stack.imgur.com/XxOME.png\" alt=\"enter image description here\"></p>\\n-RapidMiner - range by occurence-<frequency><rapidminer>',\n",
       " '<p>I have to run column checks for data consistency and the only thing that is throwing off my code is checking for character lengths for dates between certain parameters.</p>\\n\\n<pre><code>SEL\\nsum(case when ( A.date is  null or (character_length(A.date) &gt;8)) then 1 else 0 end ) as Date\\nfrom\\ntable A\\n;\\n</code></pre>\\n\\n<p>The date format of the column is YYYY-MM-DD, and the type is DA.  When I run the script in SQL Assistant, I get an error 3580 \"Illegal use of CHARACTERS, MCHARACTERS, or OCTET_LENGTH functions.\"</p>\\n\\n<p>Preliminary research suggests that SQL Assistant has issues with the character_length function, but I don\\'t know how to adjust the code to make it run.</p>\\n-character_length Teradata SQL Assistant-<sql><teradata>',\n",
       " \"<p>I've read the entire ATLAS installation guide, and it says all you need to build shared (.so) libraries is to pass the <code>--shared</code> flag to the configure script. However, when I build, the only <code>.so</code> files that appear in my <code>lib</code> folder are <code>libsatlas.so</code> and <code>libtatlas.so</code>, though the guide says that there should be six others:</p>\\n\\n<pre><code>libatlas.so, libcblas.so, libf77blas.so, liblapack.so, libptcblas.so, libptf77blas.so\\n</code></pre>\\n\\n<p>After installation some of the tests fail because these libraries are missing. Furthermore, FFPACK wants these libraries during installation.</p>\\n\\n<p>Has anyone encountered this? What am I doing incorrectly?</p>\\n-Build shared libraries in ATLAS-<installation><shared-libraries><lapack><atlas>\",\n",
       " \"<p>I'm considering using RapidMiner to store and analyse a collection of data gathered by a scripted process. Is there a way to import a CSV file into a RapidMiner repository from a command-line script?</p>\\n-Is it possible to import CSVs into a RapidMiner repository from the command-line?-<rapidminer>\",\n",
       " \"<p>There is another gotcha with Teradata 14 (courtesy of TC) that may catch people out.</p>\\n\\n<p>Using a table defined like this:</p>\\n\\n<pre><code>CREATE TABLE test( \\nCALENDAR_DATE DATE FORMAT 'YY/MM/DD' NOT NULL           \\n,RETAIL_OUTLET_NUMBER INTEGER NOT NULL   \\n,BASE_PRODUCT_NUMBER INTEGER NOT NULL      \\n) \\nPRIMARY INDEX (calendar_date, retail_outlet_number, base_product_number)\\n</code></pre>\\n\\n<p>This SQL will fail:</p>\\n\\n<pre><code>COLLECT STATS test COLUMN(\\ncalendar_date, base_product_number, retail_outlet_number); \\n</code></pre>\\n\\n<p>with the error message: </p>\\n\\n<blockquote>\\n  <p>3706: Syntax error: Multiple statistics with different column ordering on the same set of columns are not allowed</p>\\n</blockquote>\\n\\n<p>But:</p>\\n\\n<pre><code>COLLECT STATS test COLUMN(\\ncalendar_date, retail_outlet_number, base_product_number);\\n</code></pre>\\n\\n<p>will be OK. </p>\\n\\n<p>The order of the columns must now be the same in the collect stats statement and the primary index.</p>\\n\\n<p>...</p>\\n\\n<p>Is there any other way to make them work with out changing the column order???</p>\\n\\n<p>Thanks...</p>\\n-collect stats in teradata 14 - Multiple statistics with different column ordering on the same set of columns are not allowed-<teradata>\",\n",
       " '<p>I asked this question in RM forum but no one answered. I Created SVM prediction model and it used to work until one day it suddenly give error \"Cannot deliver AttributeWeights with parameter \"kernel_type\" set to \"polynomial\".  \" Anybody knows what did I clicked wrong because I could not use RM to build any models anymore. </p>\\n-RapidMiner not working-<rapidminer>',\n",
       " '<p>I have a table with a column defined as <code>TIMESTAMP(6)</code>.  I need to do a mass conversion of all time values in this column from EST to GMT.  How do I go about this?</p>\\n-Teradata: How can I convert a TIMESTAMP(6) field value to GMT?-<timezone><teradata><gmt>',\n",
       " '<p>I have a large data set with start dates and stop dates in each record. </p>\\n\\n<p>There is one unique field value we can call Uniq1. </p>\\n\\n<p>Each record has a Uniq1.</p>\\n\\n<p>Other fields can be called MID, PrDl, PsDl. </p>\\n\\n<p>Uniq1, MID are Integer. </p>\\n\\n<p>What I need to do is output a dataset (thinking SelfJoin) from this dataset that has flagged each record with a yes or no, (1 or 0), where a 1 = a record whose stop date is followed by another record’s start date within 30 days of the stop date.  </p>\\n\\n<p>This would be specific to the MId, so that if MId 1 had 6 records chronologically and the second record was an admission within 30 days of the previous records stop date the second record would be flagged = 1 while the initial record would be flagged 0 and among the 4 subsequent records for MId 1 there are no other similar 30 day occurrences, so they are flagged = 0 as well.</p>\\n\\n<p>Ideally, I would like the following output, for example:</p>\\n\\n<p>Uniq1  MId     Start      Stop       #Days          Flag           PrDl           PsDl</p>\\n\\n<p>123    1    1/24/2010   2/4/2010                    0           CharValue        CharValue</p>\\n\\n<p>321    1    2/25/2010   3/5/2010      21            1           CharValue        CharValue</p>\\n\\n<p>789    1    6/21/2010   6/28/2010    116            0           CharValue        CharValue</p>\\n\\n<p>Notice that the # days between Uniq1-123 and Uniq1-321 are 21 and therefore flag =1, however, between Uniq1-321 and Uniq1-789 the # of days are 116 and the flag = 0.</p>\\n\\n<p>Any advice would be most appreciated, thank you</p>\\n-TOAD, date values, flag, teradata, prior record-<date><toad><teradata>',\n",
       " \"<p>I'd like to be able to do something like this:</p>\\n\\n<p><code>insert into mydb.mytable (updatetimestamp) values (#1/15/2012 01:03:00#)</code></p>\\n\\n<p>...or...</p>\\n\\n<p><code>select * from mydb.mytable where updatetimestamp = #1/15/2012 01:03:00#</code></p>\\n\\n<p>Using literals wouldn't required the longwindedness of casting and whatnot since it would immediately interpret the expression as a DATE or TIMESTAMP.</p>\\n\\n<p>Does Teradata support this type of syntax?</p>\\n-Does Teradata support literals for DATE and TIMESTAMP?-<teradata>\",\n",
       " \"<p>I am trying to compare two addresses from the same ID to see whether they match. For example:</p>\\n\\n<pre><code>Id  Adress Code     Address\\n1   1               123 Main\\n1   2               123 Main\\n2   1               456 Wall\\n2   2               456 Wall\\n3   1               789 Right\\n3   2               100 Left\\n</code></pre>\\n\\n<p>I'm just trying to figure out whether the address for each ID matches. So in this case I want to return just ID 3 as having a different address for Address Code 1 and 2.</p>\\n-Find which rows have different values for a given column in Teradata SQL-<sql><join><teradata><rows><matching>\",\n",
       " '<p>I\\'m working in <strong>R</strong> with the following dataset for a metabolomics study.</p>\\n\\n<pre><code>first Name      Area    Sample Similarity\\n\\n120   Pentanone 699468  PO4:1   954\\n\\n120   Pentanone 153744  PO2:1   981\\n\\n126   Methylamine 83528 PO4:1   887\\n\\n126   Unknown     32741 PO2:1   645\\n\\n126   Sulfurous 43634   PO1:1   800\\n</code></pre>\\n\\n<p>I want to be able to selected in the first column, within the rowns with same value (for example 120), the compounds with same name (for example pentanone). From this selection I want to copy the row information that corresponds to the highest similarity and created new columns within the table. In this case the following information:</p>\\n\\n<pre><code>120 Pentanone   153744  PO2:1   981\\n</code></pre>\\n\\n<p>I know that \"send me the code posts\" are not very appreciated by I would greatly appreciated some clues on how to start.</p>\\n-Selecting/sorting values withing table R-<r>',\n",
       " \"<p>I have this data and I want to sum the field <code>USAGE_FLAG</code> but reset when it drops to 0 or moves to a new ID keeping the dataset ordered by <code>SU_ID</code> and <code>WEEK</code>:</p>\\n\\n<pre><code>SU_ID   WEEK    USAGE_FLAG\\n100        1    0\\n100        2    7\\n100        3    7\\n100        4    0\\n101        1    0\\n101        2    7\\n101        3    0\\n101        4    7\\n102        1    7\\n102        2    7\\n102        3    7\\n102        4    0\\n</code></pre>\\n\\n<p>So I want to create this table:</p>\\n\\n<pre><code>SU_ID   WEEK    USAGE_FLAG    SUM\\n100        1    0             0\\n100        2    7             7\\n100        3    7             14\\n100        4    0             0\\n101        1    0             0\\n101        2    7             7\\n101        3    0             0\\n101        4    7             7\\n102        1    7             7\\n102        2    7             14\\n102        3    7             21\\n102        4    0             0\\n</code></pre>\\n\\n<p>I have tried the <code>MSUM()</code> function using <code>GROUP BY</code> but it won't keep the order I want above. It groups the 7's and the week numbers together which I don't want.</p>\\n\\n<p>Anyone know if this is possible to do? I'm using teradata</p>\\n-How to calculate moving sum with reset based on condition in teradata SQL?-<sql><sum><teradata>\",\n",
       " '<p>I am trying to insert XML data into a MySQL database.  The SAX parser I wrote works on its own when tested.  However, whenever I try to insert records into the database, I get a NullPointerException even though I made sure to assign values to workflow elements that were null.  Here is my database Table code.\\n    package database;</p>\\n\\n<pre><code>//STEP 1. Import required packages\\nimport java.sql.*;\\nimport java.io.*;\\nimport org.w3c.dom.*;\\nimport javax.xml.parsers.*;\\n\\npublic class Table {\\n// JDBC driver name and database URL\\nstatic final String JDBC_DRIVER = \"com.mysql.jdbc.Driver\";  \\nstatic final String DB_URL = \"jdbc:mysql://baldwin.isri.cmu.edu/SciSIP\";\\n\\n//  Database credentials\\nstatic final String USER = \"user\";\\nstatic final String PASS = \"pass\";\\n\\npublic Table()  {\\n\\n}\\n\\npublic void createTable()  {    \\n    Connection con = null;\\n    Statement stmt = null;\\n    try{\\n        //STEP 2: Register JDBC driver\\n        Class.forName(\"com.mysql.jdbc.Driver\");\\n\\n        //STEP 3: Open a connection\\n        System.out.println(\"Connecting to a selected database...\");\\n        con = DriverManager.getConnection(DB_URL, USER, PASS);\\n        System.out.println(\"Connected database successfully...\");\\n\\n        //STEP 4: Execute a query\\n        System.out.println(\"Creating table in given database...\");\\n        stmt = con.createStatement();\\n\\n        String sql = \"CREATE TABLE IF NOT EXISTS workflow\" +\\n                \"(id INTEGER not NULL AUTO_INCREMENT, \" +\\n                \" annotationBean VARCHAR(255), \" + \\n                \" date VARCHAR(255), \" + \\n                \" text VARCHAR(255), \" +\\n                \" identification VARCHAR(255), \" +\\n                \" PRIMARY KEY ( id ))\"; \\n\\n        stmt.executeUpdate(sql);\\n        System.out.println(\"Created table in given database...\");\\n    } catch(SQLException se) {\\n        //Handle errors for JDBC\\n        se.printStackTrace();\\n    } catch(Exception e) {\\n        //Handle errors for Class.forName\\n        e.printStackTrace();\\n    } finally {\\n        //finally block used to close resources\\n        try {\\n            if(stmt != null)\\n                con.close();\\n        } catch(SQLException se) {\\n        }// do nothing\\n        try {\\n            if(con != null)\\n                con.close();\\n        } catch(SQLException se) {\\n            se.printStackTrace();\\n        }//end finally try\\n    }//end try\\n    System.out.println(\"Goodbye!\");\\n}\\n\\npublic void insertRecord(String annotationBean, String date, String text, String identification)  {\\n    Connection con = null;\\n    Statement stmt = null;\\n    try {\\n        Class.forName(\"com.mysql.jdbc.Driver\"); //Load the driver           \\n        con = DriverManager.getConnection(DB_URL, USER, PASS);\\n        stmt = con.createStatement();\\n        String sql = \"INSERT INTO `workflow` VALUES (\\'\"+annotationBean+\"\\', \\'\"+date+\"\\', \\'\"+text+\"\\', \\'\"+identification+\"\\')\";\\n        stmt.execute(sql); //Insert a row\\n        System.out.println(\"Record Inserted into Database...\");\\n    } catch(SQLException se) {\\n        //Handle errors for JDBC\\n        se.printStackTrace();\\n    } catch (Exception e) {\\n        e.printStackTrace();\\n    } finally {\\n        //finally block used to close resources\\n        try {\\n            if(stmt != null)\\n                con.close();\\n        } catch(SQLException se) {\\n        }// do nothing\\n        try {\\n            if(con != null)\\n                con.close();\\n        } catch(SQLException se) {\\n            se.printStackTrace();\\n        }//end finally try\\n    }//end try\\n}\\n</code></pre>\\n\\n<p>Here is my parser code with the insert record statement.</p>\\n\\n<pre><code>package parser;\\nimport java.io.IOException;\\nimport java.util.ArrayList;\\nimport java.util.Iterator;\\nimport java.util.List;\\nimport javax.xml.parsers.ParserConfigurationException;\\nimport javax.xml.parsers.SAXParser;\\nimport javax.xml.parsers.SAXParserFactory;\\nimport org.xml.sax.Attributes;\\nimport org.xml.sax.SAXException;\\nimport org.xml.sax.helpers.DefaultHandler;\\nimport database.Table;\\n\\npublic class XML_Parser_SAX extends DefaultHandler{\\nList aWorkflow;\\nprivate String tempVal;\\n//to maintain context\\nprivate Workflow tempWorkflow;\\n\\npublic XML_Parser_SAX(){\\n    aWorkflow = new ArrayList();\\n}\\n\\npublic void runExample() {\\n    parseDocument();\\n    printData();\\n}\\n\\nprivate void parseDocument() {\\n    //get a factory\\n    SAXParserFactory spf = SAXParserFactory.newInstance();\\n    try {\\n\\n        //get a new instance of parser\\n        SAXParser sp = spf.newSAXParser();\\n\\n        //parse the file and also register this class for call backs\\n        sp.parse(\"workflow.xml\", this);\\n\\n    }catch(SAXException se) {\\n        se.printStackTrace();\\n    }catch(ParserConfigurationException pce) {\\n        pce.printStackTrace();\\n    }catch (IOException ie) {\\n        ie.printStackTrace();\\n    }\\n}\\n\\n/**\\n * Iterate through the list and print\\n * the contents\\n */\\nprivate void printData(){\\n\\n    System.out.println(\"Number of Tasks in Workflow: \\'\" + aWorkflow.size() + \"\\'.\");\\n\\n    Iterator it = aWorkflow.iterator();\\n    while(it.hasNext()) {\\n        System.out.println(it.next().toString());\\n    }\\n}\\n\\n\\n//Event Handlers\\npublic void startElement(String uri, String localName, String qName, Attributes attributes) throws SAXException {\\n    //reset\\n    tempVal = \"\";\\n    if(qName.equalsIgnoreCase(\"annotationBean\")) {\\n        //create a new instance of employee\\n        tempWorkflow = new Workflow();\\n        tempWorkflow.setAnnotationBean(attributes.getValue(\"class\"));\\n        System.out.println(\"class\");\\n    }\\n}\\n\\n\\npublic void characters(char[] ch, int start, int length) throws SAXException {\\n    tempVal = new String(ch,start,length);\\n}\\n\\npublic void endElement(String uri, String localName, String qName) throws SAXException {\\n\\n    if(qName.equalsIgnoreCase(\"annotationBean\")) {\\n        //add it to the list\\n        aWorkflow.add(tempWorkflow);\\n        System.out.println(\"annotationBean\");\\n\\n    } else if (qName.equalsIgnoreCase(\"date\")) {\\n        if (tempVal != null)  {\\n            tempWorkflow.setDate(tempVal);\\n        }\\n        else  {\\n            tempWorkflow.setDate(\"null\");\\n        }\\n        System.out.println(tempWorkflow.getDate());\\n    } else if (qName.equalsIgnoreCase(\"text\")) {\\n        if (tempVal != null)  {\\n            tempWorkflow.setText(tempVal);\\n        }\\n        else  {\\n            tempWorkflow.setText(\"null\");\\n        }\\n        System.out.println(tempWorkflow.getText());\\n    } else if (qName.equalsIgnoreCase(\"identification\")) {\\n        if (tempVal != null)  {\\n            tempWorkflow.setIdentification(tempVal);\\n        }\\n        else  {\\n            tempWorkflow.setIdentification(\"null\");\\n        }\\n        System.out.println(tempWorkflow.getIdentification());\\n    }\\n    Table t = new Table();\\n    //t.createTable();\\n    t.insertRecord(tempWorkflow.getAnnotationBean(), tempWorkflow.getDate(), tempWorkflow.getText(), tempWorkflow.getIdentification());\\n}\\n\\npublic static void main(String[] args){\\n    XML_Parser_SAX parse = new XML_Parser_SAX();\\n    parse.runExample();\\n}\\n\\n}\\n</code></pre>\\n\\n<p>Can someone please tell me why this is giving me the NullPointerException?  Thanks in advance!!!</p>\\n\\n<p>Here is the stacktrace:</p>\\n\\n<pre><code>Exception in thread \"main\" java.lang.NullPointerException\\nat parser.XML_Parser_SAX.endElement(XML_Parser_SAX.java:126)\\nat     com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.endElement(AbstractSAXParser.java:606)\\nat com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanEndElement(XMLDocumentFragmentScannerImpl.java:1741)\\nat com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:2898)\\nat com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:607)\\nat com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:488)\\nat com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:835)\\nat com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:764)\\nat com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:123)\\nat com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.parse(AbstractSAXParser.java:1210)\\nat com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl$JAXPSAXParser.parse(SAXParserImpl.java:568)\\nat com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl.parse(SAXParserImpl.java:302)\\nat javax.xml.parsers.SAXParser.parse(SAXParser.java:274)\\nat parser.XML_Parser_SAX.parseDocument(XML_Parser_SAX.java:49)\\nat parser.XML_Parser_SAX.runExample(XML_Parser_SAX.java:35)\\nat parser.XML_Parser_SAX.main(XML_Parser_SAX.java:131)\\n</code></pre>\\n\\n<p>Yes, workflow.xml exists.  Here is a copy of it below.</p>\\n\\n<pre><code>&lt;workflow xmlns=\"http://taverna.sf.net/2008/xml/t2flow\" version=\"1\" producedBy=\"taverna-      2.4.0\"&gt;&lt;dataflow id=\"8781d5f4-d0ba-48a8-a1d1-14281bd8a917\" role=\"top\"&gt;&lt;name&gt;Hello_World&lt;/name&gt;       &lt;inputPorts /&gt;&lt;outputPorts&gt;&lt;port&gt;&lt;name&gt;greeting&lt;/name&gt;&lt;annotations /&gt;&lt;/port&gt;&lt;/outputPorts&gt;       &lt;processors&gt;&lt;processor&gt;&lt;name&gt;hello&lt;/name&gt;&lt;inputPorts /&gt;&lt;outputPorts&gt;&lt;port&gt;&lt;name&gt;value&lt;/name&gt;       &lt;depth&gt;0&lt;/depth&gt;&lt;granularDepth&gt;0&lt;/granularDepth&gt;&lt;/port&gt;&lt;/outputPorts&gt;&lt;annotations /&gt;       &lt;activities&gt;&lt;activity&gt;&lt;raven&gt;&lt;group&gt;net.sf.taverna.t2.activities&lt;/group&gt;      &lt;artifact&gt;stringconstant-activity&lt;/artifact&gt;&lt;version&gt;1.4&lt;/version&gt;&lt;/raven&gt;     &lt;class&gt;net.sf.taverna.t2.activities.stringconstant.StringConstantActivity&lt;/class&gt;&lt;inputMap /&gt;    &lt;outputMap&gt;&lt;map from=\"value\" to=\"value\" /&gt;&lt;/outputMap&gt;&lt;configBean encoding=\"xstream\"&gt;    &lt;net.sf.taverna.t2.activities.stringconstant.StringConstantConfigurationBean xmlns=\"\"&gt;\\n  &lt;value&gt;Hello, World!&lt;/value&gt;\\n &lt;/net.sf.taverna.t2.activities.stringconstant.StringConstantConfigurationBean&gt;&lt;/configBean&gt;    &lt;annotations /&gt;&lt;/activity&gt;&lt;/activities&gt;&lt;dispatchStack&gt;&lt;dispatchLayer&gt;&lt;raven&gt;        &lt;group&gt;net.sf.taverna.t2.core&lt;/group&gt;&lt;artifact&gt;workflowmodel-impl&lt;/artifact&gt;        &lt;version&gt;1.4&lt;/version&gt;&lt;/raven&gt;       &lt;class&gt;net.sf.taverna.t2.workflowmodel.processor.dispatch.layers.Parallelize&lt;/class&gt;        &lt;configBean encoding=\"xstream\"&gt;        &lt;net.sf.taverna.t2.workflowmodel.processor.dispatch.layers.ParallelizeConfig xmlns=\"\"&gt;\\n  &lt;maxJobs&gt;1&lt;/maxJobs&gt;\\n    &lt;/net.sf.taverna.t2.workflowmodel.processor.dispatch.layers.ParallelizeConfig&gt;    &lt;/configBean&gt;&lt;/dispatchLayer&gt;&lt;dispatchLayer&gt;&lt;raven&gt;&lt;group&gt;net.sf.taverna.t2.core&lt;/group&gt;    &lt;artifact&gt;workflowmodel-impl&lt;/artifact&gt;&lt;version&gt;1.4&lt;/version&gt;&lt;/raven&gt;    &lt;class&gt;net.sf.taverna.t2.workflowmodel.processor.dispatch.layers.ErrorBounce&lt;/class&gt;    &lt;configBean encoding=\"xstream\"&gt;&lt;null xmlns=\"\" /&gt;&lt;/configBean&gt;&lt;/dispatchLayer&gt;&lt;dispatchLayer&gt;    &lt;raven&gt;&lt;group&gt;net.sf.taverna.t2.core&lt;/group&gt;&lt;artifact&gt;workflowmodel-impl&lt;/artifact&gt;    &lt;version&gt;1.4&lt;/version&gt;&lt;/raven&gt;    &lt;class&gt;net.sf.taverna.t2.workflowmodel.processor.dispatch.layers.Failover&lt;/class&gt;&lt;configBean     encoding=\"xstream\"&gt;&lt;null xmlns=\"\" /&gt;&lt;/configBean&gt;&lt;/dispatchLayer&gt;&lt;dispatchLayer&gt;&lt;raven&gt;    &lt;group&gt;net.sf.taverna.t2.core&lt;/group&gt;&lt;artifact&gt;workflowmodel-impl&lt;/artifact&gt;    &lt;version&gt;1.4&lt;/version&gt;&lt;/raven&gt;    &lt;class&gt;net.sf.taverna.t2.workflowmodel.processor.dispatch.layers.Retry&lt;/class&gt;&lt;configBean     encoding=\"xstream\"&gt;&lt;net.sf.taverna.t2.workflowmodel.processor.dispatch.layers.RetryConfig     xmlns=\"\"&gt;\\n  &lt;backoffFactor&gt;1.0&lt;/backoffFactor&gt;\\n  &lt;initialDelay&gt;1000&lt;/initialDelay&gt;\\n  &lt;maxDelay&gt;5000&lt;/maxDelay&gt;\\n  &lt;maxRetries&gt;0&lt;/maxRetries&gt;\\n&lt;/net.sf.taverna.t2.workflowmodel.processor.dispatch.layers.RetryConfig&gt;&lt;/configBean&gt;    &lt;/dispatchLayer&gt;&lt;dispatchLayer&gt;&lt;raven&gt;&lt;group&gt;net.sf.taverna.t2.core&lt;/group&gt;        &lt;artifact&gt;workflowmodel-impl&lt;/artifact&gt;&lt;version&gt;1.4&lt;/version&gt;&lt;/raven&gt;        &lt;class&gt;net.sf.taverna.t2.workflowmodel.processor.dispatch.layers.Invoke&lt;/class&gt;&lt;configBean         encoding=\"xstream\"&gt;&lt;null xmlns=\"\" /&gt;&lt;/configBean&gt;&lt;/dispatchLayer&gt;&lt;/dispatchStack&gt;       &lt;iterationStrategyStack&gt;&lt;iteration&gt;&lt;strategy /&gt;&lt;/iteration&gt;&lt;/iterationStrategyStack&gt;         &lt;/processor&gt;&lt;/processors&gt;&lt;conditions /&gt;&lt;datalinks&gt;&lt;datalink&gt;&lt;sink type=\"dataflow\"&gt;        &lt;port&gt;greeting&lt;/port&gt;&lt;/sink&gt;&lt;source type=\"processor\"&gt;&lt;processor&gt;hello&lt;/processor&gt;       &lt;port&gt;value&lt;/port&gt;&lt;/source&gt;&lt;/datalink&gt;&lt;/datalinks&gt;&lt;annotations&gt;&lt;annotation_chain         encoding=\"xstream\"&gt;&lt;net.sf.taverna.t2.annotation.AnnotationChainImpl xmlns=\"\"&gt;\\n  &lt;annotationAssertions&gt;\\n     &lt;net.sf.taverna.t2.annotation.AnnotationAssertionImpl&gt;\\n       &lt;annotationBean class=\"net.sf.taverna.t2.annotation.annotationbeans.Author\"&gt;\\n        &lt;text&gt;Stian Soiland-Reyes&lt;/text&gt;\\n       &lt;/annotationBean&gt;\\n       &lt;date&gt;2012-01-03 15:10:48.73 GMT&lt;/date&gt;\\n      &lt;creators /&gt;\\n       &lt;curationEventList /&gt;\\n     &lt;/net.sf.taverna.t2.annotation.AnnotationAssertionImpl&gt;\\n  &lt;/annotationAssertions&gt;\\n &lt;/net.sf.taverna.t2.annotation.AnnotationChainImpl&gt;&lt;/annotation_chain&gt;&lt;annotation_chain     encoding=\"xstream\"&gt;&lt;net.sf.taverna.t2.annotation.AnnotationChainImpl xmlns=\"\"&gt;\\n   &lt;annotationAssertions&gt;\\n     &lt;net.sf.taverna.t2.annotation.AnnotationAssertionImpl&gt;\\n      &lt;annotationBean      class=\"net.sf.taverna.t2.annotation.annotationbeans.DescriptiveTitle\"&gt;\\n         &lt;text&gt;Hello World&lt;/text&gt;\\n       &lt;/annotationBean&gt;\\n           &lt;date&gt;2012-01-03 15:10:54.167 GMT&lt;/date&gt;\\n       &lt;creators /&gt;\\n  &lt;curationEventList /&gt;\\n     &lt;/net.sf.taverna.t2.annotation.AnnotationAssertionImpl&gt;\\n  &lt;/annotationAssertions&gt;\\n     &lt;/net.sf.taverna.t2.annotation.AnnotationChainImpl&gt;&lt;/annotation_chain&gt;            &lt;annotation_chain_2_2 encoding=\"xstream\"&gt;&lt;net.sf.taverna.t2.annotation.AnnotationChainImpl        xmlns=\"\"&gt;\\n  &lt;annotationAssertions&gt;\\n    &lt;net.sf.taverna.t2.annotation.AnnotationAssertionImpl&gt;\\n      &lt;annotationBean     class=\"net.sf.taverna.t2.annotation.annotationbeans.IdentificationAssertion\"&gt;\\n        &lt;identification&gt;8781d5f4-d0ba-48a8-a1d1-14281bd8a917&lt;/identification&gt;\\n      &lt;/annotationBean&gt;\\n      &lt;date&gt;2012-01-03 15:12:21.684 GMT&lt;/date&gt;\\n  &lt;creators /&gt;\\n  &lt;curationEventList /&gt;\\n&lt;/net.sf.taverna.t2.annotation.AnnotationAssertionImpl&gt;\\n</code></pre>\\n\\n<p>\\n         \\n       \\n             \\n              </p>\\n\\n<pre><code>&lt;text&gt;One of the simplest workflows possible. No workflow input ports, a         single          workflow output port \"greeting\",  outputting \"Hello, world!\" as produced by the        String          Constant \"hello\".&lt;/text&gt;\\n                &lt;/annotationBean&gt;\\n            &lt;date&gt;2012-01-03 15:12:15.643 GMT&lt;/date&gt;\\n           &lt;creators /&gt;\\n           &lt;curationEventList /&gt;\\n        &lt;/net.sf.taverna.t2.annotation.AnnotationAssertionImpl&gt;\\n      &lt;/annotationAssertions&gt;\\n       &lt;/net.sf.taverna.t2.annotation.AnnotationChainImpl&gt;&lt;/annotation_chain&gt;&lt;/annotations &gt;         \\n &lt;/dataflow&gt;&lt;/workflow&gt;\\n</code></pre>\\n-Inserting XML data into MySQL database-<java><mysql><xml><saxparser><taverna>',\n",
       " \"<p>Is there any way to submit dynamically generated SQL to Teradata?  I've written a query that will create the code to denormalize a table.  Right now, I am pulling the code down to my client (SAS) and resubmitting it in a second step.  I am not familiar with either Teradata macros or procedures; would something like that work?</p>\\n\\n<p>To illustrate, I have a table defined like this:</p>\\n\\n<pre><code>create multiset table MYTABLE\\n    (  RECID  integer generated always as identity\\n              ( start with 1\\n               increment by 1\\n               minvalue -2147483647\\n               maxvalue 2147483647\\n               no cycle )\\n     , SNAP_DATE  date format 'YYYY/MM/DD'\\n     , EMAIL_DATE date format 'YYYY/MM/DD'\\n     , FREQ integer\\n    )\\nunique primary index ( RECID  )\\n</code></pre>\\n\\n<p>The table is populated every day (<code>SNAP_DATE</code>) and is used to monitor changes to an email_date in another table.  The following query returns the code that I can run to create my denormalized view:</p>\\n\\n<pre><code>select RUN_THIS\\nfrom (\\n    select RUN_THIS, RN\\n    from (\\n        select 'select EMAIL_DATE ' (varchar(100)) as RUN_THIS\\n              , 0 (int) as RN\\n          ) x\\n\\n    union all\\n    select ', sum( case when SNAP_DATE = date '''\\n             || (SNAP_DATE (format 'yyyy-mm-dd') (char(10)) )\\n             || ''' then FREQ else 0 end ) as D'\\n             || (SNAP_DATE (format 'yyyymmdd') (char(8)) ) as RUN_THIS\\n          , row_number() over ( partition by 1 order by SNAP_DATE ) as RN\\n    from ( select distinct SNAP_DATE \\n           from MYTABLE \\n           where SNAP_DATE &gt; current_date - 30) t1\\n\\n    union all\\n    select RUN_THIS, RN\\n    from (\\n        select 'from MYTABLE group by 1 order by 1;' as RUN_THIS\\n                , 10000 as RN\\n          ) y\\n    ) t\\norder by RN\\n</code></pre>\\n\\n<p>I export the result of the above query to a file on my client, then turn around and submit that file back to Teradata.  I'm hoping there is some way to store this complete definition in some Teradata object so it can be executed directly.</p>\\n-How to execute dynamic SQL in Teradata-<sql><teradata>\",\n",
       " '<p>I have a cell value \"80/90\" in a .csv file. </p>\\n\\n<p>While importing into Teradata using sql assistant, the value shows up as \"?\". </p>\\n\\n<p>The dataype of the column given was CV (Character Variable Length).   </p>\\n\\n<p>How do I resolve this?</p>\\n-Import Issue in Teradata-<sql><teradata>',\n",
       " '<p>Designed a model for decision tree using rapidminer 5.2\\ni want to implement and view the result using java\\nI used xml generated from rapidminer and run the proces in java as process.run()\\nBut i want to see my decision tree using java</p>\\n-implementing rapidminer as a java application-<java><rapidminer><user-interface>',\n",
       " \"<p>In teradata, can we insert multiple records using single insert statement in query. If yes, how ?</p>\\n\\n<p>Say I am trying to do something like:</p>\\n\\n<pre><code>insert test_rank (storeid,prodid,sales) values (1,'A',1000) ( 2,'B',2000) ,(3,'C',3000); \\n</code></pre>\\n\\n<p>but this is not working in teradata to insert all 3 records in one statement.</p>\\n-How to insert multiple records using single insert statement in Teradata?-<sql><teradata>\",\n",
       " \"<p>Can anyone explain Query Bands in Teradata?\\nI've searched regarding this a lot, but wasnt able to get information which I can understand.\\nPlease be a bit detailed. </p>\\n\\n<p>Thanks!!!</p>\\n-Explain Query Bands in Teradata-<teradata>\",\n",
       " \"<p>Just installed Teradata SQL Assistant 13.0 and I would like for it to display a list of columns within the table I'm selecting while building the query. </p>\\n\\n<p>Example:</p>\\n\\n<p><code>SELECT *  FROM TABLE1 T WHERE T.</code>(DROP-DOWN LIST APPEAR HERE)</p>\\n\\n<p>Is this possible in this application? </p>\\n-How can I get Teradata SQL Assistant to autocomplete my columns while writing queries-<sql><teradata>\",\n",
       " '<p>I am working with rapidminer , I have a dataset with a numerical field (attribute) , I want to simply add a constant (e.g. 1) to all values of this feature ,</p>\\n\\n<p>How may I do this? I have not found anything straightforward so far.</p>\\n-Add a constant value to a numerical attribute in Rapid Miner-<machine-learning><data-mining><rapidminer>',\n",
       " \"<p>I am working with a rather large dataset (770K records , 2K attributes , almost all of these attributes are binomial but in integer form) ,</p>\\n\\n<p>I want to apply decision tree on the data with a 10-fold cross validation, but I've some problems : </p>\\n\\n<p>1.Why does decision tree (e.g. with depth of 10) takes so much time to be trained ? actually I balance the data (as it's imbalanced) to 40% of the original size (~320K records) before training the tree , but it still takes a lot of time , is there any other version of Decision Tree which result the same performance and takes less time ? \\n(Does making the attributes in binomial form makes it faster ?)</p>\\n\\n<p>2.How can I optimize parameter of decision tree ? Should I optimize it on the the whole X-validation ? </p>\\n-Optimize Decision Tree Parameters in RapidMiner-<machine-learning><data-mining><rapidminer>\",\n",
       " '<p>I have a dataset with many attributes (2k) which a few of them (about 10) are not binary and the rest are binary (0,1) , I want to change the value types of these binary attributes from integer to binomial , as the name of features are not fixed I want to do it based on their values ! (i.e change the value types of all attributes with values of 0,1 from integer to binomial)</p>\\n\\n<p>Is there any straight way of doing this in Rapid Miner ? </p>\\n-Changing feature value type in RapidMiner-<machine-learning><data-mining><rapidminer>',\n",
       " \"<p>I have recently shifted from Teradata to ParAccel and I use my BI DBMS with SAS environment. Teradata has this utility called <code>FastLoad</code> for loading large datasets fast and more efficiently. I often have to make use of this utility to transfer datasets from SAS libraries to Teradata. I would like to know if there's a similar utility/command/function for ParAccel as well. Any help will be appreciated. </p>\\n-What is the FastLoad (in Teradata) equivalent for ParAccel?-<analytics><sas><business-intelligence><teradata><paraccel>\",\n",
       " '<p>I\\'m trying to run the following piece of code:</p>\\n\\n<pre><code>SELECT a.job_name\\n     , coalesce(b.target_time, cast(\\'08:00:00\\' as time(2))) sla_time\\n\\nFROM ud812.slarpt_job_level_info a\\nleft outer join ( \\n   select job_name, target_time\\n   from ud812.slarpt_job_target_times \\n   qualify row_number() over (partition by job_name \\n                              order by established_date desc) = 1\\n   ) b\\non (a.job_name = b.job_name)\\nwhere  a.display_on_sla_report = \\'Y\\' \\n   and a.job_type = \\'LD\\'\\n   and a.decom_date is null\\n</code></pre>\\n\\n<p>In doing so, I get the error \"Select Failed 3800:  Datatype Mismatch in THEN/ELSE expression.\\nThis relates to my use of coalesce.</p>\\n\\n<p>When I check my data types:  select type(target_time) from ud812.slarpt_job_target_times qualify...  I get time(2).\\nI have tried several things to adjust my code.</p>\\n\\n<p>These include:  Using a different data types ex. time(6).\\nI have even tried the following, but the type for sla_time comes back as integer.</p>\\n\\n<pre><code>SELECT a.job_name\\n      , cast(coalesce(b.target_time, \\'08:00:00\\') as time(2)) sla_time\\n\\nFROM ud812.slarpt_job_level_info a\\nleft outer join ( \\n   select job_name, cast(target_time as char(8)) as target_time\\n   from ud812.slarpt_job_target_times \\n   qualify row_number() over (partition by job_name \\n                              order by established_date desc) = 1\\n   ) b\\non (a.job_name = b.job_name)\\nwhere  a.display_on_sla_report = \\'Y\\' \\n   and a.job_type = \\'LD\\'\\n   and a.decom_date is null\\n</code></pre>\\n\\n<p>In the end, I\"m trying to establish a default time with each job name of 8 am.\\nIdeas?</p>\\n\\n<p>I have also tried the following , but when I type() the column it returns an integer, not time(2).</p>\\n\\n<pre><code>SELECT a.job_name\\n     , cast(coalesce(b.target_time, default_time) as time(2)) sla_time\\nFROM ( \\n   select job_name, \\'08:00:00\\' as default_time\\n   from ud812.slarpt_job_level_info\\n   where  a.display_on_sla_report = \\'Y\\' \\n     and a.job_type = \\'LD\\'\\n     and a.decom_date is null\\n   ) a\\nleft outer join ( \\n   select job_name, target_time\\n   from ud812.slarpt_job_target_times \\n   qualify row_number() over (partition by job_name \\n                              order by established_date desc) = 1\\n   ) b\\non (a.job_name = b.job_name)\\n</code></pre>\\n-How do I coalesce a Time field in Teradata?-<time><teradata><coalesce>',\n",
       " \"<p>In <strong>rapidminer</strong>,the application of <strong>FPgrowth algorithm</strong> didn't gives instances of attributes but it gives the name of the attribute and its support. My question is how to get the value of the attribute via <code>Groovy</code>.\\nThanks    </p>\\n-How to get Frequent Itemset values when using FPGrowth with rapidminer?-<groovy><rapidminer>\",\n",
       " \"<p>SentiWordNet is a text file. In RapidMiner 'OpenWordNet Dictionary' can only be used to access only exe files. How can I extract the sentiment scores from SentiWordNet for further processing?</p>\\n\\n<p>Thanks in Advance.</p>\\n-How to connect SentiWordNet to RapidMiner?-<rapidminer>\",\n",
       " '<p>I am using rapidminer doing classification , I want to extract performance as a macro and print the performance on output after measuring the performance , </p>\\n\\n<p>Is there any way to do that ? </p>\\n-Extract performance as a macro in rapidminer-<machine-learning><data-mining><rapidminer>',\n",
       " '<p>I am facing an issue in our Teradata QA environment where a simple query that ran in under 1 minute is now taking 12 minutes to complete. This select is pulling 5 fields based on a simple inner join</p>\\n\\n<pre><code>select a.material\\n    , b.season\\n    , b.theme\\n    , b.collection\\nfrom SalesOrders_view.Allocation_Deliveries_cur a\\ninner join SalesOrders_view.Material_Attributes_cur b\\n    on a.material = b.material;\\n</code></pre>\\n\\n<p>I can run this same query in our Prod environment and it returns in less than a minute while running on approx 200k more records than QA.</p>\\n\\n<p>Total volume is under 1.1 M records in SalesOrders.Allocation_Deliveries and 129 k records in SalesOrders.Material_Attributes. These are small datasets.</p>\\n\\n<p>I compared the Explain plans on both environments and there is a stark difference in the estimated spool volume in the first Join step. The estimate in Production is on the money while the Estimate in QA is an order of magnitude off. However the data and table/views are identical in both systems and we have collected stats in every conceivable manner and we can see the particular table demographics in both systems as identical.</p>\\n\\n<p>Lastly, this query has always returned in under a minute in all environments including QA as it is still doing in Production. This latent behavior is recent in the last week or so. I discussed this with our DBA and we have had no changes to software or configuration. He is new, but seems to know what he\\'s doing but still getting caught up with a new environment.</p>\\n\\n<p>I am looking for some pointers on what to check next. I have compared the relavant table / view definitions across QA and Prod and they are identical. The Table demographics in each system are also the same (I went through these with our DBA to make sure)</p>\\n\\n<p>Any help is appreciated. Thanks in advance.\\nPat</p>\\n\\n<p>This is the Explain plan from QA. Note the very Low estimate in Step 5 (144 Rows). In Prod, the same Explain shows > 1 M rows which would be close to what I know.</p>\\n\\n<pre><code>Explain select a.material\\n    , b.season\\n    , b.theme\\n    , b.collection\\nfrom SalesOrders_view.Allocation_Deliveries a\\ninner join SalesOrders_view.Material_Attributes_cur b\\n    on a.material = b.material;\\n\\n  1) First, we lock SalesOrders.Allocation_Deliveries in view\\n     SalesOrders_view.Allocation_Deliveries for access, and we lock\\n     SalesOrders.Material_Attributes in view SalesOrders_view.Material_Attributes_cur for\\n     access. \\n  2) Next, we do an all-AMPs SUM step to aggregate from\\n     SalesOrders.Material_Attributes in view SalesOrders_view.Material_Attributes_cur by way\\n     of an all-rows scan with no residual conditions\\n     , grouping by field1 ( SalesOrders.Material_Attributes.material\\n     ,SalesOrders.Material_Attributes.season ,SalesOrders.Material_Attributes.theme\\n     ,SalesOrders.Material_Attributes.theme ,SalesOrders.Material_Attributes.af_grdval\\n     ,SalesOrders.Material_Attributes.af_stcat\\n     ,SalesOrders.Material_Attributes.Material_Attributes_SRC_SYS_NM).  Aggregate\\n     Intermediate Results are computed locally, then placed in Spool 4. \\n     The size of Spool 4 is estimated with high confidence to be\\n     129,144 rows (41,713,512 bytes).  The estimated time for this step\\n     is 0.06 seconds. \\n  3) We execute the following steps in parallel. \\n       1) We do an all-AMPs RETRIEVE step from Spool 4 (Last Use) by\\n          way of an all-rows scan into Spool 2 (all_amps), which is\\n          redistributed by the hash code of (\\n          SalesOrders.Material_Attributes.Field_9,\\n          SalesOrders.Material_Attributes.Material_Attributes_SRC_SYS_NM,\\n          SalesOrders.Material_Attributes.Field_7, SalesOrders.Material_Attributes.Field_6,\\n          SalesOrders.Material_Attributes.theme, SalesOrders.Material_Attributes.theme,\\n          SalesOrders.Material_Attributes.season, SalesOrders.Material_Attributes.material)\\n          to all AMPs.  Then we do a SORT to order Spool 2 by row hash\\n          and the sort key in spool field1 eliminating duplicate rows. \\n          The size of Spool 2 is estimated with low confidence to be\\n          129,144 rows (23,504,208 bytes).  The estimated time for this\\n          step is 0.11 seconds. \\n       2) We do an all-AMPs RETRIEVE step from SalesOrders.Material_Attributes in\\n          view SalesOrders_view.Material_Attributes_cur by way of an all-rows scan\\n          with no residual conditions locking for access into Spool 6\\n          (all_amps), which is redistributed by the hash code of (\\n          SalesOrders.Material_Attributes.material, SalesOrders.Material_Attributes.season,\\n          SalesOrders.Material_Attributes.theme, SalesOrders.Material_Attributes.theme,\\n          SalesOrders.Material_Attributes.Material_Attributes_SRC_SYS_NM,\\n          SalesOrders.Material_Attributes.Material_Attributes_UPD_TS, (CASE WHEN (NOT\\n          (SalesOrders.Material_Attributes.af_stcat IS NULL )) THEN\\n          (SalesOrders.Material_Attributes.af_stcat) ELSE (\\'\\') END )(VARCHAR(16),\\n          CHARACTER SET UNICODE, NOT CASESPECIFIC), (CASE WHEN (NOT\\n          (SalesOrders.Material_Attributes.af_grdval IS NULL )) THEN\\n          (SalesOrders.Material_Attributes.af_grdval) ELSE (\\'\\') END )(VARCHAR(8),\\n          CHARACTER SET UNICODE, NOT CASESPECIFIC)) to all AMPs.  Then\\n          we do a SORT to order Spool 6 by row hash.  The size of Spool\\n          6 is estimated with high confidence to be 129,144 rows (\\n          13,430,976 bytes).  The estimated time for this step is 0.08\\n          seconds. \\n  4) We do an all-AMPs RETRIEVE step from Spool 2 (Last Use) by way of\\n     an all-rows scan into Spool 7 (all_amps), which is built locally\\n     on the AMPs.  Then we do a SORT to order Spool 7 by the hash code\\n     of (SalesOrders.Material_Attributes.material, SalesOrders.Material_Attributes.season,\\n     SalesOrders.Material_Attributes.theme, SalesOrders.Material_Attributes.theme,\\n     SalesOrders.Material_Attributes.Field_6, SalesOrders.Material_Attributes.Field_7,\\n     SalesOrders.Material_Attributes.Material_Attributes_SRC_SYS_NM,\\n     SalesOrders.Material_Attributes.Field_9).  The size of Spool 7 is estimated\\n     with low confidence to be 129,144 rows (13,301,832 bytes).  The\\n     estimated time for this step is 0.05 seconds. \\n  5) We do an all-AMPs JOIN step from Spool 6 (Last Use) by way of an\\n     all-rows scan, which is joined to Spool 7 (Last Use) by way of an\\n     all-rows scan.  Spool 6 and Spool 7 are joined using an inclusion\\n     merge join, with a join condition of (\"(material = material) AND\\n     ((season = season) AND ((theme = theme) AND ((theme =\\n     theme) AND (((( CASE WHEN (NOT (af_grdval IS NULL )) THEN\\n     (af_grdval) ELSE (\\'\\') END ))= Field_6) AND (((( CASE WHEN (NOT\\n     (AF_STCAT IS NULL )) THEN (AF_STCAT) ELSE (\\'\\') END ))= Field_7)\\n     AND ((Material_Attributes_SRC_SYS_NM = Material_Attributes_SRC_SYS_NM) AND\\n     (Material_Attributes_UPD_TS = Field_9 )))))))\").  The result goes into Spool\\n     8 (all_amps), which is duplicated on all AMPs.  The size of Spool\\n     8 is estimated with low confidence to be 144 rows (5,616 bytes). \\n     The estimated time for this step is 0.04 seconds. \\n  6) We do an all-AMPs JOIN step from Spool 8 (Last Use) by way of an\\n     all-rows scan, which is joined to SalesOrders.Allocation_Deliveries in view\\n     SalesOrders_view.Allocation_Deliveries by way of an all-rows scan with no\\n     residual conditions.  Spool 8 and SalesOrders.Allocation_Deliveries are\\n     joined using a single partition hash_ join, with a join condition\\n     of (\"SalesOrders.Allocation_Deliveries.material = material\").  The result goes\\n     into Spool 1 (group_amps), which is built locally on the AMPs. \\n     The size of Spool 1 is estimated with low confidence to be 3,858\\n     rows (146,604 bytes).  The estimated time for this step is 0.44\\n     seconds. \\n  7) Finally, we send out an END TRANSACTION step to all AMPs involved\\n     in processing the request.\\n  -&gt; The contents of Spool 1 are sent back to the user as the result of\\n     statement 1.  The total estimated time is 0.70 seconds.\\n</code></pre>\\n\\n<p>Here is what the record distribution looks like and the SQL I used to generate the result set</p>\\n\\n<pre><code>SELECT HASHAMP(HASHBUCKET(HASHROW( MATERIAL ))) AS\\n\"AMP#\",COUNT(*)\\nFROM EDW_LND_SAP_VIEW.EMDMMU01_CUR\\nGROUP BY 1\\nORDER BY 2 DESC;\\n</code></pre>\\n\\n<p>Output\\nHighest: AMP 137 with 1093 rows\\nLowest: AMP 72 with 768 rows\\nTotal AMPs: 144</p>\\n-Teradata Performance issue and example-<sql><teradata>',\n",
       " '<p>I\\'m trying to query a Teradata database in Python with PyODBC.\\nThe connection to database is established alright; however, when I try to fetch result, I ran into this error \"Invalid literal for Decimal: u\\'\\'\". Help please.</p>\\n\\n<p>I am on RHEL6, with Python 2.7.3</p>\\n\\n<p>Here is the code and result:</p>\\n\\n<pre><code>import pyodbc\\n\\nsql = \"select * from table\"\\n\\npyodbc.pooling = False\\ncnx = pyodbc.connect(\"DRIVER={Teradata};DBCNAME=host;DATABASE=database;   AUTHENTICATION=LDAP;UID=user;PWD=password\", autocommit=True, ANSI=True)\\ncursor = cnx.cursor()\\nrows = cursor.execute(sql).fetchone()\\n</code></pre>\\n\\n<hr>\\n\\n<pre><code>InvalidOperation                          Traceback (most recent call last)\\n&lt;ipython-input-25-f2a0c81ca0e4&gt; in &lt;module&gt;()\\n----&gt; 1 test.fetchone()\\n\\n/usr/local/lib/python2.7/decimal.pyc in __new__(cls, value, context)\\n    546                     context = getcontext()\\n    547                 return context._raise_error(ConversionSyntax,\\n--&gt; 548                                 \"Invalid literal for Decimal: %r\" % value)\\n    549 \\n    550             if m.group(\\'sign\\') == \"-\":\\n\\n/usr/local/lib/python2.7/decimal.pyc in _raise_error(self, condition, explanation, *args)\\n   3864         # Errors should only be risked on copies of the context\\n   3865         # self._ignored_flags = []\\n-&gt; 3866         raise error(explanation)\\n   3867 \\n   3868     def _ignore_all_flags(self):\\n\\nInvalidOperation: Invalid literal for Decimal: u\\'\\'\\n</code></pre>\\n-Issue with querying Teradata in Python/Pyodbc-<python><teradata>',\n",
       " '<p>After creating a new operator and testing it, I need to set some configuration such as the definition of minsupp for frequent itemsets algorithms. Actually, I am defining this parameter inside my java code. <strong>I like the minsup parameter be viewed in the parameter list when I select the new operator in the Rapidminer GUI</strong>.</p>\\n-How to add parameters configuration of new operator with rapidminer?-<java><rapidminer>',\n",
       " \"<p>I am having problems with Teradata Assistant (verion Teradata.Net 13.11.0.1)\\nThe Problem is with Database Explorer window becuse it is fixed length and width and I can´t adjust it as answer window and history window.\\nCan you please help me with this because the width of the Assistant is very small and when I expand a database I can't see the the column names.</p>\\n-Teradata Assistant Database Explorer-<teradata>\",\n",
       " '<p>I want to start working with Teradata database and for that I need to setup it on my system. After searching a lot I didn\\'t find any setup which I can use to install it on windows machine. The only link I found was <a href=\"http://www.teradata.com/teradata-express-13-0-windows/\" rel=\"nofollow\">http://www.teradata.com/teradata-express-13-0-windows/</a> this one  but there is no download link on this page. I have also found the VMware version to use Teradata on 64-bit windows on this link <a href=\"http://downloads.teradata.com/download/database/teradata-express/vmware\" rel=\"nofollow\">http://downloads.teradata.com/download/database/teradata-express/vmware</a> but I am not sure how to install this using VMware after downloading the setup.</p>\\n\\n<p>Please provide some help for installing Teradata on 32 windows or 64 windows using VmWare.</p>\\n-Setup Teradata database on windows 7 32-bit-<windows><installation><teradata>',\n",
       " '<p>I\\'m attempting to use the \\'Read Database\\' operator in RapidMiner 5 to read an \\'ExampleSet\\' from a Mysql database. I seem to establish a working connection with my db as I notice the Manage Database Connections window displays a \\'Connection ok\\' message when prompted.</p>\\n\\n<p>However when I attempt to run the \\'Read Database\\' process I receive the following error</p>\\n\\n<p>\"Process failed:\\nFailed to lookup .javax.naming.NoInitialContextException:\\nNeed to specify class name in environment or system property, or as an applet parameter,\\nor in an application resouce file:\\njava.naming.factory.initial\"</p>\\n\\n<p>I don\\'t know how to interpret this error. Do I need to set an environmental variable?</p>\\n-Read MYSQL db in RapidMiner-<mysql><jvm><environment-variables><rapidminer>',\n",
       " \"<p>I've been tasked to take a calendar date range value from a form front-end and use it to, among other things, feed a query in a Teradata table that does not have a datetime column.  Instead the date is aggregated from two varchar columns:  one for year (CY = current year, LY = last year, LY-1, etc), and one for the date with format MonDD (like Jan13, Dec08, etc).</p>\\n\\n<p>I'm using Coldfusion for the form and result page, so I have the ability to dynamically create the query, but I can't think of a good way to do it for all possible cases.  Any ideas?  Even year differences aside, I can't think of anything outside of a direct comparison on each day in the range with a potential ton of separate OR statements in the query.  I'm light on SQL knowledge - maybe there's a better way to script it in the SQL itself using some sort of conversion on the two varchar columns to form an actual date range where date comparisons could then be made?</p>\\n-Date range comparison using varchar columns in Teradata-<sql><coldfusion><teradata>\",\n",
       " '<p>I need to pull a row ID with a select statement. Something that is similar to the row ID of oracle. How would I do that in Teradata? I am trying the following query but it is throwing error.</p>\\n\\n<pre><code>select rowid,emp_id,e_name from test;\\n\\nError msg : Syntax error: ROWID not allowed.\\n</code></pre>\\n\\n<p>Thanks in advance.</p>\\n-RowID in Teradata-<java><sql><database><teradata>',\n",
       " \"<p>I am doing data conversion and i need to get the start and end dates of a given week.  </p>\\n\\n<p>My source has the date stored in a single <code>int</code> column <code>'year_wk'</code>.</p>\\n\\n<p>Examples for this column would be </p>\\n\\n<p>201201 - meaning the first week in 2012 \\n201005 - meaning the fifth week in 2010</p>\\n\\n<p>I would like to get start and end dates for a given week in a standard <code>mm/dd/yyyy</code> format.</p>\\n\\n<p>So 201201 would give me something like 1/1/2012 and 1/7/2012.</p>\\n-Finding Start and end Date for a week given the week of a year in Teradata (SQL)-<sql><teradata>\",\n",
       " '<p>Is it possible to transfer the date from the Teradata Table into .csv file directly.\\nProblem is - my table has more that 18 million rows.\\nIf yes, please send tell me the process</p>\\n-Export From Teradata Table to CSV-<teradata>',\n",
       " \"<p>I run this SQL:</p>\\n\\n<pre><code>select * from dbc.columns\\nwhere\\n    databasename = 'mydb' and\\n    tablename = 'mytable' and\\n    columntype = 'ts' and\\n    1=1\\norder by tablename;\\n</code></pre>\\n\\n<p>The column is a Timestamp(6) and its ColumnFormat column returns <code>yyyy-mm-dd hh:mm:ss.s(6)</code>.  However, when I view the DDL with <code>show table mydb.mytable</code>, the column format for the column shows as <code>yyyy-mm-ddbhh:mi:ss.s(6)</code>.  Notice the minute format strings.  The dbc view shows <code>mm</code> while the table DDL shows <code>mi</code>.  What's up with that?</p>\\n-Teradata dbc.columns: Why does ColumnFormat column differ from the table DDL's column definition?-<teradata>\",\n",
       " \"<p>I'm finishing the implementation of a new data mining technique in our research. </p>\\n\\n<p>I want to profit by the defined measures and performance operators included in rapidminer, so I need to deliver a labeled data set to the performance operator. </p>\\n\\n<p>Can anyone help me find how to create a labeled data set? (I have the label and the prediction, and my problem is how to create the output data table in the form of a labeled set). </p>\\n-How to create labeled ouputSet with rapidminer?-<java><rapidminer>\",\n",
       " \"<p>Suppose I have a table that looks like this:</p>\\n<blockquote>\\n<p>id  attribute</p>\\n<p>1   football</p>\\n<p>1   NFL</p>\\n<p>1   ball</p>\\n<p>2   football</p>\\n<p>2   autograph</p>\\n<p>2   nfl</p>\\n<p>2    blah</p>\\n<p>2    NFL</p>\\n</blockquote>\\n<p>I would like to get a list of distinct ids where the attribute column contains the terms &quot;football&quot;, &quot;NFL&quot;, and &quot;ball&quot;. So 1 would be included, but 2 would not. What's the most elegant/efficient way to do this in Terdata?</p>\\n<p>The number of attributes can vary for each id, and terms can repeat. For example, NFL appears twice for id 2.</p>\\n-Conditioning on multiple rows in a column in Teradata-<sql><teradata>\",\n",
       " '<p>i have data in one of the  columns which looks like </p>\\n\\n<p>column data:</p>\\n\\n<p>1.A123BDG</p>\\n\\n<p>2.ADGHKKL</p>\\n\\n<p>3.12HJKLL</p>\\n\\n<p>4.78GHUBD</p>\\n\\n<p>5.GHJKUIP</p>\\n\\n<p>6.KGJHGJG</p>\\n\\n<p>Now i have to filter out the data in  such a way that i have detect which is alphanumeric and only alpahabets</p>\\n\\n<p>select </p>\\n\\n<pre><code>  case when &lt;condition for detecting alphanumneric&gt;\\n          then column_data\\n          end column1 ,\\n  case when &lt;condition for detecting alphabets&gt;\\n          then column_data\\n         end  column2 \\n</code></pre>\\n\\n<p>from source_table </p>\\n\\n<p>data in column1 should be </p>\\n\\n<p>1.A123BDG </p>\\n\\n<p>3.12HJKLL</p>\\n\\n<p>4.78GHUBD</p>\\n\\n<p>data in column2 should be </p>\\n\\n<p>2.ADGHKKL</p>\\n\\n<p>5.GHJKUIP</p>\\n\\n<p>6.KGJHGJG</p>\\n\\n<p>anyone could help me by writing the the conditions for filtering alphanumeric &amp; alphabets</p>\\n\\n<p>Thanks in Advance</p>\\n-teradata : filtering alpahanumeric & only alphabets-<sql><regex><teradata>',\n",
       " \"<p>I've touched a Teradata. I've never touched hadoop, but since yesterday, I am doing some research on that. By description of both, they seem quite interchangable, but in some papers it is written that they serve for different purposes. But all I found is vague. I am confused.</p>\\n\\n<p>Has anybody experience with both of them? What is the serious difference between them?</p>\\n\\n<p>Simple Example: I want to build ETL which will transform billions rows of raw data and organize them to DWH. Then do some resources expensive analysis on them. Why use TD? Why Hadoop? or why not?</p>\\n-hadoop vs teradata what is the difference-<database><hadoop><teradata><business-intelligence>\",\n",
       " '<p>I am trying to load a big csv file(about 18G)  into rapidminer for building a classification model. The “import configuration wizard” seems has difficulty in loading the data. Therefore, I choose to use the “Edit parameter list: data set meta data information” to set up the attribute and label information. However, the UI-interface only allows me to setup those information column-by-column. My csv file has about 80000 columns. How should I handle this kind of scenario? Thanks.</p>\\n-problems of loading large csv file into rapidminer-<rapidminer>',\n",
       " '<p>As my question was remained unanswered in Rapidminer forum via this address : </p>\\n\\n<p><a href=\"http://rapid-i.com/rapidforum/index.php/topic,6256.msg21862.html#msg21862\" rel=\"nofollow\">my original question</a></p>\\n\\n<p>I would like to propose it here again, please inform me if by any chance you know the answer.</p>\\n\\n<p>I have noticed no matter what type of extensions we use for getting  export (i.e. .pdf, .ps .jpg , etc), the export engine do like a screenshot of the current view instead of the whole output space, this has caused lots of issues for me exporting gigantic decision trees of my research.</p>\\n\\n<p>Please someone advise me on this issue.</p>\\n\\n<p>Best,</p>\\n\\n<p>Amir</p>\\n-Rapidminer Graph view suspected export bug?-<export><rapidminer>',\n",
       " '<p>May know the basic requirements for a asp.net and teradata application. </p>\\n\\n<p>How to connect to Teradata using asp.net(c# code). I have zero knowledge on Teradata. I know that we need a provider for DB connection. How can we get all those things. </p>\\n\\n<p>How to work with teradata. Does it work like SQL Server with queries, procedures and all DB things. </p>\\n\\n<p>Whats the difference between SQL Server and Teradata. Can we find trail version of teradata</p>\\n\\n<p>After installation how to connect and work with teradata from asp.net application (Most important for me)</p>\\n\\n<p>Thanks in Advance.</p>\\n-Teradata and Asp.Net-<c#><asp.net><teradata>',\n",
       " '<p>I am adding some new columns to a table and want to add documentation to the table DDL for future developers.  How does one go about this?</p>\\n-How do I add comments to existing table DDL in Teradata?-<ddl><teradata>',\n",
       " '<p>So I am stuck on this Teradata problem and I am looking to the community for advice as I am new to the TD platform. I am currently working with a Teradata Data Warehouse and have an interesting task to solve. Currently we store our information in a live production database but want to stage tables in another database before using FastExport to export the files. Basically we want to move our tables into a database to take a quick snapshot.</p>\\n\\n<p>I have been exploring different solutions and am unsure how to proceed. I need to be able to automate a create table process from one DB in Teradata to another. The tricky part is I would like to create many tables off of the source table using a WHERE clause. For example, I have a transaction table and want to take a snapshot of the transaction table for a certain date range month by month. Meaning that the original table Transaction would be split into many tables such as Transaction_May2001, Transaction_June2001, Transaction_July2001 and so on and so forth. </p>\\n\\n<p>Thanks </p>\\n-How to transfer data between databases with SELECT statement in Teradata-<sql><teradata>',\n",
       " '<p>I have written my own implementation of LOF and I\\'m trying to compare results with the implementations in ELKI and RapidMiner, but all 3 give different results! I\\'m trying to work out why.</p>\\n\\n<p>My reference dataset is one-dimensional, 102 real values with many duplicates. I\\'ll try and post it below. </p>\\n\\n<p>First, the RapidMiner implementation. The LOF scores are wildly different from ELKI and from my results; many come back with a LOF of infinity. Has this implementation been validated as being correct?</p>\\n\\n<p>My results are similar to ELKI, but I don\\'t get exactly the same LOF values. From a quick scan of the comments in the ELKI source code, I think this may be because of differences in the way the k-neighbourhood is calculated.</p>\\n\\n<p>In the LOF paper, the MinPts parameter (elsewhere called k) specifies the minimum no. of points to be included in the k-neighbourhood. In the ELKI implementation, I think they are defining the k-neighbourhood as exactly k points rather than all the points within the k-distance or k-distinct distance. Can anyone confirm exactly how ELKI constructs the k-neighbourhood? Also there is a private variable which allows the point itself to be included in its own neighbourhood, but it looks like the default is not to include it.</p>\\n\\n<p>Does anyone know of a public reference dataset which has the LOF scores attached for validation purposes?</p>\\n\\n<p>--- more details follow ---</p>\\n\\n<p>Reference: ELKI source code is here:</p>\\n\\n<p><a href=\"http://elki.dbs.ifi.lmu.de/browser/elki/trunk/src/de/lmu/ifi/dbs/elki/algorithm/outlier/lof/LOF.java\" rel=\"nofollow\">http://elki.dbs.ifi.lmu.de/browser/elki/trunk/src/de/lmu/ifi/dbs/elki/algorithm/outlier/lof/LOF.java</a></p>\\n\\n<p>RapidMiner source code is here:</p>\\n\\n<p><a href=\"http://code.google.com/p/rapidminer-anomalydetection/source/browse/trunk/src/de/dfki/madm/anomalydetection/evaluator/nearest_neighbor_based/LOFEvaluator.java\" rel=\"nofollow\">http://code.google.com/p/rapidminer-anomalydetection/source/browse/trunk/src/de/dfki/madm/anomalydetection/evaluator/nearest_neighbor_based/LOFEvaluator.java</a></p>\\n\\n<p>Here is my test dataset:</p>\\n\\n<p>4.32323\\n5.12595 5.12595 5.12595 5.12595 5.7457 5.7457 5.7457\\n5.7457 5.7457 5.7457 5.97766 5.97766\\n6.07352 6.07352 6.12015 6.12015 6.12015 6.44797 6.44797\\n6.48131 6.48131 6.48131 6.48131 6.48131 6.48131 6.6333\\n6.6333 6.6333 6.70872 6.70872 6.70872 6.70872 6.70872\\n6.77579 6.77579 6.77579 6.77579 6.77579 6.77579 6.77579\\n6.77579 6.77579 6.77579 6.77579 6.77579 6.77579 6.77579\\n6.77579\\n7.03654 7.03654 7.03654 7.03654 7.03654 7.03654 7.03654\\n7.03654 7.03654 7.03654 7.03654 7.03654 7.03654 7.03654\\n7.03654 7.10361 7.10361 7.10361 7.10361 7.10361 7.10361\\n7.10361 7.10361 7.15651 7.15651 7.15651 7.15651 7.15651\\n7.15651 7.15651 7.15651\\n8.22598 8.22598 8.22598 8.22598 8.5538 8.5538 8.5538\\n8.5538 8.5538 8.5538 8.5538 8.5538 8.5538 8.5538 8.5538\\n8.5538 8.5538 8.5538 8.5538 8.5538 8.5538 8.5538</p>\\n\\n<p>For example, I get the following LOF score for the first number (4.32323):</p>\\n\\n<ul>\\n<li>RapidMiner: infinity (with MinPts lower/upper bounds set to 10,100)</li>\\n<li>ELKI: 2.6774 (with k = 10 and distfunction/reachdistfunction set to default)</li>\\n<li>My implementation: 1.9531</li>\\n</ul>\\n\\n<p>Some more details on what my implementation is doing:</p>\\n\\n<ol>\\n<li>MinPts is 10, so I\\'m finding the 10 distinct neighbours of the point. So the neighbourhood of 4.32323 is actually 48 points, from 5.12595 up to 6.77579.</li>\\n<li>That gives me a k-distinct distance of 2.45256</li>\\n<li>I\\'m calculating the reachability distance of the first neighbour as 1.58277</li>\\n<li>I\\'m calculating the LRD of the sample as 1/(99.9103/48)</li>\\n<li>Sum of lrd(o)/lrd(p) for all 48 neighbours is 93.748939</li>\\n<li>Divide by 48 to get a LOF of 1.9531</li>\\n</ol>\\n-Different results from LOF implementation in ELKI and RapidMiner-<java><rapidminer><outliers><elki>',\n",
       " '<p>I can swear that I heard that TD14 will support bitwise, but I see no mention of it in searches nor searching \"bitwise\" in the TD14 manual.</p>\\n\\n<p>Can someone confirm whether or not bitwise is supported?</p>\\n\\n<p>BTW, the manual I am looking at is the Teradata User Documentation in case there\\'s another doc that I should be reading.</p>\\n-Does Teradata 14 support bitwise?-<bit-manipulation><bitwise-operators><teradata>',\n",
       " '<p>Pasting following url to address bar prompts for a username and password. Once authenticated it sends results in json format.\\n<a href=\"http://metabolomics.pharm.uconn.edu:2480/connect/iimdb\" rel=\"nofollow\">http://metabolomics.pharm.uconn.edu:2480/connect/iimdb</a>  </p>\\n\\n<p>However following code doesn\\'t work. Results in alert \"failed\". I also tried hard coding username and password. Can someone spot the problem. Thanks in advance.  </p>\\n\\n<pre><code>    &lt;script type=\"text/javascript\" src=\"jquery-1.9.1.min.js\"&gt;&lt;/script&gt;\\n        &lt;script type=\"text/javascript\"&gt;\\n        $(document).ready(function(){   \\n            var userName = $(\\'#username\\').val();\\n            var password = $(\\'#password\\').val();\\n            var submitButton = $(\\'#submitButton\\');\\n            submitButton.click(function(){\\n    $.ajax( {\\n        type: \\'Get\\',\\n    url : \\'http://metabolomics.pharm.uconn.edu:2480/connect/iimdb\\',\\n    dataType : \\'json\\',\\n        username : userName,\\n        password : password,\\n    success : function(data) {\\n        alert(\"success\");\\n        response(data);\\n    },\\n        error : function()\\n        {\\n        alert(\"failed\");\\n        }\\n});\\n    });\\n        }); \\n        &lt;/script&gt;\\n</code></pre>\\n-jquery RESTful web service-<jquery><rest>',\n",
       " \"<p>I have a portion of a code where I need to get the data from the table for a particular condition \\n( where accountid = '  ' ) . The data retreival is a very slow process . Retreving record it self takes 2 hours . I tried adding cocurrent threads to speed up the process , but did not have much benefit in terms of time . </p>\\n\\n<p>It would be helpful if some one could suggest me a better and faster way for retreival .</p>\\n\\n<p>PS : I am making an ODBC connection and then running a query and getting the output of the query . </p>\\n-Retrieve huge number of records ( 300000 ) using a c# code from a table (Teradata Database)-<c#><java><sql><odbc><teradata>\",\n",
       " \"<ol>\\n<li>We often have Excel connect to a teradata database and run some sql for various reports/models we produce. If I'm right this will always connect and 'Execute' as per a standard query run in Queryman.</li>\\n<li>However, I'd like to be able to tell it to connect and 'Execute Parallel'. </li>\\n<li>Because my query takes around 2-3 hrs to get the data and if I run it from excel VBA then I am not able to do anything on my Excel application. </li>\\n<li>I want that once i have fired query on teradata, it will run parallerly and my excel is also free for the time required to get the data.</li>\\n</ol>\\n\\n<p>Thanks in advance!!!</p>\\n-Connect to teradata from VBA and run codes parallerly-<excel><teradata><vba>\",\n",
       " '<p>I am trying to migrate my data from sybase to monetdb.(or opposite) for this I am using bcp out from sybase and copy into in monetdb.</p>\\n\\n<p>\\nThe problem is some of the data types are not available in monetdb for example datatime. To solve this problem I can define a  function in monetdb which coneverts datetime format to monetdb specific format.</p>\\n\\n<p>But How to call this function on specific column when I am trying bulkcopy command<br/>\\n<pre>COPY INTO TABLE from file using delimiters;</pre>\\n</p>\\n\\n<p>\\nSame situation for bcp in sybase\\n</p>\\n\\n<p>EDIT 2: \\n(I guess I solved my problem although original question still remains) I wanted to bulk copy data from a csv file in both monetdb and sybase. The csv file had one column with unix timestamps (seconds from 1970). As it is difficult to query using unix time stamp compared to human readable date time (YYYY-MM-DD HH:MM:SS) format, I wanted to convert that timestamp column to date time format).</p>\\n\\n<p>I could not change the datatype at time of bulk copy but defined(used) functions to translate timestamp in human readable format.</p>\\n\\n<pre>\\n--For Sybase used dateadd function :\\nselect * from gaurav.table where dateadd(ss,t_stamp,\\'1/1/1970\\') \\'2013-02-27 (less than sign) 10:17:29.463114\\';\\n</pre>\\n\\n<pre>\\n--For Monetdb defined a function : \\ncreate function \"epoch\"(sec INT) returns TIMESTAMP external name timestamp.\"epoch\";\\nselect * from gaurav.table where epoch(t_stamp)  (less than sign) \\'2013-02-27 10:17:29.463114\\'; \\n</pre>\\n-bulk copy[copy into] How do I replace or change the datatype of one column(during bulk copying) in monet db/sybase?-<sap-ase><monetdb>',\n",
       " '<p>I have a table something like this:</p>\\n\\n<pre><code>ID|Value\\n01|1\\n02|4\\n03|12\\n01|5\\n02|14\\n03|22\\n01|9\\n02|32\\n02|62\\n01|13\\n03|92\\n</code></pre>\\n\\n<p>I want to know how much progress have each id made (from initial or minimal value)<br>\\nso in sybase I can type:</p>\\n\\n<pre><code>select ID, (value-min(value)) from table group by id;\\n\\nID|Value\\n01|0\\n01|4\\n01|8\\n01|12\\n02|0\\n02|10\\n02|28\\n02|58\\n03|0\\n03|10\\n03|80\\n</code></pre>\\n\\n<p>But <code>monetdb</code> does not support this (I am not sure may be cz it uses SQL\\'99).<br>\\nGroup by only gives one column or may be average of other values but not the desired result.</p>\\n\\n<p>Are there any alternative to group by in <code>monetdb</code>?</p>\\n-Why is \"group by\" giving only one column as output?-<monetdb>',\n",
       " '<p>Is it possible to add a referential integrity constraint on a column which references a non primary-key column in another table.</p>\\n\\n<p>I basically have a table with an ID column <strong>which is non-unique</strong>.</p>\\n\\n<p>I have another table with REFERENCE column which refers to the ID column in the above table.</p>\\n\\n<p>I want to enforce a constraint on REFERENCE column that it only contain values present in the ID column.</p>\\n\\n<p>Can this be done through the Reference Constraint in Teradata 12.0?</p>\\n\\n<p>thanks</p>\\n-Teradata 12.0: Referential Integrity Constraint-<constraints><teradata><referential-integrity>',\n",
       " '<p>I am new to RapidMiner...What I am trying to do is that I have a list of 10 Documents which I tokenize using the ProcessDocuments Operator (subtask)-> Tokenize...the result is an <strong>10 by 800</strong> exampleset with 10 rows (one for each document) and 800 attributes (one for each token).</p>\\n\\n<p>Now I want to filterbylength the 800 tokens,  I again use the ProcessDocuments Operator (subtask)-> FilterByLength <strong>on the worldlist generated by the previous ProcessDocuments operator</strong>...the result is a 800 by 700 matrix...800 for the 800 tokens from the previous ProcessDocuments Operator and 700 the reduced set of tokens.</p>\\n\\n<p>What I want to accomplish is a <strong>10 by 700 exampleset</strong> which i can pass to the Kmeans clustering operator. How can I do that?</p>\\n\\n<p>thanks</p>\\n-RapidMiner Multiple Filters in sequence-<rapidminer>',\n",
       " '<p>In rapid miner i am trying to take data using xpath from an xml page, i have tried a number of different statements but no success. Below is the data that im trying to retrieve, i want all the features from the un-ordered list.</p>\\n\\n<pre><code>enter code here\\n\\n&lt;div id=\"features\"&gt;\\n&lt;h3&gt;Features:&lt;/h3&gt;\\n&lt;ul&gt;&lt;li&gt;Front  garden&lt;/li&gt;\\n&lt;li&gt;Rear Large Shed&lt;/li&gt;\\n&lt;li&gt;Superb condition and tastefully decorated&lt;/li&gt;\\n&lt;li&gt;Energy Efficent with a B2 Ber rating&lt;/li&gt;\\n&lt;li&gt;Gravel &amp;amp; driveway&lt;/li&gt;\\n&lt;/ul&gt;&lt;/div&gt;\\n</code></pre>\\n-apath statement for rapidminer-<xpath><rapidminer>',\n",
       " \"<p>My Questions:\\n<li> Is there a way to add quotes around %{macro_name} in my SQL query ? \\n<li> Is there a better way to create the required report (i.e. not using RapidMiner)?</p>\\n\\n<p>My Process:</p>\\n\\n<p>I'm currently trying to create a custom report of data stored in a MySQL database. Here is an hypothetical example of my table data:</p>\\n\\n<pre><code>Item_Name  Item_Price Item_Stock Item_Timestamp\\nDish Soap    3.99        25      1/1/2013 12:00am\\nFrogs        0.69        26      1/1/2013 12:00am\\nFrogs        0.69        19      1/1/2013 1:00am\\nDish Soap    3.99        28      1/1/2013 1:00am\\n</code></pre>\\n\\n<p>Item_Timestamp refers to the datetime of when the entry was made.</p>\\n\\n<p>I'm attempting to use RapidMiner to do the following:\\n    <li> Provide a summation of increases in Item_Stock for each unique Item_Name\\n    <li> Provide a summation of decreases in Item_Stock for each unique Item_Name\\n    <li> Provide the average rate of change over a specified time period</p>\\n\\n<p>My goal is to create a report that tells me whether items are being restocked at a rate of equilibrium with demand.</p>\\n\\n<p>In order to create a report for each unique Item_Name, I have created a RapidMiner process which loads unique Item_Name as an example set, then attempts to loop through the exampleset by using the extract macro operator which sends the Item_Name from each example to another SQL query. RapidMiner uses %{macro_name} as the syntax for the macro. My SQL query looks like:</p>\\n\\n<pre><code>Select Item_Name\\nFrom thisTable\\nWhere Item_Name = %{macro_name}\\n</code></pre>\\n\\n<p>The problem is that this query throws an exception, but I'm not sure why. Perhaps the problem is that %{macro_name} returns a string without the necessary quotes, but I am unsure.</p>\\n\\n<p>My questions are: \\n<li> Is there a way to add quotes around %{macro_name} in my SQL query ? \\n<li> Is there a better way to create the required report (i.e. not using RapidMiner)?</p>\\n-Problems implementing SQL query in Rapidminer to create report-<sql><business-intelligence><rapidminer>\",\n",
       " '<p>I have written a REST client plugin for KNIME (a software that is based on eclipse). I have used jersey as library.</p>\\n\\n<p>Everything works fine. However, if I send an client request a APPLET ALERT dialog pops up asking me whether I want to allow, disallow or stop the applet. This dialog get quite annoying, especially if you want to submit multiple requests. \\nHow do I get rid of it?</p>\\n\\n<p>All I have found so far, referred to anti-virus software adding stuff to .jar file during download, adjusting the arguments of a maven server and ant builds. However, I have created this plugin on my machine and I am running the target REST server locally (it is based on python).</p>\\n\\n<p>Ideally I would like to know how to suppress this plugin in my code or plugin settings or via the preferences. If thats not possible I would need a workaround that can also be explained to user outside of our company, since the plugin is going to be published. </p>\\n\\n<p>The dialog is created by the code line</p>\\n\\n<blockquote>\\n  <p>client.handle(request)</p>\\n</blockquote>\\n\\n<p>with client being a jersey Client and request being a jersey ClientRequest.</p>\\n\\n<p>The console output is </p>\\n\\n<pre><code>   -- PolicyProps uninitialized on access of \\'jscan.session.policyname\\n   -- PolicyProps uninitialized on access of \\'jscan.session.origin_uri\\'\\n   -- PolicyProps uninitialized on access of \\'misc.no_user_interaction\\'\\n   -- PolicyProps uninitialized on access of \\'misc.prompt_user\\'\\n   -- PolicyProps uninitialized on access of \\'misc.notify_user\\'\\n   -- PolicyProps uninitialized on access of \\'misc.max_offense_cnt\\'\\n   -- PolicyProps uninitialized on access of \\'misc.strict_enforcement\\'\\n   -- PolicyProps uninitialized on access of \\'jscan.session.origin_uri\\'\\n   -- PolicyProps uninitialized on access of \\'net.bind_enable\\'\\n   -- PolicyProps uninitialized on access of \\'net.connect_src\\'\\n   -- PolicyProps uninitialized on access of \\'net.connect_other\\'\\n   -- PolicyProps uninitialized on access of \\'net.connect_in_dom_list\\'\\n   Msgs: Failed to find localized message for key \"instr.action.connect\" - using default English form\\n   Msgs: Failed to find localized message for key \"instr.msg.dialog.action_warning_start\" - using default English form\\n   --&gt;&gt; returning Frame NULL\\n   Msgs: Failed to find localized message for key \"instr.dialog.title.applet_alert\" - using default English form\\n   Msgs: Failed to find localized message for key \"instr.button.allow\" - using default English form\\n   Msgs: Failed to find localized message for key \"instr.button.disallow\" - using default English form\\n   Msgs: Failed to find localized message for key \"instr.button.stop_applet\" - using default English form\\n   BaseDialog: owner frame is a java.awt.Frame\\n</code></pre>\\n-Suppressing Applet Alert in a eclipse extension plugin that uses jersey-<eclipse-plugin><applet><jersey><knime><javapolicy>',\n",
       " \"<p>iam new to rapidminer and i know how to add neuralnets and aply model and how to use the crossvalidation. but now i want to use the stacking module but i get the error that my neural net can't work with binominal fields but it did before i started using the stack module.</p>\\n\\n<p>in my stack module in the left part i have 2 adaboosts with in the first a neural net and in de second a naive byas. in the right part i have a singel neural net wich gives the error how can i fix this? i have looked at the questions but i couldn't find any thing about using multiple learners. and google isn't as usefull as it could be.</p>\\n-using the stacking module in rapidminer-<machine-learning><classification><rapidminer>\",\n",
       " \"<p>It is not a strictly programming question, but knowing hardware is the critical part of programming.</p>\\n\\n<p>So I start this thread, hope people here can share their experience in  programming on Kepler (GK10X or GK110).</p>\\n\\n<p>First I start mine:</p>\\n\\n<p>I am doing some programming on GK110 at the moment, for some applications, GK110 is significantly faster than a Fermi, close to its theoretical peak (e.g. 2.5-3X faster). But for others, it isn't, (e.g. only ~ 50%-60% faster).</p>\\n\\n<p>Correct me if I am wrong, but it seems to me, the main performance bottleneck of Kepler is resource pressure is very high here:</p>\\n\\n<p>On a per-SM level, Fermi actually have far more resources comparing to GK110, on each SM, Fermi only has one SIMT unit, whilst Kepler has 6.</p>\\n\\n<p>Yet on each SM,  Fermi has 32K registers file, a maximum of 1536 active threads, whilst on each SM of Kepler, there are only 33% more active threads, 100% more registers, with 800% insturction-issue units, and same amount of L1 cache.</p>\\n\\n<p>The latencies regarding memory and computation are about the same in absolute terms (half in terms of GPU cycles).</p>\\n\\n<p>So resource-pressure is much higher on GK110, comparing to GF110.</p>\\n\\n<p>With 800% of instruction-issue units, it seems that Nvidia want to use more aggressive TLP and ILP to hide latency on Kepler, but it is certainly not as flexible, since L1 cache is the same, and active threads is only increased by 33% instead of 500% like its SIMT units.</p>\\n\\n<p>So, to utilize maximum efficiency of Kepler, it is much harder,  first code has to contain significantly more ILP yet take significantly less shared-memory to take the advantage of a massive instruction-issue unit of Kepler, secondly, on a per-warp level, workflow has to be very computationally intensive such that Kepler schedule don't need to switch a lot of warps to hide latency (and it certainly don't have a lot of available warps to chose from to begin with).</p>\\n-What are the limiting factors of Kepler, comparing to Fermi?-<cuda><gpu><nvidia>\",\n",
       " \"<p>I'm trying to create a model with a training dataset and want to label the records in a test data set. </p>\\n\\n<p>All tutorials or help I find online has information on only using cross validation with one data set, i.e., training dataset. I couldn't find how to use test data. I tried to apply the result model on to the test set. But the test set seems to give different no. of attributes than training set after pre-processing. This is a text classification problem.</p>\\n\\n<p>At the end I get some output like this</p>\\n\\n<pre><code>18.03.2013 01:47:00 Results of ResultWriter 'Write as Text (2)' [1]: \\n18.03.2013 01:47:00 SimpleExampleSet:\\n5275 examples,\\n366 regular attributes,\\nspecial attributes = {\\nconfidence_1 = #367: confidence(1) (real/single_value)\\nconfidence_5 = #368: confidence(5) (real/single_value)\\nconfidence_2 = #369: confidence(2) (real/single_value)\\nconfidence_4 = #370: confidence(4) (real/single_value)\\nprediction = #366: prediction(label) (nominal/single_value)/values=[1, 5, 2, 4]\\n}\\n</code></pre>\\n\\n<p>But what I wanted is all my examples to be labelled.</p>\\n\\n<p>It seems that my test data and training data have different no. of attributes, I see many of following in the logs.</p>\\n\\n<p>Mar 18, 2013 1:46:41 AM WARNING: Kernel Model: The given example set does not contain a regular attribute with name 'wireless'. This might cause problems for some models depending on this particular attribute.</p>\\n\\n<p>But how do we solve such problem in text classification as we cannot know no. of and name of attributes before hand.</p>\\n\\n<p>Can some one please throw some pointers.</p>\\n-Using test data set in RapidMiner-<machine-learning><classification><rapidminer>\",\n",
       " '<p>I am trying to connect <code>monetdb</code> with <code>node.js</code>. I have a simple (20 line) c program which can query <code>moentdb</code> using mapi libraries.</p>\\n\\n<p>Can I use those libraries to build something(module/addon) for <code>node.js</code> which uses these libraries and connect to <code>monetdb</code>?</p>\\n\\n<p>(using odbc is an option but it have its own disadvantages.)</p>\\n\\n<p><strong>Update1 :</strong><br>\\nnode-ffi is pretty awesome. I was able to create a fetch table program pretty easily. (I have added my working code for example.)</p>\\n\\n<p>So if I have 3 options<br>\\n1. ODBC<br>\\n2. node-ffi<br>\\n3. a c program to fetch database data and listens to connection from node.js through socket  </p>\\n\\n<p>In terms of performance which is better option to implement, if I have little less time to develop a addon for node.js</p>\\n\\n<pre><code>var ffi = require(\"ffi\");\\nvar libmylibrary = ffi.Library(\\'/usr/local/lib/libmapi.so\\', {\\n    \"mapi_connect\":[\"int\",[\"string\",\\'int\\',\"string\",\"string\",\"string\",\"string\"]],\\n    \"mapi_query\":[\\'int\\',[\"int\",\"string\"]],\\n    \"mapi_fetch_row\":[\"int\",[\"int\"]],\\n    \"mapi_fetch_field\":[\"string\",[\"int\",\"int\"]]\\n});\\n\\n\\nvar res = libmylibrary.mapi_connect(\"localhost\", 50000,\"monetdb\", \"monetdb\", \"sql\", \"demo\");\\nconsole.log(res);\\nvar ret=libmylibrary.mapi_query(res,\"select * from table\");\\nwhile(libmylibrary.mapi_fetch_row(ret)){\\n    console.log(libmylibrary.mapi_fetch_field(ret,0));\\n    console.log(libmylibrary.mapi_fetch_field(ret,1));\\n}\\n</code></pre>\\n\\n<p><strong>Update 2:</strong><br>\\nAbove code is not recommended for production use...it does not use async functionality of node.js so please use it for baby steps</p>\\n-Is it possible to create a node.js module which uses c libraries for monetdb connection?-<javascript><node.js><database><monetdb><node-ffi>',\n",
       " \"<h1>Here is my simple BTEQ Script</h1>\\n\\n<pre><code>.LOGON 127.0.0.1/tduser,tduser\\n.EXPORT DATA FILE=C:\\\\Documents and Settings\\\\Owner\\\\Desktop\\\\Study\\\\Google Drive\\\\TD\\\\BTEQ\\\\Exported_File.txt\\n    SELECT      account_number\\n    FROM    samples.accounts\\n    WHERE   balance_current  &lt; 500 ;\\n.EXPORT RESET\\n.QUIT\\n</code></pre>\\n\\n<p>=====================================================================================</p>\\n\\n<h1>The output of the script:</h1>\\n\\n<pre><code>BTEQ 12.00.00.01 Tue Mar 26 19:58:59 2013\\n\\n+---------+---------+---------+---------+---------+---------+---------+----\\n.LOGON 127.0.0.1/tduser,\\n\\n *** Logon successfully completed.\\n *** Teradata Database Release is 12.00.00.10\\n *** Teradata Database Version is 12.00.00.10\\n *** Transaction Semantics are BTET.\\n *** Character Set Name is 'ASCII'.\\n\\n *** Total elapsed time was 1 second.\\n\\n+---------+---------+---------+---------+---------+---------+---------+----\\n.EXPORT DATA FILE=C:\\\\Documents and Settings\\\\Owner\\\\Desktop\\\\Study\\\\Google Driv\\ne\\\\TD\\\\BTEQ\\\\Exported_File.txt\\n *** To reset export, type .EXPORT RESET\\n+---------+---------+---------+---------+---------+---------+---------+----\\n SELECT   account_number\\n FROM  samples.accounts\\n WHERE  balance_current  &lt; 500 ;\\n\\n *** Success, Stmt# 1 ActivityCount = 4\\n *** Query completed. 4 rows found. One column returned.\\n *** Total elapsed time was 1 second.\\n\\n\\n+---------+---------+---------+---------+---------+---------+---------+----\\n.EXPORT RESET\\n *** Output returned to console.\\n+---------+---------+---------+---------+---------+---------+---------+----\\n.QUIT\\n *** You are now logged off from the DBC.\\n *** Exiting BTEQ...\\n *** RC (return code) = 0\\n</code></pre>\\n\\n<p>=====================================================================================</p>\\n\\n<p>Once, I check the file it shows invalid character's in the text file. Any possible workaround? Unicode problem? I am not sure.I even tried pasting the file content below, but its empty.\\n <br>\\n <br>\\n <br>\\n    </p>\\n\\n<p>Thanks in advance.</p>\\n-Teradata BTEQ Export Script Resulting Invalid Character in O/P File-<teradata>\",\n",
       " \"<p>When using multiple CTE's in MSSQL 2008, I normally separate them with a comma.</p>\\n\\n<p>But when I try this in a Teradata environment, I get an error with the syntax.</p>\\n\\n<p>Works in MS SQL:</p>\\n\\n<pre><code>WITH CTE1 AS \\n(SELECT TOP 2 Name FROM Sales.Store)\\n,CTE2 AS \\n(SELECT TOP 2 ProductNumber, Name FROM Production.Product)\\n,CTE3 AS \\n(SELECT TOP 2 Name FROM Person.ContactType)\\nSELECT * FROM CTE1,CTE2,CTE3\\n</code></pre>\\n\\n<p>Now, attempting to put into Teradata syntax:</p>\\n\\n<pre><code>WITH RECURSIVE CTE1 (Name) AS \\n(SELECT TOP 2 Name FROM Sales.Store)\\n,RECURSIVE CTE2 (ProductNumber, Name) AS \\n(SELECT TOP 2 ProductNumber, Name FROM Production.Product)\\n,RECURSIVE CTE3 (Name) AS \\n(SELECT TOP 2 Name FROM Person.ContactType)\\nSELECT * \\nFROM CTE1,CTE2,CTE3\\n</code></pre>\\n\\n<blockquote>\\n  <p>Syntax error, expected something like a name or a Unicode delimited\\n  identifier between ',' and the 'RECURSIVE' keyword.</p>\\n</blockquote>\\n\\n<p>2nd attempt (without using RECURSIVE multiple times)</p>\\n\\n<pre><code>WITH RECURSIVE CTE1 (Name) AS \\n(SELECT TOP 2 Name FROM Sales.Store)\\n,CTE2 (ProductNumber, Name) AS \\n(SELECT TOP 2 ProductNumber, Name FROM Production.Product)\\n,CTE3 (Name) AS \\n(SELECT TOP 2 Name FROM Person.ContactType)\\nSELECT * \\nFROM CTE1,CTE2,CTE3\\n</code></pre>\\n\\n<blockquote>\\n  <p><em>Multiple WITH definitions are not supported.</em></p>\\n</blockquote>\\n-Teradata SQL Syntax - Common Table Expressions-<sql><common-table-expression><teradata><recursive-query>\",\n",
       " \"<p>I am trying to load SAS dataset into a teradata table using FASTLOAD utility. This works fine in some cases, but I want to separate the error tables and create them in my own/other database in teradata environment. </p>\\n\\n<p>Could some one provide me the syntax (I do know it but it's not working)  for how to make it possible?</p>\\n\\n<p>Any method is fine either using <code>proc sql command</code> or <code>proc append command</code>. Thanks in advance.</p>\\n-Load SAS dataset into Teradata table using Fast LOAD-<sas><teradata>\",\n",
       " '<p>I have special character in Teradata database, and I want to remove it.</p>\\n\\n<pre><code>Example - special character: \"aa€bb\"  (special charcter is \"€\" sign)\\nDesired output: \"aabb\"\\n</code></pre>\\n\\n<p>How can I do that?</p>\\n-Replacing Special Characters in Teradata-<teradata>',\n",
       " '<p>I deploy some .bteq and .sql scripts on a TERADATA database. For doing this, I use a client on my desktop called BTEQWin version 13.10.0.03.</p>\\n\\n<p>I get the .bteq/.sql from a version control like pvcs/svn etc and all I do once the files are in my workspace folder (from Version control tool), to just drag and drop the files from Windows browser to BTEQWin client (which I connect to a database prior to drag/drop for running those scripts).</p>\\n\\n<p>Now, I have to automate this whole process in UNIX.</p>\\n\\n<p>I have written a SHELL KSH/BASH script which is getting all the .bteq/.sql from a TAG/LABEL in the version control tool to a given UNIX folder. Now, all I need to do is the pass these files one by one (i\\'ll take care of the order) to Teradata client.</p>\\n\\n<p>My ?\\n- what client do I need to tell Unix admin team to install on Unix server - so that I can run something like below:</p>\\n\\n<p>someTeraDataCommand -u username -p password -h hostname -d database -f filenametoexectue | tee output_filename.log</p>\\n\\n<p>Where, someTeraDataCommand is the client / executable - which will let me run Teradata scripts (like I was doing using BTEQWin on my desktop - GUI session). Other parameters can be username, password, which database to connect on what server and which file to run or make that file passed to the command using \"&lt;\" operator at command line.</p>\\n\\n<p>Any idea?\\n- What client ?</p>\\n-Teradata client on Unix Solaris-<sql><shell><command><teradata>',\n",
       " '<p>I have a text classification process in RapidMiner. It reads the test data from specified excel ssheet and does the classification. I have also a small Java application which is just running this process. Now I want to make the file input part in my aplication, so that everytime I would be able to specify the excel file from my application (not from RapidMiner).\\nAny hints?</p>\\n\\n<p>This is the code:</p>\\n\\n<pre><code>import com.rapidminer.RapidMiner;\\nimport com.rapidminer.Process;\\nimport com.rapidminer.example.Attribute;\\nimport com.rapidminer.example.Example;\\nimport com.rapidminer.example.ExampleSet;\\nimport com.rapidminer.operator.IOContainer;\\nimport com.rapidminer.operator.Operator;\\nimport com.rapidminer.operator.OperatorException;\\n\\n\\n\\nimport java.io.File;\\nimport java.io.IOException;\\nimport java.util.Iterator;\\nimport com.rapidminer.operator.io.ExcelExampleSource; \\nimport com.rapidminer.tools.XMLException;\\n\\n\\npublic class Classification {\\n\\n    public static void main(String [] args) throws Exception{\\n         ExampleSet resultSet1 = null;\\n         IOContainer ioInput = null;\\n        IOContainer ioResult;\\n        try {\\n            RapidMiner.setExecutionMode(RapidMiner.ExecutionMode.COMMAND_LINE);\\n            RapidMiner.init();\\n            Process pr = new Process(new File(\"C:\\\\\\\\Users\\\\\\\\MP-TEST\\\\\\\\Desktop\\\\\\\\Rapid_Test\\\\\\\\Wieder_Model.rmp\"));\\n            Operator op = pr.getOperator(\"Read Excel\");\\n            op.setParameter(ExcelExampleSource.PARAMETER_EXCEL_FILE, \"C:\\\\\\\\Users\\\\\\\\MP-TEST\\\\\\\\Desktop\\\\\\\\Rapid_Test\\\\\\\\HaendlerRatings_neu.xls\");\\n            ioResult = pr.run(ioInput);\\n            if (ioResult.getElementAt(0) instanceof ExampleSet) {\\n                resultSet1 = (ExampleSet)ioResult.getElementAt(0);\\n\\n                for (Example example : resultSet1) {\\n                    Iterator&lt;Attribute&gt; allAtts = example.getAttributes().allAttributes();\\n                    while(allAtts.hasNext()) {\\n                        Attribute a = allAtts.next();\\n                                if (a.isNumerical()) {\\n                                        double value = example.getValue(a);\\n                                        System.out.println(value);\\n\\n                                } else {\\n                                        String value = example.getValueAsString(a);\\n                                        System.out.println(value);\\n                                }\\n                         }\\n                }\\n                    }\\n        } catch (IOException | XMLException | OperatorException e) {\\n            // TODO Auto-generated catch block\\n            e.printStackTrace();\\n        }\\n\\n\\n\\n\\n          }\\n}\\n</code></pre>\\n\\n<p>This is the error:</p>\\n\\n<pre><code>Apr 09, 2013 9:06:05 AM com.rapidminer.Process run\\nINFO: Process C:\\\\Users\\\\MP-TEST\\\\Desktop\\\\Rapid_Test\\\\Wieder_Model.rmp starts\\ncom.rapidminer.operator.UserError: A value for the parameter \\'excel_file\\' must be specified! \\n    at com.rapidminer.operator.nio.model.ExcelResultSetConfiguration.makeDataResultSet(ExcelResultSetConfiguration.java:316)\\n    at com.rapidminer.operator.nio.model.AbstractDataResultSetReader.createExampleSet(AbstractDataResultSetReader.java:127)\\n    at com.rapidminer.operator.io.AbstractExampleSource.read(AbstractExampleSource.java:52)\\n    at com.rapidminer.operator.io.AbstractExampleSource.read(AbstractExampleSource.java:1)\\n    at com.rapidminer.operator.io.AbstractReader.doWork(AbstractReader.java:126)\\n    at com.rapidminer.operator.Operator.execute(Operator.java:855)\\n    at com.rapidminer.operator.execution.SimpleUnitExecutor.execute(SimpleUnitExecutor.java:51)\\n    at com.rapidminer.operator.ExecutionUnit.execute(ExecutionUnit.java:711)\\n    at com.rapidminer.operator.OperatorChain.doWork(OperatorChain.java:379)\\n    at com.rapidminer.operator.Operator.execute(Operator.java:855)\\n    at com.rapidminer.Process.run(Process.java:949)\\n    at com.rapidminer.Process.run(Process.java:873)\\n    at com.rapidminer.Process.run(Process.java:832)\\n    at com.rapidminer.Process.run(Process.java:827)\\n    at Classification.main(Classification.java:29)\\n</code></pre>\\n\\n<p>Best regards</p>\\n\\n<p>Armen</p>\\n-Integration of RapidMiner in Java application-<java><rapidminer>',\n",
       " \"<p>I am working with visual studio's 2010 and using .Net Data Provider for Teradata 14.0 and receiving the following error.  [Teradata Database] [3710] Insufficient memory to parse this request, during QueryRewrite phase.  I am trying to bring in a simple small table using ADO.net entity model.</p>\\n\\n<p>Anyone know how to fix this error?</p>\\n-Teradata Database 3710 Insufficient memory to parse this request, during QueryRewrite phase-<visual-studio-2010><teradata>\",\n",
       " '<p>I am building an app in extjs connecting to teradata at the back-end.  It works fine locally but when deployed its giving </p>\\n\\n<blockquote>\\n  <p>[Error 1277] [SQLState 08S01] Login time-out for Connection to server after 12seconds. </p>\\n</blockquote>\\n\\n<p>I am trying to increase the timeout now. </p>\\n\\n<pre><code>Class.forName(\"com.teradata.jdbc.TeraDriver\");\\nDriverManager.setLoginTimeout(100);\\nConnection conn = DriverManager.getConnection(connectionString, \"user\", \"pass\");\\n</code></pre>\\n\\n<p>Still I\\'m getting same time-out error after 12 seconds. It seems like setLoginTimeout didn\\'t work. Where am I going wrong? Is there any other solution other than increasing time-out ? </p>\\n\\n<p>P.S: For one server it worked fine now I changed only the server name to point to another server and I\\'m getting timeout. </p>\\n-Teradata setLoginTimeout not working-<jdbc><timeout><teradata>',\n",
       " \"<p>I'm using Rapidminer to do an analysis. I used cross-validation on several models to get the best working model. Now I want to use this model to test on a separate testset that I made using Split Data to estimate the performance. </p>\\n\\n<p>How do I use the test set? As far as I can tell, all the validation modules use the training set that the model was made on. Which performance measure can I use that takes in a model and my test set?</p>\\n-How to test on testset using Rapidminer?-<performance><evaluation><rapidminer>\",\n",
       " '<p>I want to apply a decision tree learning algorithm to a dataset I have imported from a CSV.\\nThe problem is that the \"tra\" input of the Decision Tree block is still red, stating \"Input example set must have special attribute \\'label\\'.\".\\nHow do I add that label? What is it?\\nI have been playing around with it for some time without results. Any help appreciated.</p>\\n\\n<p>ADDENDUM: the column\\'s titles have been correctly inferred, so I have no clue on what the \\'label\\' is.</p>\\n-rapid miner: how to add a \\'label\\' attribute to a dataset?-<machine-learning><data-mining><decision-tree><rapidminer>',\n",
       " '<p>I\\'ve looked all over, and I just can\\'t find a clear answer on the Eclipse pages on upgrading milestones (or really upgrading general) What is the correct way to upgrade an Eclipse 4.3 milestone to the next milestone, for example 4.3 M5 to 4.3 M6.</p>\\n\\n<p>Even after adding <a href=\"http://download.eclipse.org/eclipse/updates/4.3milestones\" rel=\"nofollow noreferrer\">http://download.eclipse.org/eclipse/updates/4.3milestones</a>, the core Eclipse components do not show up when checking for updates.</p>\\n\\n<p>Do I need to explicitly install the new milestone and then copy over the packages even for milestones (as described in <a href=\"https://stackoverflow.com/a/11264210/255961\">https://stackoverflow.com/a/11264210/255961</a>)? </p>\\n\\n<p>There is a related question at <a href=\"https://stackoverflow.com/questions/13585438/upgrading-eclipse-ide-for-java-ee-developers\">Upgrading Eclipse IDE for Java EE Developers</a>, but its focused on 4.2 and complicated with the general question of fixing the Juno slowness rather than how to upgrade.</p>\\n-How to upgrade an Eclipse 4.2 milestone (e.g. 4.3 M5 to 4.3 M6)-<eclipse><eclipse-kepler>',\n",
       " \"<p>I have a table which has the primary key on one column and is partitioned by a date column. This is sample format of the DDL: </p>\\n\\n<pre><code>CREATE MULTISET TABLE DB.TABLE_NAME,\\nNO FALLBACK ,\\nNO BEFORE JOURNAL,\\nNO AFTER JOURNAL,\\nCHECKSUM = DEFAULT,\\nDEFAULT MERGEBLOCKRATIO\\n(  FIRST_KEY                 DECIMAL(20,0) NOT NULL,\\n   SECOND_KEY                DECIMAL(20,0) ,\\n   THIRD_COLUMN              VARCHAR(5),     \\n   DAY_DT                    DATE FORMAT 'YYYY-MM-DD')\\nPRIMARY INDEX TABLE_NAME_IDX_PR (FIRST_KEY)\\nPARTITION BY RANGE_N(DAY_DT  BETWEEN DATE '2007-01-06' \\n                                 AND DATE '2016-01-02' EACH  INTERVAL '1' DAY );\\n\\nCOLLECT STATS ON DB.TABLE_NAME COLUMN(FIRST_KEY);\\n</code></pre>\\n\\n<p>The incoming data can be of size 30 million each day and I have loaded the data for 2012-04-11. Now i have to collect stats for only '2012-04-11' partition instead of whole table.</p>\\n\\n<p>Is there any way to collect partition for a particular day?</p>\\n-Collect statistics for a single partition in Teradata-<teradata>\",\n",
       " '<p>I am trying to upload a folder with three excel files in it to Rapidminer at once. \\nWhat operator do I need to use to do so (without selecting each of them and use <code>read excel</code> operator)? </p>\\n-How to import multiple excel files to Rapidminer-<rapidminer>',\n",
       " \"<p>I am able to do the following query  </p>\\n\\n<pre><code>select (current_date-interval '1' day) ,a,b from (select '1' as a, 2 as b) as t2;\\n</code></pre>\\n\\n<p>But I am not able to put variables in place  of '1'.\\nI have tried the following methods with no success  </p>\\n\\n<pre><code>select (current_date-interval b day) ,a,b from (select '1' as a, 2 as b) as t2;\\nselect (current_date-interval a day) ,a,b from (select '1' as a, 2 as b) as t2;\\n</code></pre>\\n\\n<p>I have also tried casting but still no result.</p>\\n-Date queries in monetdb?-<sql><database><monetdb>\",\n",
       " '<p>I am working with a Teradata warehouse, and I am using row_number in one of my sql scripts. The record set I am trying to use my script on is larger than the maximum of the integer value threshold. What to do in such a situation? </p>\\n\\n<p>(casting the \"row_number() over (par..)\" expression to bigint did not work)</p>\\n\\n<p>If you have found a solution for this problem in another DBMS, then I also welcome your solution as it might work in Teradata too. </p>\\n-Row_Number() over 2 billion records (Teradata)-<sql><teradata>',\n",
       " '<p>Q1: I want to convert a unix timestamp (INT) to monetdb timestamp (\\'YYYY-MM-DD HH:MM:SS\\') format\\nbut it is giving me the GMT time not my actual time.</p>\\n\\n<p>When I do  </p>\\n\\n<pre><code>select (epoch(cast(current_timestamp as timestamp))-epoch(timestamp \\'2013-04-25 11:49:00\\'))\\n</code></pre>\\n\\n<p>where 2013-04-25 11:49:00 is my systems current time it gives the same difference</p>\\n\\n<p>I tried using  </p>\\n\\n<pre><code>set time zone interval \\'05:30\\' HOUR TO MINUTE;\\n</code></pre>\\n\\n<p>but it did not change the result</p>\\n\\n<p>How can I solve this problem??</p>\\n\\n<p>Example Problem:<br>\\nI wanted to convert unix timestamp 1366869289 which should be around \"2013-04-25 11:25:00\" but monetdb gives \"2013-04-25 05:55:00\"</p>\\n-How to convert a unix timestamp (INT) to monetdb timestamp (\\'YYYY-MM-DD HH:MM:SS\\') local time format-<sql><timezone><epoch><monetdb>',\n",
       " '<p>I need to execute a query on a teradata database on a daily basis (select + insert).</p>\\n\\n<p>Can this be done within the (teradata-) database or should I consider external means (e.g. a cron-job).</p>\\n-Teradata Event/Time triggered execution of SQL-<scheduled-tasks><teradata>',\n",
       " '<pre><code>debian@debian:~$ echo $PYTHONPATH  \\n/home/qiime/lib/:  \\ndebian@debian:~$ python  \\nPython 2.7.3 (default, Jan  2 2013, 16:53:07)   \\n[GCC 4.7.2] on linux2  \\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.  \\n&gt;&gt;&gt; import sys  \\n&gt;&gt;&gt; sys.path  \\n[\\'\\', \\'/usr/local/lib/python2.7/dist-packages/feedparser-5.1.3-py2.7.egg\\',   \\n\\'/usr/local/lib/python2.7/dist-packages/stripogram-1.5-py2.7.egg\\', \\'/home/qiime/lib\\', \\n\\'/home/debian\\', \\'/usr/lib/python2.7\\', \\'/usr/lib/python2.7/plat-linux2\\',   \\n\\'/usr/lib/python2.7/lib-tk\\', \\'/usr/lib/python2.7/lib-old\\', \\'/usr/lib/python2.7/lib-\\ndynload\\',   \\'/usr/local/lib/python2.7/dist-packages\\', \\'/usr/lib/python2.7/dist-packages\\', \\n\\'/usr/lib/python2.7/dist-packages/PIL\\', \\'/usr/lib/python2.7/dist-packages/gst-0.10\\',  \\n\\'/usr/lib/python2.7/dist-packages/gtk-2.0\\', \\'/usr/lib/pymodules/python2.7\\']    \\n</code></pre>\\n\\n<p>How can I get all of <code>PYTHONPATH</code> output in bash?<br>\\nWhy  <code>echo  $PYTHONPATH</code> can not get all of them?</p>\\n-How to get the PYTHONPATH in shell?-<python><linux><pythonpath>',\n",
       " \"<p>I have the following SQL statement:</p>\\n\\n<pre><code>select  cast (count(*) as bigint) from \\n(SELECT  oldtable.id,oldtable.day,newtable.newid from oldtable\\nleft outer join newtable on oldtable.day between newtable.FROM_DAY \\nand newtable.TO_DAY and oldtable.id = newtable.id) a\\n</code></pre>\\n\\n<p>this results 4.5 billion</p>\\n\\n<p>but when I say this:</p>\\n\\n<pre><code>INSERT  INTO AnotherTable\\n(id, day, newid)\\nSELECT  oldtable.id,oldtable.day,newtable.newid from oldtable\\nleft outer join newtable on oldtable.day between newtable.FROM_DAY \\nand newtable.TO_DAY and oldtable.id = newtable.id\\n</code></pre>\\n\\n<p>it only inserts 300 million records (oldtable contains 4.5 billion records, newtable 430 million). </p>\\n\\n<p>Why?</p>\\n\\n<p>The definiton of AnotherTable:</p>\\n\\n<pre><code>CREATE MULTISET TABLE AnotherTable ,NO FALLBACK ,\\n NO BEFORE JOURNAL,\\n NO AFTER JOURNAL,\\n CHECKSUM = DEFAULT,\\n DEFAULT MERGEBLOCKRATIO\\n (  \\n  id INTEGER NOT NULL,\\n  day DATE FORMAT 'YYYY-MM-DD',  \\n   newid INTEGER NOT NULL\\n )\\n PRIMARY INDEX ( id) \\n PARTITION BY RANGE_N(day  BETWEEN DATE '2000-09-20' AND DATE '2030-02-15' EACH INTERVAL '1' DAY );\\n</code></pre>\\n\\n<p>I made the following checking: </p>\\n\\n<pre><code> SELECT oldtable.id,oldtable.day,newtable.newid from oldtable \\n left outer join newtable on oldtable.day between newtable.FROM_DAY and newtable.TO_DAY\\n and oldtable.id = newtable.id \\n where newtable.newid is null \\n</code></pre>\\n\\n<p>It resulted 0 records, so outer join is not needed at all, I just use it here to demonstrate that the record number is different, but it shouldn't be</p>\\n-Insert from Select (Teradata)-<sql><teradata>\",\n",
       " '<p>I am able to run Normal DML scripts (INSERT, UPDATE &amp; DELETE) scripts through VBA ODBC connection.</p>\\n\\n<p><strong>But Is it possible to run FLOAD or MLOAD scripts through VBA ODBC ?</strong> </p>\\n\\n<p>I have large set of data and i want to write a VBA code for FLOAD / MLOAD to insert the data very quickly rather than normal DML scripts.</p>\\n\\n<p>I heard that FLOAD / MLOAD can be executed from Unix or Mainframe systems. Is it possible to run from VBA ? If yes can you please help with some sample ?</p>\\n\\n<p>Below is sample Teradata FLOAD script.</p>\\n\\n<pre><code>.LOGTABLE DB.Table_Log;\\n.LOGON oneview/uid,pwd;\\n.BEGIN IMPORT MLOAD TABLES DB.FRC_RPT_PERD SESSIONS 5;\\n.LAYOUT InputFile_layout;\\n....\\n....\\n</code></pre>\\n-Is it possible to execute Teradata FastLoad or MultiLoad scripts through VBA?-<vba><excel><odbc><teradata>',\n",
       " '<p>Sorry for the poor title/description. What is happening is I\\'m joining to another table by ID but the data is coming from two different admin systems (CLONE and GRASN) Is there any code I can add to say \"if CLONE and GRASN exist, use CLONE\"?</p>\\n\\n<p>a simple query looks like this:</p>\\n\\n<pre><code> ` select sorce_claim_id\\n  ,claim_sorce_syst_cd\\n  ,sorce_agrmt_id\\n  ,asgn_sorce_syst_cd\\n  from edw_p.claim_agrmt\\n  where sorce_claim_id = \\'4513049\\'`\\n</code></pre>\\n\\n<p>and is returning the following:</p>\\n\\n<pre><code>    SORCE_CLAIM_ID  CLAIM_SORCE_SYST_CD SORCE_AGRMT_ID  ASGN_SORCE_SYST_CD\\n4513049             NVLNK              00932419       GRASN\\n4513049             NVLNK              R0035600       CLONE\\n</code></pre>\\n\\n<p>I would like only to pull the second record. I can\\'t just say WHERE ASGN_SORCE_SYST_CD = CLONE as that will bring only the clones back. The table basically consists of mostly GRASN but there are duplicate records (like in this case) where if there are, I want to use the CLONE record.</p>\\n\\n<p>I hope this makes enough sense. I\\'m working in Teradata.</p>\\n-SQL If two records exist, choose which one you want to display Teradata-<sql><olap><teradata>',\n",
       " \"<p>i have recently discovered MonetDB and i am evaluating it for an internal project, so probably my questions are from a really newbie point of view. Maybe someone could point me to a site and/or document where i could find more info (i haven't found too much googling)</p>\\n\\n<p>regarding scalability, correct me please if i am wrong, but what i understand is that if i need to scale, i would launch more server instances and discover them from the control node, is it right?\\nis there any limit on the number of servers?</p>\\n\\n<p>the other point is about storage, is it possible to use amazon S3 to back MonetDB readonly instances?</p>\\n\\n<p><strong>update</strong> we would need to store a massive amount of Call Detail Records from different sources, on a read-only basis. We would aggregate/reduce that data for the day-to-day operation, accessing the bigger tables only when the full detail is required. \\nWe would store the historical data as well to perform longer-term analysis. My concern is mostly about memory, disk storage wouldn't be the issue i think; if the hot dataset involved in a report/analysis eats up the whole memory space (fast response times needed, not sure about how memory swapping would impact), i would like to know if i can scale somehow instead of reingeneering the report/analysis process (maybe i am biased by the horizontal scaling thing :-) )</p>\\n\\n<p>thanks!</p>\\n-monetdb in the cloud, scalability, amazon s3-<amazon-s3><scalability><monetdb>\",\n",
       " \"<p>I was not able to start rapidminer by double clicking on lib/rapidminer.jar. I also tried java -jar lib/rapidminer.jar that also doesn't work.</p>\\n\\n<p>I tried <code>./scripts/rapidminer</code> and rapid miner started</p>\\n\\n<pre><code>RAPIDMINER_HOME is not set. Trying the directory '.'...\\nNo maximum Java memory defined, using 1024 Mb...\\nJava version: 16\\ngrep: /proc/cpuinfo: No such file or directory\\nNumber of cores: 4\\nNumber of GC Threads: 3\\nStarting RapidMiner from '.', using classes from './lib/rapidminer.jar'...\\nJava version: 16\\nStarting with CMS garbage collector\\nStarting in multi-core mode\\nRapidMiner version 5.3.008, Copyright (C) 2001-2012\\nRapidMiner comes with ABSOLUTELY NO WARRANTY; This is free software,\\nand you are welcome to redistribute it under certain conditions;\\nsee license information in the file named LICENSE.\\nUsage: com.rapidminer.RapidMinerCommandLine [-f] PROCESS [-Mname=value]\\n  PROCESS       a repository location containing a process\\n  -f            interpret PROCESS as a file rather than a repository location (deprecated)\\n  -Mname=value  sets the macro 'name' with the value 'value'\\n</code></pre>\\n\\n<p>I want to launch the GUI also\\nI tried : ./scripts/RapidMinerGUI</p>\\n\\n<pre><code>RAPIDMINER_HOME is not set. Trying the directory '.'...\\ngrep: /proc/cpuinfo: No such file or directory\\nNumber of cores: 4\\nNumber of GC Threads: 3\\nNo maximum Java memory defined, using 1024 Mb...\\nStarting RapidMiner from '.', using classes from './lib/rapidminer.jar'...\\nJava version: 16\\nStarting with CMS garbage collector\\nStarting in multi-core mode\\nMay 7, 2013 2:38:31 PM com.rapid_i.Launcher ensureRapidMinerHomeSet\\nINFO: rapidminer.home is '.'.\\nMay 7, 2013 2:38:31 PM com.rapid_i.Launcher main\\nINFO: Launching RapidMiner, platform ANY\\nMay 7, 2013 2:38:32 PM com.rapidminer.tools.FileSystemService getUserRapidMinerDir\\nWARNING: Unable to create user home rapidminer directory /Users/.RapidMiner5\\nMay 7, 2013 2:38:32 PM com.rapidminer.tools.ParameterService init\\nINFO: Reading configuration resource com/rapidminer/resources/rapidminerrc.\\nMay 7, 2013 2:38:32 PM com.rapidminer.tools.FileSystemService getUserRapidMinerDir\\nWARNING: Unable to create user home rapidminer directory /Users/.RapidMiner5\\nMay 7, 2013 2:38:32 PM com.rapidminer.tools.I18N &lt;clinit&gt;\\n</code></pre>\\n\\n<p>Please help me run the rapidminer gui in Mac.</p>\\n-Unable to start the graphical user interface of RapidMiner in Mac OS-<rapidminer>\",\n",
       " '<p>I have the client/TTU installed on Unix box for Teradata.</p>\\n\\n<p>If I do the following, it works. Where \"...\" is Teradata BTEQ normal output and once the following is done, I\\'m back at the prompt.</p>\\n\\n<p><strong>$ bteq</strong>\\n<br/>...<br/>\\n....<br/></p>\\n\\n<pre><code>. logon dbname/dbuser,dbpassword\\nSELECT DATE, TIME;\\n.LOGOFF;\\n.QUIT;\\n</code></pre>\\n\\n<p>..<br/>\\n...<br/>\\n<strong>$</strong></p>\\n\\n<p>Now, lets say I put the following lines in a file called \"<strong><em>testtd.bteq</em></strong>\"</p>\\n\\n<pre><code>. logon dbname/dbuser,$dbpassword\\nSELECT DATE, TIME;\\n.LOGOFF;\\n.QUIT;\\n</code></pre>\\n\\n<p>What I want now is ... how can I run this script (.bteq) at Unix $ prompt ???</p>\\n\\n<p>I tried the following methods so far, but they didn\\'t work, may be Im missing anything: <br/>\\n1. bteq &lt; /path/to/testtd.bteq > testtd.log<br/><br/>\\n2. bteq &lt;\\n.run /path/to/testtd.bteq<br/>\\nHereDocEndsHere<br/></p>\\n\\n<p>Any ideas? DO I HAVE to provide \". logon dbname/dbuser,dbpassword\" FIRST, if I\\'m using the HereDocument way?</p>\\n\\n<p>Running bteq command on $ prompt doesn\\'t even give me any HELP/options that I can use, like other commands.</p>\\n\\n<p>i.e.\\ncmd -u user -p password -f file etc... </p>\\n-Teradata - run a file/script at Unix Linux command prompt-<unix><client><teradata><automated-deploy>',\n",
       " \"<p>I'm trying to do multiple select from diff tables and just have a result in one column.</p>\\n\\n<pre><code>SELECT COUNT(*) FROM tb1 union\\nSELECT COUNT(*) FROM tb2 union\\nSELECT COUNT(*) FROM tb3;\\n</code></pre>\\n\\n<p>output should be like:</p>\\n\\n<pre><code> 593643\\n 18103600\\n 0\\n</code></pre>\\n\\n<p>Problem with this is that the result is being arranged on desc order.\\nLike below:</p>\\n\\n<pre><code> 0\\n 593643\\n 18103600\\n</code></pre>\\n\\n<p>I would want the result to be as I put the select statement.\\nPlease advise. Btw, I'm using teradata.\\nThank you.</p>\\n-multiple select in one query [Teradata]-<sql><oracle><select><count><teradata>\",\n",
       " '<p>Which data type is appropriate to store address details?\\nHow to specifiy new line character while inserting data into this column?</p>\\n-Teradata Store multiple line value like address in a single column-<teradata>',\n",
       " \"<p>i've a TFIDF vectors that is saved in a MYSQL table inside the database the table has the following schema :</p>\\n\\n<pre><code>id | docid | word    | weight | class/label | timestamp\\n1  | 1     | argon   | 0.2123 | pos         | 2013-03-25 16:22:48\\n2  | 1     | apple   | 0.1523 | pos         | 2013-03-25 16:22:48\\n3  | 2     | orange  | 0.8823 | pos         | 2013-03-25 16:22:48\\n4  | 2     | diffuse | 0.9812 | pos         | 2013-03-25 16:22:48\\n5  | 3     | master  | 0.2653 | neg         | 2013-03-25 16:22:48\\n6  | 3     | mouse   | 0.7623 | neg         | 2013-03-25 16:22:48\\n</code></pre>\\n\\n<p>the vectors of all documents are on the same tables vertically and differentiated by the <code>docid</code> field</p>\\n\\n<p>i want to load them inside RapidMiner in order to build a classifier for pos and neg classes \\nand as far as i know the format that the Classifier model in RapidMiner accepts is each document is represented horizontally in a row , like this : </p>\\n\\n<pre><code>docid | class/label | argon |apple   | orange  | diffuse | .... \\n1     | pos         | 0.154 |0       | 0.1326  | 0.7741  | ....\\n2     | pos         | 0.545 |0       | 0       | 0.77    | ....\\n3     | neg         | 0.565 |0.122   | 0.1555  | 0       | ....\\n</code></pre>\\n\\n<p>I can write some code to do this task and save them in CSV files and hence upload them to rapid miner , i wanted a more faster task within rapid miner. either by making it accepts the first format or change it to the second or even a MYSQL Query to do that.</p>\\n\\n<p>taking into consideration that vectors table is very large ( around 500 mb ) so scalability is an issue </p>\\n-formatting vectors when importing to Rapidminer-<mysql><sql><rapidminer>\",\n",
       " \"<p>In Teradata I can use a statement like ... </p>\\n\\n<pre><code>collect statistics on my_table column(col1)\\n</code></pre>\\n\\n<p>This will gather stats on the table and store them in DBC views like ColumnStats, IndexStats and MultiColumnStats.  I'm also under the impression that the optimizer (parsing engine) will find the statistics when they are available and use them instead of estimated table cardinality/index value counts to make better decisions on how to execute a query. </p>\\n\\n<p>This all sounds great, but I have some questions.</p>\\n\\n<ul>\\n<li>are there any disadvantages to using <code>collect stats</code>?</li>\\n<li>When is it appropriate/inappropriate to use collect statistics in your SQL scripting? </li>\\n<li>What's the performance benefit to collect statistics on a field that's already indexed?</li>\\n<li>How long are statistics stored for (table, volatile tables)?   </li>\\n<li>Any other comments concerning <code>collect statistics</code> would be appreciated. </li>\\n</ul>\\n-Using COLLECT STATISTICS in Teradata-<sql><database><query-optimization><teradata>\",\n",
       " '<p>I am using R JDBC and teradataR to connect R to teradata.  I would like to write a table using the function dbWriteTable, but am receiving this error</p>\\n\\n<p>[Error 3932] [SQLState 25000] Only an ET or null statement is legal after a DDL Statement.)</p>\\n\\n<p>This works perfectly fine using RMySQL on a MySQL database.</p>\\n\\n<p>There is a similar thread here where the problem was believed to be the lack of a \\'begin transaction\\', but turned out that wasn\\'t the case\\n<a href=\"http://forums.teradata.com/forum/extensibility/teradata-r-create-table-based-on-a-data-frame-using-jdbc\" rel=\"nofollow\">http://forums.teradata.com/forum/extensibility/teradata-r-create-table-based-on-a-data-frame-using-jdbc</a></p>\\n-insert into Teradata using R JDBC-<database><r><jdbc><teradata>',\n",
       " \"<p>I have data like this:</p>\\n\\n<pre><code>Date             User ID\\n2012-10-11         a\\n2012-10-11         b\\n2012-10-12         c\\n2012-10-12         d \\n2012-10-13         e\\n2012-10-14         b\\n2012-10-14         e\\n</code></pre>\\n\\n<p>What I want to do is group by the most recent two-day range (in my real query, it will be 7 days) for every day and get the count of distinct user IDs.</p>\\n\\n<p>For instance, I want the result to look like this:</p>\\n\\n<pre><code>Date             count(distinct userIDs)\\n2012-10-12         4\\n2012-10-13         3\\n2012-10-14         2\\n</code></pre>\\n\\n<p>For instance for 2012-10-12, I get a count of 4 because I have <em>'a'</em>, <em>'b'</em>, <em>'c'</em>, and <em>'d'</em>. ' <strong>==></strong>\\n<em>'a'</em> and <em>'b'</em> come from the previous day, and <em>'c'</em> and <em>'d'</em> come from the same day, 2012-10-12.</p>\\n\\n<p>Likewise, for 2012-10-13, I am looking at 2012-10-13 and 2012-10-12 and I get <em>'c'</em>, <em>'d'</em>, and <em>'e'</em>.</p>\\n\\n<p>Date column's data type is date. I am using Teradata.</p>\\n\\n<p>I've been trying to research it, but couldn't find a straightforward answer that applies to my situation yet. :-/ Sorry if this is a repetition. Your help is greatly appreciated. Thank you!</p>\\n-Group by date ranges (teradata)-<sql><date><group-by><teradata>\",\n",
       " '<p>I have data  as follows in a Table ( 3 columns ):</p>\\n\\n<pre><code>Name    StartDt         EndDt\\n A     01/01/2009    12/31/2009\\n A     01/01/2010    11/30/2010\\n B     03/01/2011    10/31/2011\\n A     04/01/2012    12/31/2012\\n A     01/01/2013    08/01/2013\\n</code></pre>\\n\\n<p>Now I want to create a Output using Terdata Sql query as follows:</p>\\n\\n<pre><code>Name    Min_Startdt    Max_Startdt\\n A       01/01/2009    11/30/2010\\n A       04/01/2012    08/01/2013\\n B       03/01/2011    10/31/2011\\n</code></pre>\\n\\n<p>Please let me how this can be achieved via a Teradata Query</p>\\n-Teradata Sql query-<sql><group-by><teradata>',\n",
       " '<p>I make a custom Stemmer in java and I need a operator for this code.\\nHow can I import a custom Java class in rapidminer 5?</p>\\n\\n<p>Thank you!!!</p>\\n-Import a custom class in RapidMiner process-<java><rapidminer><custom-operator>',\n",
       " \"<p>I have the new version of eclipse: Eclipse Kepler 4.3.\\nI'm looking for GWT plugin for that version of eclipse but I can not find.\\nIf I try to install the plugin for 4.2 eclipse version, what could happen?\\nThanks</p>\\n-GWT plugin for eclipse 4.3-<eclipse><gwt><installation><google-eclipse-plugin><eclipse-kepler>\",\n",
       " '<p>In eclipse, it\\'s possible to set not null by default for a single package.  Is there any way to set it across all packages, or for a tree of packages?</p>\\n\\n<p><strong>Edit</strong></p>\\n\\n<p>I\\'m talking about the <code>@Nonnull</code> annotations that eclipse respects and polices. At the moment, we are putting them on every function, every variable every parameter. I know we can just set them package wide, by putting the attribute in package-info - but we have hundreds of packages, and it\\'s easy to miss one. I am hoping to be able to set eclipse to say \"assume <code>@Nonnull</code> by default, unless <code>@Nullable</code> is specified\".</p>\\n-@Nonnull by default in eclipse/kepler-<java><eclipse><eclipse-kepler>',\n",
       " '<p>My requirement is to</p>\\n\\n<blockquote>\\n  <ol>\\n  <li>Move data from Oracle to HDFS</li>\\n  <li>Process the data on HDFS</li>\\n  <li>Move processed data to Teradata.</li>\\n  </ol>\\n</blockquote>\\n\\n<p>It is also required to do this entire processing every 15 minutes. The volume of source data may be close to 50 GB and the processed data also may be the same.</p>\\n\\n<p>After searching a lot on the internet, i found that</p>\\n\\n<blockquote>\\n  <ol>\\n  <li>ORAOOP to move data from Oracle to HDFS (Have the code withing the shell script and schedule it to run at the required interval).</li>\\n  <li>Do large scale processing either by Custom MapReduce or Hive or PIG.</li>\\n  <li>SQOOP - Teradata Connector to move data from HDFS to Teradata (again have a shell script with the code and then schedule it).</li>\\n  </ol>\\n</blockquote>\\n\\n<p>Is this the right option in the first place and is this feasible for the required time period (Please note that this is not the daily batch or so)?</p>\\n\\n<p>Other options that i found are the following</p>\\n\\n<blockquote>\\n  <ol>\\n  <li>STORM (for real time data processing). But i am not able to find the oracle Spout or Teradata bolt out of the box.</li>\\n  <li>Any open source ETL tools like Talend or Pentaho.</li>\\n  </ol>\\n</blockquote>\\n\\n<p>Please share your thoughts on these options as well and any other possibilities.</p>\\n-Move data from oracle to HDFS, process and move to Teradata from HDFS-<oracle><hadoop><teradata><apache-storm>',\n",
       " \"<p>I would like to do a bunch of statistics on a huge data set, so I'm looping through all the attributes with <em>Loop Attributes</em>. In the operator, I have an <em>Aggregation</em> operator, which doesn't work in itself of course. The solution would be doing a <em>Branch</em> on different data types.</p>\\n\\n<p>Does anybody know, how to do that?</p>\\n-How to branch based on attribute type in Rapidminer?-<rapidminer>\",\n",
       " '<p>I have a process in rapidminer that applied k mean clustering on my provided excel sheet records.\\nNow I want that when I add some new record from c# desktop application, it uses that trained model to generate its appropriate cluster id.\\nPlease suggest me some solution? </p>\\n-way to use rapidminer clustering model using C#-<c#><integration><data-mining><cluster-analysis><rapidminer>',\n",
       " '<p>After recently re-installing pydev (3rd party eclipse plugin), I was surprised to find that folding doesn\\'t work for some large files (25k+ lines). I used another (recent) version of pydev previously and folding worked fine. After quite some wondering/googling/fumbling through menus I stumbled upon :</p>\\n\\n<blockquote>\\n  <p><a href=\"http://pydev.org/history_pydev.html\" rel=\"nofollow\">Pydev Changelog</a>:\\n  Code folding marks won\\'t be shown on really large files for performance reasons</p>\\n</blockquote>\\n\\n<p>The files I\\'m working with <em>did have folds</em> in a 2.x release I had at that time, but the changelog states they were disabled under version 1.5.7. I am absolutely sure they worked in a more recent version. Could be a conflict with another plugin ?</p>\\n\\n<p>The real question is <strong><em>how do I force the folding feature to be enabled on big files</em></strong> (exactly where it is needed most that is) ?<br>\\nAn answer in the form of a patch - or just an indication of where in pydev code can this be disabled will be enough - if it is not possible via the GUI</p>\\n\\n<p>Eclipse Kepler - up to date</p>\\n-pydev enable folding on big files-<eclipse><pydev><eclipse-kepler><eclipse-luna>',\n",
       " '<p>I have this scenario in which purchase orders for each items falling under a particular date range (each 7 days) has to be aggregated.</p>\\n\\n<pre><code>ITEM_NUMBER  ORDERED_DATE  ORDER_QUANTITY\\n1            06/01/2013    5\\n1            06/02/2013    12\\n1            06/03/2013    3\\n1            06/06/2013    8\\n1            06/18/2013    7\\n1            06/21/2013    9\\n1            06/29/2013    3\\n1            06/30/2013    4\\n\\n2            06/03/2013    24\\n2            06/07/2013    1\\n2            06/19/2013    22\\n2            06/23/2013    12\\n2            06/30/2013    9\\n</code></pre>\\n\\n<p>The above records has to be aggregated into 7 days window with respect to ITEM like below</p>\\n\\n<pre><code>Item 1 - Orders from 06/01/2013 to 06/07/2013 as one group (7 days)\\nItem 1 - Orders from 06/18/2013 to 06/24/2013 as second group (7 days)\\nItem 1 - Orders from 06/29/2013 to 07/05/2013 as third group (7 days)\\nItem 2 - Orders from 06/03/2013 to 07/09/2013 as forth group (7 days) and so on\\n</code></pre>\\n\\n<p>Hence the final output should be</p>\\n\\n<pre><code>ITEM_NUMBER  MIN(ORDERED_DATE)  SUM(ORDER_QUANTITY)\\n1            06/01/2013         28\\n1            06/18/2013         16\\n1            06/29/2013         7\\n\\n2            06/03/2013         25\\n2            06/19/2013         34\\n2            06/30/2013         9\\n</code></pre>\\n\\n<p>Please apologize if this is a repetition.\\nThanks in advance!</p>\\n-Teradata - Group by date rage-<sql><teradata>',\n",
       " '<p>I want to get the number of users who have used the particular table or all the tables in any of the DML scripts in teradata.</p>\\n-How to find the count of users who viewed/used the table in the last 2 weeks in Teradata?-<sql><teradata>',\n",
       " \"<p>Glassfish 4.0 final gives a timeout everytime I try to start it in Debug mode.\\nThe normal run on server (from Eclipse) does work without a timeout.\\n<br/>I've tried running Glassfish 4.0 from Eclipse Juno and Eclipse Kepler, both have the correct port configuration for debugging (9009), both are failing to start Glassfish in debug.</p>\\n\\n<p>Is this a known problem? How do I fix this?</p>\\n-Eclipse - Glassfish 4 Debugging startup timeout-<eclipse-kepler><glassfish-4>\",\n",
       " \"<p>We're migrating database from Oracle to Teradata.</p>\\n\\n<p>We have <code>.sql</code> files with valid trigger DDL and <code>.bteq</code> files with <code>.compile</code> commands for these triggers. But when we run these <code>.bteq</code> files we get errors and trigger is not loaded.\\nFor example, file <code>td_instrg1.sql</code> contains trigger definition:</p>\\n\\n<pre><code>CREATE TRIGGER TD_INSTRG1\\nAFTER INSERT\\nON TD_EMPLOYEES\\nREFERENCING NEW AS X1\\nFOR EACH ROW\\nWHEN(X1.id is not null)\\nBEGIN ATOMIC\\n   (INSERT INTO TD_EMPLOYEES1     VALUES(X1.id, X1.name, X1.monthly_income);)\\nEND;\\n</code></pre>\\n\\n<p>and file td_instrg1.bteq contains the following commands:</p>\\n\\n<pre><code>.logon vmdbsrv016/dbc, dbc;\\n\\nDATABASE twm;\\n\\n.compile FILE=td_instrg1.sql;\\n\\n.logoff;\\n</code></pre>\\n\\n<p>Please advise how to load triggers from scripts using <code>bteq</code> utility.</p>\\n-How to load Triggers to Teradata Server using bteq-<oracle><migration><database-migration><teradata>\",\n",
       " \"<p>I want to create table in Teradata. Therefore I am using this syntax:</p>\\n\\n<pre><code>    CREATE VOLATILE TABLE a AS\\n    (\\n        Select * FROM ...\\n    ) WITH DATA PRIMARY INDEX ( ACCOUNT_ID )\\n;\\n</code></pre>\\n\\n<p>The inner SELECT statement results in 4 rows. However, when I run the entire query, the resulting data set does not have any rows. Strange, I know - that`s why I'm writing. Please help. Thanks.</p>\\n-Teradata Volatile Table Statement is not creating any rows-<teradata>\",\n",
       " '<p>Is there a function in Teradata, that \"makes\" a date by giving day, month any year as parameters?\\nSO if I have integer parameters p1_day, p2_month, and p3_year (which are, by the way, attributes), is there a function like </p>\\n\\n<pre><code>date_function (p1_day, p2_month, and p3_year) -&gt; for example \\'2013-12-11\\'\\n</code></pre>\\n-Date Function in Teradata-<teradata>',\n",
       " '<p>snakemake is a python-like replacement for make that is geared more towards workflows than compilation.  It\\'s quite nice, but also quite new, and I cannot seem to find a mode for it in Emacs.  I just want something very simple: a very slight modification from fundamental-mode, so I in perusing the emacs manual, I started the following in init.el:</p>\\n\\n<pre><code>(define-derived-mode snake-mode fundamental-mode\\n   ...\\n)\\n</code></pre>\\n\\n<p>like make, snakemake is strict about indents (actual tab \"\\\\t\" characters, not how emacs behaves by default when one types TAB).  When I instead type \"C-q TAB\" it puts a real tab character in the buffer : this works, I tried it with a Snakefile in fundamental-mode and it runs perfectly.  So to avoid typing \"C-q TAB\" each time I want a TAB in this mode, the first addition I would like to make to snake-mode is to rebind the TAB key to \"C-q TAB\" (or something like this).  So I perused the emacs manual and tried:</p>\\n\\n<pre><code>(define-derived-mode snake-mode fundamental-mode\\n  (global-set-key (kbd \"\\\\t\") (kbd \"C-q \\\\t\"))\\n  ...\\n)\\n</code></pre>\\n\\n<p>but this and other alternatives aren\\'t working ... maybe rebinding standard keys like the TAB key is not a recommended practice?</p>\\n\\n<p>the other addition to the snake-mode I would like is for it to highlight syntax according to python (but <em>not</em> have any python behaviour, e.g., python indenting behaviour)</p>\\n\\n<p>To conclude, just these 2 simple modifications to fundamental-mode in creating a \"snake-mode\" and a way to also invoke snake-mode if the filename is \"Snakefile\" was all I was looking for, but I\\'ve already spent several hours perusing the emacs manual and doing some googling, and it seems I\\'m not even close.  This is so simple, and I\\'m quite sure it is possible; any advice?</p>\\n\\n<p>Thanks</p>\\n\\n<p>Murray</p>\\n-emacs mode for snakemake?-<python><emacs><makefile><snakemake>',\n",
       " '<p>There is an old stack post (<a href=\"https://stackoverflow.com/questions/11204136/first-in-first-out-fifo-inventory-costing\">First-in-first-out (FIFO) inventory costing</a>) that contains the Set-based Speed Phreakery: The FIFO Stock Inventory SQL Problem: (<a href=\"https://www.simple-talk.com/sql/performance/set-based-speed-phreakery-the-fifo-stock-inventory-sql-problem/\" rel=\"nofollow noreferrer\">https://www.simple-talk.com/sql/performance/set-based-speed-phreakery-the-fifo-stock-inventory-sql-problem/</a>).  </p>\\n\\n<p>I have been trying to adapt it  from SQL Server to Teradata SQL but have discovered that:</p>\\n\\n<p>(a) Teradata can only handle one CTE with statement</p>\\n\\n<p>(b) You cannot use cross apply</p>\\n\\n<p>(c) You cannot use hint indexes?</p>\\n\\n<p>My questions are:</p>\\n\\n<p>Is there an alternative in Teradata (other than volatile tables) to get around (a) above?</p>\\n\\n<p>Is the Terdata \"Cross Join\" the same as Cross Apply in SQL Server?</p>\\n\\n<p>Has anyone adapated this script to Teradata?</p>\\n-Teradata FIFO Script-<sql><teradata><fifo>',\n",
       " '<p>I have 2 tables in Teradata - \"a\" and \"ch\". Table ch contains 2 colums \"amt\" and \"code\". Between tables \"a\" and \"ch\" is LEFT JOIN. Join is made, and in the SELECT part I am trying to SUM amt values. But when a \"code\" attribute has a speific values it hast to take only 70% of the \"amt\" value.</p>\\n\\n<p>This is what I have tried:</p>\\n\\n<pre><code>SELECT SUM \\n       (\\n       CASE WHEN ch.code IN (SELECT code from ...)\\n          then 0.7*ch.amt\\n       )   \\n       else ch.amt\\n       END\\nFROM a LEFT JOIN ch\\n</code></pre>\\n\\n<p>I get an error:</p>\\n\\n<blockquote>\\n  <p>Illegal expression in WHEN Clause of CASE expression.</p>\\n</blockquote>\\n\\n<p>Google says, that it is because <code>CASE</code> does not allow <code>SELECT</code> statements.</p>\\n\\n<p>Any suggestion how can I achieve the above described functionality?</p>\\n-Teradata CASE WHEN attribute IN SELECT-<sql><teradata>',\n",
       " '<p>Is there a way i can create a column in a table which will only hold milliseconds value. I have a requirement where i have to store only LLLCCC(milliseconds) value in a column.</p>\\n-Teradata- DDL with column holding milliseconds only-<teradata><milliseconds>',\n",
       " \"<p>I'm very much new to Rapid Miner and I'm currently doing a research on web usage mining. I want to analyze some apache and IIS web server logs and detect some fraudulent activities. I have googled and couldn't find some tutorials for this kind of web log file mining using Rapid Miner? \\nSo my questions:</p>\\n\\n<pre><code>1) Is it possible to do this with Rapid Miner(As I know it has a web mining extension)\\n2) Can somebody please advice me how to do this?some tutorials etc.\\n</code></pre>\\n\\n<p>Thanks very much in advance.</p>\\n-Web usage mining with rapidminer-<rapidminer>\",\n",
       " \"<p>I'm trying to extract Stored procedure DDL by querying system tables.<br>\\nIf I run the following query</p>\\n\\n<p><strong>select * from dbc.tvm where TableKind = 'P'</strong></p>\\n\\n<p>both fields <strong>RequestText</strong> and <strong>CreateText</strong> contain NULL.\\nIs there any way to query Stored Procedure body apart from using SHOW PROCEDURE?</p>\\n\\n<p>Thank you.</p>\\n-How to extract Stored Procedure body in Teradata-<procedure><ddl><teradata><system-tables>\",\n",
       " \"<p>I would like your help for the next issue. It is a real problem, but I will present you like an example.</p>\\n\\n<p>I have a view which is composed as below</p>\\n\\n<pre><code>viewa =\\nsel * from\\n(\\ncalendar_dim\\ninner join\\n(\\nsel * from table_a\\ninner join table_b\\nunion all\\nsel * from table_c\\ninner join table_D\\n)\\non...\\n)\\n</code></pre>\\n\\n<p>All the tables, table_a table_b table_c table_d have the same pi and partition columns (the 1st partition level is at the date), stats are up to date.</p>\\n\\n<p>My question is the following:</p>\\n\\n<p>If I will try to do a query such as</p>\\n\\n<pre><code>sel *\\n  from viewa\\n where cal_Date = '2013-05-31'\\n--&gt;&gt; the explain plan works perfect. It takes only one partition from each table and produce the result.\\n</code></pre>\\n\\n<p>The same happens with multiple date , /......where cal_Date in ('2013-05-31','','',.......)\\n(it takes the correct number of partitions ).</p>\\n\\n<p>But, if I try to inner join this view with a table, which contains 10 distinct dates, I was waiting to have 10 partitions from each table of the view  inner join this spool with the external table.</p>\\n\\n<pre><code>sel a.*\\nfrom viewa a\\n  inner join table_e b\\n   on a.cal_Date = b.cal_Date\\n</code></pre>\\n\\n<p>it does an all amp retrieve from each table of the view and then duplicates the external table. The issue is that the 4 tables inside the view are very big, about 1 billion rows.</p>\\n\\n<p>Solution for this?</p>\\n\\n<p>I tried to make a volatile table (with one column =  date) which contains only the distinct date that I am expected but nothing. Still the same behaviour.</p>\\n-How to force Teradata parser to understand the join-<sql><database><view><teradata>\",\n",
       " '<p>I am trying to release mload from DBC.ABC as below.</p>\\n\\n<p>Release mload DBC.ABC ;\\nor \\nrelease mload DBC.ABC  in apply.</p>\\n\\n<p>But getting below error.</p>\\n\\n<p>RELEASE Failed. 2561:  MLoad related table DBC.ABC has bad usage field in table header</p>\\n-RELEASE Failed. 2561 for mload in teradata-<teradata>',\n",
       " '<p>I have some troubles invoking SQL User Defined Functions in Teradata.\\nI\\'ve created the following function</p>\\n\\n<pre><code>*REPLACE FUNCTION \"twm_source\".\"TD_FN_CALC\" (\\n        \"func\" CHARACTER(1) CHARACTER SET LATIN,\\n        \"a\" INTEGER,\\n        \"b\" INTEGER)\\n    RETURNS INTEGER\\n    SPECIFIC \"td_fn_calc\"\\n    LANGUAGE SQL\\n    CONTAINS SQL\\n    DETERMINISTIC\\n    CALLED ON NULL INPUT\\n    SQL SECURITY DEFINER\\n    COLLATION INVOKER\\n    INLINE TYPE 1\\n    RETURN CASE\\n    WHEN func = \\'A\\'\\n    THEN A + B\\n    WHEN func = \\'S\\'\\n    THEN A - B\\n    WHEN func = \\'M\\'\\n    THEN A * B\\n    ELSE A / B\\nEND;*\\n</code></pre>\\n\\n<p>But when I execute the following query against Teradata 14.0 Server </p>\\n\\n<pre><code>select \"twm_source\".\"TD_FN_CALC\"(\\'M\\',3,8);\\n</code></pre>\\n\\n<p>it gives error</p>\\n\\n<blockquote>\\n  <p>Failed [5589 : HY000] Function \\'TD_FN_CALC\\' does not exist. </p>\\n</blockquote>\\n\\n<p>Could anyone please help me to find out what is wrong.\\nAny help is deeply appreciated.</p>\\n-Invoking UDF in Teradata-<sql><user-defined-functions><invoke><teradata>',\n",
       " '<p>I\\'m having trouble using xpath in Rapidminer.  Below is a sample html that I\\'m trying to pull data from.  I\\'m having trouble getting the number 7001 and Calfornia.  </p>\\n\\n<p>I use <code>//h:span[@class=\\'detail-block\\']//h:/text()</code> and I can get \"Number:\"\\nThen I try <code>//h:span[@class=\\'detail-block\\']/span//h:/text()</code> and get nothing.  I tried a bunch of variation of this and still come up with nothing.  I\\'m able to get things to work on google spreadsheet =importXML, but not rapidminer.</p>\\n\\n<pre><code>&lt;div class=\"information\"&gt;\\n&lt;h2 class=\"underline\"&gt;Information&lt;/h2&gt;\\n&lt;span class=\"detail-block\"&gt;&lt;span class=\"detail-attribute\"&gt;Number:&amp;nbsp;&lt;/span&gt;         \\n&lt;span&gt;7001&lt;/span&gt;&lt;/span&gt;\\n&lt;span class=\"detail-block\"&gt;&lt;span class=\"detail-attribute\"&gt;Location:&amp;nbsp;&lt;/span&gt; &lt;span&gt;California&lt;/span&gt;&lt;/span&gt;\\n</code></pre>\\n-trouble with xpath query in rapid miner-<xpath><rapidminer>',\n",
       " \"<p>I wanna create a table in teradata with YYYY-MM format and give default value as '0001-01'. Is it possible to do both?..However i am able to do this..</p>\\n\\n<p>create table test(t1 date format 'yyyy-mm' not null);</p>\\n-Teradata DDL with Date Formatting-<date-format><ddl><teradata>\",\n",
       " \"<p>Let's suppose you run a linear regression.</p>\\n\\n<p>The model equation in Linear Regression process and the prediction plot in Apply Model process end up in different result windows.</p>\\n\\n<p>Is it possible to annotate the Apply Model plot (showing data and linear regression fit/prediction) with the linear regression model equation?  (the way Excel does hum hum!)</p>\\n\\n<p>If so, how?</p>\\n-How to add a model equation to a rapidminer Apply Model plot?-<rapidminer>\",\n",
       " '<p>Lets say, for a system with 8 nodes and 368 AMPS, what is the number of PE that be configured? And How many sessions can each PE can handle (The default is 120)</p>\\n-How many PE (Parsing Engine) can be configured in a Teradata Box-<teradata>',\n",
       " \"<p>I have a simple Access database I use to create a number of reports. I've linked to a Teradata database server in our organization to pull some additional employee-level details. There is a simple left join on employee number, and all I pull is the name and the role. </p>\\n\\n<p>The query without the connect takes maybe a minute or so to run and is very quick once loaded. Left joining on the Teradata connection slows everything down to a crawl. It can take 10 minutes or so to run the query through Excel. When the query is loaded in Access, scrolling through it is very slow.</p>\\n\\n<p>I should note there's no performance issues with the Teradata server. I pull unrelated reports from the same and different tables, with complex joins and the speed is very quick.</p>\\n\\n<p>I tried creating an even simpler query that does almost noting, and the performance issues are still there. Here is the code:</p>\\n\\n<pre><code>SELECT EMPL_DETAILS_CURR.NM_PREFX, EMPL_DETAILS_CURR.NM_GIVEN, \\nMC.DT_APP_ENTRY, MC.CHANNEL_IND\\n\\nFROM MC LEFT JOIN EMPL_DETAILS_CURR ON MC.EMP_ID = EMPL_DETAILS_CURR.EMP_ID;\\n</code></pre>\\n\\n<p>There are only 7000 records in <code>MC</code>. </p>\\n-Access slow when joining on Teradata SQL connections?-<sql><performance><ms-access><teradata>\",\n",
       " '<p>I want to classify text data using classifier model SVM with Rapidminer tool. Classification would be of multilable type. Since my data is of text type, how SVM can be used for this classification. I know that SVM works with numeric data only.</p>\\n-Multilabel classification with SVM using rapidminer-<classification><svm><rapidminer>',\n",
       " \"<p>I try to create SQLCLR proc that access teradata server and I get this error:  </p>\\n\\n<blockquote>\\n  <p>System.IO.FileLoadException: LoadFrom(), LoadFile(), Load(byte[]) and LoadModule() have been disabled by the host.</p>\\n</blockquote>\\n\\n<p>I google about this error and I found that I need to create <code>&lt;dllname&gt;.XmlSerializers.dll</code> and add this to the DB. I did that and still I get same error.</p>\\n\\n<p>The code work stably with Oracle and SQL Server, but I can't add Teradata.</p>\\n\\n<p>I'm using <code>Teradata.Client.Provider</code> namespace from the assembly <code>Teradata.Client.Provider</code>.</p>\\n\\n<p>The server version is 2012.</p>\\n-can't connect teradata from sql clr code-<.net><sql-server><teradata><sqlclr>\",\n",
       " \"<pre><code>    cust_id acct_id trxn_amt    trxn_cnt\\n 1  66685638    10,028,717,398  199.75  5\\n 2  66685638    10,028,849,377  76.16   2\\n</code></pre>\\n\\n<p>Say I have a table of customers with multiple account IDs and I want to create a new column for  that's the summation of all of the transaction amounts for each customer (199.75+76.16 for cust_id=66685638) as well as another column thats the % of total spend for each account (for first account its 199.75/(76.15+199.75)). Each customer may have anywhere from 2-4 acct_ids. </p>\\n\\n<p>Thanks so much.</p>\\n-SQL-Teradata: How to create a new column composed of a formula-<sql><teradata>\",\n",
       " \"<p>I'm using Eclipse Kepler WTP + the Glassfish Kepler plugins.</p>\\n\\n<p>Problem is I'm not able to start my locally installed Glassfish any more with this configuration.</p>\\n\\n<p>After run the launch configuration the progress bar is at 69% and does not continue. There is nothing in the logs etc. I also tried using a fresh workspace etc. but no success.</p>\\n\\n<p>The Glassfish version I'm running is 3.1.2. With Eclipse Juno and the corresponding Glassfish plugins everything works just fine.</p>\\n\\n<p><strong>EDIT</strong> Seems to be an issue with the already existing domain inside GF. It is possible to use the plugin when creating a new domain.</p>\\n-Eclipse Kepler - Glassfish Plugin - unable to start GF 3.1-<glassfish><eclipse-kepler>\",\n",
       " '<p>I have managed to get Eclipse Juno connecting to the internet. For this to accomplish, there are many posts here on stackoverflow. here is one of this links:\\n<a href=\"https://stackoverflow.com/questions/11539681/eclipse-not-connecting-to-internet-via-proxy\">Eclipse not connecting to internet via proxy</a></p>\\n\\n<p>Today i installed Eclipse Kepler on my Windows7 box. But accessing the eclipse marketplace or installing new software does not work.\\nI am using exactly the same proxy configuration as i used for Eclipse Juno. Juno can access the marketplace but Kepler does not.\\nThe proxy uses NTLM Authentification.\\nAny one here with similar problem?</p>\\n\\n<p>No success til now. I am still wondering why Juno can connect whereas Kepler can not:-(</p>\\n-Eclipse Kepler not connecting to internet via proxy-<eclipse><proxy><eclipse-kepler>',\n",
       " \"<p>After uninstalling JRebel from Eclipse Kepler (on Ubuntu 12.04), when I try to run my web app on server I get the following error:</p>\\n\\n<p>'Starting Tomcat v7.0 Server at localhost' has encountered a problem.\\nReference to undefined variable jrebel_args.</p>\\n\\n<p>How can I remove this reference?</p>\\n-Eclipse - Reference to undefined variable jrebel_args-<eclipse><jrebel><eclipse-kepler>\",\n",
       " '<p>I have to text in fireworks that I kerned at \"12\". I am not trying to translate this value into css. Does it translate as 12%? .12em... How would I convert it?</p>\\n-Fireworks kern value into css-<css><fireworks>',\n",
       " '<p>I\\'m using the J2EE bundle of Eclipse Kepler, and I\\'m working with Git. </p>\\n\\n<p>I\\'m also using <a href=\"http://sourceforge.net/projects/gitextensions/\" rel=\"noreferrer\">git-extension</a> as a UI to git, and I wouldn\\'t trade it for anything else, so for me egit is just an annoyance that brings information that I don\\'t want on my screen.</p>\\n\\n<p>How can I permanently remove it from eclispe ?</p>\\n-How to remove EGIT from Eclipse Kepler?-<java><egit><eclipse-kepler>',\n",
       " '<p>The <a href=\"http://pic.dhe.ibm.com/infocenter/streams/v3r1/topic/com.ibm.swg.im.infosphere.streams.spl-language-specification.doc/doc/dynamicappcomposition.html\" rel=\"nofollow\">Dynamic application composition</a> topic says that </p>\\n\\n<blockquote>\\n  <p>Properties can also be added, updated, or removed at runtime, and so can subscriptions. The compile-time properties and subscriptions just serve as initial settings.</p>\\n</blockquote>\\n\\n<p>but it doesn\\'t say how to do that.  So, how do you do that?</p>\\n-How can I change the properties of an Export stream at runtime?-<infosphere-spl><ibm-streams>',\n",
       " '<p>EDIT: It turns out this really isn\\'t specific to Eclipse Kepler. I had to use the same process for Eclipse Juno. The problem was that there seem to be missing steps in other posts answering this same question.</p>\\n\\n<p>I\\'m using Eclipse Kepler for C++ and I\\'m trying to use C++11 and getting errors. When I compile I get the error</p>\\n\\n<p>error: range-based-for loops are not allowed in C++98 mode</p>\\n\\n<p>I\\'ve followed the instructions from the post</p>\\n\\n<p><a href=\"https://stackoverflow.com/questions/9131763/eclipse-cdt-c11-c0x-support\">Eclipse CDT C++11/C++0x support</a></p>\\n\\n<p>and the solution given for Eclipse Juno isn\\'t working. </p>\\n\\n<p>Different comments have suggested restarting eclipse and cleaning and rebuilding. That hasn\\'t made a difference.</p>\\n-How to enable C++11 in Eclipse Juno/Kepler/Luna CDT?-<eclipse><eclipse-cdt><eclipse-juno><eclipse-kepler><eclipse-luna>',\n",
       " '<p>I am new to Rapidminer. I have many XML files and I want to classify these files manually based on keywords. Then I would like to train a classifier like Naive Bayer and SVM on these data and calculate their performances using cross- validator.</p>\\n\\n<p>Could you please let me know different steps for this?</p>\\n\\n<p>Should I need to use text processing activities like  tokenising, TFIDF etc.?</p>\\n-How to process XML files using Rapidminer for classification-<machine-learning><classification><rapidminer>',\n",
       " \"<p>I have applied different clustering algos like kmean, kmediod kmean-fast and expectation max clustering on my biomedical dataset using Rapidminer. now i want to check the performance of these algos that which algo gives better clustering results.\\nFor that i have applied some operators like 'cluster density performance' and 'cluster distance performance' which gives me avg within cluster distance for each cluster and davis bouldin. but I am confused is it the right way to check clustering performance of each algo with these operators?\\nI am also interested in Silhouette method which i can apply for each algo to check performance but can't understand from where i can get b(i) and a(i) values from clustering algo output.</p>\\n-comparison of clustering algorithms performance in rapidminer-<validation><data-mining><cluster-analysis>\",\n",
       " '<p>Eclipse TableItem does not display a long String value.\\nWhen I open up the RCP, the TableItem is only displaying only ~260 characters per column.</p>\\n\\n<p><img src=\"https://i.stack.imgur.com/M8Sma.png\" alt=\"Character Column Limit\">\\nI am using Windows 7 (64bit), Eclipse Kepler (64bit), Java 1.7.0_25 (64bit)</p>\\n\\n<p>Here is the source code:</p>\\n\\n<pre><code>public class RecordTableViewerPart {\\n\\nprivate TableViewer tableViewer;\\nprivate int numberOfColumns = 2;\\n\\n@PostConstruct\\npublic void createComposite(Composite parent) {\\n    parent.setLayout(new GridLayout());\\n\\n    // Define the TableViewer\\n    TableViewer tableViewer = new TableViewer(parent, SWT.VIRTUAL | SWT.NONE | SWT.MULTI | SWT.H_SCROLL | SWT.V_SCROLL | SWT.FULL_SELECTION);\\n\\n    // Create the columns\\n    for(int i=0; i &lt; numberOfColumns; i++) {\\n        TableViewerColumn tableViewerColumn = new TableViewerColumn(tableViewer, SWT.NONE);\\n        tableViewerColumn.getColumn().setWidth(5000);\\n        tableViewerColumn.getColumn().setText(\"Column: \" + i);\\n    }\\n\\n\\n    // Create the sample data\\n    for(int i=0; i &lt; numberOfColumns; i++) {\\n        TableItem tableItem = new TableItem(tableViewer.getTable(), SWT.NONE);\\n        tableItem.setText(i, i + \":\" + createLongString());\\n    }\\n\\n    final Table table = tableViewer.getTable();\\n    table.setLayoutData(new GridData(GridData.FILL_BOTH));\\n    table.setHeaderVisible(true);\\n    table.setLinesVisible(true);\\n}\\n\\nprivate String createLongString() {\\n    String numbers = new String(\"0123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012\" +\\n                                \"9012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901\" + \\n                                \"2345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234\" + \\n                                \"5678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567\" +\\n                                \"8901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890\" +\\n                                \"1234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123\" +\\n                                \"4567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456\" + \\n                                \"7890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789\" + \\n                                \"0123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012\" + \\n                                \"3456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345\" +\\n                                \"6789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678\" + \\n                                \"9012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901\" +\\n                                \"2345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234\" + \\n                                \"5678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567\" +\\n                                \"8901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890\" +\\n                                \"1234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123\" +\\n                                \"4567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456\" + \\n                                \"7890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789\" + \\n                                \"0123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012\" + \\n                                \"3456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345\" +\\n                                \"6789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678\" + \\n                                \"9012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901\" +\\n                                \"2345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234\" +\\n                                \"5678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567\" +\\n                                \"8901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890\" +\\n                                \"1234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123\" +\\n                                \"4567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456\" + \\n                                \"7890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789\" +\\n                                \"0123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012\" +\\n                                \"3456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345\" +\\n                                \"6789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678\" +\\n                                \"9012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901\" +\\n                                \"2345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234\" +\\n                                \"0123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012\" +\\n                                \"9012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901\" + \\n                                \"2345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234\" + \\n                                \"5678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567\" +\\n                                \"8901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890\" +\\n                                \"1234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123\" +\\n                                \"4567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456\" + \\n                                \"7890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789\" + \\n                                \"0123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012\" + \\n                                \"3456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345\" +\\n                                \"6789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678\" + \\n                                \"9012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901\" +\\n                                \"2345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234\" +\\n                                \"5678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567\" +\\n                                \"8901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890\" +\\n                                \"1234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123\" +\\n                                \"4567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456\" + \\n                                \"7890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789\" +\\n                                \"0123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012\" +\\n                                \"3456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345\" +\\n                                \"9012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901\" +\\n                                \"2345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890123456789012345678901234\");\\n    System.out.println(numbers.length());\\n    return numbers.toString();\\n}\\n</code></pre>\\n\\n<p>}</p>\\n-TableItem does not display a long String Value-<java><eclipse><swt><tableviewer><eclipse-kepler>',\n",
       " '<p>I am using eclipse Kepler I imported some java projects into workspace, and all <code>web.xml</code> files are showing error like this </p>\\n\\n<blockquote>\\n  <p>Referenced file contains errors<br>\\n  (jar:file:/J:/eclipse/plugins/org.eclipse.jst.standard.schemas_1.2.1.v201302050732.jar!   /dtdsAndSchemas/javaee_6.xsd)\\n  <strong>My web.xml</strong> </p>\\n</blockquote>\\n\\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"ISO-8859-1\"?&gt;\\n\\n&lt;web-app xmlns=\"http://java.sun.com/xml/ns/javaee\"\\nxmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\\nxsi:schemaLocation=\"http://java.sun.com/xml/ns/javaee\\n                  http://java.sun.com/xml/ns/javaee/web-app_3_0.xsd\"\\nversion=\"3.0\"\\nmetadata-complete=\"true\"&gt;  \\n</code></pre>\\n\\n<p>is it the problem with jar file?</p>\\n-Referenced file contains errors (jar:file:/J:/eclipse/plugins/org.eclipse.jst.standard.schemas_1.2.1.v201302050732.jar!/dtdsAndSchemas/javaee_6.xsd)-<xml><eclipse><eclipse-plugin><eclipse-juno><eclipse-kepler>',\n",
       " '<p>I am using Eclipse Kepler 64-bit and Sun JDK 1.7.0_25 64-bit on Ubuntu 64-bit. Eclipse worked when I had JDK 1.6 but stopped working after upgrading to JDK 1.7 (using <a href=\"http://www.webupd8.org/2012/01/install-oracle-java-jdk-7-in-ubuntu-via.html\" rel=\"nofollow noreferrer\">webupd8.org</a>\\'s method and the popular <a href=\"https://askubuntu.com/a/55960\">askubuntu</a> solution) and now at start-up it tells me there was an error and to check the <code>configuration/xxx.log</code> files.</p>\\n\\n<p>This is the beginning of the contents of each log file it creates at start-up:</p>\\n\\n<pre><code>!SESSION 2013-07-08 15:38:11.495 -----------------------------------------------\\neclipse.buildId=4.3.0.I20130605-2000\\njava.version=1.7.0_25\\njava.vendor=Oracle Corporation\\nBootLoader constants: OS=linux, ARCH=x86_64, WS=gtk, NL=en_US\\nCommand-line arguments:  -os linux -ws gtk -arch x86_64\\n\\n!ENTRY org.eclipse.equinox.ds 4 0 2013-07-08 15:38:12.212\\n!MESSAGE [SCR] Exception while activating instance org.eclipse.e4.ui.css.swt.internal.theme.ThemeEngineManager@b1320f9 of component org.eclipse.e4.ui.css.swt.theme  \\n!STACK 0\\njava.lang.NoClassDefFoundError: org/eclipse/swt/widgets/Display\\n        at java.lang.Class.getDeclaredMethods0(Native Method)\\n        at java.lang.Class.privateGetDeclaredMethods(Class.java:2521)\\n        at java.lang.Class.getDeclaredMethods(Class.java:1845)\\n        at org.eclipse.equinox.internal.ds.model.ServiceComponent.getMethod(ServiceComponent.java:126)\\n        at org.eclipse.equinox.internal.ds.model.ServiceComponent.activate(ServiceComponent.java:213)\\n        at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.activate(ServiceComponentProp.java:146)\\n        at org.eclipse.equinox.internal.ds.model.ServiceComponentProp.build(ServiceComponentProp.java:345)\\n        at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponent(InstanceProcess.java:620)\\n        at org.eclipse.equinox.internal.ds.InstanceProcess.buildComponents(InstanceProcess.java:197)\\n</code></pre>\\n\\n<p>I have cleaned the workspace, ran <code>./eclipse -clean</code> but it doesn\\'t pass the logo. Is there anything left to check / clean?</p>\\n-Eclipse Kepler startup error \"NoClassDefFoundError: org/eclipse/swt/widgets/Display\"-<java><eclipse><eclipse-kepler>',\n",
       " \"<p>I seem to have come across a strange restriction in using bind variables in MonetDB. If i use bind variables in the normal way by saying where field=${var} then it works. But not in this case:</p>\\n\\n<p>Here is a sample:</p>\\n\\n<pre><code>select case when ${Brand} = 'All'\\n</code></pre>\\n\\n<p>And here is the error:</p>\\n\\n<pre><code>EXEC: wrong type for argument 1 of prepared statement: char, expected char\\n</code></pre>\\n\\n<p>Using the latest version of the jdbc driver (2.9) via Pentaho.</p>\\n-Bind variables in case statement in MonetDB-<pentaho><bind-variables><monetdb>\",\n",
       " '<p>I  just download eclipse kelper  and  found out  the  m2e and m2e-wtp plugins had been installed,and the new m2e plugins has  no \"package\"  of the  lifecycle,how do i  uninstall it?</p>\\n\\n<p>=================================================\\n<a href=\"http://m2eclipse.sonatype.org/sites/m2e\" rel=\"nofollow\">http://m2eclipse.sonatype.org/sites/m2e</a>\\nthis is the update site i used in eclipse juno(now it seems 404 error).</p>\\n-How do i uninstall m2e in eclipse kepler-<m2e><eclipse-kepler><m2e-wtp>',\n",
       " \"<p>Ok. I am using eclipse kepler.</p>\\n\\n<p>Ihave an eclipse project called 'afd-core'. It has some classes, and a directory etc/hibernate containing hibernate config - lookup.hbm.xml.</p>\\n\\n<p>I have another project afd-public. It is a webapp, and needs the stuff in afd-core on it's classpath.</p>\\n\\n<p>And I think I have tried everything.</p>\\n\\n<p>afd-public->project referencers: added add-core as a referenced project\\nafd-public->java build path->projects: added afd-core\\nafd-public->java build path->order and export: marked afd-core as exported\\nafd-core->java build path->added etc/hibernate as a classpath entry\\nafd-core->order and export->marked etc/hibernate as exported</p>\\n\\n<p>Didn't work. When I run add-public as a web app, complains that it cant find the hibernate config.</p>\\n\\n<p>So I made etc/hibernate a source rather than a class direcrory. Still didn't work.</p>\\n\\n<p>Explicitly added add-core/etc/hibernate as a class directory in the add-public project. Still didnt work. Marked those class drectories as 'exported'. Still didn't work.</p>\\n\\n<p>Manually copied the hibernate config into afd-public/webapp/WEB-INF/classes . Ok, it finds the hibernate config, but it does not find the core class files.</p>\\n\\n<p>In other words, the afd-public webapp is not including dependencies from afd-core AT ALL, not in any way, shape, or form into the webapp that it deploys locally to tomcat. Whether or not I mark them as exported from afd-core. Whether or not I include the project or the directories explicitly. Whether or not I do or dont export them from the afd-public webapp.</p>\\n\\n<p>Nothing. nada. Won't go.</p>\\n\\n<p>Help?</p>\\n-eclipse - cannot get project export/dependencies working-<eclipse><jakarta-ee><eclipse-kepler>\",\n",
       " '<p>I have a wokflow configured with \"To Source Push Down Optimization\". If I take a look to the Push Down optimization option (I edit the workflow task, Mapping tab, and select Pushdonw Optimization), I find that there are two push down groups, that\\'s ok, it is what I expected.</p>\\n\\n<p>Then I start the workflow, check the log and I see that PWC is not throwing this two querys to the database, instead five different querys are thrown to the database (not one per source, it is like a different push down grouping).</p>\\n\\n<p>I \\'ve checked the two querys that are suppossed to be launched to the database and they are correct.</p>\\n\\n<p>Any ideas of what Am I doing wrong?</p>\\n\\n<p>PD: I am using Teradata with PWC 9.1</p>\\n\\n<p>Thanks in advance.</p>\\n-Push down optimization is throwing to the database unexpected querys (teradata)-<informatica-powercenter>',\n",
       " '<p>I have researched thoroughly on this problem but no use. Hopefully you guys can help me. Thanks very much in advance!</p>\\n\\n<p>The test code is below:</p>\\n\\n<pre><code>import javax.swing.JOptionPane;\\npublic class JOptionPane_Test {\\n    public static void main(String[] args){\\n        String userExit=\"a\";\\n        while (userExit!=null){\\n            userExit = JOptionPane.showInputDialog(null, \"Message\");\\n        }\\n    }\\n}\\n</code></pre>\\n\\n<p>It simply displays an input box waiting for user\\'s response, repeats if user hits OK, and stops if user hits Cancel or X button. The box halts after a random number of hitting OK (or Enter Key). It only shows the frame of the box, with the X button, and nothing inside (no message, no input field, no ok or cancel button). The only thing I can do is hitting the X button to exit out of the program.</p>\\n\\n<p>It happens with both showMessageDialog and showInputDialog.</p>\\n\\n<p>My log shows this warning: </p>\\n\\n<blockquote>\\n  <p>!SESSION 2013-07-09 14:00:12.666 -----------------------------------------------\\n  eclipse.buildId=4.3.0.I20130605-2000\\n  java.version=1.7.0_25 java.vendor=Oracle Corporation BootLoader\\n  constants: OS=win32, ARCH=x86, WS=win32, NL=en_US Framework arguments:\\n  -product org.eclipse.epp.package.standard.product Command-line arguments:  -os win32 -ws win32 -arch x86 -product\\n  org.eclipse.epp.package.standard.product</p>\\n  \\n  <p>!ENTRY org.eclipse.egit.ui 2 0 2013-07-09 14:00:30.147\\n  !MESSAGE Warning: EGit couldn\\'t detect the installation path\\n  \"gitPrefix\" of native Git. Hence EGit can\\'t respect system level Git\\n  settings which might be configured in ${gitPrefix}/etc/gitconfig under\\n  the native Git installation directory. The most important of these\\n  settings is core.autocrlf. Git for Windows by default sets this\\n  parameter to true in this system level configuration. The Git\\n  installation location can be configured on the Team > Git >\\n  Configuration preference page\\'s \\'System Settings\\' tab. This warning\\n  can be switched off on the Team > Git > Confirmations and Warnings\\n  preference page.</p>\\n</blockquote>\\n\\n<p>but I suspect that this warning does not have anything to do with the JOptionPane error. Or am I wrong?</p>\\n\\n<p>I found on this <a href=\"http://ubuntuforums.org/archive/index.php/t-684511.html\" rel=\"nofollow\">forum</a> that someone had exactly the same problem I have, and the solution was to check on the compiler that Eclipse uses, making sure it\\'s Sun\\'s instead of GCJ. However, mine has always been Sun\\'s, java 7.</p>\\n\\n<p>I have tried both Eclipse for 64 and 32 bit.</p>\\n\\n<p>Any help is much appreciated!!</p>\\n-JOptionPane halts after a random number of run (using Eclipse Kepler, java 7, Windows 7 64bit on MacbookAir)-<crash><windows-7-x64><joptionpane><halt><eclipse-kepler>',\n",
       " '<p>I\\'ve been using Indigo reasonably successfully for quite a while. Today I installed the STS for Kepler. I exported/imported my preferences and a Team Project Set for my (many) projects checked out from SVN.</p>\\n\\n<p>I\\'ve noticed that when I switch to editor buffers for a particular Maven project, I get the following error often repeated multiple times:</p>\\n\\n<p>\"Path must include project and resource name: /\"</p>\\n\\n<p>This project was just checked out from SVN, and I was working on the same project in the Indigo workspace, and I wasn\\'t seeing this error.</p>\\n\\n<p>I\\'ve carefully examined everything in the Build Path tab, and I\\'ve manually examined both the .classpath and .project files.</p>\\n\\n<p>The only thing that seems vaguely suspicious is the following block in my .project file:</p>\\n\\n<pre><code>        &lt;buildCommand&gt;\\n        &lt;name&gt;org.eclipse.ui.externaltools.ExternalToolBuilder&lt;/name&gt;\\n        &lt;triggers&gt;full,incremental,&lt;/triggers&gt;\\n        &lt;arguments&gt;\\n            &lt;dictionary&gt;\\n                &lt;key&gt;LaunchConfigHandle&lt;/key&gt;\\n                &lt;value&gt;&amp;lt;project&amp;gt;/.externalToolBuilders/org.eclipse.wst.validation.validationbuilder.launch&lt;/value&gt;\\n            &lt;/dictionary&gt;\\n        &lt;/arguments&gt;\\n    &lt;/buildCommand&gt;\\n</code></pre>\\n-First Kepler workspace is giving \"Path must include project and resource name: /\" for Maven project from SVN-<eclipse><maven><eclipse-kepler>',\n",
       " \"<p>I try to install Symfony2 Feature to Eclipse Kepler (I tried to 32-bit and 64-bit version as well) but I have only this error:</p>\\n\\n<pre><code>Cannot complete the install because one or more required items could not be found.\\n  Software being installed: Symfony Feature 1.0.85 (com.dubture.symfony.feature.feature.group 1.0.85)\\n  Missing requirement: Twig Editor Feature 1.0.109 (com.dubture.twig.feature.feature.group 1.0.109) requires 'org.eclipse.php 0.0.0' but it could not be found\\nCannot satisfy dependency:\\n  From: Symfony Feature 1.0.85 (com.dubture.symfony.feature.feature.group 1.0.85)\\n  To: com.dubture.twig.feature.feature.group 1.0.109\\n</code></pre>\\n\\n<p>I have installed a PDT.</p>\\n\\n<p>Does someone know what to do?</p>\\n-I can't install symfony2 feature to eclipse from symfony dubture com - strange error-<eclipse><symfony><eclipse-kepler>\",\n",
       " '<p>I\\'ve just downloaded the OEPE (Kepler) and installed m2e and m2e-wtp connectors.\\nI found out that under this path: Preferences ->Maven->Lifecycle mappings->Open workspace lifecycle mapping data there is a preconfigured xml file which says that maven should ignore the compile goal for AspectJ and I assume that\\'s why the AspectJ runtime libraries are not added to the project hence the project is not recognized as an AspectJ project by eclipse.</p>\\n\\n\\n\\n<pre class=\"lang-xml prettyprint-override\"><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;lifecycleMappingMetadata&gt;\\n&lt;pluginExecutions&gt;\\n    &lt;pluginExecution&gt;\\n        &lt;pluginExecutionFilter&gt;\\n            &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;\\n            &lt;artifactId&gt;aspectj-maven-plugin&lt;/artifactId&gt;\\n            &lt;versionRange&gt;1.6&lt;/versionRange&gt;\\n            &lt;goals&gt;\\n                &lt;goal&gt;compile&lt;/goal&gt;\\n            &lt;/goals&gt;\\n        &lt;/pluginExecutionFilter&gt;\\n        &lt;action&gt;\\n            &lt;ignore /&gt;\\n        &lt;/action&gt;\\n    &lt;/pluginExecution&gt;\\n&lt;/pluginExecutions&gt;\\n</code></pre>\\n\\n<p></p>\\n\\n<p>I commented out these lines in the xml file and reloaded it once again.\\nNow the IDE does not ignore AspectJ plugin tag in the lifecycle but  pom file is complaining that it cannot recognize the execution tag.</p>\\n\\n<pre class=\"lang-xml prettyprint-override\"><code>&lt;plugin&gt;\\n&lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;\\n&lt;artifactId&gt;aspectj-maven-plugin&lt;/artifactId&gt;\\n&lt;version&gt;1.4&lt;/version&gt;\\n&lt;dependencies&gt;\\n    &lt;dependency&gt;\\n        &lt;groupId&gt;org.aspectj&lt;/groupId&gt;\\n        &lt;artifactId&gt;aspectjrt&lt;/artifactId&gt;\\n        &lt;version&gt;${aspectj.version}&lt;/version&gt;\\n    &lt;/dependency&gt;\\n&lt;/dependencies&gt;\\n&lt;configuration&gt;\\n    &lt;source&gt;1.6&lt;/source&gt;\\n    &lt;target&gt;1.6&lt;/target&gt;\\n&lt;/configuration&gt;\\n&lt;executions&gt;\\n    &lt;execution&gt;\\n        &lt;goals&gt;\\n            &lt;goal&gt;compile&lt;/goal&gt;\\n        &lt;/goals&gt;\\n    &lt;/execution&gt;\\n&lt;/executions&gt;\\n&lt;/plugin&gt;\\n</code></pre>\\n\\n<p>Using indigo the m2e-wtp was able to recognize the <code>&lt;execution&gt;</code> tag for aspectj plugin and able to add the AspectJ runtime libraries\\nautomatically to the project, though this is not the case in Kepler.  (I think it is m2e-wtp\\'s job to make an AspectJ project out of the pom but not quite sure.)</p>\\n\\n<p>Btw. the how can I make things work like in Indigo?\\nI know I can right click on the project and convert it to aspect project in order to solve the problem but I want the IDE and plugins realize from the pom file that this project needs AspectJ jars.  Any idea?</p>\\n-aspectj-maven-plugin not covered by lifecycle in Kepler-<maven><aspectj><lifecycle><eclipse-kepler>',\n",
       " '<pre><code>    CUST_ID     ACCT_ID         FICO_SCORE  DATE_APPLICATION_RECEIVED\\n1   48378281    2,200,263,271   577         5/3/2001\\n2   48378281    2,346,832,797   611         5/14/2003\\n3   48378281    2,210,263,271   560         5/2/2002\\n4   48378281    2,416,532,797   575         5/14/2005\\n</code></pre>\\n\\n<p>Say that I have a list of customers with multiple accounts with my company in teradata, and I have their fico scores, and the date that their application for a new account with us are opened. A customer must have more than 2 accounts and may have up to 5 accounts. Above is how that table would look like for just one customer. How do I now delimit this population to just those customers that have accounts where their opening date was at least 6 months apart from each other for between their first opened account and second opened account? I just need count(unique(cust_id)) at the end.</p>\\n-How to delimit a population based on the dates that accounts were opened on in teradata?-<sql><sql-server><teradata>',\n",
       " '<p>![enter image description here][1]Testswarm is a Continuous Integration Testing Framework.</p>\\n\\n<p>Can anybody tell me how to install and use it?</p>\\n\\n<p>I have checked the read me on github and installed Testswarm, but I am unable to add jobs or run them. </p>\\n\\n<p>I added one project by inserting details in database. </p>\\n\\n<p>I have downloaded qUnit, so please tell me how to write tests in it for Testswarm.</p>\\n-How to install and setup Testswarm?-<debugging><configuration><continuous-integration><qunit><testswarm>',\n",
       " '<p>I want to get table creation script on teradata with jdbc.\\nI used this code which I found it on stackoverflow :</p>\\n\\n<pre><code>   StringBuilder sb = new StringBuilder( 1024 );\\n                if ( columnCount &gt; 0 ) { \\n                    sb.append( \"Create table \").append( rsmd.getTableName( 1 )  ).append( \" ( \" );\\n                }\\n                for ( int i = 1; i &lt;= columnCount; i ++ ) {\\n                    if ( i &gt; 1 ) sb.append( \", \" );\\n                    String columnName = rsmd.getColumnLabel( i );\\n                    String columnType = rsmd.getColumnTypeName( i );\\n\\n                    sb.append( columnName ).append( \" \" ).append( columnType );\\n\\n                    int precision = rsmd_ddl.getPrecision( i );\\n                    if ( precision != 0 ) {\\n                        sb.append( \"( \" ).append( precision  ).append( \" )\" );\\n                    }\\n                } // for columns\\n                sb.append( \" ) \" );\\n</code></pre>\\n\\n<p>But the problem is : when the type is <code>VARCHAR</code> the precision is <code>0</code> but in teradata the column is <code>VARCHAR(100)</code> but how can I find <code>100</code> ? </p>\\n\\n<p>Thanks.</p>\\n-How can I get table creation scripts on teradata with jdbc?-<jdbc><metadata><resultset><teradata>',\n",
       " \"<p>I am writing a plugin which I want to use the CDT-plugin (since I don't want to implement my own brand new C/C++ editor).</p>\\n\\n<p>Questions:</p>\\n\\n<ul>\\n<li>How is it possible to set the dependencies of my plugin to this particular editor </li>\\n<li>How can I set this editor as a nature of this project?</li>\\n</ul>\\n-Eclipse Plugin Development: Make plugin dependent on CDT-plugin-<java><eclipse><eclipse-plugin><eclipse-kepler>\",\n",
       " '<p>I\\'ve successfully installed testswarm on my machine.</p>\\n\\n<p>I just can\\'t find how to create project and start running some tests. On GUI there is no option for creating a project. </p>\\n\\n<p>There is a text on login \"Login for projects. Projects can only be created by swarm operators.\". So, how to create swarm operator and/or project?</p>\\n-How to create project in testswarm-<testswarm>',\n",
       " \"<p>I have to check performance of various clustering algos using different performance operators in rapidminer. For that I want to know the following things:</p>\\n\\n<ol>\\n<li>what does cluster number index value shows which is output of cluster count performance operator?</li>\\n<li>what does small and large value of avg within cluster distance and avg. within centroid distance mean in terms of good and bad clustering?</li>\\n<li>I also want to check other indexes value like Dunn index,Jaccard index, Fowlkes–Mallows for various clustering algos. but rapidminer don't have any operator for this, what to do for that. I don't have experience with R.</li>\\n</ol>\\n-rapidminer: cluster performance operators..what does different value mean?-<cluster-analysis><k-means><rapidminer>\",\n",
       " '<p>Until now I was using Eclipse Indigo and m2eclipse. When wanted to create a new project I chose new maven project > skip archetype selection > gave names > finish.</p>\\n\\n<p>This would result in a directory structure like :\\nsrc/main/java\\nsrc/main/resources\\nsrc/test/java\\nsrc/test/resources</p>\\n\\n<p>and with the directory src/main/webapp including\\nMETA-INF\\n --MANIFEST.MF\\nWEB-INF\\n --web.xml</p>\\n\\n<p>Recently (yesterday) I switched to the Eclipse Kepler. Maven is integrated, thus I didn\\'t download m2eclipse wtp. </p>\\n\\n<p>When I tried new project > maven project > skip archetype > naming > war packaging > finish , I got the correct structure in the src directory\\nsrc/main/java\\nsrc/main/resources\\nsrc/test/java\\nsrc/test/resources</p>\\n\\n<p>But the directory webapp, was empty! META-INF and WEB-INF did not exist, no web.xml also. I read about using the maven-archetype-webapp, but i got the error:\\n\"Could not resolve artifact org.apache.maven.archetypes:maven-archetype-webapp:pom:RELEASE\\nFailed to resolve version for org.apache.maven.archetypes:maven-archetype-webapp:pom:RELEASE: Could not find metadata org.apache.maven.archetypes:maven-archetype-webapp/maven-metadata.xml in local (C:\\\\Users\\\\stef.m2\\\\repository)\"</p>\\n\\n<p>With this article\\n<a href=\"https://stackoverflow.com/questions/12742467/failed-to-resolve-version-for-org-apache-maven-archetypes\">Failed to resolve version for org.apache.maven.archetypes</a></p>\\n\\n<p>I found out that I had to add the remote maven catalog. I did and was able to create a project using the maven-archetype-webapp version 1.0 . But then the problem was that the directory structure in the new project was: </p>\\n\\n<p>Java Resources\\n--src/main/resources</p>\\n\\n<p>and \\nsrc/main/webapp\\n--index.jsp\\n--WEB-INF\\n  |--web.xml</p>\\n\\n<p>I expected(and wanted) the old directory structure which was ,\\nsrc/main/java\\nsrc/main/resources\\nsrc/test/java\\nsrc/test/resources</p>\\n\\n<p>and with the directory src/main/webapp including\\nMETA-INF\\n --MANIFEST.MF\\nWEB-INF\\n --web.xml</p>\\n\\n<p>Is there a step I am missinng? something changed in archetypes? Maybe it is a really stupid question, but my research returned very little results. Please feel free to any comment that can get me and other with the same issue, back in track</p>\\n\\n<p>Thanks!</p>\\n-maven-archetype-webapp directory structure using Eclipse Kepler-<java><maven><java-ee-7><eclipse-kepler>',\n",
       " '<p>Getting a error while creating the volatile table illegal usage of identity column.. </p>\\n\\n<pre><code>CREATE VOLATILE TABLE t1 (\\n    ID1 INTEGER GENERATED ALWAYS AS IDENTITY (START WITH 1 INCREMENT BY 1 MINVALUE 0 MAXVALUE 100 NO CYCLE),\\n    NoSec BigInt\\n) ON COMMIT PRESERVE ROWS;\\n</code></pre>\\n-illegal usage of identity column teradata-<teradata>',\n",
       " '<p>I installed the teradata client successfully. When I configure the connection I dont see the connection test button.I wrote ip and username and password.\\nWhere is it?</p>\\n-How can I test teradata odbc?-<odbc><teradata>',\n",
       " '<p>I\\'ve upgraded to Eclipse Kepler from Juno, and I find that it handles the <kbd>Esc</kbd> key completely differently.</p>\\n\\n<p>Usually, with the <kbd>Esc</kbd> key I could abort smaller views/menus like auto-completion: <kbd>Alt</kbd>+<kbd>Space</kbd> to open the auto-completion dropdown, <kbd>Esc</kbd> to close it.</p>\\n\\n<p>However, with Kepler I find that <kbd>Esc</kbd> hides basically everything. Before:</p>\\n\\n<p><img src=\"https://i.stack.imgur.com/lgjy1.png\" alt=\"State before pressing Esc:\"></p>\\n\\n<p>After:</p>\\n\\n<p><img src=\"https://i.stack.imgur.com/r6OZS.png\" alt=\"State after pressing Esc:\"></p>\\n\\n<p><kbd>Esc</kbd> is not mapped to any function (it\\'s not listed in the configuration under keys). Is this the default behavior now? Does anybody else have this?</p>\\n\\n<p>Resetting the perspective doesn\\'t change this behavior. I think it might have something to do with the floaty windows style. When I installed Kepler and started it up, I had to manually resize the inner window (containing all the usual views) to the size of the screen.</p>\\n\\n<p>Edit: I can get the UI back if I click the small Java perspective button in the little menu bar on the top left side. The <kbd>Esc</kbd> functionality seems to be the same with all the views (focusing each view, then pressing <kbd>Esc</kbd> does the same for every view).</p>\\n-Escape hides complete UI-<eclipse><eclipse-kepler>',\n",
       " '<p>I am trying to add Teradata in vs 2010. Can anyone please help me?</p>\\n\\n<p>You can check the below post they are trying for mysql. I need for teradata.</p>\\n\\n<p><a href=\"https://stackoverflow.com/questions/4235291/how-to-connect-to-a-mysql-data-source-in-visual-studio\">How to connect to a MySQL Data Source in Visual Studio</a></p>\\n-How to connect Teradata in entity data model wizard-<asp.net><entity-framework><teradata>',\n",
       " '<p>I\\'ve downloaded Eclipse Kepler 4.3 from official site in <a href=\"http://www.eclipse.org/downloads/packages/eclipse-standard-43/keplerr\" rel=\"nofollow\">here</a>.</p>\\n\\n<p>And installed PDT plugin successfully (<code>Eclipse -&gt; Help -&gt; Install new software</code>).</p>\\n\\n<p><strong>The problem :</strong></p>\\n\\n<p>I need to install JSDT jQuery plugin. </p>\\n\\n<p>I have tried :</p>\\n\\n<pre><code>Help -&gt; Eclipse MarketPlace -&gt; JSDT jQuery (Install)\\n</code></pre>\\n\\n<p>But get me following error :</p>\\n\\n<blockquote>\\n  <p>No repository found at\\n  <a href=\"https://svn.codespot.com/a/eclipselabs.org/jsdt-jquery/updatesite\" rel=\"nofollow\">https://svn.codespot.com/a/eclipselabs.org/jsdt-jquery/updatesite</a></p>\\n</blockquote>\\n\\n<p>Thanks in advance.</p>\\n-Installing JSDT jQuery plugin in Eclipse Kepler 4.3-<eclipse><eclipse-plugin><eclipse-kepler>',\n",
       " '<p>I have had a working Indigo workspace for quite a while, with many projects checked out from SVN.  Mostly Ant projects, but a few critical Maven projects.</p>\\n\\n<p>I\\'m struggling to get a Kepler workspace working with the same projects.  I\\'ve essentially given up on the pure STS installation, as that gives me (different) errors in both Maven projects.  I filed both Eclipse and STS bugs, and they both say it\\'s the others problem, so I\\'ve given up on that path for now.</p>\\n\\n<p>I\\'m now trying to start with pure Eclipse and install the Spring pieces I want.  I thought I had all the plugins installed that I need, and I\\'m not getting the error dialogs that I was getting with the STS installation, but I\\'m getting a build error I can\\'t handle.  It\\'s saying this:</p>\\n\\n<pre><code>Build path is incomplete. Cannot find class file for Could not initialize class org.springframework.ide.eclipse.springframework.aop.aspectj.AspectJExpressionPointcut\\n</code></pre>\\n\\n<p>I suppose I\\'m missing some plugin, but I don\\'t know what it would be.</p>\\n\\n<p>I even did a somewhat exhaustive search for that class (\"AspectJExpressionPointcut\") in my workspace and Kepler distro, and it can\\'t find it, unless it\\'s in a jar file that\\'s inside a jar file (my search just looks for classes in jar files in a directory tree).  I see references to it on the internet, but I can\\'t get much out of those references.</p>\\n-Maven project on new Kepler workspace fails with \"Could not initialize class AspectJExpressionPointcut\"-<spring><maven><sts-springsourcetoolsuite><eclipse-indigo><eclipse-kepler>',\n",
       " \"<p>I need to do something like this in MonetDB:</p>\\n\\n<pre><code>select\\n  If(value &gt; 0, value, -1) as selected_value\\nfrom\\n  table\\n</code></pre>\\n\\n<p>In mysql, this means 'return value if value > 0, otherwise return -1.'\\nIs there an equivalent function to this in MonetDB?</p>\\n\\n<p>And I am not looking for a case statement. I know it exists in both MonetDB and other sql languages. I would like specifically to know of something like the example I gave.</p>\\n\\n<p>Thank you</p>\\n-Is there a function in MonetDB that is equivalent to IF?-<sql><monetdb>\",\n",
       " '<p>I came across this document \\n<a href=\"http://www.clear.rice.edu/comp422/resources/cuda/pdf/cuobjdump.pdf\" rel=\"nofollow\">cuobjdump.pdf</a>.</p>\\n\\n<p>It lists the Fermi and Kepler instruction but with no additional explanation. </p>\\n\\n<p>Apart from the usual add, subtract, multiply, etc .. I do not get what other instruction mean or do. Can anybody help me with that or point me to a document or link that explain them ??</p>\\n-understanding Nvidia Kepler Assembly instructions-<assembly><cuda><gpu><nvidia><kepler>',\n",
       " '<p>I have an Eclipse plugin editor (now running on Kepler), and I want to use dependency injection to get contexts such as workbench, logger, etc\\'. My purpose is to make my code \"more testable\".</p>\\n\\n<ol>\\n<li>What plugin dependencies should I add to use the new DI?</li>\\n<li>Are there goo examples on how to use it?</li>\\n</ol>\\n-How to use dependency injection in Eclipse plugin-<java><dependency-injection><eclipse-plugin><eclipse-rcp><eclipse-kepler>',\n",
       " '<p>Alright. I worked so hard over the past few weeks to learn the .NET data provider for teradata and set up an infrastructure to query Teradata data sources. Everything is great except....</p>\\n\\n<p>Canonical Entity functions are not supported by the provider. Alright. Then I look at the link below which says I should use \"Teradata.DiffDays\" and \"EntityFunctions.DiffDays\". </p>\\n\\n<p><a href=\"http://developer.teradata.com/doc/connectivity/tdnetdp/14.10/webhelp/EntityProviderCanonicalFunctions.html\" rel=\"nofollow\">http://developer.teradata.com/doc/connectivity/tdnetdp/14.10/webhelp/EntityProviderCanonicalFunctions.html</a></p>\\n\\n<p>I merrily try to use \"Teradata.DiffDays\" in my LINQ to Entities query thusly:</p>\\n\\n<p>var results = IMAccts.Where(acct => acct.ACCT_CLSD_DT != null &amp;&amp; Teradata.DiffDays(ACCT_OPEN_DT, ACCT_CLSD_DT) >= 90).Dump();</p>\\n\\n<p>The type or namespace name \\'DiffDays\\' does not exist in the namespace \\'Teradata\\'.</p>\\n\\n<p>However, I observe that the same Teradata.DiffDays can be used in Entity SQL like this:</p>\\n\\n<p>select Teradata.DiffDays(ACCT_OPEN_DT,ACCT_CLSD_DT) as diff from IMAccts</p>\\n\\n<p>My questions are: </p>\\n\\n<p>1) How is it possible that the function under the \"Teradata\" namespace is recognized in Entity SQL and not in LINQ to Entities? </p>\\n\\n<p>2) Is it possible for me to work around this by supplying my own DateDiff function? (Please note that collecting the data locally and then applying date manipulation is definitely not an option)</p>\\n-Teradata .NET provider code first fiasco - Entity Functions not supported-<.net><linq-to-entities><provider><teradata><entity-sql>',\n",
       " \"<p>Does teradata have an inbuilt way of generating ER diagrams of databases within it? If so, can you tell me how to do it?</p>\\n\\n<p>I can't use third party tools.</p>\\n-Teradata inbuilt ER diagram generator-<teradata>\",\n",
       " '<p>I use sqoop to dial with my TD database.\\nWhen i try this, everything is OK (my table is create in default hive database)</p>\\n\\n<pre><code>sqoop import \\\\\\n -libjars $LIB_JARS \\\\\\n -Dteradata.db.input.job.type=hive \\\\\\n -Dteradata.db.input.target.table=hive_table \\\\\\n -Dteradata.db.input.target.table.schema=\"c1 bigint\" \\\\\\n -m 1 \\\\\\n --connect jdbc:teradata://PRD/Database=database \\\\\\n --connection-manager org.apache.sqoop.teradata.TeradataConnManager \\\\\\n --username userTD \\\\\\n --password passTD \\\\\\n --table tableTD\\n</code></pre>\\n\\n<p>But when i try to specify another hive database with :</p>\\n\\n<pre><code> -Dteradata.db.input.target.database=hive_database \\\\\\n</code></pre>\\n\\n<p>The script return OK, the table is create but without any data inside...</p>\\n\\n<p>Need somme help...</p>\\n\\n<p>Thanks</p>\\n-SQOOP Import from Teradata : Create table Ok but without data-<hadoop><import><hive><teradata><sqoop>',\n",
       " '<p>I started working on a project recently and after I imported it in Eclipse (the usual way, <code>File</code> -> <code>Import</code> -> <code>Existing Projects</code> into Workspace) it started giving me this strange error. The project has a little red icon, like it is not configure properly, that is really bugging me.</p>\\n\\n<p>The error below happens also when I click on Project Facets. When I try to click OK it alerts me with the usual java-style <code>NullPointerException</code>. Otherwise, the project works just fine. I\\'m using <code>Eclipse Kepler</code>.</p>\\n\\n<p><img src=\"https://dl.dropboxusercontent.com/u/82097299/eclipse-error.png\" alt=\"eclipse-error\"></p>\\n-NullPointerException in Eclipse itself-<java><eclipse><eclipse-kepler>',\n",
       " \"<p>Is it possible to do an 'Order by lower(field_name)' in MonetDB?</p>\\n-Is it possible to sort by lower case in MonetDB?-<sql><monetdb>\",\n",
       " '<p>I developed an Eclipse plugin using Juno (version 4.2) and specified in the plugin.xml file that it is supposed to target Eclipse version 3.5.</p>\\n\\n<p>However, when I copy the jar file to the \"dropins\" folder of Eclipse Indigo (previous version) or Eclipse Kepler (current version), it is listed as an installed plugin, but doesn\\'t load on startup.</p>\\n\\n<p>Could it be that the particular instances of Kepler/Indigo that I tried the plugin on are missing at least one of the plugin\\'s dependencies?</p>\\n-how come an Eclipse plugin that targets Eclipse version 3.5 works with Juno but not Indigo or Kepler?-<eclipse><plugins><eclipse-kepler>',\n",
       " '<p>I want to get data from one database through a spout and process the data and store it in another database using trident.I am new to storm and trident and i am not sure how to implement it.I got the data from the database in a spout(separate java class which implements IRichSpout which is supported by trident) and i emit it as an object.I need to pass it to the trident topology for processing(counting the number of records) and storing it to a database.</p>\\n\\n<pre><code> TridentTopology topology = new TridentTopology();  \\n TridentState wordCounts =\\n          topology.newStream(\"spout1\",spout)\\n</code></pre>\\n\\n<p>now the new stream takes a spout as an input i.e the syntax is</p>\\n\\n<pre><code> Stream storm.trident.TridentTopology.newStream(String txId, IRichSpout spout)\\n</code></pre>\\n\\n<p>but i want to give the object emitted by the spout as an input to the stream for the trident to process and save to database.So how can i bring my spout class inside trident and pass it to new stream or should i combine both spout and trident as a same class??</p>\\n\\n<p>can someone help plz.....</p>\\n-Getting data from one database and process and store it to another database using trident topology-<apache-storm><trident>',\n",
       " '<p>I hope this question is appropriate for Stack Overflow; if not, I apologize. For some reason the close all functionality on my installation of eclipse stopped working. I can still close windows, but I can\\'t close all. Neither the hot key nor the menu functionality works anymore. My theory is that it has something to do with me occasionally breaking source pages out onto other screens. Is there a way to fix this or get some sort of output out of eclipse that will provide some clues?</p>\\n-Is there a way to fix the \"Close All\" functionality on Eclipse Kepler?-<eclipse><eclipse-kepler>',\n",
       " \"<p>I am using Eclipse Kepler with PyDev and initially it was giving me errors for all of my local imports (imports from the same project that weren't in the same folder) so I removed the PyDev configuration and then added it back and now it isn't giving me errors for anything but indentation errors.  Any idea what to do here?</p>\\n\\n<p>Or how to solve the initial problem?</p>\\n-Eclipse Kepler/PyDev not showing any errors-<python><eclipse><pydev><eclipse-kepler>\",\n",
       " '<p>I have installed Kepler yesterday, but I want to use the old workspace instead of creating from scratch.</p>\\n\\n<p>Is there any way to migrate eclipse Juno workspace to Kepler ?</p>\\n-How to migrate eclipse Juno workspace to Kepler?-<java><eclipse><eclipse-kepler>',\n",
       " '<p>I have a working plugin in Eclipse 3.7 with <code>Views</code> and a <code>Perspective</code>.<br>\\nNow I try to move this to Eclipse Kepler 4.3 therefore i follow the <a href=\"http://eclipsesource.com/blogs/tutorials/eclipse-4-e4-tutorial-soft-migration-from-3-x-to-eclipse-4-e4/\" rel=\"nofollow noreferrer\">Tutorial from Jonas Helming</a><br>\\nAll my Views are based on a <code>TemplateView</code> in another Plugin. This <code>TemplateView</code> was transformed to an <code>e4</code> style <code>POJO</code> View.<br>\\nMy Plugin extends this <code>TemplateView</code> and adds the wrapper class (as described in the Tutorial).</p>\\n\\n<p>Iff i run my application in <code>e3</code> style (means <code>TemplateView</code> is extending <code>ViewPart</code>) everything is fine and the <code>Perspective</code> gets added.\\nBut when i change it to the <code>e4</code> style (means <code>TemplateView</code> is extending <code>POJO</code> and i use the Wrapper in my <code>plugin.xml</code>) eclipse does no longer dispay the <code>Perspective</code> and its <code>Views</code>.</p>\\n\\n<p>my <code>e3</code> <code>View</code>is not really interesting, as its mostly using my custom api (which would only confuse here &amp; it is working correctly - as it starts as <code>e3</code>)</p>\\n\\n<p>and this is the <code>e4</code> Wrapper class:</p>\\n\\n<pre><code>import org.eclipse.e4.tools.compat.parts.DIViewPart;\\nimport lumo.views.contact.e3.PojoDetailView;\\n\\npublic class E4DetailView extends DIViewPart&lt;PojoDetailView&gt; {\\n    public E4DetailView() {\\n        super(PojoDetailView.class);\\n    }\\n}\\n</code></pre>\\n\\n<p>this problem is <a href=\"https://stackoverflow.com/questions/17826182/eclipse-e3-to-e4-migration-adaptionpreferred-recommendations\">part of this question</a>, but no dupe</p>\\n-Eclipse Kepler Dependency Injection in 3.x-<dependency-injection><eclipse-plugin><e4><eclipse-kepler>',\n",
       " '<p>I need some help with setting local javadoc in Eclipse Kepler, since it seems the behavior has changed from Juno to Kepler.</p>\\n\\n<p>I have set the url for the installed jdk as follows</p>\\n\\n<pre><code>file:///C:/Program%20Files%20(x86)/Java/jdk1.7.0_21/docs/api\\n</code></pre>\\n\\n<p>In Juno clicking on a class name and pressing F1 the class javadoc page opens. The address from properties is </p>\\n\\n<pre><code>file:///C:/Program%20Files%20(x86)/Java/jdk1.7.0_21/docs/api/java/io/FileInputStream.html?noframes=true#FileInputStream(java.lang.String)\\n</code></pre>\\n\\n<p>Opening the same workspace using Kepler and following the same procedure, the javadoc page does not open, instead showing a \"page cannot be displayed\" message. The address retrieved  from properties in this case is </p>\\n\\n<pre><code>res://ieframe.dll/dnserrordiagoff_webOC.htm#file:///C:/Program%20Files%20(x86)/Java/jdk1.7.0_21/docs/api/noframes=true\\n</code></pre>\\n\\n<p>The documentation pages for the particular configuration have not changed between Juno and Kepler, therefore I would expect no difference in behavior.</p>\\n-Eclipse Kepler local javadoc confguration-<eclipse><javadoc><eclipse-kepler>',\n",
       " '<p>I am migrating a database from Sql Server 2008 to Teradata\\nand I am facing a problem:</p>\\n\\n<p>In Sql Server in the ddl of a table column is defined as follows:</p>\\n\\n<pre><code>[rowguid] uniqueidentifier ROWGUIDCOL NOT NULL CONSTRAINT [DF_Address_rowguid] DEFAULT (NEWID())\\n</code></pre>\\n\\n<p>This column uses newid() function to generate and insert random varchar value in the column [rowguid] if the user doesnt provide any input.</p>\\n\\n<p>There is no similar function in Teradata to generate this value.</p>\\n\\n<p>What can be used instead of of NEWID() function of Sql Server while creating similar table ddls for Teradata?</p>\\n-i am migrtaing database from sql server 2008 to teradata-<sql-server-2008><ddl><teradata><create-table><newid>',\n",
       " '<p>Problem with too many parameter to put as columns into example set.</p>\\n\\n<p>My problem-task is that I have customers with a unique ID and they have parameter (binomial) and I would like to predict the value of certain target variables, so far only one but possible multiple.\\nIn my test case I used the following input dataset, see meta data, each customer is represented in a row and the parameter are in the columns – simply the usual way.</p>\\n\\n<p><strong>meta data:</strong><br>\\n<em>Role - Name - Type</em> <br>\\nid   - Customer_Id  - integer <br>\\nlabel - Target - binominal <br>\\nregular - Para1 - binominal <br>\\nregular - Para2 - binominal <br>\\nregular - Para3 - binominal <br>\\nregular - Para4 -  binominal <br>\\n<strong>dataset:</strong> <br>\\n*Customer_Id -  Target -  Para1  - Para2 -  Para3 -  Para4* <br>\\n1 -  M -  1  - 0 -  1 -  0 <br>\\n2 -  V  - 1 -  0  - 0 -  1 <br>\\n3 -  M -  0  - 1 -  1 -  1 <br></p>\\n\\n<p>=> With Naïve Bayes I get great prediction results in the test case with limited dimensions.</p>\\n\\n<p>Problem with the actual dataset: \\nI have some 100,000s of parameter and the number is growing a lot. The actual number of active parameter for a customer is very small and so the table would be extremely large and sparse. So my idea was to use the following dataset format as input: <br>\\n<strong>meta data:</strong><br>\\n<em>Role -  Name -   Type</em> <br>\\nid -  Customer_Id  - integer <br>\\nlabel - Target -   binominal <br>\\nregular -  ActivePara -  polynominal <br>\\n<strong>data:</strong><br>\\n*Customer_Id -  Target  - ActivePara* <br>\\n1 -  M -  Para1 <br>\\n1 -  M -  Para3 <br>\\n2 -  V -  Para1 <br>\\n2 -  V -  Para4 <br>\\n3 -  M  - Para2 <br>\\n3 -  M -  Para3 <br>\\n3 -  M -  Para4 <br></p>\\n\\n<p>BUT now I do not get consistent predictions per customer what I get is something like this</p>\\n\\n<p>*Customer_Id -  Target  - ActivePara  - Prediction of Target <br>\\n1 -  <strong>M -  Para1 -  V</strong> <br>\\n1 -  M -  Para3 -  M <br>\\n2 -  V -  Para1 -  V <br>\\n2 -  V -  Para4 -  V <br>\\n3 -  M -  Para2 -  M <br>\\n3 -  M -  Para3 -  M <br>\\n3 -  <strong>M -  Para4 -  V</strong> <br></p>\\n\\n<p><b>But I want/need the target prediction per customer_id to be consistent.</b></p>\\n\\n<p>How do I need to set up the input data/ the model to get the result!</p>\\n\\n<p><strong>Thanks a lot in advance for any hints and help!!!</strong></p>\\n-rapid-miner formating datsets with many parameter-<classification><modeling><prediction><rapidminer>',\n",
       " '<p>I\\'ve recently installed Eclipse Kepler CDT on a Win7 64 machine. It runs fine (well, sort of), but - it keeps crunching, with the status bar saying </p>\\n\\n<blockquote>\\n  <p>Updating RPM packages proposal list</p>\\n</blockquote>\\n\\n<p>Why is this happening and how can I make it stop?</p>\\n-Eclipse keeps crunching while \"updating rpm packages proposal list\"-<eclipse><windows-7><eclipse-cdt><eclipse-kepler>',\n",
       " '<p>I have eclipse kepler (really jbdev studio 7 alpha).  The JPA Entities from Tables feature is amazing!  Very sophisticated.  Start it up with right-clicking on a project to which the JPA facet has been added.  Then \"New->JPA Entities from Tables\" and follow the wizard.</p>\\n\\n<p>There are lots of ways to customize the mapping of tables to entities, including lots of flexibility regarding relations between tables, but one thing I can not find is how to map database datatypes to java datatypes.</p>\\n\\n<p>The result is that my entities were generated with many primitive datatypes and it\\'s causing me problems.</p>\\n\\n<p>How to do a \"JPA Entities from Tables\" and tell the generator to map MySQL \"int\" to Java \"Integer\" and not java \"int\"?</p>\\n\\n<p>Big pain if I have to go in and edit these manually. :-(  There are many attributes in many classes.</p>\\n-Eclipse \"JPA Entities from Tables\" how to map mysql datatypes to java datatypes?-<jpa-2.0><eclipse-kepler>',\n",
       " '<p>R3.0 is not compatible with <code>TeradataR</code> and there are no plans to update the package or release the source code.</p>\\n\\n<p>Does anyone have a code snippet that shows how to write a dataframe to a new or existing Teradata table?</p>\\n\\n<pre><code>sql &lt;- \"CREATE TABLE teradata.aaa (yr BIGINT, ct BIGINT, tax BIGINT)\"  \\ntbl &lt;- sqlQuery(ch, sql)  # Creates a table, works \\n# What syntax to insert a dataframe \\'myData\\' into this table?\\n</code></pre>\\n-Write from R to Teradata in 3.0-<r><teradata>',\n",
       " '<p>Please provide teradata syntax for the below scenario:</p>\\n\\n<ol>\\n<li>get the valid values from xml tags apart from nulls in column 1.</li>\\n<li>get the name of the tags where value is available in column 2</li>\\n</ol>\\n\\n<p>For example:refer the below xml code.</p>\\n\\n<p><em><strong>Column 1</em></strong><br>\\n<strong>55: MENS DENIM</strong><br>\\n<strong>10k</strong>        </p>\\n\\n<p><em><strong>Column 2</em></strong><br>\\n<strong>Department</strong></p>\\n\\n<p><strong>Major</strong></p>\\n\\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;Filters&gt;&lt;Filter&gt;&lt;Max/&gt;&lt;Joiners/&gt;&lt;Department&gt;55: MENS DENIM&lt;/Department&gt;\\n &lt;Major&gt;10&lt;/Major&gt;&lt;Sub/&gt;&lt;Min/&gt;&lt;/Filter&gt;&lt;/Filters&gt;\\n</code></pre>\\n-Teradata syntax for Xml codes-<syntax><teradata>',\n",
       " '<p>I need help with interval overplaps. I have these records in one table (and much more):</p>\\n\\n<p><strong>Example 1:</strong></p>\\n\\n<pre><code>Id---------StartDate------EndDate\\n\\n794122    2011-05-10    2999-12-31\\n\\n794122    2011-04-15    2999-12-31\\n\\n794122    2008-04-03    2999-12-31\\n\\n794122    2008-03-31    2999-12-31\\n\\n794122    2008-02-29    2999-12-31\\n\\n794122    2008-02-04    2999-12-31\\n\\n794122    2007-10-10    2999-12-31\\n\\n794122    2007-09-15    2999-12-31\\n</code></pre>\\n\\n<p><strong>Example 2:</strong></p>\\n\\n<pre><code>Id---------StartDate------EndDate\\n\\n5448    2012-12-28      2999-12-31\\n\\n5448    2011-06-30      2999-12-31\\n\\n5448    2005-12-26      2011-06-30\\n\\n5448    2005-06-15      2011-06-30\\n\\n5448    2006-07-31      2006-12-31\\n\\n5448    2001-03-31      2006-07-15\\n</code></pre>\\n\\n<p><strong>Example 3:</strong></p>\\n\\n<pre><code>Id---------StartDate------EndDate\\n\\n214577    2007-02-28    2999-12-31\\n\\n214577    2003-06-20    2007-03-04\\n\\n214577    2003-06-20    2007-02-28\\n</code></pre>\\n\\n<p><strong>Example 4:</strong></p>\\n\\n<pre><code>Id---------StartDate-------EndDate\\n\\n9999    2008-05-28      2999-01-01\\n\\n9999    2005-03-03      2008-05-31\\n\\n9999    2005-05-31      2005-12-31\\n\\n9999    2003-12-01      2005-08-12\\n\\n9999    2001-01-01      2002-03-05\\n\\n9999    2000-01-08      2002-01-01\\n</code></pre>\\n\\n<p><strong>I would like to get:</strong></p>\\n\\n<pre><code>*Example1* - 2007-09-15-&gt;3000-01-01\\n\\n*Example2* - 2001-03-31-&gt;3000-01-01\\n\\n*Example3* - 2003-06-20-&gt;3000-01-01\\n\\n*Example4* - 2003-12-01-&gt;3000-01-01\\n</code></pre>\\n\\n<p>Have you any suggestions how I do it? Because i dont choose max and min values(group by Id) -> This problem is in the example 4.</p>\\n\\n<p>Thanks!</p>\\n-Time interval overlaps - teradata-<sql><teradata>',\n",
       " '<p>I started using Eclipse Kepler and I am not sure if that is at fault or some other change caused it.   Strangely * is treated as a meta character (Shift-8).  To get * to be printed I have to use Option-Shift-8.  </p>\\n\\n<p>For example import is bound to Shift-8 I which essentially causes the editor to be useless.   </p>\\n\\n<p>Choosing between editors did not help</p>\\n\\n<p>I am using Macbook pro running 10.8.3</p>\\n-Eclipse kepler release keyboard mapping issue in Mac-<eclipse><osx-mountain-lion><eclipse-kepler>',\n",
       " '<p>I am using <code>DI</code> (Dependency Injection) to access the <code>IEclipsePreferences</code> from my handlers.\\nBut I need to access the same <code>IEclipsePreferences</code> from other code parts to save/load these settings.</p>\\n\\n<p>This is the example how I get the preferences and show em in a dialog:</p>\\n\\n<pre class=\"lang-java prettyprint-override\"><code>import java.lang.reflect.InvocationTargetException;\\nimport javax.inject.Inject;\\nimport javax.inject.Named;\\nimport org.eclipse.core.runtime.preferences.IEclipsePreferences;\\nimport org.eclipse.e4.core.di.annotations.Execute;\\nimport org.eclipse.e4.core.di.extensions.Preference;\\nimport org.eclipse.e4.ui.model.application.MApplication;\\nimport org.eclipse.e4.ui.services.IServiceConstants;\\nimport org.eclipse.jface.dialogs.MessageDialog;\\nimport org.eclipse.swt.widgets.Shell;\\nimport org.osgi.service.prefs.BackingStoreException;\\n\\npublic class PojoShowPersistencePreferencesHandler {\\n@Inject\\n@Preference\\nIEclipsePreferences preferences;\\n\\n@Inject\\nMApplication application;\\n\\n@Execute\\npublic void execute(@Named(IServiceConstants.ACTIVE_SHELL) Shell shell\\n) throws InvocationTargetException, InterruptedException {\\n    System.out.println(getClass().getSimpleName() + \".execute()\");\\n    System.out.println(\"preferences: \" + preferences);\\n    System.out.println(\"application: \" + application);\\n    try {\\n\\n        String msg = \"Driver:\\\\t\"    + preferences.get(\"jdbc_driver\", \"404\") + \"\\\\n\" + //\\n                \"URL:\\\\t\" + preferences.get(\"jdbc_url\", \"404\") + \"\\\\n\" + //\\n                \"User:\\\\t\" + preferences.get(\"jdbc_user\", \"404\") + \"\\\\n\" + //\\n                \"Password:\\\\t\" + preferences.get(\"jdbc_pwd\", \"404\");\\n        preferences.flush();\\n        MessageDialog.openInformation(shell,\\n                \"Info :: Save Persistence Preferences\",msg //\\n        );\\n        System.out.println(msg);\\n    } catch (BackingStoreException e) {\\n        e.printStackTrace();\\n    }\\n}\\n</code></pre>\\n\\n<p>}</p>\\n\\n<p>a) How can I do the same without using an <code>Handler</code>?\\nb) I need to SET these values by code without <code>DI</code> or as a Handler with Parameters, which can be called Programmatically</p>\\n\\n<p>I\\'ve been trying <a href=\"http://www.vogella.com/articles/EclipsePreferences/\" rel=\"nofollow\">this (Vogella) article</a> but somehow I cannot find the values stored in these (<code>Instance</code>, <code>Configuration</code>) scopes, but they are stored, as they get shown with the <code>Handler</code>!</p>\\n\\n<p>NOTE: i am using 3.x <code>Plugin</code> style, so using old style is not a problem.</p>\\n-Eclipse e4 Kepler: IEclipsePreferences read/write-<eclipse><eclipse-kepler>',\n",
       " '<p>I want to have a support form use the content of the \\'problem title\\' field as a trigger to add a div below with possible answers just as StackOverflow does when you submit a new question.</p>\\n\\n<p>To clarify, I do not want the suggested answers to be presented as a \\'auto-complete\\' option, I want them to appear below as a suggested answer.</p>\\n\\n<p>The markup in the page might be something like:</p>\\n\\n<pre><code>&lt;input type=\"text\" value=\"\" placeholder=\"Your problem in brief\"&gt;\\n&lt;div id=\"PossibleAnswers\" class=\"hidden\"&gt;\\n&lt;ul&gt;\\n    &lt;li&gt;&lt;a href=\"#\"&gt;Possible answer 1&lt;/a&gt;&lt;/li&gt;\\n    &lt;li&gt;&lt;a href=\"#\"&gt;Possible answer 2&lt;/a&gt;&lt;/li&gt;\\n    &lt;li&gt;&lt;a href=\"#\"&gt;Possible answer 3&lt;/a&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n&lt;/div&gt;\\n</code></pre>\\n\\n<p>As the user starts to type the summary of their problem the div below would populate with suggested answers.</p>\\n-jQuery FAQ autosuggest answer below title as per StackOverflow-<jquery><autosuggest><proactive>',\n",
       " '<p>I am trying to insert records in a table in the below format </p>\\n\\n<pre><code>Name              Amount       Date       Counter    \\nA                  100        Jan 1          1\\nA                  100        Jan2           1\\nA                  200        Jan 10         2\\nA                  300        Mar 30         3\\nB                   50        Jan 7          1\\nC                   20        Jan 7          1\\n</code></pre>\\n\\n<p>Could someone tell me the sql for generating the value for the Counter field .\\nThe counter value should increment whenever the amount changes and reset when the name changes.</p>\\n-Counters in Teradata while inserting records-<sql><teradata>',\n",
       " '<p>In Oracle, the LIKE operator can be used either with or without a wildcard.\\nFor example \"where name like \\'SMITH\\'\" will return rows where the name column is exactly \\'SMITH\\',\\nand \"where name like \\'SMITH%\\'\" will return rows where the name column begins with \\'SMITH\\' followed by 0 or more other characters.</p>\\n\\n<p>However in Teradata, \"where name like \\'SMITH\\'\" returns nothing.  Does ANSI standard not specify behavior for the LIKE operator if used without a wildcard?</p>\\n\\n<p>Boolean algebra says \\'SMITH\\' like \\'SMITH\\' is true.  Oracle behaves this way; Teradata does not. Can Teradata SQL be tweaked to enable the LIKE operator to work both with and without wildcards?</p>\\n-Teradata LIKE operator without wildcard?-<sql><oracle><teradata>',\n",
       " '<p>I am trying to use the FTP server (factory) in Geronimo 3.0.1 on Fedora 19, in eclipse kepler. I have the following import which produces no error:</p>\\n\\n<pre><code>import org.apache.mina.*;\\n</code></pre>\\n\\n<p>However, when I declare</p>\\n\\n<pre><code>FTPServerFactory ftpFactory;\\nFTPServer ftpServer;\\n</code></pre>\\n\\n<p>neither of FTPServer and FTPServerFactory is resolvable. The usual eclipse hints in the editor, which are very cool, offer no help in this case. My build path has the mina-core.jar (This is the only MINA jar that I find in /usr/share/java/apache-mina). The build path dialog flags errors, not explicitly for mina, stating the the following are missing:</p>\\n\\n<pre><code>org.eclipse.JRE_CONTAINER/\\norg.eclipse.jdt.internal.debug.uio.launcher.StandardVMType/\\njava-1.7.0-openjdk-1.7.0.25.x86-64\\n</code></pre>\\n\\n<p>I suspect that my installation is missing other mina jars and am at a loss for the three errors above except that the last one is strange given that the that the build path has</p>\\n\\n<pre><code>java-1.7.0-openjdk-1.7.0\\n</code></pre>\\n\\n<p>My environment is all relatively new, so there could be problems in a number of places. Any advice on where to start?</p>\\n\\n<p>Thanks in advance.</p>\\n-Geronimo FTP Server on Fedora 19-<configuration><fedora><apache-mina><geronimo><eclipse-kepler>',\n",
       " '<p>I tried the new Eclipse Kepler that already comes with a maven plugin.</p>\\n\\n<p>But when I want to add a dependency (open the pom.xml, go to dependencies tab and click add) I can input some string like \"jetty\" in the search box but nothing happens. In older versions I got a list of all dependencies containing \"jetty\".</p>\\n\\n<p>I don\\'t know if this is important but I directly get an info when opening the \"add dependency screen\":\\n\"Artifact Id cannot be empty\".</p>\\n\\n<p>I do not use a proxy or have any network issues.</p>\\n\\n<p>I also missed an information something like: \"indexing maven repo...\"</p>\\n\\n<p>Any idea?</p>\\n\\n<p>Thanks!</p>\\n-Cannot search for artifact in Eclipse Kepler using m2e plugin-<eclipse><maven><m2e><eclipse-kepler>',\n",
       " \"<p>I'm new to teradata. I want to insert numbers 1 to 1000 into the table <code>test_seq</code>, which is created as below.</p>\\n\\n<pre><code>create table test_seq(\\n    seq_id integer\\n);\\n</code></pre>\\n\\n<p>After searching on this site, I came up with recusrive query to insert the numbers.</p>\\n\\n<pre><code>insert into test_seq(seq_id)\\nwith recursive cte(id) as (\\n    select 1 from test_dual\\n    union all\\n    select id + 1 from cte\\n    where id + 1 &lt;= 1000\\n    )\\nselect id from cte;\\n</code></pre>\\n\\n<p><code>test_dual</code> is created as follows and it contains just a single value. (something like DUAL in Oracle)</p>\\n\\n<pre><code>create table test_dual(\\n    test_dummy varchar(1)\\n);\\n\\ninsert into test_dual values ('X');\\n</code></pre>\\n\\n<p>But, when I run the insert statement, I get the error, <code>Failure 2616 Numeric overflow occurred during computation.</code></p>\\n\\n<p>What did I do wrong here? Isn't the <code>integer</code> datatype enough to hold numeric value 1000?\\nAlso, is there a way to write the query so that i can do away with <code>test_dual</code> table?</p>\\n-Numeric Overflow in Recursive Query : Teradata-<sql><teradata><recursive-cte>\",\n",
       " '<p>When a validation is performed, a part of the dataset is used to build a model and then the model is tested on the remaining records in the dataset. I need to look at the result for each record in the testing process, say, for a classification task, I need to know which record is classified as what in the testing phase and what are the records exactly that are used for testing. Can anyone point me to the section in RapidMiner where I can find a table of the tested records and their results?</p>\\n\\n<p>Thanks!</p>\\n-How to look at each record in RapidMiner Testing Results?-<rapidminer>',\n",
       " \"<p>I just upgraded Eclipse from Juno to Kepler, and suddenly I can no longer type asterisks.  I have to open up a text editor, type the asterisk there, then copy-paste it into Eclipse.  I <em>can</em> type an asterisk in Eclipse when I am using Find/Replace, but I am unable to when I am editing a python or Java file.</p>\\n-Why can't I type an asterisk in Eclipse?-<eclipse><macos><eclipse-kepler>\",\n",
       " \"<p>I have a database where I can see actions taken by users. These actions have a timestamp. All the timestamps are in EST.\\nLet's say the action is, downloading content.\\nI need to find, x amount of users downloaded content y in GMT(these users are in GMT).\\nI need to find, x amount of users downloaded content y in PST(these users are in PST).</p>\\n\\n<p>I have the mapping of countries etc. However, I can't find a good query which creates a table with the timezone mappings for the database.</p>\\n\\n<p>Can anyone give me an idea on how I would do the conversion in SQL and point me in the direction of a query which recreates the timezone mapping table.</p>\\n-Teradata SQL Timezone adjustments-<sql><timezone><teradata>\",\n",
       " '<p>eclipse kepler, in my case: D:\\\\Eclipse Kepler\\\\dropins, does not locate new bundles in the dropins folder. I tried a lot of methods described on official sites, for example starting eclipse with: eclipse -clean -console -consoleLog -debug /path/to/.options</p>\\n\\n<p>\"Create an .options file with the following content:\\norg.eclipse.equinox.p2.core/debug=true\\norg.eclipse.equinox.p2.core/reconciler=true\"</p>\\n\\n<p>It does not work. All the bundles i copied in dropins folder wont detect by eclipse, so i cant find this bundles in OSGI RunConfiguration. On the other side, if i manually edit the bundles.info (in my case: \"D:\\\\Eclipse Kepler\\\\eclipse\\\\configuration\\\\org.eclipse.equinox.simpleconfigurator\\\\bundles.info\"), it would show me the bundle, which i inserted into bundles.info, in OSGI RunConig. But i dont want to edit manually all the time because of time-wasting. Does anyone have an idea? </p>\\n\\n<p>Thanx</p>\\n-Dropins folder does not find new bundles automatically-<eclipse><osgi><bundle><eclipse-kepler>',\n",
       " \"<p>I have a column date like\\n<code>date CHAR(7) CHARACTER SET LATIN NOT CASESPECIFIC,</code> where i have some values like 2010-12, 2011-10, etc. i want to change these into the date format yyyy/mm . How do i do it?\\nThanks</p>\\n-How to change a 'date' in Char format to date format in teradata SQL?-<sql><date><teradata>\",\n",
       " '<p>I have defined primary index on three columns as below</p>\\n\\n<p>primary index ( col1,col2,col3)</p>\\n\\n<p>To query the table using this index will the order of col1,col2,col3 in the predicate matter ?</p>\\n-Primary index in teradata-<sql><teradata>',\n",
       " '<p>Sorry if I am just really dense, but I read the <a href=\"http://eclipsesource.com/blogs/2013/06/26/top-10-eclipse-kepler-features/\" rel=\"nofollow\">Top 10 Eclipse Kepler Features</a> and I see Orion at #1. It claims to run all in the browser. I find it really interesting, but it looks like a completely separate tool. Am I wrong?</p>\\n\\n<p><a href=\"http://www.eclipse.org/orion/\" rel=\"nofollow\">http://www.eclipse.org/orion/</a></p>\\n-Eclipse Orion, does it not run in Eclipse?-<eclipse><browser><web><eclipse-kepler><eclipse-orion>',\n",
       " '<p>we can query the available GPU with nvenc hardware like this:</p>\\n\\n<pre><code>     cuResult = cuInit(0);\\n\\nif (cuResult != CUDA_SUCCESS)\\n{\\n    printf(\"&gt;&gt; GetNumberEncoders() - cuInit() failed error:0x%x\\\\n\", cuResult);\\n    exit(EXIT_FAILURE);\\n}\\n\\ncheckCudaErrors(cuDeviceGetCount(&amp;deviceCount));\\n\\nif (deviceCount == 0)\\n{\\n    printf(\"&gt;&gt; GetNumberEncoders() - reports no devices available that support CUDA\\\\n\");\\n    exit(EXIT_FAILURE);\\n}\\nelse\\n{\\n    printf(\"&gt;&gt; GetNumberEncoders() has detected %d CUDA capable GPU device(s) &lt;&lt;\\\\n\", deviceCount);\\n\\n    for (int currentDevice=0; currentDevice &lt; deviceCount; currentDevice++)\\n    {\\n        checkCudaErrors(cuDeviceGet(&amp;cuDevice, currentDevice));\\n        checkCudaErrors(cuDeviceGetName(gpu_name, 100, cuDevice));\\n        checkCudaErrors(cuDeviceComputeCapability(&amp;SMmajor, &amp;SMminor, currentDevice));\\n        printf(\"  [ GPU #%d - &lt; %s &gt; has Compute SM %d.%d, NVENC %s ]\\\\n\",\\n               currentDevice, gpu_name, SMmajor, SMminor,\\n               (((SMmajor &lt;&lt; 4) + SMminor) &gt;= 0x30) ? \"Available\" : \"Not Available\");\\n\\n        if (((SMmajor &lt;&lt; 4) + SMminor) &gt;= 0x30)\\n        {\\n            encoderInfo[NVENC_devices].device = currentDevice;\\n            strcpy(encoderInfo[NVENC_devices].gpu_name, gpu_name);\\n            NVENC_devices++;\\n        }\\n    }\\n}\\n</code></pre>\\n\\n<p>I have 8 GPU whit NVENC capability:</p>\\n\\n<p>How can we check that specific NVENC hardware is now running or Idle. Is there any way to monitoring NVENC hardware ?</p>\\n\\n<p>What about specific NVENC API function \"OR\" CUDA Driver or API functions that help me to find out which GPU or NVENC hardware is Idle?</p>\\n\\n<p>NOTE: I know that CUDA and NVENC hardware are completely separate things but I am looking for Direct or Indirect (using Cuda API like using Cuda for specifying the available NVENC hardware) way for checking specific NVENC\\'s status ???  </p>\\n-Monitoring NVENC hardware (Active or idle)-<cuda><gpu><nvidia><video-encoding><kepler>',\n",
       " '<p>I decided to handle a problem in a Java project with AOP, so I installed AJDT 2.2.3 in my Eclipse Kepler. Everything seemed to work fine, as I can create a new AspectJ project and create aspects and everything works as it should. Aspects are properly woven and execute as I expected. After that I tried to integrate AspectJ in my already existing Java project. I converted it by using <code>Configure &gt; Convert to AspectJ Project</code>. Still everything works fine, but when I come to the point of actually create an aspect in this project using the create aspect wizard, I encounter the folling error message:\\n<code>Creation of element failed. See error log for more details.</code> I can still finish the wizard, but the created aspect file looks different to the aspect files created in my first attempts in a AspectJ Project. </p>\\n\\n<p><img src=\"https://i.stack.imgur.com/PbAmE.png\" alt=\"File\"></p>\\n\\n<p>If I edit the created file, the same aspect that worked in the first try in a original AspectJ Project don\\'t seem to have any effect.</p>\\n\\n<p>The log message (some lines in between omitted):</p>\\n\\n<pre><code>java.lang.reflect.InvocationTargetException\\n    at org.eclipse.jface.operation.ModalContext.run(ModalContext.java:421)\\nat org.eclipse.jface.wizard.WizardDialog.run(WizardDialog.java:1028)\\nat org.eclipse.jdt.internal.ui.wizards.NewElementWizard.performFinish(NewElementWizard.java:134)\\n...\\nat org.eclipse.equinox.launcher.Main.invokeFramework(Main.java:636)\\nat org.eclipse.equinox.launcher.Main.basicRun(Main.java:591)\\nat org.eclipse.equinox.launcher.Main.run(Main.java:1450)\\nCaused by: java.lang.NullPointerException\\nat org.eclipse.ajdt.internal.ui.wizards.AJNewTypeWizardPage.ajc$interMethod$org_eclipse_ajdt_internal_ui_wizards_AJNewTypeWizardPage$org_eclipse_ajdt_internal_ui_wizards_NewTypeWizardPage$createAJType(AJNewTypeWizardPage.aj:267)\\nat org.eclipse.ajdt.internal.ui.wizards.NewTypeWizardPage.ajc$interMethodDispatch2$org_eclipse_ajdt_internal_ui_wizards_AJNewTypeWizardPage$createAJType(NewTypeWizardPage.java:1)\\nat org.eclipse.ajdt.internal.ui.wizards.AJNewTypeWizardPage.ajc$interMethodDispatch1$org_eclipse_ajdt_internal_ui_wizards_AJNewTypeWizardPage$org_eclipse_ajdt_internal_ui_wizards_NewTypeWizardPage$createAJType(AJNewTypeWizardPage.aj)\\n...\\norg.eclipse.jdt.internal.core.JavaModelOperation.run(JavaModelOperation.java:728)\\nat org.eclipse.core.internal.resources.Workspace.run(Workspace.java:2345)\\nat org.eclipse.jdt.core.JavaCore.run(JavaCore.java:5331)\\nat org.eclipse.jdt.internal.ui.actions.WorkbenchRunnableAdapter.run(WorkbenchRunnableAdapter.java:106)\\nat org.eclipse.jface.operation.ModalContext$ModalContextThread.run(ModalContext.java:121)\\n</code></pre>\\n\\n<p>Does anybody know what this error wants to tell me and what I can do to use aspects in my project?</p>\\n-Create an Aspect with AJDT in former Java Project-<eclipse><aspectj><eclipse-kepler><ajdt>',\n",
       " '<p>I am new to teradata. I run a sql code on teradata which gives me a table as the output. I want this table to be populated in an excel sheet directly whenever I run this code. how can I do it?\\nThank You.</p>\\n-How to link the output of teradata sql query to an excel sheet?-<sql><excel><teradata>',\n",
       " '<p>I tried to change the font size in the dialog of the views increasing all the font settings in preferences->general->appearance->color and fonts, but it was completely useless.<p>\\nDoes anyone know how can I increase the font size in the view boxes in eclipse? (on a mac the text is very small)\\n<p>\\nI need to change the font size for example in the project explorer, outline etc..</p>\\n-Change font size in the project explorer view in Eclipse-<eclipse><eclipse-plugin><ide><eclipse-rcp><eclipse-kepler>',\n",
       " \"<p>I have a task to compare some Teradata Views with actual Oracle Tables.\\nI need a script for that.I have taken Java approach in which I connect to a specific schema from Oracle and then call the <code>SELECT * FROM all_tables order by TABLE_NAME</code> query and write this into a file.\\nI do the same for other schema but now my problem is Teradata.\\nCan you people please suggest me some script or query by which I can get proper details like it does with Oracle.</p>\\n\\n<p>There is no complex Java Code but if you still want I can post it.</p>\\n\\n<p>Edited:</p>\\n\\n<p>Okay now I have a schema in Oracle which has all the tables.so views for those tables are created in Teredata.\\nI have to compare oracle tables and Teradata views every morning and send the differances.\\nSo I use <code>SELECT * FROM all_tables order by TABLE_NAME</code> in Oracle and for Teradata I use <code>SELECT * FROM dbc.tables WHERE tablekind='V' AND databasename='SCHEMA' order by TableName</code> so now when I compare them I dont get accurate results, so I wanted to know does any script exists or how do I approach.</p>\\n-Script to compare Oracle Schema with Teradata Views-<oracle><compare><view><teradata>\",\n",
       " \"<p>I am writing SQL for Teradata.  I need to use joins to connect data from multiple tables.  Is it typically faster to use subqueries or create temporary tables and append columns one join at a time?  I'm trying to test it myself but network traffic makes it hard for me to tell which is faster.</p>\\n\\n<p>Example A:</p>\\n\\n<pre><code>SELECT a.ID, a.Date, b.Gender, c.Age\\nFROM mainTable AS a\\nLEFT JOIN (subquery 1) AS b ON b.ID = a.ID\\nLEFT JOIN (subquery 2) AS c ON c.ID = a.ID\\n</code></pre>\\n\\n<p>Or I could...</p>\\n\\n<p>Example B:</p>\\n\\n<pre><code>CREATE TABLE a AS (\\n SELECT mainTable.ID, mainTable.Date, sq.Gender\\n FROM mainTable\\n LEFT JOIN (subquery 1) AS sq ON sq.id = mainTable.ID\\n)\\nCREATE TABLE b AS (\\n  SELECT a.ID, a.Date, a.Gender, sq.Age\\n  FROM a\\n  LEFT JOIN (subquery 2) AS sq ON sq.id = a.ID\\n)\\n</code></pre>\\n\\n<p>Assuming I clean everything up afterward, is one approach preferable to another?  Again, I would like to just test this myself but the network traffic is kind of messing me up.</p>\\n\\n<p><strong>EDIT:</strong>  The main table has anywhere from 100k to 5 million rows.  The subqueries return a 1:1 relationship to the main table's IDs, but require WHERE clauses to filter dates.  The subquery SQL isn't trivial, I guess is what I'm trying to convey.</p>\\n-Multiple Joins in Teradata SQL - Faster to Use Subqueries or Temp Tables?-<sql><teradata>\",\n",
       " '<p>Within Teradata, when executing an ALTER TABLE command to modify the data type for an existing column from VARCHAR(10) to CHAR(10), I receive a 3558 error indicating that the specified attribute can not be altered. Is there an alternate method of coding this to achieve the desired objective or does the column need to be dropped and re-created in order to change the data type?</p>\\n-Teradata Alter Table Command to Modify existing column data type from varchar to char with same length-<teradata><alter>',\n",
       " \"<p>I am trying to do an agregate multiplication.  I'll use a baseball example.</p>\\n\\n<p>Sample Table</p>\\n\\n<pre><code>team  player     battingavg  1-battingavg = nohitavg\\n1       A       0.123         0.877\\n1       B       0.144         0.856\\n1       C       0.210         0.790\\n2       E       0.142         0.858\\n2       F       0.231         0.769\\n2       G       0.125         0.875\\n</code></pre>\\n\\n<p>I want to aggregate multiply to get the overall probability of having at no hits.</p>\\n\\n<p>For team 1 =    (0.877) * (0.856) * (0.790) = probability of have no hits\\nFor team 2 =    (0.858) * (0.769) * (0.875) = probability of have no hits</p>\\n\\n<p>1 - probability of no hits = probability of at least 1 hit.</p>\\n\\n<pre><code>SELECT team\\n     , SOMEFUNCTION(nohitavg) as probofnohit \\nFROM table \\nGROUP BY team\\n</code></pre>\\n\\n<p>Desired output</p>\\n\\n<pre><code>team  probofnohit\\n1        0.593\\n2        0.577\\n</code></pre>\\n-Teradata Multiplying percentages in an Aggregate-<sql><aggregate><teradata><multiplication>\",\n",
       " '<p>I have a jar file that contains some tag-files.</p>\\n\\n<p>My <code>*.tag</code> files are inside <code>/META-INF/tags/</code> folder (jar)</p>\\n\\n<p>I also have a <code>mytags.tld</code> inside <code>/META-INF/</code> folder (jar)</p>\\n\\n<p>After pack all <code>war</code> project (with <code>mytags.jar</code> inside <code>WEB-INF/lib</code> folder), it works fine in JBoss. But Eclipse still cannot recognize the tag, getting the error <code>Can not find the tag library descriptor for \"http://www.mycompany.com\"</code></p>\\n\\n<p><br>\\nIs there a way to Eclipse recognize my tags?</p>\\n\\n<hr>\\n\\n<p>follow the sources:</p>\\n\\n<p><strong>block.tag</strong></p>\\n\\n<pre><code>&lt;%@tag description=\"Item do block\" pageEncoding=\"UTF-8\"%&gt;\\n&lt;%@taglib uri=\"http://java.sun.com/jsp/jstl/core\" prefix=\"c\" %&gt;\\n\\n&lt;%@attribute name=\"id\" required=\"true\" %&gt;\\n&lt;%@attribute name=\"label\" required=\"true\" %&gt;\\n&lt;%@attribute name=\"description\" required=\"false\" %&gt;\\n&lt;%@attribute name=\"icon\" required=\"false\" %&gt;\\n\\n&lt;div id=\"${id}\" class=\"block\"&gt;\\n    &lt;div class=\"block-box ${icon}\"&gt;\\n        &lt;div class=\"label\"&gt;\\n            &lt;span&gt;${label}&lt;/span&gt;\\n        &lt;/div&gt;\\n        &lt;div class=\"description\"&gt;\\n            ${description}\\n            &lt;jsp:doBody&gt;&lt;/jsp:doBody&gt;\\n        &lt;/div&gt;\\n    &lt;/div&gt;\\n&lt;/div&gt;\\n</code></pre>\\n\\n<p><br/></p>\\n\\n<p><strong>mytags.tld</strong></p>\\n\\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;\\n&lt;taglib xmlns=\"http://java.sun.com/xml/ns/javaee\"\\n    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\\n    xsi:schemaLocation=\"http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-jsptaglibrary_2_1.xsd\"\\n    version=\"2.1\"&gt;\\n\\n    &lt;description&gt;My Tags&lt;/description&gt;\\n    &lt;display-name&gt;MyTags&lt;/display-name&gt;\\n    &lt;tlib-version&gt;1.0&lt;/tlib-version&gt;\\n    &lt;short-name&gt;mytags&lt;/short-name&gt;\\n    &lt;uri&gt;http://www.mycompany.com&lt;/uri&gt;\\n\\n    &lt;tag-file&gt;\\n        &lt;name&gt;block&lt;/name&gt;\\n        &lt;path&gt;/META-INF/tags/block.tag&lt;/path&gt;\\n    &lt;/tag-file&gt;\\n&lt;/taglib&gt;\\n</code></pre>\\n\\n<p><br/>\\n<strong>some.jsp</strong></p>\\n\\n<pre><code>&lt;%@page contentType=\"text/html; charset=ISO-8859-1\" pageEncoding=\"UTF-8\" %&gt;\\n&lt;%@taglib uri=\"http://java.sun.com/jsp/jstl/core\" prefix=\"c\" %&gt;\\n&lt;%@taglib uri=\"http://www.mycompany.com\" prefix=\"mytags\" %&gt; &lt;-- ECLIPSE MARKS ERROR HERE\\n&lt;!DOCTYPE html&gt;\\n&lt;html&gt;\\n\\n    &lt;head&gt;\\n        &lt;title&gt;Test&lt;/title&gt;\\n    &lt;/head&gt;\\n    &lt;body&gt;\\n        &lt;mytags:block id=\"users\" label=\"Users\" icon=\"user\"&gt;\\n            &lt;!-- some content --&gt;\\n        &lt;/mytags:block&gt;\\n    &lt;/body&gt;\\n&lt;/html&gt;\\n</code></pre>\\n\\n<hr>\\n\\n<p>But everything works fine in JBoss. Only eclipse accuses error.</p>\\n\\n<p>Thanks</p>\\n-Eclipse cannot find tld inside jar file-<eclipse><jsp-tags><taglib><eclipse-kepler><tagfile>',\n",
       " '<p>Suddenly breakpoints have stopped working in Eclipse PyDev. When I double click on the left margin in the code editor everything seems to work fine except that a blue diagonal overstrike appears on top of the break point symbol in the margin. Note that the overstrike is diagonal so it has nothing todo with the breakpoint being disabled. When I disable it I get yet another horizontal overstrike as usual. But when I run the code (with breakspoints enable) the debugger ignores all breakpoint still. The problem occurs on both Juno and Kepler. What is wrong?</p>\\n-Overstriken and Ignored PyDev BreakPoints-<eclipse><pydev><breakpoints><eclipse-juno><eclipse-kepler>',\n",
       " '<p>I do not have access to the internet from eclipse so I can not add software using update sites.  I have tried several different methods but none seems to be working.</p>\\n\\n<p>I am using the JBoss Dev Studio version of kepler, but I figured this might be a general eclipse question.</p>\\n\\n<p>Tried Help- Install New Software - Add... - browse to zip file and I get \"could not find jar:file:/blahblahblah/jautodoc_1.10.0.zip!/\"  Nothing.</p>\\n\\n<p>Tried unzipping it so we end up with eclipse/dropins/jautodoc_1.10.0/[features | plugins followed by restart. Nothing.</p>\\n\\n<p>Tried unzipping it so we end up with eclipse/dropins/[features | plugins] followed by restart.  Still nothing.</p>\\n\\n<p>What is the definitive way to do this?</p>\\n-How to manually install JAutodoc in eclipse kepler?-<eclipse-kepler><jautodoc>',\n",
       " '<p><strong>Environment:</strong></p>\\n\\n<p>Java/Spring application that uses JPA/Hibernate for persistence and connects to a Teradata datasource configured in the app container (Tomcat) which is accessed through JNDI.</p>\\n\\n<p>Versions that I am using:</p>\\n\\n<pre><code>java: 6\\nspring: 3.2.4.RELEASE\\nhibernate.core: 4.2.4.Final\\nhibernate.entitymanager: 4.2.4.Final\\nhibernate.validator: 5.0.1.Final\\nspringdata: 1.3.4.RELEASE\\njavax.validation: 1.1.0.Final\\n</code></pre>\\n\\n<p><strong>Problem:</strong></p>\\n\\n<p>There are two Teradata databases in the same server that have a same named table but with different columns:</p>\\n\\n<pre><code>DatDe001.SFITEM\\nColumns: [iipcst, iidesc, iivend, updated_at, iisku#, created_at, item_expdt, item_effdt]\\n\\nDEV_DIG_UMT.SFITEM\\nColumns: [iipcst, iidesc, iivend, row_updt_tms, iisku#, row_insrt_tms, item_expdt, item_effdt]\\n</code></pre>\\n\\n<p>As you can see the columns that differ are updated_at -> <strong>row_updt_tms</strong> and created_at -> <strong>row_insrt_tms</strong></p>\\n\\n<p>I am using a JNDI datasource which is configured using this jdbc url:</p>\\n\\n<pre><code>jdbc:teradata://&lt;server_ip&gt;/DATABASE=DEV_DIG_UMT,DBS_PORT=1025,COP=OFF,CHARSET=UTF8,TMODE=ANSI  \\n</code></pre>\\n\\n<p>It is supposed that the jdbc connection will resolve the location of the table using the DATABASE value in that jdbc url. However Hibernate seems to be taking the wrong one: <strong>DatDe001.SFITEM</strong> when performing the initial schema validation, that is at the moment of context initialization when Spring tries to create the EntityManagerFactory bean:</p>\\n\\n<pre><code>2013-08-15 13:32:03,635 INFO localhost-startStop-1 org.hibernate.tool.hbm2ddl.TableMetadata - HHH000261: Table found: DatDe001.SFITEM\\n2013-08-15 13:32:03,635 INFO localhost-startStop-1 org.hibernate.tool.hbm2ddl.TableMetadata - HHH000037: Columns: [iipcst, iidesc, iivend, updated_at, iisku#, created_at, item_expdt, item_effdt]\\n</code></pre>\\n\\n<p>So as my JPA entity (see the entity below in the post) does not have those columns, the hibernate validation throws an exception (see the summarized stack trace):</p>\\n\\n<pre><code>org.springframework.beans.factory.BeanCreationException: Error creating bean with name \\'org.springframework.dao.annotation.PersistenceExceptionTranslationPostProcessor#0\\': Initialization of bean failed; nested exception is org.springframework.beans.factory.BeanCreationException: Error creating bean with name \\'entityManagerFactory\\' defined in file [C:\\\\APP\\\\springsource\\\\vfabric-tc-server-developer-2.9.2.RELEASE\\\\base-instance\\\\wtpwebapps\\\\profile-items\\\\WEB-INF\\\\classes\\\\META-INF\\\\spring\\\\applicationContext.xml]: Invocation of init method failed; nested exception is javax.persistence.PersistenceException: [PersistenceUnit: persistenceUnit] Unable to build EntityManagerFactory\\nat org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:529)\\nat org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:458)\\n...\\nCaused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name \\'entityManagerFactory\\' defined in file [C:\\\\APP\\\\springsource\\\\vfabric-tc-server-developer-2.9.2.RELEASE\\\\base-instance\\\\wtpwebapps\\\\profile-items\\\\WEB-INF\\\\classes\\\\META-INF\\\\spring\\\\applicationContext.xml]: Invocation of init method failed; nested exception is javax.persistence.PersistenceException: [PersistenceUnit: persistenceUnit] Unable to build EntityManagerFactory\\nat org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1482)\\nat org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:521)\\n...\\nCaused by: javax.persistence.PersistenceException: [PersistenceUnit: persistenceUnit] Unable to build EntityManagerFactory\\n            at org.hibernate.ejb.Ejb3Configuration.buildEntityManagerFactory(Ejb3Configuration.java:924)\\n            at org.hibernate.ejb.Ejb3Configuration.buildEntityManagerFactory(Ejb3Configuration.java:899)\\n...\\nCaused by: org.hibernate.HibernateException: Missing column: row_updt_tms in DatDe001.SFITEM\\nat org.hibernate.mapping.Table.validateColumns(Table.java:366)\\nat org.hibernate.cfg.Configuration.validateSchema(Configuration.java:1305)\\nat org.hibernate.tool.hbm2ddl.SchemaValidator.validate(SchemaValidator.java:155)\\nat org.hibernate.internal.SessionFactoryImpl.&lt;init&gt;(SessionFactoryImpl.java:508)\\nat org.hibernate.cfg.Configuration.buildSessionFactory(Configuration.java:1790)\\nat org.hibernate.ejb.EntityManagerFactoryImpl.&lt;init&gt;(EntityManagerFactoryImpl.java:96)\\nat org.hibernate.ejb.Ejb3Configuration.buildEntityManagerFactory(Ejb3Configuration.java:914)\\n</code></pre>\\n\\n<p>After I saw that, I was wondering if this behavior will persist when executing a query statement to the db through JPA/hibernate, or if it will point to the right table in that case.</p>\\n\\n<p>Then just for investigation purposes I changed my JPA entity to have the same columns that DatDe001.SFITEM table:</p>\\n\\n<pre><code>@Entity\\npublic class Sfitem implements Serializable {\\n    private static final long serialVersionUID = 1L;\\n\\n    @EmbeddedId\\n    private SfitemPK id;\\n\\n    @Column(name=\"\\\\\"iidesc\\\\\"\")\\n    private String iidesc;\\n\\n    @Column(name=\"\\\\\"iipcst\\\\\"\")\\n    private BigDecimal iipcst;\\n\\n    @Column(name=\"\\\\\"iivend\\\\\"\")\\n    private BigDecimal iivend;\\n\\n    @Temporal\\n    @Column(name=\"\\\\\"item_expdt\\\\\"\")\\n    private Date itemExpdt;\\n\\n    @Temporal\\n    @Column(name=\"\\\\\"created_at\\\\\"\")\\n    private Date createdAt;\\n\\n    @Temporal\\n    @Column(name=\"\\\\\"updated_at\\\\\"\")\\n    private Date updatedAt;\\n\\n    ...\\n}\\n</code></pre>\\n\\n<p>I started the application and it got loaded successfully. Instead of showing the exception now the log looked good:</p>\\n\\n<pre><code>...\\n2013-08-15 14:42:52,056 INFO localhost-startStop-1 org.hibernate.tool.hbm2ddl.TableMetadata - HHH000261: Table found: DatDe001.SFITEM\\n2013-08-15 14:42:52,056 INFO localhost-startStop-1 org.hibernate.tool.hbm2ddl.TableMetadata - HHH000037: Columns: [iipcst, iidesc, iivend, updated_at, iisku#, created_at, item_expdt, item_effdt]\\n2013-08-15 14:42:52,061 DEBUG localhost-startStop-1 org.hibernate.internal.SessionFactoryImpl - Checking 0 named HQL queries\\n2013-08-15 14:42:52,061 DEBUG localhost-startStop-1 org.hibernate.internal.SessionFactoryImpl - Checking 0 named SQL queries\\n2013-08-15 14:42:52,063 TRACE localhost-startStop-1 org.hibernate.service.internal.AbstractServiceRegistryImpl - Initializing service [role=org.hibernate.service.config.spi.ConfigurationService]\\n2013-08-15 14:42:52,113 TRACE localhost-startStop-1 org.hibernate.service.internal.AbstractServiceRegistryImpl - Initializing service [role=org.hibernate.stat.spi.StatisticsImplementor]\\n...\\n</code></pre>\\n\\n<p>I tried to execute a query to the table and surprisingly found that this time Hibernate was pointing to the right database/schema: <strong>DEV_DIG_UMT</strong>, the query failed because now the entity had the columns for the other database: <strong>DatDe001</strong>, see the log:</p>\\n\\n<pre><code>2013-08-15 14:50:05,731 TRACE tomcat-http--4 org.hibernate.engine.query.spi.QueryPlanCache - Located HQL query plan in cache (SELECT o FROM Sfitem o WHERE o.id.iisku = :iisku AND o.id.itemEffdt &lt;= :date AND coalesce(o.itemExpdt, cast(\\'9999-12-31\\' as date)) &gt;= :date)\\n2013-08-15 14:50:05,766 TRACE tomcat-http--4 org.hibernate.engine.query.spi.QueryPlanCache - Located HQL query plan in cache (SELECT o FROM Sfitem o WHERE o.id.iisku = :iisku AND o.id.itemEffdt &lt;= :date AND coalesce(o.itemExpdt, cast(\\'9999-12-31\\' as date)) &gt;= :date)\\n2013-08-15 14:50:05,768 TRACE tomcat-http--4 org.hibernate.engine.query.spi.HQLQueryPlan - Find: SELECT o FROM Sfitem o WHERE o.id.iisku = :iisku AND o.id.itemEffdt &lt;= :date AND coalesce(o.itemExpdt, cast(\\'9999-12-31\\' as date)) &gt;= :date\\n2013-08-15 14:50:05,772 TRACE tomcat-http--4 org.hibernate.engine.spi.QueryParameters - Named parameters: {iisku=387671, date=2013-08-08}\\n2013-08-15 14:50:05,810 DEBUG tomcat-http--4 org.hibernate.SQL - select sfitem0_.\"iisku#\" as iisku1_0_, sfitem0_.\"item_effdt\" as item_eff2_0_, sfitem0_.\"created_at\" as created_3_0_, sfitem0_.\"iidesc\" as iidesc4_0_, sfitem0_.\"iipcst\" as iipcst5_0_, sfitem0_.\"iivend\" as iivend6_0_, sfitem0_.\"item_expdt\" as item_exp7_0_ from sfitem sfitem0_ where sfitem0_.\"iisku#\"=? and sfitem0_.\"item_effdt\"&lt;=? and coalesce(sfitem0_.\"item_expdt\", cast(\\'9999-12-31\\' as DATE))&gt;=?\\n2013-08-15 14:50:05,832 DEBUG tomcat-http--4 org.hibernate.engine.jdbc.spi.SqlExceptionHelper - could not prepare statement [select sfitem0_.\"iisku#\" as iisku1_0_, sfitem0_.\"item_effdt\" as item_eff2_0_, sfitem0_.\"created_at\" as created_3_0_, sfitem0_.\"iidesc\" as iidesc4_0_, sfitem0_.\"iipcst\" as iipcst5_0_, sfitem0_.\"iivend\" as iivend6_0_, sfitem0_.\"item_expdt\" as item_exp7_0_ from sfitem sfitem0_ where sfitem0_.\"iisku#\"=? and sfitem0_.\"item_effdt\"&lt;=? and coalesce(sfitem0_.\"item_expdt\", cast(\\'9999-12-31\\' as DATE))&gt;=?]\\ncom.teradata.jdbc.jdbc_4.util.JDBCException: [Teradata Database] [TeraJDBC 14.00.00.21] [Error 3810] [SQLState 42S22] Column/Parameter \\'DEV_DIG_UMT.sfitem0_.created_at\\' does not exist.\\n    at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDatabaseSQLException(ErrorFactory.java:307)\\n    at com.teradata.jdbc.jdbc_4.statemachine.ReceiveInitSubState.action(ReceiveInitSubState.java:102)\\n    at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.subStateMachine(StatementReceiveState.java:320)\\n    at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.action(StatementReceiveState.java:201)\\n    at com.teradata.jdbc.jdbc_4.statemachine.StatementController.runBody(StatementController.java:121)\\n    at com.teradata.jdbc.jdbc_4.statemachine.StatementController.run(StatementController.java:112)\\n...\\n    at org.hibernate.engine.jdbc.internal.StatementPreparerImpl$5.doPrepare(StatementPreparerImpl.java:161)\\n    at org.hibernate.engine.jdbc.internal.StatementPreparerImpl$StatementPreparationTemplate.prepareStatement(StatementPreparerImpl.java:182)\\n    at org.hibernate.engine.jdbc.internal.StatementPreparerImpl.prepareQueryStatement(StatementPreparerImpl.java:159)\\n    at org.hibernate.loader.Loader.prepareQueryStatement(Loader.java:1859)\\n        at org.hibernate.loader.Loader.executeQueryStatement(Loader.java:1836)\\n        at org.hibernate.loader.Loader.executeQueryStatement(Loader.java:1816)\\n        at org.hibernate.loader.Loader.doQuery(Loader.java:900)\\n        at org.hibernate.loader.Loader.doQueryAndInitializeNonLazyCollections(Loader.java:342)\\n        at org.hibernate.loader.Loader.doList(Loader.java:2526)\\n        at org.hibernate.loader.Loader.doList(Loader.java:2512)\\n        at org.hibernate.loader.Loader.listIgnoreQueryCache(Loader.java:2342)\\n        at org.hibernate.loader.Loader.list(Loader.java:2337)\\n        at org.hibernate.loader.hql.QueryLoader.list(QueryLoader.java:495)\\n</code></pre>\\n\\n<p><strong>This means that hibernate validation and the query executor routines are behaving differently</strong></p>\\n\\n<p>The entity with the correct fields:</p>\\n\\n<pre><code>@Entity\\npublic class Sfitem implements Serializable {\\n    private static final long serialVersionUID = 1L;\\n\\n    @EmbeddedId\\n    private SfitemPK id;\\n\\n    @Column(name=\"\\\\\"iidesc\\\\\"\")\\n    private String iidesc;\\n\\n    @Column(name=\"\\\\\"iipcst\\\\\"\")\\n    private BigDecimal iipcst;\\n\\n    @Column(name=\"\\\\\"iivend\\\\\"\")\\n    private BigDecimal iivend;\\n\\n    @Column(name=\"\\\\\"item_expdt\\\\\"\")\\n    private Date itemExpdt;\\n\\n    @Column(name=\"\\\\\"row_insrt_tms\\\\\"\")\\n    private Timestamp rowInsrtTms;\\n\\n    @Column(name=\"\\\\\"row_updt_tms\\\\\"\")\\n    private Timestamp rowUpdtTms;\\n\\n    ...\\n}\\n</code></pre>\\n\\n<p>Persistence.xml</p>\\n\\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?&gt;\\n&lt;persistence xmlns=\"http://java.sun.com/xml/ns/persistence\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" version=\"2.0\" xsi:schemaLocation=\"http://java.sun.com/xml/ns/persistence http://java.sun.com/xml/ns/persistence/persistence_2_0.xsd\"&gt;\\n&lt;persistence-unit name=\"persistenceUnit\" transaction-type=\"RESOURCE_LOCAL\"&gt;\\n        &lt;provider&gt;org.hibernate.ejb.HibernatePersistence&lt;/provider&gt;\\n        &lt;properties&gt;\\n            &lt;property name=\"hibernate.dialect\" value=\"org.hibernate.dialect.TeradataDialect\"/&gt;\\n            &lt;!-- value=\"create\" to build a new database on each run; value=\"update\" to modify an existing database; value=\"create-drop\" means the same as \"create\" but also drops tables when Hibernate closes; value=\"validate\" makes no changes to the database --&gt;\\n            &lt;property name=\"hibernate.hbm2ddl.auto\" value=\"validate\"/&gt;\\n            &lt;property name=\"hibernate.ejb.naming_strategy\" value=\"org.hibernate.cfg.ImprovedNamingStrategy\"/&gt;\\n            &lt;property name=\"hibernate.connection.charSet\" value=\"UTF-8\"/&gt;\\n            &lt;!-- Uncomment the following two properties for JBoss only --&gt;\\n            &lt;!-- property name=\"hibernate.validator.apply_to_ddl\" value=\"false\" /--&gt;\\n            &lt;!-- property name=\"hibernate.validator.autoregister_listeners\" value=\"false\" /--&gt;\\n        &lt;/properties&gt;\\n    &lt;/persistence-unit&gt;\\n&lt;/persistence&gt;\\n</code></pre>\\n\\n<p>Datasource and entity manager beans:</p>\\n\\n<pre><code>&lt;bean id=\"dataSource\" class=\"org.springframework.jndi.JndiObjectFactoryBean\"&gt;\\n  &lt;property name=\"jndiName\" value=\"${datasource.jndiName}\"/&gt;\\n  &lt;property name=\"lookupOnStartup\" value=\"true\"/&gt;\\n  &lt;property name=\"resourceRef\" value=\"true\" /&gt;\\n&lt;/bean&gt;\\n\\n&lt;bean class=\"org.springframework.orm.jpa.JpaTransactionManager\" id=\"transactionManager\"&gt;\\n    &lt;property name=\"entityManagerFactory\" ref=\"entityManagerFactory\"/&gt;\\n&lt;/bean&gt;\\n\\n&lt;bean class=\"org.springframework.orm.jpa.LocalContainerEntityManagerFactoryBean\" id=\"entityManagerFactory\"&gt;\\n    &lt;property name=\"persistenceUnitName\" value=\"persistenceUnit\"/&gt;\\n    &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt;\\n&lt;/bean&gt;\\n</code></pre>\\n\\n<p>Is that a bug or a configuration issue? Has anyone faced this same issue? </p>\\n\\n<p>I don\\'t want to configure a default schema in the persistence unit nor in the entities, because the approach we are following is to keep the datasource configuration outside the application and in a single place by using the JNDI datasource defined in the container context. That way we don\\'t need to worry when deploying to different environments (Dev, QA, Prod, etc)</p>\\n-How does hibernate resolve schema for initially validating to a Teradata database?-<java><spring><hibernate><jpa><teradata>',\n",
       " '<p>I am trying to connect to Teradata using jdbc. My application is running in JDK 1.4.</p>\\n\\n<p>The url i am using is \"jdbc:teradata:///TMODE=ANSI,CHARSET=UTF8\";</p>\\n\\n<p>I get \"No suitable driver error\" when i try to run using JDK 1.4. \\nHowever, I am able to run my app using JDK 1.7 with the same url.</p>\\n\\n<p>I have both terajdbc4.jar and tdgssconfig.jar in my Netbeans libraries. </p>\\n-Not able to connect to Teradata using jdbc-<java><jdbc><sdk><teradata>',\n",
       " \"<p>I'm trying to write a simple IF/ELSE statement in Teradata. As far as I understand you have to use a CASE. What I want to do is write a statement that will check if a column IS NOT NULL and display something. ELSE if it IS NULL, display something else.</p>\\n\\n<p>Every example I found simply replaces a single value with a hard coded string or int. I'm looking for something more along the lines of this which uses the THEN statement to make another SELECT:</p>\\n\\n<pre><code>  SELECT CARS.VIN_NUM, CARS.DRIVER_NAMES\\n     CASE\\n       WHEN CARS.FUEL IS NOT NULL THEN SELECT CARS.DESTINATIONS\\n       WHEN CARS.FUEL IS NULL THEN SELECT CARS.GAS_STATIONS\\n     END\\n  FROM AUTOMOBILES CARS\\n  WHERE CARS.VIN_NUM IN\\n  ('345353',\\n   '354632',\\n   '535231')\\n  ORDER BY CARS.VIN_NUM\\n</code></pre>\\n\\n<p>The end result should be a table displaying the VIN_NUM, DRIVER_NAMES, DESTINATIONS OR GAS_STATIONS based on the CASE. Is something like this possible or am I going about it the wrong way?</p>\\n-SQL case statement in Teradata-<sql><if-statement><case><teradata>\",\n",
       " '<p>I want to report on various statistics on a specific Teradata database, particularly \"free space\".  Should table skew be included in the calculation?  For example, someone suggested the following query:</p>\\n\\n<pre><code>SELECT databasename\\n     , SUM(maxperm)/1024/1024/1024 (DECIMAL(10,2))     AS space_allocated\\n     , SUM(currentperm)/1024/1024/1024 (DECIMAL(10,2)) AS space_Used\\n     , (MAX(currentperm)*COUNT(*)-SUM(currentperm))\\n        /1024/1024/1024 (DECIMAL(10, 2)) AS skew_Size\\n\\n     , (space_used + skew_size) AS total_space_used\\n\\n     , (MIN(maxperm-currentperm)/1024/1024/1024) * COUNT(*) (DECIMAL(10,2)) \\n        AS free_Space\\n\\n     ,  CAST(total_space_used AS DECIMAL(10,2)) * 100 \\n      / CAST(space_allocated AS DECIMAL(10,2)) AS pct_used\\n\\nFROM   DBC.diskspace\\nWHERE  databasename = \\'MyDatabase\\'\\n   AND maxperm &gt; 0\\nGROUP   BY 1;\\n</code></pre>\\n\\n<p>I\\'m particularly curious about the calculation of <code>total_space_used</code> and <code>pct_used</code>.  Is it \"proper\" to account for skewed tables like this? </p>\\n-How to calculate available space in a Teradata database-<teradata>',\n",
       " \"<p>I couldn't find an answer to this in any other questions and I wanted to see if anyone knew. I'm using JBoss AS 7.1 on Kepler eclipse, and I was wondering if there is a way to change your standalone.xml while the server is running and have Jboss push the change. Would just cleaning the server do this?</p>\\n-Cleaning JBoss AS 7.1 Standalone.xml-<jboss7.x><eclipse-kepler>\",\n",
       " '<p>I\\'m trying Eclipse Kepler 4.3 EE</p>\\n\\n<p>After open it over my old workspace, my project starts with an build error:</p>\\n\\n<pre><code>This project has the JPA facet, but no JPA project could be created. See the error log for more details.\\n</code></pre>\\n\\n<p>This is the details:</p>\\n\\n<pre><code>eclipse.buildId=4.3.0.I20130605-2000\\njava.version=1.7.0_21\\njava.vendor=Oracle Corporation\\nBootLoader constants: OS=win32, ARCH=x86_64, WS=win32, NL=pt_BR\\nFramework arguments:  -product org.eclipse.epp.package.jee.product\\nCommand-line arguments:  -data D:\\\\JJW\\\\srcWeb -os win32 -ws win32 -arch x86_64 -product org.eclipse.epp.package.jee.product\\n\\nError\\nMon Aug 19 14:46:53 BRT 2013\\nnull JPA platform: P/jjwxp-web\\n\\njava.lang.IllegalArgumentException\\n    at org.eclipse.jpt.jpa.core.internal.InternalJpaProjectManager.buildJpaProject(InternalJpaProjectManager.java:643)\\n    at org.eclipse.jpt.jpa.core.internal.InternalJpaProjectManager.buildJpaProject(InternalJpaProjectManager.java:635)\\n    at org.eclipse.jpt.jpa.core.internal.InternalJpaProjectManager.buildJpaProject(InternalJpaProjectManager.java:628)\\n    at org.eclipse.jpt.jpa.core.internal.InternalJpaProjectManager.addJpaProject(InternalJpaProjectManager.java:609)\\n    at org.eclipse.jpt.jpa.core.internal.InternalJpaProjectManager.buildJpaProject_(InternalJpaProjectManager.java:312)\\n    at org.eclipse.jpt.jpa.core.internal.InternalJpaProjectManager$BuildJpaProjectCommand.execute(InternalJpaProjectManager.java:306)\\n    at org.eclipse.jpt.common.core.internal.utility.command.CommandJobCommandAdapter.execute(CommandJobCommandAdapter.java:50)\\n    at org.eclipse.jpt.common.core.internal.utility.command.JobCommandJob.run(JobCommandJob.java:42)\\n    at org.eclipse.core.internal.jobs.Worker.run(Worker.java:53)\\n</code></pre>\\n\\n<p>Project facet: JPA 2.0</p>\\n\\n<p>Persistence.xml</p>\\n\\n<pre class=\"lang-xml prettyprint-override\"><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;persistence version=\"2.0\"\\n    xmlns=\"http://java.sun.com/xml/ns/persistence\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\\n    xsi:schemaLocation=\"http://java.sun.com/xml/ns/persistence http://java.sun.com/xml/ns/persistence/persistence_2_0.xsd\"&gt;\\n\\n\\n    &lt;persistence-unit name=\"jjwxp-web-unit\"\\n        transaction-type=\"RESOURCE_LOCAL\"&gt;\\n\\n        &lt;provider&gt;org.eclipse.persistence.jpa.PersistenceProvider&lt;/provider&gt;\\n\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.Pais&lt;/class&gt;\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.Estado&lt;/class&gt;\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.Cidade&lt;/class&gt;\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.Usuario&lt;/class&gt;\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.Empresa&lt;/class&gt;\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.Pessoa&lt;/class&gt;\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.PessoaCliente&lt;/class&gt;\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.PessoaRepresentante&lt;/class&gt;\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.PessoaContato&lt;/class&gt;\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.ListaPrecos&lt;/class&gt;\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.ListaPrecosRepresentante&lt;/class&gt;\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.ListaPrecosColuna&lt;/class&gt;\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.ListaPrecosItem&lt;/class&gt;\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.Produto&lt;/class&gt;\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.ProdutoGrade&lt;/class&gt;\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.ProdutoEstoque&lt;/class&gt;\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.ProdutoCor&lt;/class&gt;\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.FaixaTamanho&lt;/class&gt;\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.GrupoProduto&lt;/class&gt;\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.Tamanho&lt;/class&gt;\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.Cor&lt;/class&gt;\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.ProdutoImagem&lt;/class&gt;\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.ProdutoCorImagem&lt;/class&gt;\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.PVE&lt;/class&gt;\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.PVEItem&lt;/class&gt;\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.PVEItemGrade&lt;/class&gt;\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.CondicaoPagamento&lt;/class&gt;\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.ModalidadeVenda&lt;/class&gt;\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.Moeda&lt;/class&gt;\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.ClassificacaoFiscal&lt;/class&gt;\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.ClassificacaoFiscalEmpresa&lt;/class&gt;\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.RepresentanteProduto&lt;/class&gt;\\n        &lt;class&gt;br.com.jjw.jjwxp.web.model.ModuloSistema&lt;/class&gt;\\n\\n\\n        &lt;shared-cache-mode&gt;NONE&lt;/shared-cache-mode&gt;\\n\\n        &lt;properties&gt;\\n            &lt;property name=\"eclipselink.ddl-generation\" value=\"NONE\" /&gt;\\n            &lt;property name=\"eclipselink.weaving\" value=\"dynamic\" /&gt;\\n\\n            &lt;property name=\"eclipselink.logging.level\" value=\"FINE\" /&gt;\\n            &lt;property name=\"eclipselink.logging.logger\"\\n                value=\"org.eclipse.persistence.logging.CommonsLoggingSessionLog\" /&gt;\\n        &lt;/properties&gt;\\n\\n    &lt;/persistence-unit&gt;\\n\\n&lt;/persistence&gt;\\n</code></pre>\\n\\n<p>Any ideas about what is happening ?</p>\\n-eclipse 4.3 EE facet jpa 2.0 build error-<eclipse><jpa-2.0><eclipselink><eclipse-kepler>',\n",
       " '<p>I recently switched a project from java 1.6 to 1.7. Now I\\'m getting these \"Redundant specification of type arguments xxx\"-warnings from Eclipse. Since I\\'m too lazy to remove these warnings by hand I wanted this be done by a code cleanup. Eclipse offers me the quickfix \"remove type arguments\". I couldn\\'t find anything in the code-cleanup menu.</p>\\n\\n<p>Any hints how to achive this?</p>\\n-\"remove type arguments\" in code-cleanup-<eclipse><eclipse-kepler>',\n",
       " \"<p>i am trying to pull data from Teradata and put it into hadoop.</p>\\n\\n<p>i have written a script to do so. </p>\\n\\n<p>Well this is not a direct process.</p>\\n\\n<p>It is staged to Hadoop's local and then put into Hadoop.</p>\\n\\n<p>While running the script i am getting the following error:</p>\\n\\n<pre>\\n0002 .LOGTABLE  log_1;\\n**** 16:06:28 UTY1006 CLI error: 235, MTDP: EM_GSSINITFAIL(235): call to\\n     gss_init failed.\\n**** 16:06:28 UTY2410 Total processor time used = '0 Seconds'\\n.       Start : 16:06:28 - TUE AUG 20, 2013\\n.       End   : 16:06:28 - TUE AUG 20, 2013\\n.       Highest return code encountered = '12'.\\n</pre>\\n\\n<p>Can anyone help me and tell what is the mistake here? What does that error mean?</p>\\n-Fast Export script of Teradata-<teradata>\",\n",
       " \"<p>Many people have posted complaints that m2e resets their compiler settings to 1.5. The answer to these complaints is often to  set the source and target levels as shown as below:</p>\\n\\n<pre><code>&lt;plugins&gt;\\n    &lt;plugin&gt;\\n        &lt;inherited&gt;true&lt;/inherited&gt;\\n        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\\n        &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;\\n        &lt;version&gt;3.1&lt;/version&gt;\\n        &lt;configuration&gt;\\n            &lt;source&gt;1.7&lt;/source&gt;\\n            &lt;target&gt;1.7&lt;/target&gt;\\n        &lt;/configuration&gt;\\n    &lt;/plugin&gt;\\n&lt;/plugins&gt;\\n</code></pre>\\n\\n<p>I am using Eclipse Kepler which come with m2e built in, and this does not work for me. I've checked for updates to m2e and there are none. Every time I select Maven > Update Project, my compiler version gets set back to 1.5 and I have to open the project setting and reset it to 1.7.</p>\\n\\n<p>Is there something else that needs to be done or is this a bug in m2e for Kepler?</p>\\n-Maven to Eclipse (m2e) ignores compiler version setting in POM file in Kepler-<eclipse><maven><m2e><eclipse-kepler>\",\n",
       " '<p>I have tried to install UML2 Tools SDK plugin according to <a href=\"http://www.vogella.com/articles/UML/article.html\" rel=\"nofollow\">this</a> tutorial in Eclipse 4.3 (kepler) version. I am working with windows 7 OS. But i did\\'t get the solution. will you help me?</p>\\n-Eclipse: How to install uml2 Tools SDK in Eclipse 4.3 (kepler)?-<eclipse><eclipse-plugin><uml><eclipse-kepler>',\n",
       " '<p>I followed the instructions on page \\n<a href=\"http://wiki.eclipse.org/FAQ_How_do_I_upgrade_Eclipse%3F\" rel=\"nofollow\">http://wiki.eclipse.org/FAQ_How_do_I_upgrade_Eclipse%3F</a>\\nbut unfortunately eclipse does not start anymore. I can choose a workspace and then it stops with error \"An error has occured. See the log file ....metadata.log</p>\\n\\n<p>The log file shows a bunch of errors which are:</p>\\n\\n<pre><code>!ENTRY org.eclipse.e4.ui.workbench 4 0 2013-08-22 06:43:34.234\\n!MESSAGE Unable to create class \\'org.eclipse.e4.core.commands.CommandServiceAddon\\' from bundle \\'283\\'\\n!STACK 0\\norg.eclipse.e4.core.di.InjectionException: Unable to process \"CommandServiceAddon#init()\": no actual value was found for the argument \"IEclipseContext\".\\n    at org.eclipse.e4.core.internal.di.InjectorImpl.reportUnresolvedArgument(InjectorImpl.java:412)\\n    at org.eclipse.e4.core.internal.di.InjectorImpl.processAnnotated(InjectorImpl.java:874)\\n    at org.eclipse.e4.core.internal.di.InjectorImpl.inject(InjectorImpl.java:119)\\n    at org.eclipse.e4.core.internal.di.InjectorImpl.internalMake(InjectorImpl.java:333)\\n    at org.eclipse.e4.core.internal.di.InjectorImpl.make(InjectorImpl.java:267)\\n    at org.eclipse.e4.core.contexts.ContextInjectionFactory.make(ContextInjectionFactory.java:186)\\n    at org.eclipse.e4.ui.internal.workbench.ReflectionContributionFactory.createFromBundle(ReflectionContributionFactory.java:105)\\n    at org.eclipse.e4.ui.internal.workbench.ReflectionContributionFactory.doCreate(ReflectionContributionFactory.java:71)\\n    at org.eclipse.e4.ui.internal.workbench.ReflectionContributionFactory.create(ReflectionContributionFactory.java:49)\\n    at org.eclipse.e4.ui.internal.workbench.swt.E4Application.createE4Workbench(E4Application.java:276)\\n    at org.eclipse.ui.internal.Workbench$5.run(Workbench.java:581)\\n    at org.eclipse.core.databinding.observable.Realm.runWithDefault(Realm.java:332)\\n    at org.eclipse.ui.internal.Workbench.createAndRunWorkbench(Workbench.java:567)\\n    at org.eclipse.ui.PlatformUI.createAndRunWorkbench(PlatformUI.java:150)\\n    at org.eclipse.ui.internal.ide.application.IDEApplication.start(IDEApplication.java:124)\\n    at org.eclipse.equinox.internal.app.EclipseAppHandle.run(EclipseAppHandle.java:196)\\n    at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.runApplication(EclipseAppLauncher.java:110)\\n    at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.start(EclipseAppLauncher.java:79)\\n    at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:354)\\n    at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:181)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n    at java.lang.reflect.Method.invoke(Method.java:601)\\n    at org.eclipse.equinox.launcher.Main.invokeFramework(Main.java:636)\\n    at org.eclipse.equinox.launcher.Main.basicRun(Main.java:591)\\n    at org.eclipse.equinox.launcher.Main.run(Main.java:1450)\\n    at org.eclipse.equinox.launcher.Main.main(Main.java:1426)\\n\\n!ENTRY org.eclipse.e4.ui.workbench 4 0 2013-08-22 06:43:34.250\\n!MESSAGE Unable to create class \\'org.eclipse.e4.ui.internal.workbench.addons.CommandProcessingAddon\\' from bundle \\'299\\'\\n!STACK 0\\norg.eclipse.e4.core.di.InjectionException: Unable to process \"CommandProcessingAddon.commandService\": no actual value was found for the argument \"ECommandService\".\\n    at org.eclipse.e4.core.internal.di.InjectorImpl.reportUnresolvedArgument(InjectorImpl.java:412)\\n    at org.eclipse.e4.core.internal.di.InjectorImpl.resolveRequestorArgs(InjectorImpl.java:403)\\n    at org.eclipse.e4.core.internal.di.InjectorImpl.inject(InjectorImpl.java:108)\\n    at org.eclipse.e4.core.internal.di.InjectorImpl.internalMake(InjectorImpl.java:333)\\n    at org.eclipse.e4.core.internal.di.InjectorImpl.make(InjectorImpl.java:267)\\n    at org.eclipse.e4.core.contexts.ContextInjectionFactory.make(ContextInjectionFactory.java:186)\\n    at org.eclipse.e4.ui.internal.workbench.ReflectionContributionFactory.createFromBundle(ReflectionContributionFactory.java:105)\\n    at org.eclipse.e4.ui.internal.workbench.ReflectionContributionFactory.doCreate(ReflectionContributionFactory.java:71)\\n    at org.eclipse.e4.ui.internal.workbench.ReflectionContributionFactory.create(ReflectionContributionFactory.java:49)\\n    at org.eclipse.e4.ui.internal.workbench.swt.E4Application.createE4Workbench(E4Application.java:276)\\n    at org.eclipse.ui.internal.Workbench$5.run(Workbench.java:581)\\n    at org.eclipse.core.databinding.observable.Realm.runWithDefault(Realm.java:332)\\n    at org.eclipse.ui.internal.Workbench.createAndRunWorkbench(Workbench.java:567)\\n    at org.eclipse.ui.PlatformUI.createAndRunWorkbench(PlatformUI.java:150)\\n    at org.eclipse.ui.internal.ide.application.IDEApplication.start(IDEApplication.java:124)\\n    at org.eclipse.equinox.internal.app.EclipseAppHandle.run(EclipseAppHandle.java:196)\\n    at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.runApplication(EclipseAppLauncher.java:110)\\n    at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.start(EclipseAppLauncher.java:79)\\n    at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:354)\\n    at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:181)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n    at java.lang.reflect.Method.invoke(Method.java:601)\\n    at org.eclipse.equinox.launcher.Main.invokeFramework(Main.java:636)\\n    at org.eclipse.equinox.launcher.Main.basicRun(Main.java:591)\\n    at org.eclipse.equinox.launcher.Main.run(Main.java:1450)\\n    at org.eclipse.equinox.launcher.Main.main(Main.java:1426)\\n\\n!ENTRY org.eclipse.e4.ui.workbench 4 0 2013-08-22 06:43:34.250\\n!MESSAGE Unable to create class \\'org.eclipse.e4.ui.workbench.swt.util.BindingProcessingAddon\\' from bundle \\'302\\'\\n!STACK 0\\norg.eclipse.e4.core.di.InjectionException: Unable to process \"BindingProcessingAddon.commandService\": no actual value was found for the argument \"ECommandService\".\\n    at org.eclipse.e4.core.internal.di.InjectorImpl.reportUnresolvedArgument(InjectorImpl.java:412)\\n    at org.eclipse.e4.core.internal.di.InjectorImpl.resolveRequestorArgs(InjectorImpl.java:403)\\n    at org.eclipse.e4.core.internal.di.InjectorImpl.inject(InjectorImpl.java:108)\\n    at org.eclipse.e4.core.internal.di.InjectorImpl.internalMake(InjectorImpl.java:333)\\n    at org.eclipse.e4.core.internal.di.InjectorImpl.make(InjectorImpl.java:267)\\n    at org.eclipse.e4.core.contexts.ContextInjectionFactory.make(ContextInjectionFactory.java:186)\\n    at org.eclipse.e4.ui.internal.workbench.ReflectionContributionFactory.createFromBundle(ReflectionContributionFactory.java:105)\\n    at org.eclipse.e4.ui.internal.workbench.ReflectionContributionFactory.doCreate(ReflectionContributionFactory.java:71)\\n    at org.eclipse.e4.ui.internal.workbench.ReflectionContributionFactory.create(ReflectionContributionFactory.java:49)\\n    at org.eclipse.e4.ui.internal.workbench.swt.E4Application.createE4Workbench(E4Application.java:276)\\n    at org.eclipse.ui.internal.Workbench$5.run(Workbench.java:581)\\n    at org.eclipse.core.databinding.observable.Realm.runWithDefault(Realm.java:332)\\n    at org.eclipse.ui.internal.Workbench.createAndRunWorkbench(Workbench.java:567)\\n    at org.eclipse.ui.PlatformUI.createAndRunWorkbench(PlatformUI.java:150)\\n    at org.eclipse.ui.internal.ide.application.IDEApplication.start(IDEApplication.java:124)\\n    at org.eclipse.equinox.internal.app.EclipseAppHandle.run(EclipseAppHandle.java:196)\\n    at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.runApplication(EclipseAppLauncher.java:110)\\n    at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.start(EclipseAppLauncher.java:79)\\n    at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:354)\\n    at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:181)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n    at java.lang.reflect.Method.invoke(Method.java:601)\\n    at org.eclipse.equinox.launcher.Main.invokeFramework(Main.java:636)\\n    at org.eclipse.equinox.launcher.Main.basicRun(Main.java:591)\\n    at org.eclipse.equinox.launcher.Main.run(Main.java:1450)\\n    at org.eclipse.equinox.launcher.Main.main(Main.java:1426)\\n\\n!ENTRY org.eclipse.e4.ui.workbench 4 0 2013-08-22 06:43:34.281\\n!MESSAGE Unable to create class \\'org.eclipse.e4.ui.internal.workbench.addons.HandlerProcessingAddon\\' from bundle \\'299\\'\\n!STACK 0\\norg.eclipse.e4.core.di.InjectionException: java.lang.NullPointerException\\n    at org.eclipse.e4.core.internal.di.MethodRequestor.execute(MethodRequestor.java:63)\\n    at org.eclipse.e4.core.internal.di.InjectorImpl.processAnnotated(InjectorImpl.java:877)\\n    at org.eclipse.e4.core.internal.di.InjectorImpl.inject(InjectorImpl.java:119)\\n    at org.eclipse.e4.core.internal.di.InjectorImpl.internalMake(InjectorImpl.java:333)\\n    at org.eclipse.e4.core.internal.di.InjectorImpl.make(InjectorImpl.java:267)\\n    at org.eclipse.e4.core.contexts.ContextInjectionFactory.make(ContextInjectionFactory.java:186)\\n    at org.eclipse.e4.ui.internal.workbench.ReflectionContributionFactory.createFromBundle(ReflectionContributionFactory.java:105)\\n    at org.eclipse.e4.ui.internal.workbench.ReflectionContributionFactory.doCreate(ReflectionContributionFactory.java:71)\\n    at org.eclipse.e4.ui.internal.workbench.ReflectionContributionFactory.create(ReflectionContributionFactory.java:49)\\n    at org.eclipse.e4.ui.internal.workbench.swt.E4Application.createE4Workbench(E4Application.java:276)\\n    at org.eclipse.ui.internal.Workbench$5.run(Workbench.java:581)\\n    at org.eclipse.core.databinding.observable.Realm.runWithDefault(Realm.java:332)\\n    at org.eclipse.ui.internal.Workbench.createAndRunWorkbench(Workbench.java:567)\\n    at org.eclipse.ui.PlatformUI.createAndRunWorkbench(PlatformUI.java:150)\\n    at org.eclipse.ui.internal.ide.application.IDEApplication.start(IDEApplication.java:124)\\n    at org.eclipse.equinox.internal.app.EclipseAppHandle.run(EclipseAppHandle.java:196)\\n    at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.runApplication(EclipseAppLauncher.java:110)\\n    at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.start(EclipseAppLauncher.java:79)\\n    at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:354)\\n    at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:181)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n    at java.lang.reflect.Method.invoke(Method.java:601)\\n    at org.eclipse.equinox.launcher.Main.invokeFramework(Main.java:636)\\n    at org.eclipse.equinox.launcher.Main.basicRun(Main.java:591)\\n    at org.eclipse.equinox.launcher.Main.run(Main.java:1450)\\n    at org.eclipse.equinox.launcher.Main.main(Main.java:1426)\\nCaused by: java.lang.NullPointerException\\n    at org.eclipse.e4.ui.internal.workbench.addons.HandlerProcessingAddon.processActiveHandler(HandlerProcessingAddon.java:162)\\n    at org.eclipse.e4.ui.internal.workbench.addons.HandlerProcessingAddon.postConstruct(HandlerProcessingAddon.java:59)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n    at java.lang.reflect.Method.invoke(Method.java:601)\\n    at org.eclipse.e4.core.internal.di.MethodRequestor.execute(MethodRequestor.java:56)\\n    ... 27 more\\n\\n!ENTRY org.eclipse.osgi 4 0 2013-08-22 06:43:34.343\\n!MESSAGE Application error\\n!STACK 1\\njava.lang.LinkageError: loader constraint violation: when resolving method \"org.eclipse.e4.core.commands.ExpressionContext.&lt;init&gt;(Lorg/eclipse/e4/core/contexts/IEclipseContext;)V\" the class loader (instance of org/eclipse/osgi/internal/baseadaptor/DefaultClassLoader) of the current class, org/eclipse/ui/internal/services/EvaluationService, and the class loader (instance of org/eclipse/osgi/internal/baseadaptor/DefaultClassLoader) for resolved class, org/eclipse/e4/core/commands/ExpressionContext, have different Class objects for the type se/e4/core/contexts/IEclipseContext;)V used in the signature\\n    at org.eclipse.ui.internal.services.EvaluationService.&lt;init&gt;(EvaluationService.java:85)\\n    at org.eclipse.ui.internal.Workbench.init(Workbench.java:1529)\\n    at org.eclipse.ui.internal.Workbench.access$39(Workbench.java:1511)\\n    at org.eclipse.ui.internal.Workbench$58.run(Workbench.java:2672)\\n</code></pre>\\n\\n<p>Is there any eclipse guru out there who could help me with that ?</p>\\n\\n<p>Thank you.</p>\\n-Eclipse doesn\\'t start after upgrading from Juno to Kepler-<eclipse><upgrade><eclipse-kepler>',\n",
       " \"<p>How can I run a classification for new examples against my trained model, without re-running the trained model again?</p>\\n\\n<p>The trained model takes some time to process (1 hour), and I'd like to classify new observations without having to wait every time for the training data to be used create the model again.</p>\\n\\n<p>I've never separated these two processes before, I always had them in the same process flow window, as I don't know to execute these processes independently.</p>\\n-RapidMiner: Classifying new examples without re-running the existing trained model-<classification><execution><rapidminer>\",\n",
       " '<p>today I opened my eclipse as usual but I wasn\\'t able to do anything. All menus except the preferences are grayed out, disabled.</p>\\n\\n<p>I\\'m on a Mac Mavericks Beta 6 &amp; Eclipse Kepler</p>\\n\\n<p>Do u have any hints for me? Restart of Eclipse or the complete mac didn\\'t solve the problem</p>\\n\\n<p><img src=\"https://i.stack.imgur.com/xSFpg.png\" alt=\"Eclipse Menu\"></p>\\n-Eclipse menus disabled on OS X 10.9 Beta 6-<eclipse><macos><eclipse-kepler>',\n",
       " '<p>I have imported a dataset in which the gender (m/f) has been interpreted as \"binominal\". If I am not mistaken, this means that there is a mapping true/false is assigned to the two values m/f.</p>\\n\\n<p>Since I need to find association rules, this is not correct.</p>\\n\\n<p>My question is: is it possible to convert a binominal field to nominal and then expand it again to two nominal attributes (one for male and the other for female) in order to be correctly used with the \"FP-Growth\" and \"Create Association Rules\" operators?</p>\\n-convert binominal to nominal in Rapid Miner-<rapidminer>',\n",
       " '<p>In Eclipse 4.x, we finally have the ability to detach editor tabs into floating windows. Is there any way to open files (e.g. via <kbd>Ctrl</kbd><kbd>Shift</kbd><kbd>R</kbd>) directly in a new tab in a floating editor window, rather than having to open them in the main window and manually drag them over to the floating window? This is a basic feature I really miss from IntelliJ-based IDEs.</p>\\n-Open resource directly in detached editor window-<eclipse><eclipse-kepler>',\n",
       " '<p>I get this error message whenever i try to establish a connection to Teradata server using Teradata client provider 13.10.</p>\\n\\n<p>[.NET Data Provider for Teradata[1150009] Message truncation error, not all data was received.</p>\\n\\n<p>Please advise me to get rid of this error.</p>\\n-.NET Data Provider for Teradata 1150009 Message truncation error, not all data was received-<teradata><provider>',\n",
       " \"<p>We have a multi-module maven project.  The context root for our JAX-RS is specified in the POM and an application.xml gets generated when we do a maven build on the project.</p>\\n\\n<p>Inside eclipse when we clean/build the project and deploy to Glassfish this context-root gets ignored and the context-root ends up being the name of the WAR file.</p>\\n\\n<p>Is there some way to override this?    This didn't seem to be an issue in the older version of eclipse we had been using.</p>\\n-Eclipse (Kepler) Ignoring Context Root-<eclipse><glassfish><eclipse-kepler>\",\n",
       " '<p>I have created a process flow within RapidMiner that utilizes some loops. I\\'m not exactly sure where my Store Model operator should be connected to, in order to save the model parameters derived through this process to be in a new process. </p>\\n\\n<p>The attached example has my data replaced with some sample data, however the rest of the process is what I have for my actual data set.</p>\\n\\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?&gt;\\n&lt;process version=\"5.3.012\"&gt;\\n  &lt;context&gt;\\n    &lt;input/&gt;\\n    &lt;output/&gt;\\n    &lt;macros/&gt;\\n  &lt;/context&gt;\\n  &lt;operator activated=\"true\" class=\"process\" compatibility=\"5.3.012\" expanded=\"true\" name=\"Process\"&gt;\\n    &lt;process expanded=\"true\"&gt;\\n      &lt;operator activated=\"true\" class=\"retrieve\" compatibility=\"5.3.012\" expanded=\"true\" height=\"60\" name=\"Retrieve Sonar\" width=\"90\" x=\"45\" y=\"30\"&gt;\\n        &lt;parameter key=\"repository_entry\" value=\"//Samples/data/Sonar\"/&gt;\\n      &lt;/operator&gt;\\n      &lt;operator activated=\"true\" class=\"numerical_to_binominal\" compatibility=\"5.3.012\" expanded=\"true\" height=\"76\" name=\"Numerical to Binominal\" width=\"90\" x=\"179\" y=\"30\"&gt;\\n        &lt;parameter key=\"attribute_filter_type\" value=\"single\"/&gt;\\n        &lt;parameter key=\"attribute\" value=\"20_OV_COVER\"/&gt;\\n      &lt;/operator&gt;\\n      &lt;operator activated=\"true\" class=\"set_role\" compatibility=\"5.3.012\" expanded=\"true\" height=\"76\" name=\"Set Role\" width=\"90\" x=\"45\" y=\"120\"&gt;\\n        &lt;parameter key=\"attribute_name\" value=\"class\"/&gt;\\n        &lt;parameter key=\"target_role\" value=\"label\"/&gt;\\n        &lt;list key=\"set_additional_roles\"/&gt;\\n      &lt;/operator&gt;\\n      &lt;operator activated=\"true\" class=\"normalize\" compatibility=\"5.3.012\" expanded=\"true\" height=\"94\" name=\"Normalize\" width=\"90\" x=\"179\" y=\"120\"/&gt;\\n      &lt;operator activated=\"true\" class=\"nominal_to_numerical\" compatibility=\"5.3.012\" expanded=\"true\" height=\"94\" name=\"Nominal to Numerical (2)\" width=\"90\" x=\"45\" y=\"210\"&gt;\\n        &lt;list key=\"comparison_groups\"/&gt;\\n      &lt;/operator&gt;\\n      &lt;operator activated=\"true\" class=\"replace_missing_values\" compatibility=\"5.3.012\" expanded=\"true\" height=\"94\" name=\"Replace Missing Values\" width=\"90\" x=\"179\" y=\"210\"&gt;\\n        &lt;list key=\"columns\"/&gt;\\n      &lt;/operator&gt;\\n      &lt;operator activated=\"true\" class=\"independent_component_analysis\" compatibility=\"5.3.012\" expanded=\"true\" height=\"94\" name=\"ICA\" width=\"90\" x=\"313\" y=\"210\"&gt;\\n        &lt;parameter key=\"number_of_components\" value=\"700\"/&gt;\\n      &lt;/operator&gt;\\n      &lt;operator activated=\"true\" class=\"optimize_selection_forward\" compatibility=\"5.3.012\" expanded=\"true\" height=\"94\" name=\"Forward Selection\" width=\"90\" x=\"514\" y=\"75\"&gt;\\n        &lt;parameter key=\"maximal_number_of_attributes\" value=\"100\"/&gt;\\n        &lt;parameter key=\"speculative_rounds\" value=\"10\"/&gt;\\n        &lt;process expanded=\"true\"&gt;\\n          &lt;operator activated=\"true\" class=\"x_validation\" compatibility=\"5.3.012\" expanded=\"true\" height=\"112\" name=\"Validation\" width=\"90\" x=\"112\" y=\"30\"&gt;\\n            &lt;parameter key=\"number_of_validations\" value=\"5\"/&gt;\\n            &lt;process expanded=\"true\"&gt;\\n              &lt;operator activated=\"true\" class=\"naive_bayes\" compatibility=\"5.3.012\" expanded=\"true\" height=\"76\" name=\"Naive Bayes\" width=\"90\" x=\"112\" y=\"30\"/&gt;\\n              &lt;connect from_port=\"training\" to_op=\"Naive Bayes\" to_port=\"training set\"/&gt;\\n              &lt;connect from_op=\"Naive Bayes\" from_port=\"model\" to_port=\"model\"/&gt;\\n              &lt;portSpacing port=\"source_training\" spacing=\"0\"/&gt;\\n              &lt;portSpacing port=\"sink_model\" spacing=\"0\"/&gt;\\n              &lt;portSpacing port=\"sink_through 1\" spacing=\"0\"/&gt;\\n            &lt;/process&gt;\\n            &lt;process expanded=\"true\"&gt;\\n              &lt;operator activated=\"true\" class=\"apply_model\" compatibility=\"5.3.012\" expanded=\"true\" height=\"76\" name=\"Apply Model\" width=\"90\" x=\"45\" y=\"30\"&gt;\\n                &lt;list key=\"application_parameters\"/&gt;\\n              &lt;/operator&gt;\\n              &lt;operator activated=\"true\" class=\"performance\" compatibility=\"5.3.012\" expanded=\"true\" height=\"76\" name=\"Performance\" width=\"90\" x=\"276\" y=\"30\"/&gt;\\n              &lt;connect from_port=\"model\" to_op=\"Apply Model\" to_port=\"model\"/&gt;\\n              &lt;connect from_port=\"test set\" to_op=\"Apply Model\" to_port=\"unlabelled data\"/&gt;\\n              &lt;connect from_op=\"Apply Model\" from_port=\"labelled data\" to_op=\"Performance\" to_port=\"labelled data\"/&gt;\\n              &lt;connect from_op=\"Performance\" from_port=\"performance\" to_port=\"averagable 1\"/&gt;\\n              &lt;portSpacing port=\"source_model\" spacing=\"0\"/&gt;\\n              &lt;portSpacing port=\"source_test set\" spacing=\"0\"/&gt;\\n              &lt;portSpacing port=\"source_through 1\" spacing=\"0\"/&gt;\\n              &lt;portSpacing port=\"sink_averagable 1\" spacing=\"0\"/&gt;\\n              &lt;portSpacing port=\"sink_averagable 2\" spacing=\"0\"/&gt;\\n            &lt;/process&gt;\\n          &lt;/operator&gt;\\n          &lt;connect from_port=\"example set\" to_op=\"Validation\" to_port=\"training\"/&gt;\\n          &lt;connect from_op=\"Validation\" from_port=\"averagable 1\" to_port=\"performance\"/&gt;\\n          &lt;portSpacing port=\"source_example set\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"sink_performance\" spacing=\"0\"/&gt;\\n        &lt;/process&gt;\\n      &lt;/operator&gt;\\n      &lt;connect from_op=\"Retrieve Sonar\" from_port=\"output\" to_op=\"Numerical to Binominal\" to_port=\"example set input\"/&gt;\\n      &lt;connect from_op=\"Numerical to Binominal\" from_port=\"example set output\" to_op=\"Set Role\" to_port=\"example set input\"/&gt;\\n      &lt;connect from_op=\"Set Role\" from_port=\"example set output\" to_op=\"Normalize\" to_port=\"example set input\"/&gt;\\n      &lt;connect from_op=\"Normalize\" from_port=\"example set output\" to_op=\"Nominal to Numerical (2)\" to_port=\"example set input\"/&gt;\\n      &lt;connect from_op=\"Nominal to Numerical (2)\" from_port=\"example set output\" to_op=\"Replace Missing Values\" to_port=\"example set input\"/&gt;\\n      &lt;connect from_op=\"Replace Missing Values\" from_port=\"example set output\" to_op=\"ICA\" to_port=\"example set input\"/&gt;\\n      &lt;connect from_op=\"ICA\" from_port=\"example set output\" to_op=\"Forward Selection\" to_port=\"example set\"/&gt;\\n      &lt;connect from_op=\"ICA\" from_port=\"original\" to_port=\"result 1\"/&gt;\\n      &lt;connect from_op=\"ICA\" from_port=\"preprocessing model\" to_port=\"result 2\"/&gt;\\n      &lt;connect from_op=\"Forward Selection\" from_port=\"example set\" to_port=\"result 3\"/&gt;\\n      &lt;connect from_op=\"Forward Selection\" from_port=\"attribute weights\" to_port=\"result 4\"/&gt;\\n      &lt;connect from_op=\"Forward Selection\" from_port=\"performance\" to_port=\"result 5\"/&gt;\\n      &lt;portSpacing port=\"source_input 1\" spacing=\"0\"/&gt;\\n      &lt;portSpacing port=\"sink_result 1\" spacing=\"18\"/&gt;\\n      &lt;portSpacing port=\"sink_result 2\" spacing=\"0\"/&gt;\\n      &lt;portSpacing port=\"sink_result 3\" spacing=\"0\"/&gt;\\n      &lt;portSpacing port=\"sink_result 4\" spacing=\"0\"/&gt;\\n      &lt;portSpacing port=\"sink_result 5\" spacing=\"0\"/&gt;\\n      &lt;portSpacing port=\"sink_result 6\" spacing=\"0\"/&gt;\\n    &lt;/process&gt;\\n  &lt;/operator&gt;\\n&lt;/process&gt;\\n</code></pre>\\n-RapidMiner : Where/how is the Store (Model) operator connected in this process flow-<process><rapidminer>',\n",
       " '<p>I recently upgraded to Eclipse Kepler, and am having issues with Ant.  I am getting the Java Virtual Machine Launcher - A Java Exception has occurred error no matter which target I choose in my build files.</p>\\n\\n<p>I tried reinstalling my JDK, and I still get the error.  I am running the 7u25 version of the JDK.  I have my JAVA_HOME environment variable set to <code>C:\\\\Progra~1\\\\Java\\\\jdk1.7.0_25</code>, so I don\\'t think this is the problem.  What else could be causing the problem?</p>\\n\\n<p>EDIT: I also tested ant in the command line and it works just fine.  Is this a bug in Kepler?</p>\\n\\n<p>EDIT 2: Here is the log of the errors:</p>\\n\\n<pre><code>org.eclipse.core.runtime.CoreException: Could not find one or more classes: \"org.apache.tools.ant.BuildLogger\". Please check the Ant classpath.\\nat org.eclipse.ant.core.AntRunner.problemLoadingClass(AntRunner.java:467)\\nat org.eclipse.ant.core.AntRunner.run(AntRunner.java:380)\\nat org.eclipse.ant.internal.launching.launchConfigurations.AntLaunchDelegate.runInSameVM(AntLaunchDelegate.java:307)\\nat org.eclipse.ant.internal.launching.launchConfigurations.AntLaunchDelegate.launch(AntLaunchDelegate.java:260)\\nat org.eclipse.debug.internal.core.LaunchConfiguration.launch(LaunchConfiguration.java:858)\\nat org.eclipse.debug.internal.core.LaunchConfiguration.launch(LaunchConfiguration.java:707)\\nat org.eclipse.debug.internal.core.LaunchConfiguration.launch(LaunchConfiguration.java:700)\\nat org.eclipse.core.externaltools.internal.model.ExternalToolBuilder.launchBuild(ExternalToolBuilder.java:181)\\nat org.eclipse.core.externaltools.internal.model.ExternalToolBuilder.doBuildBasedOnScope(ExternalToolBuilder.java:169)\\nat org.eclipse.core.externaltools.internal.model.ExternalToolBuilder.build(ExternalToolBuilder.java:88)\\nat org.eclipse.core.internal.events.BuildManager$2.run(BuildManager.java:726)\\nat org.eclipse.core.runtime.SafeRunner.run(SafeRunner.java:42)\\nat org.eclipse.core.internal.events.BuildManager.basicBuild(BuildManager.java:199)\\nat org.eclipse.core.internal.events.BuildManager.basicBuild(BuildManager.java:239)\\nat org.eclipse.core.internal.events.BuildManager$1.run(BuildManager.java:292)\\nat org.eclipse.core.runtime.SafeRunner.run(SafeRunner.java:42)\\nat org.eclipse.core.internal.events.BuildManager.basicBuild(BuildManager.java:295)\\nat org.eclipse.core.internal.events.BuildManager.basicBuildLoop(BuildManager.java:351)\\nat org.eclipse.core.internal.events.BuildManager.build(BuildManager.java:374)\\nat org.eclipse.core.internal.resources.Workspace.buildInternal(Workspace.java:514)\\nat org.eclipse.core.internal.resources.Workspace.build(Workspace.java:423)\\nat org.eclipse.debug.internal.core.LaunchConfiguration.launch(LaunchConfiguration.java:830)\\nat org.eclipse.debug.internal.core.LaunchConfiguration.launch(LaunchConfiguration.java:707)\\nat org.eclipse.debug.internal.ui.DebugUIPlugin.buildAndLaunch(DebugUIPlugin.java:1018)\\nat org.eclipse.debug.internal.ui.DebugUIPlugin$8.run(DebugUIPlugin.java:1222)\\nat org.eclipse.core.internal.jobs.Worker.run(Worker.java:53)\\n\\nCaused by: java.lang.NoClassDefFoundError: org/apache/tools/ant/BuildLogger\\nat java.lang.Class.getDeclaredConstructors0(Native Method)\\nat java.lang.Class.privateGetDeclaredConstructors(Unknown Source)\\nat java.lang.Class.getConstructor0(Unknown Source)\\nat java.lang.Class.newInstance(Unknown Source)\\nat org.eclipse.ant.core.AntRunner.run(AntRunner.java:324)\\n... 24 more\\n\\nCaused by: java.lang.ClassNotFoundException: org.apache.tools.ant.BuildLogger\\nat java.net.URLClassLoader$1.run(Unknown Source)\\nat java.net.URLClassLoader$1.run(Unknown Source)\\nat java.security.AccessController.doPrivileged(Native Method)\\nat java.net.URLClassLoader.findClass(Unknown Source)\\nat org.eclipse.ant.internal.core.AntClassLoader.findClass(AntClassLoader.java:54)\\nat java.lang.ClassLoader.loadClass(Unknown Source)\\nat java.lang.ClassLoader.loadClass(Unknown Source)\\n... 29 more\\n</code></pre>\\n-Ant not working in Eclipse Kepler (Java Virtual Machine Launcher - A Java Exception has occured)-<java><eclipse><ant><eclipse-kepler>',\n",
       " '<p>does any one know how to over come from this issue ?<br>\\nI have followed this <a href=\"https://stackoverflow.com/questions/17337526/how-to-upgrade-eclipse-for-java-ee-developers-from-juno-to-kepler\">link</a> but after updating it, every thing seems to correct but my existing projects are not able to run via tomcat (previously in juno it was working). I can see my tomcat server under servers view, when I click on <strong>File</strong> menu -> New it show <strong>No Applicable Item</strong>.. I am not even able to create new Dynamic web project, neither I am able to change my project facets from project properties...<br>\\nCan any one help me out in this case ?</p>\\n-Updating Juno to Kepler cause existing project in workspace non functional-<java><eclipse><eclipse-kepler>',\n",
       " '<p>I am not sure, if this is the right forum for my issue. Please let me know if not and will try it in another one.</p>\\n\\n<p>Since it seems to be amazing extension, I am trying to run Rapid Miner\\'s R-Extension. Without any success for over a day now. I don\\'t know how to help myself anymore, that\\'s why I decided to write this post.</p>\\n\\n<p>After starting Rapid Miner I get the error message:</p>\\n\\n<p><i><p>Could not load native library.</p>\\nR Extension could not be initialized. Error while loading native R library. Please check PATH,R_HOME and JAVA_HOME environment variables settings.\\n<p>Reason: \\'C:\\\\Program Files\\\\R\\\\R-3.0.1\\\\library\\\\rJava\\\\jri\\\\x64\\\\jri.dll: Can\\'t find dependent libraries\\'</p></i></p>\\n\\n<p><hr>\\nFor helping you guys helping me: In the following some info, which might be useful.</p>\\n\\n<p><b>System Info</b></p>\\n\\n<pre><code>OS:Windows 7 Enterprise SP 1 64 bit\\n\\nRapidminer-version: 5.2.008 64 bit\\n\\nrapidminer.r.native lib: \\nC:\\\\Program Files\\\\R\\\\R-3.0.1\\\\library\\\\rJava\\\\jri\\\\x64\\\\jri.dll \\n\\nR-Version: \\nplatform       x86_64-w64-mingw32\\narch           x86_64\\nos             mingw32\\nsystem         x86_64, mingw32\\n[...]\\nversion.string R version 3.0.1 (2013-05-16)\\nnickname       Good Sport  \\n\\nJava version: \\njava version \"1.7.0_25\"\\nJava(TM) SE Runtime Environment (build 1.7.0_25-b17)\\nJava HotSpot(TM) 64-Bit Server VM (build 23.25-b01, mixed mode) \\n\\nJAVA_HOME: C:\\\\Program Files\\\\Java\\\\jdk1.7.0_25 \\n\\nR_HOME: C:\\\\Program Files\\\\R\\\\R-3.0.1 \\n\\nPATH (part): \\nC:\\\\Windows\\\\SysWOW64; C:\\\\Windows\\\\System32; \\nC:\\\\Program Files\\\\Internet Explorer; C:\\\\Program Files\\\\Java\\\\jdk1.7.0_25\\\\jre\\\\bin; \\nC:\\\\Program Files\\\\Java\\\\jdk1.7.0_25\\\\jre\\\\bin\\\\server; \\nC:\\\\Program Files\\\\R\\\\R-3.0.1\\\\bin\\\\x64; \\nC:\\\\Program Files\\\\R\\\\R-3.0.1\\\\library\\\\rJava\\\\jri\\\\x64 \\n</code></pre>\\n\\n<p><strong>After starting Rapid Miner (console): Part of console output</strong></p>\\n\\n<pre><code>java.lang.UnsatisfiedLinkError: C:\\\\Program Files\\\\R\\\\R-3.0.1\\\\library\\\\rJava\\\\jri\\\\x64\\\\jri.dll: Can\\'t find dependent libraries\\n28.08.2013 12:34:48 com.rapidminer.PluginInitR initPlugin\\nINFO: Trying to load R Library...\\n\\n28.08.2013 12:34:48 org.rosuda.JRI.Rengine [clinit]\\nINFO: Creating Connection to R...\\n\\n28.08.2013 12:34:48 org.rosuda.JRI.Rengine loadLibrary\\nINFO: Trying to loaded R library from C:\\\\Program Files\\\\R\\\\R-3.0.1\\\\library\\\\rJava\\\\jri\\\\x64\\\\jri.dll\\n\\n28.08.2013 12:34:48 org.rosuda.JRI.Rengine loadLibrary\\nINFO: Engine ID: class org.rosuda.JRI.Rengine ClassLoader:PluginClassLoader ([file:/C:/Users/&lt;...&gt;/.RapidMiner5/managed/rmx_r-5.3.0.jar])\\n\\n28.08.2013 12:34:48 com.rapidminer.tools.I18N getMessage\\nWARNING: Missing I18N key: r.could_not_load_native_lib\\n\\n28.08.2013 12:34:48 com.rapidminer.PluginInitR loadNativeLibrary\\nSEVERE: r.could_not_load_native_lib\\njava.lang.UnsatisfiedLinkError: C:\\\\Program Files\\\\R\\\\R-3.0.1\\\\library\\\\rJava\\\\jri\\\\x64\\\\jri.dll: Can\\'t find dependent libraries\\n*Exception-Stacktrace*\\n\\n28.08.2013 12:34:48 com.rapidminer.PluginInitR initPlugin\\nSEVERE: Failed to load R library! Check your R and rJava installation and PATH,R_HOME and JAVA_HOME environment variables.\\njava.lang.UnsatisfiedLinkError: C:\\\\Program Files\\\\R\\\\R-3.0.1\\\\library\\\\rJava\\\\jri\\\\x64\\\\jri.dll: Can\\'t find dependent libraries\\n*Exception Stacktrace*\\n</code></pre>\\n\\n<p><strong>Checking Measure for …\\\\x64\\\\jri.dll</strong></p>\\n\\n<p>Checked dependencies of …\\\\x64\\\\jri.dll using Dependency Walker.</p>\\n\\n<p>Result: Delay-load module warning for module c:\\\\windows\\\\system32\\\\IEFRAME.DLL.</p>\\n\\n<p>Message: <em>Warning: At least one module has an unresolved import due to a missing export function in a delay-load dependent module.</em></p>\\n\\n<p>! But: It is possible to load rJava (version: rJava_0.9-4) into a R-workspace. Tried out with RStudio. </p>\\n\\n<hr>\\n\\n<p>That\\'s it! I don\\'t know, what to do anymore to get out of this situation. As you can see, I read all posts in <i>stackoverflow </i>with the same issue. But nothing helped so far. Is there anything I did not consider?</p>\\n\\n<p>I appreciate each help very much. If there any things you need to know for further steps please let me know.  </p>\\n\\n<p>Thanks in advance,</p>\\n\\n<p>Rudi</p>\\n-Rapidminer R-Extension: Cannot load native library jri.dll-<r><rapidminer><rjava>',\n",
       " \"<p>Is there a way to determine the first day of a year in Teradata SQL?  </p>\\n\\n<p>For example, I use this to find the first day of the month:</p>\\n\\n<pre><code>SELECT dt + (1 - EXTRACT(DAY FROM dt)) AS dt\\n</code></pre>\\n\\n<p>I know I can extract the year directly using <code>YEAR()</code> but I want to output the results as a date so it will work in some external charts.</p>\\n\\n<p>I tried this, but it added a bunch of spaces at the start of the date:</p>\\n\\n<pre><code>CONCAT(YEAR(dt),'-01-01')\\n</code></pre>\\n-Teradata Determine First Day of Year-<sql><teradata>\",\n",
       " '<p><strong>Issue</strong>\\nHi all I have arounf 7 files each having 7000 million records each and 1 have to laod them in 4 tables in teradata .\\nI dont have any constraint and i am using Teradata Parallel Transpoter\\nmy Load is Full Refresh and I am using Informatica.</p>\\n\\n<p>I am getting of throughput of around 45000Rows/sec .</p>\\n\\n<p><strong>Requirement</strong>\\nCan anyone suugest any way to improve performance in informatica, or by partitioning in table.\\nI have 6 columns in table and all are composite keys.</p>\\n-load 70 Billion records in teradata-<sql><database><database-performance><teradata>',\n",
       " '<p>Hi i tried to upgrade my eclipse using like this.\\n<a href=\"https://stackoverflow.com/questions/17337526/how-to-upgrade-eclipse-for-java-ee-developers-from-juno-to-kepler\">How to upgrade Eclipse for Java EE Developers?</a>\\nBut now my Eclipse start and close. I tried clean start. \\nI don\\'t want to install clean 4.3 because i have to much settings and plugins to migrate.</p>\\n\\n<p>At the moment i am working on clean 4.3 with copied old plugins folder but it sometimes closing automatically when building workspace.\\nI would prefer to resurrect old eclipse because i have to much things to configure and it will take a lot of time to do that. I will try working on new one but i would be glad to resurrect old one.</p>\\n-Eclipse upgrade juno to kepler. Eclipse wont start-<java><eclipse><eclipse-plugin><eclipse-juno><eclipse-kepler>',\n",
       " '<p>The error icon in the package explorer is not showing (on top of the file icon), even though it shows on the right bar when the file is opened.</p>\\n<p><em>.project</em> has <em>org.eclipse.jdt.core.javabuilder</em> in its build command.</p>\\n<p>What else could be wrong?</p>\\n-This Eclipse error icon in the package explorer is not showing-<java><eclipse><eclipse-kepler>',\n",
       " '<p>In <strong>Teradata</strong> Is there any way to get column name as well with the error message. For example I have a table</p>\\n\\n<ul>\\n<li>tablename column1 int, </li>\\n<li>tablename column2 timestamp, </li>\\n<li>tablename column3 timestamp, </li>\\n<li>tablename column4 timestamp, </li>\\n<li>tablename column5 char(20)</li>\\n</ul>\\n\\n<p>When i insert a wrong value in a column, it does not return me COLUMNNAME.  <strong>For Example if i insert wrong time it just say 6760 : invalid timestamp</strong> \\nbut which column is having problem we dont know. </p>\\n\\n<p><strong>is there is any mathod to know about it</strong>.</p>\\n-Teradata - get tablename and column name with the error message-<sql><teradata>',\n",
       " '<p>I have around 5 qunit tests which have timed out according to testswarm. I tried to reset those tests using cleanp action.\\n     <code>api.php?action=cleanup</code>\\nIt didn\\'t work. The response from the action is\\n<code>{\"cleanup\":{\"resetTimedoutRuns\":0}}</code></p>\\n\\n<p>What can be the issue here ?</p>\\n-testswarm cleanup action doesn\\'t seem to work correctly-<javascript><testswarm>',\n",
       " '<p>I have just moved from 4.2 to 4.3 (kepler). I was working on Eclipse RCP(OSGi framework) which was working fine with 4.2, but once i moved to Kepler- started getting \\nbelow mentioned error while launching. Any kind of help is appreciated.</p>\\n\\n<pre><code>java.lang.RuntimeException: Application \"org.eclipse.e4.ui.workbench.swt.E4Application\" could not be found in the registry. The applications available are: org.eclipse.equinox.app.error.\\nat org.eclipse.equinox.internal.app.EclipseAppContainer.startDefaultApp(EclipseAppContainer.java:248)\\nat org.eclipse.equinox.internal.app.MainApplicationLauncher.run(MainApplicationLauncher.java:29)\\nat org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.runApplication(EclipseAppLauncher.java:110)\\nat org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.start(EclipseAppLauncher.java:79)\\nat org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:354)\\nat org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:181)\\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\\nat java.lang.reflect.Method.invoke(Method.java:597)\\nat org.eclipse.equinox.launcher.Main.invokeFramework(Main.java:636)\\nat org.eclipse.equinox.launcher.Main.basicRun(Main.java:591)\\nat org.eclipse.equinox.launcher.Main.run(Main.java:1450)\\nat org.eclipse.equinox.launcher.Main.main(Main.java:1426)\\n</code></pre>\\n-Application \"org.eclipse.e4.ui.workbench.swt.E4Application\" could not be found in the registry-<eclipse><registry><eclipse-kepler><workbench>',\n",
       " \"<p>I am getting following error message when I tried to install cobertura plugin in eclipse kepler (java/j2ee version) through eclipse market place.</p>\\n<blockquote>\\n<p>Cannot complete the install because one or more required items could\\nnot be found.   Software being installed: eCobertura\\n0.9.8.201007202152 (ecobertura.feature.group 0.9.8.201007202152)</p>\\n<p>Missing requirement: eCobertura 0.9.8.201007202152\\n(ecobertura.feature.group 0.9.8.201007202152) requires 'org.junit4\\n0.0.0' but it could not be found</p>\\n</blockquote>\\n-Cannot install Cobertura plugin for Eclipse-<eclipse><code-coverage><eclipse-kepler><cobertura><eclipse-luna>\",\n",
       " '<p>When i start eclipse, on splash loading window, eclise auto exit with error:</p>\\n\\n<pre><code>        !SESSION 2013-09-05 14:52:04.771 -----------------------------------------------\\n    eclipse.buildId=4.3.0.I20130605-2000\\n    java.version=1.7.0_25\\n    java.vendor=Oracle Corporation\\n    BootLoader constants: OS=win32, ARCH=x86, WS=win32, NL=en_US\\n    Framework arguments:  -product org.eclipse.epp.package.jee.product\\n    Command-line arguments:  -os win32 -ws win32 -arch x86 -product org.eclipse.epp.package.jee.product\\n\\n    !ENTRY org.eclipse.ui.workbench 4 2 2013-09-05 14:52:09.260\\n    !MESSAGE Problems occurred when invoking code from plug-in: \"org.eclipse.ui.workbench\".\\n    !STACK 0\\n    java.lang.NullPointerException\\n        at org.eclipse.equinox.internal.p2.core.helpers.ServiceHelper.getService(ServiceHelper.java:74)\\n        at org.eclipse.equinox.internal.p2.engine.SimpleProfileRegistry.updateRoamingProfile(SimpleProfileRegistry.java:156)\\n        at org.eclipse.equinox.internal.p2.engine.SimpleProfileRegistry.updateSelfProfile(SimpleProfileRegistry.java:147)\\n        at org.eclipse.equinox.internal.p2.engine.SimpleProfileRegistry.getProfileMap(SimpleProfileRegistry.java:344)\\n        at org.eclipse.equinox.internal.p2.engine.SimpleProfileRegistry.internalGetProfile(SimpleProfileRegistry.java:248)\\n        at org.eclipse.equinox.internal.p2.engine.SimpleProfileRegistry.getProfile(SimpleProfileRegistry.java:178)\\n        at org.eclipse.equinox.internal.p2.ui.sdk.scheduler.AutomaticUpdateScheduler.earlyStartup(AutomaticUpdateScheduler.java:88)\\n        at org.eclipse.ui.internal.EarlyStartupRunnable.runEarlyStartup(EarlyStartupRunnable.java:87)\\n        at org.eclipse.ui.internal.EarlyStartupRunnable.run(EarlyStartupRunnable.java:66)\\n        at org.eclipse.core.runtime.SafeRunner.run(SafeRunner.java:42)\\n        at org.eclipse.ui.internal.Workbench$55.run(Workbench.java:2552)\\n        at org.eclipse.core.internal.jobs.Worker.run(Worker.java:53)\\n\\n    !ENTRY org.eclipse.ui.workbench 4 2 2013-09-05 14:52:09.264\\n    !MESSAGE Problems occurred when invoking code from plug-in: \"org.eclipse.ui.workbench\".\\n    !STACK 0\\n    java.lang.NullPointerException\\n        at org.eclipse.core.internal.runtime.InternalPlatform.getLog(InternalPlatform.java:354)\\n        at org.eclipse.core.runtime.Plugin.getLog(Plugin.java:291)\\n        at org.eclipse.ui.internal.WorkbenchPlugin.log(WorkbenchPlugin.java:830)\\n        at org.eclipse.ui.statushandlers.StatusManager.logError(StatusManager.java:285)\\n        at org.eclipse.ui.statushandlers.StatusManager.handle(StatusManager.java:200)\\n        at org.eclipse.ui.internal.progress.ProgressManager$2.done(ProgressManager.java:467)\\n        at org.eclipse.core.internal.jobs.JobListeners$3.notify(JobListeners.java:39)\\n        at org.eclipse.core.internal.jobs.JobListeners.doNotify(JobListeners.java:96)\\n        at org.eclipse.core.internal.jobs.JobListeners.done(JobListeners.java:152)\\n        at org.eclipse.core.internal.jobs.JobManager.endJob(JobManager.java:647)\\n        at org.eclipse.core.internal.jobs.WorkerPool.endJob(WorkerPool.java:105)\\n        at org.eclipse.core.internal.jobs.Worker.run(Worker.java:70)\\n\\n    !ENTRY org.eclipse.core.jobs 4 2 2013-09-05 14:52:09.267\\n    !MESSAGE An internal error occurred during: \"Workbench early startup\".\\n    !STACK 0\\n    java.lang.NullPointerException\\n        at org.eclipse.core.internal.runtime.InternalPlatform.getLog(InternalPlatform.java:354)\\n        at org.eclipse.core.runtime.Plugin.getLog(Plugin.java:291)\\n        at org.eclipse.ui.internal.WorkbenchPlugin.log(WorkbenchPlugin.java:818)\\n        at org.eclipse.ui.internal.EarlyStartupRunnable.handleException(EarlyStartupRunnable.java:81)\\n        at org.eclipse.core.runtime.SafeRunner.handleException(SafeRunner.java:75)\\n        at org.eclipse.core.runtime.SafeRunner.run(SafeRunner.java:44)\\n        at org.eclipse.ui.internal.Workbench$55.run(Workbench.java:2552)\\n        at org.eclipse.core.internal.jobs.Worker.run(Worker.java:53)\\n</code></pre>\\n\\n<p>I\\'ve tried:</p>\\n\\n<ul>\\n<li>deleting workspace/.metadata/.lock</li>\\n<li>delete the file WORKSPACE/.metadata/.plugins/org.eclipse.core.resources/.snap</li>\\n</ul>\\n\\n<p>But didn\\'t solve this problem.</p>\\n-Eclipse Kepler auto exit on loading workbench with error-<eclipse><eclipse-kepler>',\n",
       " '<p>I\\'ve just updated from Eclipse Juno to Kepler. What happened to the Eclipse perspective layout and how can I restore it to fill the entire window?</p>\\n\\n<p><img src=\"https://i.stack.imgur.com/N2l8x.png\" alt=\"Eclipse Layout\"></p>\\n-Reset Eclipse perspective layout-<eclipse><layout><eclipse-juno><perspective><eclipse-kepler>',\n",
       " \"<p>Have a problem with Eclipse Kepler.\\nI have a workspace with many projects, organized in many working sets. Projects are located on external USB harddrive. When i've opened workspace with HDD being occidentally detached those projects were in closed  state and many of them(but not all) lost their working set bindings.Reopening workspace with HDD attached didn't help - workspace remains in that broken  state ( projects are closed and working set bindings are lost). So had to manually readd projects to working sets.</p>\\n\\n<p>Is it some sort of a bug in eclipse or i can avoid this behaviour using some eclipse settings ?</p>\\n\\n<p>Thanks.</p>\\n-Eclipse kepler loses working sets bindings-<eclipse><eclipse-kepler>\",\n",
       " \"<p>I'm working with a fresh copy of Eclipse Kepler but I'm unable to find the marketplace. I know it's normally under 'Help' but it's not there.</p>\\n\\n<p>Is there an option to make it visible again?</p>\\n\\n<p>Mayby good to know, I'm working on Ubuntu 12.10</p>\\n-Can't find the marketplace in Eclipse Kepler-<eclipse><eclipse-kepler><eclipse-marketplace>\",\n",
       " '<p>I am new to <code>Teradata</code>. I have been using Oracle for quite a bit. However, when I tried to run the following queries in <code>Teradata</code>, they simply did not work. How can I translate the following queries into <code>Teradata</code>:</p>\\n\\n<pre><code>select Table_name, constraint_name, constraint_type,\\n   r_constraint_name, Delete_rule, search_condition\\n from user_constraints\\n  order by table_name, constraint_name;\\n\\nselect object_name, object_id, Object_type,\\n   Created, last_DDL_time, status\\n from user_Objects\\n  order by object_name;\\n\\nselect table_name, column_name, data_type, data_length,\\n   data_precision, nullable, column_id, data_default\\n from user_tab_columns\\n  order by table_name, column_name;\\n</code></pre>\\n-How to specify user_constraints and other tables in Teradata-<teradata>',\n",
       " '<p>When tying to start Hybris in debug mode using Eclipse Kepler showing error </p>\\n\\n<pre><code>“Failed to connect to remote VM. Connection refused.\\nConnection refused: connect”\\n</code></pre>\\n\\n<p>Even I  tried by changing the default port also</p>\\n\\n<pre><code>tomcat.debugjavaoptions=-Xdebug -Xnoagent -Xrunjdwp:transport=dt_socket,server=y,address=8000,suspend=n\\n</code></pre>\\n\\n<p>but showing the same error. Any help/suggestion will be appreciated. </p>\\n-Not able to start hybris in debug mode-<java><eclipse><e-commerce><eclipse-kepler><sap-commerce-cloud>',\n",
       " '<p>My table contains a column named \"notes\". I need to create a column <code>Temper</code> in my report.  If I spot a word \"angry\" in the notes, then my new variable <code>Temper</code> should be <code>3</code>. If I spot a word \"indifferent\" then <code>Temper=2</code>, otherwise <code>Temper</code> should equal to <code>0</code>.</p>\\n\\n<p>The initial table:</p>\\n\\n<pre><code>Id  State  Gender  Notes\\n1    IL     M      Kind of angry, but...\\n2    MI     F      Maybe indifferent also...\\n3    IL     F      Was cool but not necces...\\n</code></pre>\\n\\n<p>The result should be like this:</p>\\n\\n<pre><code>    Id      Temper\\n    1          3\\n    2          2\\n    3          0\\n</code></pre>\\n\\n<p>I am not sure how to do this in Teradata environment:</p>\\n\\n<pre><code>select Id, (notes like \\'%angry%\\')=3 else // ?????\\nfrom customers\\nwhere\\ncase\\n</code></pre>\\n-How to create a new column based on conditional statement in Teradata query report?-<teradata>',\n",
       " '<p>I want to display the timestamp(GMT) of my table as CST with daylight savings .</p>\\n\\n<p>From march 2nd sunday 2:00AM to November 1st sunday 2:00AM = timestamp(GMT) - 5 hours\\nother days of the year = timestamp(GMT) - 6 hours</p>\\n\\n<p>It appears to be working. Now having another problem. Teradata table is having CST timestamp. Another table (different source, but loaded to teradata staging tables) has GMT. Both timestamp looks alike. But when i minus both , it doesnot give me NULL value. Timestamp looks like 12/20/2012 08:30:00.000000</p>\\n-convert the timestamp from gmt to cst in teradata with day light savings-<sql><date><calendar><teradata>',\n",
       " \"<p>I have <code>Eclipse Standard 4.3 (Kepler)</code> installed, but the jsp pages are opening in black and white. I guess that I haven't downloaded the EE version, that's why its the case. What can I do now ? The xml files are opening in coloured format.</p>\\n\\n<p>I searched for jsp plugins, but the editors that came up in the options didn't have '<code>Install</code>' option beside them.</p>\\n-JSPs opening in black-white in Kepler (Eclipse Standard 4.3)-<java><eclipse><eclipse-kepler>\",\n",
       " '<p>I would like to know how can we see the predictable labels given by a classifier using RapidMiner?</p>\\n\\n<p>Can we have it as an attribute output as in the clustering results?</p>\\n\\n<p>Thanks in advance!\\nTelma</p>\\n-How can we see the predictable labels in RapidMiner?-<rapidminer>',\n",
       " '<p>I am trying to troubleshoot an XML validation problem I am having with Eclipse.</p>\\n\\n<p>When I try to validate the xml against a specified schema, I <em>instantly</em> get a </p>\\n\\n<p><code>\\nNo grammar constraints (DTD or XML Schema) referenced in the document.\\n</code></p>\\n\\n<p>error. The XML schema I am trying to validate it against is located on a remote webserver. Some folk in my office are able to validate against it, whilst others are not. We are using the same xml. </p>\\n\\n<p>I have noticed that when validation successfully occurs, Eclipse takes a few minutes, presumably because it\\'s searching for the schema online. Alas, when it fails to validate on my machine, the failure is instant; it\\'s as if Eclipse isn\\'t even trying to search online. </p>\\n\\n<p>I\\'m currently using Eclipse IDE JD (4.3 I think), Kepler release, but validation has been both successful and unsuccessful on Juno (3.8), again with the same xml. The machines that have successfully been able to validate were successful regardless of Eclipse version. Similarly, the machines that were not successful were so regardless of Eclipse version. </p>\\n\\n<p>I\\'m completely lost.</p>\\n-\"No grammar constraints\" Eclipse Error?-<eclipse><xsd><xml-validation><eclipse-kepler>',\n",
       " '<p>Very new to RM making headway slowly.</p>\\n\\n<p>I have split a column such that the new column is a column of numbers this field is set automatically to <code>polynomial</code>.</p>\\n\\n<p>How do I set it to <code>Numerical</code> or even better <code>Integer</code>?</p>\\n\\n<p>I have tried <code>Geuss Type</code> but this did not work</p>\\n\\n<p>Thanks</p>\\n-In Rapidminer after splitting a column how do I change the type of the new column column?-<rapidminer>',\n",
       " '<p>My requirement is: I have to insert data from excel sheet to teradata tabels without using olap connector and import/export option thats available in TeraData.</p>\\n\\n<p>Is this possible, if so could you please help on this?</p>\\n-How to get data from Excel to Teradata table without using teradata olap connector-<excel><excel-2007><teradata>',\n",
       " '<p>I open multiple windows when using Eclipse on a machine running Windows 7.</p>\\n\\n<p>I would like to be able to boot Eclipse and find the windows displayed (stacked) in the Taskbar in the same order as I left them when I last exited Eclipse.</p>\\n\\n<p>With Eclipse Indigo, the order in which the windows where opened and displayed in the Windows Taskbar could be predicted by the order in which the windows had previously been selected and viewed.</p>\\n\\n<p>By activating (selecting to view) the windows one-at-a-time from left to right just before exiting, I was guaranteed to find them opened in same order next time I booted Eclipse.</p>\\n\\n<p>Eclipse Kepler seems to set the window ordering in some other way that I cannot predict.</p>\\n\\n<p>Is there anything I can do about this?</p>\\n-Eclipse Kepler - How To Maintain Multiple Window Order Across Restarts-<eclipse><eclipse-kepler>',\n",
       " '<p>I have been using KNIME 2.7.4 for running analysis algorithm. I have integrated KNIME with our existing application to run in BATCH mode using the below command.</p>\\n\\n<pre><code>&lt;&lt;KNIME_ROOT_PATH&gt;&gt;\\\\\\\\plugins\\\\\\\\org.eclipse.equinox.launcher_1.2.0.v20110502.jar -application org.knime.product.KNIME_BATCH_APPLICATION -reset -workflowFile=&lt;&lt;Workflow Archive&gt;&gt; -workflow.variable=&lt;&lt;parameter&gt;&gt;,&lt;&lt;value&gt;&gt;,&lt;&lt;DataType&gt;\\n</code></pre>\\n\\n<p>Knime provide different kinds of plot which I want to use. However I am running the workflow in batch mode. Is there any option in KNIME where I can specify the Node Id and \"View\" option as a parameter to KNIME_BATCH_APPLICATION.</p>\\n\\n<p>Would need suggestion or guidance to achieve this functionality.</p>\\n-View plot for Node in KNIME_BATCH_APPLICATION-<knime>',\n",
       " '<p>I just wanna get into EJB, but i refuse to use this all-in-one program netbeans+glassfisch. My jboss 7.1 is already running and added as server. I tried to create an Enterprise Application Project but it won\\'t let me create any beans, says \"source folder is not a java project.\"</p>\\n\\n<p>Maybe I need to install some updates? In Indigo there seems to be an option for creating an ejb-project. </p>\\n\\n<p>Thanks in advance!</p>\\n-How do I create an EJB 3.1 Project with eclipse kepler?-<java><eclipse><ejb><eclipse-kepler>',\n",
       " \"<p><code>[Teradata Database] [3130] Response limit exceeded</code></p>\\n\\n<p>I have no idea what is causing this random error message.  It happens when I am making a call to the database for a SELECT or to execute a stored procedure.  I wish I had more information on how to reproduce this, but it appears to be intermittent.</p>\\n\\n<p>What does this error actually mean?  What types of conditions could cause this?</p>\\n\\n<p><strong>Edit:</strong>  I've discovered that the issue goes away when I build my ASP.NET app (vs2012).  It's like something related to connections is being cached somewhere on my machine.  After I recycle the app pool with a rebuild, it resets everything.  The next time this happens, I will try saving the web.config file which automatically recycles the app pool without rebuilding the DLL.</p>\\n-Teradata Database 3130 Response limit exceeded?-<teradata>\",\n",
       " \"<p>1 ad-jerry ad-bruckheimer ad-chase ad-premier ad-sept ad-th ad-clip ad-bruckheimer ad-chase page found</p>\\n\\n<p>-1 ad-symptom ad-muscle ad-weakness ad-genetic ad-disease ad-symptom ad-include ad-search ad-learn page found</p>\\n\\n<p>1 1:1 2:1 3:1 4:1 5:1 6:1 7:1 8:1 9:1</p>\\n\\n<p>-1 8:1 9:1 429:1 430:1 431:1 432:1 433:1 434:1 435:1 436:1</p>\\n\\n<p>I've text vector &amp; its corresponding term vector, I want to learn a Decision Tree using ID3 algorithm in rapid miner, But I don't know how to process such data for ID3 Algorithm. I've tried to run ID3(Read CSV->ID3->Model) over term vector but I don't know whether It's working correct or not. Please help.</p>\\n-Text Vector to Decision Tree in RapidMiner-<rapidminer>\",\n",
       " '<p>After reading <a href=\"http://www.michael-noll.com/blog/2012/10/16/understanding-the-parallelism-of-a-storm-topology/\" rel=\"nofollow\">this</a> and <a href=\"http://www.datasalt.com/2013/04/an-storms-trident-api-overview/\" rel=\"nofollow\">this</a> I\\'m having difficulties understanding how to configure my trident topology.</p>\\n\\n<p>Basically my storm application is reading from <a href=\"http://kafka.apache.org/projects.html\" rel=\"nofollow\">kafka</a>, doing some data manipulations and finally writing to <a href=\"http://cassandra.apache.org/\" rel=\"nofollow\">Cassandra</a>.</p>\\n\\n<p>Here is how I\\'m currently building my topology:</p>\\n\\n<pre><code>private static StormTopology buildTopology() {\\n// connection to kafka\\nZkHosts zkHosts = new ZkHosts(broker_zk, broker_path);\\nTridentKafkaConfig kafkaConfig = new TridentKafkaConfig(zkHosts, topic);\\nkafkaConfig.scheme = new RawMultiScheme();\\nStateFactoryFields[] cassandraStateFactories = createStateFactories();\\nTransactionalTridentKafkaSpout spout = new TransactionalTridentKafkaSpout(kafkaConfig);\\nTridentTopology topology = new TridentTopology();\\nStream kafkaSpout = topology.newStream(\"kafkaspout\", spout).parallelismHint(1).shuffle();\\nStream filterValidatStream = kafkaSpout.each(new Fields(\"bytes\"), new SplitKafkaInput(), EventData.getEventDataFields()).parallelismHint(1);\\nfor (StateFactoryFields stateFactoryFields : cassandraStateFactories) {\\n    filterValidatStream.groupBy(stateFactoryFields.groupingFields)\\n        .persistentAggregate(stateFactoryFields.cassandraStateFactor, new Count(), new Fields(\"count\")).parallelismHint(2);\\n}\\nlogger.info(\"Building topology\");\\nreturn topology.build();\\n}\\n</code></pre>\\n\\n<p>So I got a spout and a few operations (filter, grouopBy) with parallelismHint.\\nI don\\'t understant hor to determine the optimal parallelismHint, moreover if I\\'m setting this value in my code, how does it work in conjunction with storm standard topology configurations such as </p>\\n\\n<pre><code>topology.max.task.parallelism\\ntopology.workers\\ntopology.acker.executors\\n</code></pre>\\n\\n<p>Thanks in advance</p>\\n-parallelism configuration in trident topology (storm)-<apache-storm><apache-kafka><trident>',\n",
       " '<p>I have a variety of tables that I am joining together.  Each table has a primary index and most but not all are partitioned on a date field.  Each table has an associated view.</p>\\n\\n<p>If I write a query in the form</p>\\n\\n<pre><code>select\\n*\\nfrom view1\\njoin view2\\non pi1 = pi2\\njoin view3\\non pi1 = pi3\\njoin view4\\non pi1 = pi4\\n</code></pre>\\n\\n<p>...</p>\\n\\n<p>I run into a out of spool space problem.  Would it be better to query the tables directly?  Would it be better to create some intermediate tables and do a few joins at a time, then create new indices and partitions on the intermediate tables?    </p>\\n-Teradata performance of joining tables vs joining views-<sql><join><view><teradata>',\n",
       " '<p>I was asked to develop a project that will use Rapidminer library from which i will provide a interface to view results in some graphical form,. i am NOT an advanced programmer but can work around with codes.</p>\\n\\n<p>i have included all packages, libraries and necessary jars in my project, that i am building using eclipse.</p>\\n\\n<p>referring some sites and codes from internet, i have started integration of Rapidminer in my program, but FAILED to do so.</p>\\n\\n<p>please guide me to start my project, or any good source from internet where i can learn and be able to develop my project from scratch.</p>\\n\\n<p>i have seen Rapidminer blogs and web, but i am NOT able to understand thing properly..</p>\\n\\n<p>Thanx...for your help and precious time!!</p>\\n-Starting a java project that uses Rapid miner-<java><eclipse><rapidminer>',\n",
       " \"<p>I am experiencing difficulty in the retrieval of repositories from URL sites for eclipse kepler. I have several URL's that I am attempting to fetch to no avail; they are mostly zip files, a few check sums, and a few tar bzip files. Each URL, when you navigate to it in a web browser, has intact files and none of them are broken sites. This tells me that there must be some kind of error in my eclipse build regarding its repository management. I am sharing my current build with a colleague who is not experiencing these repository issues, which is what has led me to post a question here.</p>\\n\\n<p>I am going to keep tracing the stack, but I would greatly appreciate any cues!</p>\\n\\n<pre><code>org.eclipse.equinox.p2.core.ProvisionException: No repository found at http://googleappengine.googlecode.com/files/appengine-java-sdk-1.8.1.1.ziptype%2520or%2520select%2520a%2520site.\\nat org.eclipse.equinox.internal.p2.repository.helpers.AbstractRepositoryManager.fail(AbstractRepositoryManager.java:395)\\nat org.eclipse.equinox.internal.p2.repository.helpers.AbstractRepositoryManager.loadRepository(AbstractRepositoryManager.java:692)\\nat org.eclipse.equinox.internal.p2.metadata.repository.MetadataRepositoryManager.loadRepository(MetadataRepositoryManager.java:96)\\nat org.eclipse.equinox.internal.p2.metadata.repository.MetadataRepositoryManager.loadRepository(MetadataRepositoryManager.java:92)\\nat org.eclipse.equinox.p2.ui.LoadMetadataRepositoryJob.doLoad(LoadMetadataRepositoryJob.java:117)\\nat org.eclipse.equinox.p2.ui.LoadMetadataRepositoryJob.runModal(LoadMetadataRepositoryJob.java:102)\\nat org.eclipse.equinox.internal.p2.ui.sdk.PreloadingRepositoryHandler$2.runModal(PreloadingRepositoryHandler.java:83)\\nat org.eclipse.equinox.p2.operations.ProvisioningJob.run(ProvisioningJob.java:177)\\nat org.eclipse.core.internal.jobs.Worker.run(Worker.java:53)\\n</code></pre>\\n\\n<p>And here is another example of a repository that I am trying to access:</p>\\n\\n<pre><code>org.eclipse.equinox.p2.core.ProvisionException: No repository found at http://ftp.mozilla.org/pub/mozilla.org/xulrunner/releases/2.0/runtimes/xulrunner-2.0.en-US.linux-i686.tar.bz2.\\nat org.eclipse.equinox.internal.p2.repository.helpers.AbstractRepositoryManager.fail(AbstractRepositoryManager.java:395)\\nat org.eclipse.equinox.internal.p2.repository.helpers.AbstractRepositoryManager.loadRepository(AbstractRepositoryManager.java:692)\\nat org.eclipse.equinox.internal.p2.metadata.repository.MetadataRepositoryManager.loadRepository(MetadataRepositoryManager.java:96)\\nat org.eclipse.equinox.internal.p2.metadata.repository.MetadataRepositoryManager.loadRepository(MetadataRepositoryManager.java:92)\\nat org.eclipse.equinox.p2.ui.LoadMetadataRepositoryJob.doLoad(LoadMetadataRepositoryJob.java:117)\\nat org.eclipse.equinox.p2.ui.LoadMetadataRepositoryJob.runModal(LoadMetadataRepositoryJob.java:102)\\nat org.eclipse.equinox.internal.p2.ui.sdk.PreloadingRepositoryHandler$2.runModal(PreloadingRepositoryHandler.java:83)\\nat org.eclipse.equinox.p2.operations.ProvisioningJob.run(ProvisioningJob.java:177)\\nat org.eclipse.core.internal.jobs.Worker.run(Worker.java:53)\\n</code></pre>\\n\\n<p>Does someone mind telling me why this provision exception occurs? Am I missing critical plugins for eclipse? I'd be interested to know if there is a way to automate the resolution of these exceptions in finding the repositories as well...</p>\\n-URL Repository Not Found: Provisioning Exception in Eclipse Kepler-<eclipse><url><repository><eclipse-kepler><equinox>\",\n",
       " '<p>Table2 has structureas\\nv_1, v_2, v_3</p>\\n\\n<p>each time I only want to select one variable so want to reference loop index i in a stored procedure. Is this possible in TD?</p>\\n\\n<p>++++++++++++++++++++++</p>\\n\\n<pre><code>replace procedure abc();\\nbegin;\\n    declare i integer default 0;\\n    loopi: loop\\n    set i = i + 1;\\n        if i &gt; 10 then leave loopi;\\n        end if;\\n        insert into table1 \\n        select * from table2\\n        where v_i = 1;\\n    end loop loopi;\\nend;\\n</code></pre>\\n-How to reference loop index i in teradata SP?-<teradata>',\n",
       " '<p>I am getting the following error while starting eclipse Kepler.</p>\\n\\n<p><strong>An internal error occurred during: \"JSP Index Manager: Processing Resource Events\".\\njava.lang.NullPointerException</strong></p>\\n\\n<p>Do you know what can be the cause of this problem?</p>\\n\\n<p>More details:</p>\\n\\n<p><strong>Severity:</strong> Error</p>\\n\\n<p><strong>Message:</strong> An internal error occurred during: \"JSP Index Manager: Processing Resource Events\".</p>\\n\\n<p><strong>Exception Stack Trace:</strong></p>\\n\\n<pre><code>java.lang.NullPointerException\\nat org.eclipse.wst.sse.core.indexing.AbstractIndexManager$ResourceEventProcessingJob.run(AbstractIndexManager.java:1520)\\nat org.eclipse.core.internal.jobs.Worker.run(Worker.java:53)\\n</code></pre>\\n\\n<p><strong>Session Data:</strong></p>\\n\\n<pre><code>eclipse.buildId=4.3.0.I20130605-2000\\njava.version=1.6.0_45\\njava.vendor=Sun Microsystems Inc.\\nBootLoader constants: OS=linux, ARCH=x86_64, WS=gtk, NL=en_US\\nFramework arguments:  -product org.eclipse.epp.package.jee.product\\nCommand-line arguments:  -os linux -ws gtk -arch x86_64 -product org.eclipse.epp.package.jee.product\\n</code></pre>\\n-eclipse kepler - An internal error occurred during: \"JSP Index Manager: Processing Resource Events\". java.lang.NullPointerException-<eclipse><eclipse-kepler>',\n",
       " '<p>I have just started using \\'Eclipse Kepler\\' IDE for PHP programming and I\\'m now looking to use the same IDE for JavaScript, but I\\'m not sure which plugin to install?</p>\\n\\n<p>I\\'m new to JavaScript so I\\'m not sure what is good to have; when I look at the Aptana and the JSDT plugin descriptions separately I\\'m not sure what the difference is and what I should be looking for. Can someone please help me. I have spent a few hours searching for a comparison but I find nothing on this topic; it feels like they are not even competitors (?). </p>\\n\\n<p>I know I want auto-completion and debugging for HTML, CSS JavaScript and PHP (I think my php auto-completion and debugging is already good enough since I have PDT and Xdebug), but I\\'m not sure what else will make me more productive. If you have some other plugin for \\'Eclipse Kepler\\' which I should be using for web application programming with JavaScript together with PHP please advice me. </p>\\n\\n<p>Ps. I downloaded Aptana 3 as a seperate IDE but I have decided to go for Eclipse Kepler since it seemed to give me more functions and more freedom to expand my IDE (this might not be true but I especially missed the \\'ctrl + hover + click\\' function in eclipse that takes me to object\\'s and method\\'s files which Aptana doesn\\'t seem to have). </p>\\n\\n<p>Best Regards \\nKriss</p>\\n-Eclipse \"aptana plugin\" vs. jsdt (JavaScript Development Tools); where can I find a comparison chart?-<javascript><php><eclipse><eclipse-plugin><eclipse-kepler>',\n",
       " '<p>I want to build new wizard and to extends from WizardNewFileCreationPage</p>\\n\\n<p>I have 2 questions :</p>\\n\\n<ol>\\n<li><p>is it possible add new button after the FileName ? Do I need to override the create\\ncreateControl and then to add my logic ?</p></li>\\n<li><p>is it possible to hide the \"Advanced\" button ? </p></li>\\n</ol>\\n-WizardNewFileCreationPage - is it possible to add new element-<java><eclipse><eclipse-kepler>',\n",
       " '<p>We are developing a solution for moving hive tables to teradata and we are using <strong>mapr-sqoop1.4.2</strong> with <strong>cloudera connector for teradata 1.0.5</strong>. Mapr version is M5.\\nHowever the performance is fluctuating as some tables are getting exported from Hadoop to Teradata in 15-20 mins whereas others take 3-4 hours.\\nThe no. of records and the file size is same for all the tables, as is the no. of unique values for the column that is used as the primary index in teradata.\\n(There is no skewing). </p>\\n\\n<p>The question raised by the client is whether <strong>cloudera connector 1.0.5</strong> actually uses <strong>FASTLOAD</strong> or not. The data that has been transfered are showing up as\\ninsert statements in DBQL table(Teradata) which does not seem to be using FASTLOAD.</p>\\n\\n<p>So how can I be sure that the exports are actually using FASTLOAD ?</p>\\n-Issue with using Cloudera Connector for Teradata with Sqoop-<teradata><cloudera><sqoop>',\n",
       " \"<p>I am building a report to display sales at a product level, and need to include a count of stores which have sold each product. My sales data is broken down by weeks, but I also need to include service level statistics which is at daily level.</p>\\n\\n<p>The problem I am having is that when I try to count the number of stores each product is sold in, it is being multiplied by up to 7 times (depending how many days of the week there is a service level record for), so my question is can I get a distinct count of stores within the one query.</p>\\n\\n<p>The data is structured as follows:</p>\\n\\n<p><strong>Sales data</strong></p>\\n\\n<pre><code>WeekNo   StoreNo   ProductNo   SalesValue\\n201301   123       123456       10,000.00\\n201301   123       654321        5,000.00\\n201301   124       123456        9,400.00\\n201301   124       654321        3,500.00\\n201302   123       123456       11,500.00\\netc.\\n</code></pre>\\n\\n<p><strong>Service Level data</strong> <em>Joined to sales via a table which converts calendar date to WeekNo</em></p>\\n\\n<pre><code>Date         StoreNo   ProductNo   ServiceLevel\\n01/01/2013   123       123456                99\\n03/01/2013   123       123456                98\\n04/01/2013   123       123456               100\\netc.\\n</code></pre>\\n\\n<p>My SQL is as follows:</p>\\n\\n<pre><code> select sales.ProductNo,\\n        prod.ProductDesc,\\n        sum(case when sales.WeekNo = 201330 then 1 end) as StoresSoldInThisWeek,\\n        sum(case when sales.WeekNo = 201329 then 1 end) as StoresSoldInLastWeek,\\n        sum(case when sales.WeekNo between 201324 and 201327 then 1 end) / 4 as StoresSoldInAverage,\\n        sum(case when sales.WeekNo = 201330 then sales.SalesValue end) as SalesValueThisWeek,\\n        sum(case when sales.WeekNo = 201329 then sales.SalesValue end) as SalesValueLastWeek,\\n        sum(case when sales.WeekNo between 201324 and 201327 then wks.SalesValue end) / 4 as SalesValueAverage,\\n        sum(case when sales.WeekNo = 201330 then sales.SalesVolume end) as SalesVolumeThisWeek,\\n        sum(case when sales.WeekNo = 201329 then sales.SalesVolume end) as SalseVolumeLastWeek,\\n        sum(case when sales.WeekNo between 201324 and 201327 then sales.SalesVolume end) / 4 as SalesVolumeAverage,\\n        cast(sum(case when serv.WeekNo = 201330 then serv.Delivered end) as float) / nullifzero(cast(sum(case when serv.WeekNo = 201330 then serv.Ordered end) as float)) as ServiceLevelThisWeek,\\n        cast(sum(case when serv.WeekNo = 201329 then serv.Delivered end) as float) / nullifzero(cast(sum(case when serv.WeekNo = 201329 then serv.Ordered end) as float)) as ServiceLevelLastWeek,\\n        cast(sum(case when serv.WeekNo between 201324 and 201327 then serv.Delivered end) as float) / nullifzero(cast(sum(case when serv.WeekNo between 201324 and 201327 then serv.Ordered end) as float)) as ServiceLevelAverage\\n\\n   from Products prod\\n        inner join Sales sales\\n                on prod.ProductNo = sales.ProductNo\\n               and sales.WeekNo in (201324,201325,201326,201327,201329,201330)\\n         left join ServiceLevel serv\\n                on sales.ProductNo = serv.ProductNo\\n               and sales.StoreNo = serv.StoreNo\\n         left join Weeks week\\n                on serv.CalDate between week.WeekStartDate and week.WeekEndDate\\n               and sales.WeekNo = week.WeekNo\\n\\ngroup by 1,2\\n\\norder by 1\\n</code></pre>\\n\\n<p>My attempt to get number of stores each product is sold in is:</p>\\n\\n<pre><code>sum(case when sales.WeekNo = 201302 then 1 end) as StoresSoldInThisWeek,\\nsum(case when sales.WeekNo = 201301 then 1 end) as StoresSoldInLastWeek,\\n</code></pre>\\n\\n<p>I have tried summarising the Service Level data into weeks however this causes the query to take too long to run, so my question is what is the most efficient way of getting a distinct count of stores? I suspect this needs the ROW_NUMBER() function, but it doesn't seem to like me doing that in a Group By field</p>\\n-Count distinct items in teradata result set-<sql><teradata>\",\n",
       " \"<p>I have something like this:</p>\\n\\n<pre><code>ID      Result\\n1       value1\\n2       value1\\n2       value2\\n3       value1\\n4       value1\\n4       value2\\n4       value3\\n</code></pre>\\n\\n<p>And I'd like to return something like this:</p>\\n\\n<pre><code>ID      Result1      Result2      Result3\\n1       value1\\n2       value1       value2\\n3       value1\\n4       value1       value2       value3\\n</code></pre>\\n\\n<p>I've searched on pivots and concats and breaks and I just can't find a simple, sensible solution.</p>\\n\\n<p>TIA</p>\\n-teradata sql pivot multiple occurrences into additional columns-<sql><pivot><teradata>\",\n",
       " '<p>I have something like this:</p>\\n\\n<pre><code>ID      Result\\n1       value1\\n2       value1\\n2       value2\\n3       value1\\n3       value1\\n4       value1\\n4       value2\\n4       value3\\n</code></pre>\\n\\n<p>Notice that ID 3 has the same result appearing in two rows.</p>\\n\\n<p>Thanks to bluefeet (<a href=\"https://stackoverflow.com/questions/19035081/teradata-sql-pivot-multiple-occurrences-into-additional-columns\">teradata sql pivot multiple occurrences into additional columns</a>), I was able to produce something like this:</p>\\n\\n<pre><code>ID      Result1      Result2      Result3\\n1       value1\\n2       value1       value2\\n3                    value1\\n4       value1       value2       value3\\n</code></pre>\\n\\n<p>I\\'m getting what I want, but because ID 3 has multiple results, they are counted as 2 and then included in the second column, which is for those results that return 2 occurrences for one ID.</p>\\n\\n<p>What I would like it do is simply return the first occurrence of the duplicate ID 3 result in the Result1 column, like this:</p>\\n\\n<pre><code>ID      Result1      Result2      Result3\\n1       value1\\n2       value1       value2\\n3       value1\\n4       value1       value2       value3\\n</code></pre>\\n\\n<p>How can I eliminate that duplicate result and prevent it from counting?</p>\\n-Teradata SQL pivot multiple results and eliminate duplicate values-<sql><teradata>',\n",
       " '<p>I have just updated to eclipse kepler (Java EE), but one thing I feel missing is the validation of jsp fragments inside <code>jsp:include</code> tags.</p>\\n\\n<p>For example:</p>\\n\\n<pre><code>&lt;jsp:include page=\"/jsp/includes/included.jsp\"&gt;&lt;/jsp:include&gt;\\n</code></pre>\\n\\n<p>If I change the included.jsp it does not issues any warning or error in the problems tab (previously it warned me).</p>\\n\\n<p>It may be related to some validators I disabled, but I could not find after some research.</p>\\n\\n<p>Where is the option to control this kind of validation in eclipse?</p>\\n-Option to control the validation of the jsp:include tag in eclipse Kepler-<java><eclipse><jsp><eclipse-kepler>',\n",
       " '<p><strong>Problem Statement</strong></p>\\n\\n<p>Example is shown in below image :</p>\\n\\n<p>The last 2 rows have the patterns like \"1.283  2  3\" in a single cell. The numbers are seperated by space in the column. We need to add those nos and represent in the format given in Output.</p>\\n\\n<p>So, the cell having \"1.283 2 3\" must be converted to 6.283</p>\\n\\n<p><strong>Challenges facing :</strong> </p>\\n\\n<ol>\\n<li>The column values are in string format.</li>\\n<li>Add nos after casting them into integer</li>\\n<li>Donot want to take data in UNIX box and manipulate the same.</li>\\n</ol>\\n\\n<p><img src=\"https://i.stack.imgur.com/3ZQib.png\" alt=\"enter image description here\"></p>\\n-Teradata : Sum up values in a column-<sql><teradata>',\n",
       " \"<p>I started to use the Eclipse Kepler this week and there is a new feature which is annoying me a lot.</p>\\n\\n<p>When I'm in a Debug Perspective with the source screen maximized, some times, the Debug Screen opens over the source code. This behaviour bother me because the Debug Screen covers the code making it impossible to see.</p>\\n\\n<p>Anybody knows what do I need to do to disable this feature?</p>\\n-Eclipse shows Debug Window over the code-<eclipse><eclipse-kepler>\",\n",
       " '<p>Is there is any way to redraw or refresh the <code>MPart</code> so that following method will call again.</p>\\n\\n<pre><code>@PostConstruct\\n    public void createPartControl(Composite parent) {\\n }\\n</code></pre>\\n\\n<p>Because I want to show one SWT control in this view on some conditions. </p>\\n-How to refresh/ redraw MPart-<java><eclipse><eclipse-rcp><eclipse-juno><eclipse-kepler>',\n",
       " '<p>here\\'s my problem:\\nI\\'m developing an Eclipse Plugin which should start an Javafx-application. Should be pretty easy actually, but i still got an issue.\\nHere my Code-example for the simple fx-app:</p>\\n\\n<pre><code>public class UIContainer extends Application{\\n\\n\\n\\npublic static void main(String[] args){\\n    launch(args);\\n}\\n\\n@Override\\npublic void start(Stage primaryStage) throws Exception {\\n    primaryStage.setTitle(\"First FXML Example\");\\n    Pane myPane = (Pane)FXMLLoader.load(getClass().getResource(\"gui.fxml\"));\\n    Scene myScene = new Scene(myPane);\\n    primaryStage.setScene(myScene);\\n    primaryStage.show();\\n}\\n</code></pre>\\n\\n<p>Now i want to run the application from an Eclipse Plugin Handler like this:</p>\\n\\n<pre><code>public Object execute(ExecutionEvent event) throws ExecutionException {\\n\\n   IWorkbenchWindow window = HandlerUtil.getActiveWorkbenchWindowChecked(event);\\n   //Call the UIContainer here\\n   //do sth. else\\n   return null;\\n}\\n</code></pre>\\n\\n<p>There is actually no difference in calling the application via the main or the start-method according to the error-message.</p>\\n\\n<pre><code>org.eclipse.e4.core.di.InjectionException: java.lang.NoClassDefFoundError: javafx/application/Application\\nat org.eclipse.e4.core.internal.di.MethodRequestor.execute(MethodRequestor.java:63)\\nat org.eclipse.e4.core.internal.di.InjectorImpl.invokeUsingClass(InjectorImpl.java:243)\\nat org.eclipse.e4.core.internal.di.InjectorImpl.invoke(InjectorImpl.java:224)\\nat org.eclipse.e4.core.contexts.ContextInjectionFactory.invoke(ContextInjectionFactory.java:132)\\nat org.eclipse.e4.core.commands.internal.HandlerServiceHandler.execute(HandlerServiceHandler.java:167)\\nat org.eclipse.core.commands.Command.executeWithChecks(Command.java:499)\\n...\\nCaused by: java.lang.ClassNotFoundException: javafx.application.Application cannot be found by XODR-Validator_0.0.1.qualifier\\nat org.eclipse.osgi.internal.loader.BundleLoader.findClassInternal(BundleLoader.java:501)\\nat org.eclipse.osgi.internal.loader.BundleLoader.findClass(BundleLoader.java:421)\\nat org.eclipse.osgi.internal.loader.BundleLoader.findClass(BundleLoader.java:412)\\nat org.eclipse.osgi.internal.baseadaptor.DefaultClassLoader.loadClass(DefaultClassLoader.java:107)\\nat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\\n... 60 more\\n</code></pre>\\n\\n<p>Hopefully you can track my problem in order to find the solution.</p>\\n\\n<p>My Configuration:\\nEclipse Kepler (IDE for EE Developer)\\nJDK1.7.0_40 </p>\\n\\n<p>Thanks in advance!</p>\\n-Eclipse Plugin executes Javafx application-<eclipse><plugins><javafx><eclipse-kepler>',\n",
       " '<p>We are creating an eclipse-plugin with somw views; one of them is the project explorer. How can we change its title from \"Project Explorer\" to something else? </p>\\n\\n<p>I understand that there is <code>org.eclipse.ui.navigator.resources.ProjectExplorer</code> which has a <code>setTitle</code> method, but how do I get the instance used by the Workbench?</p>\\n-How can I change the title of the Project Explorer?-<eclipse><eclipse-plugin><eclipse-kepler>',\n",
       " \"<p>Table A has the value <code>'abc123'</code> \\nTable B has the value <code>'abc123'</code></p>\\n\\n<p>I confirm this by </p>\\n\\n<pre><code>SELECT * FROM tableA a WHERE value='abc123'\\nand\\nSELECT * FROM tableB b WHERE value='abc123'\\n</code></pre>\\n\\n<p>and both queries return results</p>\\n\\n<p>however when i do:</p>\\n\\n<pre><code>select\\n    a.*\\nfrom\\n    tablea a\\njoin\\n    tableb b\\non trim(trailing from a.value) = trim(trailing from b.value)\\n</code></pre>\\n\\n<p>nothing returns.</p>\\n\\n<p>now the string I am dealing with in one of the these tables was imported over from unix and I made sure to strip out the whitespace, etc.</p>\\n-joining on known values - teradata SQL-<sql><teradata>\",\n",
       " '<p>On Eclipse, with Window -> New Editor or by drag/drop method* I could have one more editor panes in my eclipse work area. Now how do I remove that extra pane when I do not need it anymore. All it allows me now is to minimize/maximize that editor extra pane. It just sticks around. Shouldn\\'t there be a collapse or close extra editor panes (that is without me requiring to do Window->Reset Perspective!)</p>\\n\\n<ul>\\n<li>Ref: answers to another question: <a href=\"https://stackoverflow.com/questions/9471578/how-do-you-split-a-window-view-in-eclipse-ide\">How do you split and unsplit a window/view in Eclipse IDE?</a></li>\\n</ul>\\n-How do you join back the split editor panes in eclipse-<eclipse><eclipse-kepler>',\n",
       " \"<p>I'm using TAP and Fireworks to prototype an iOS app. I'm optimizing my prototype for use on an iPad mini and I recently (maybe mistakenly) upgraded the mini to iOS7.</p>\\n\\n<p>Now when I add the prototype to my homescreen it has its new icon, but when I open it ..it shows old content. :(</p>\\n\\n<p>Then when I return to the homescreen the old icon is back too. </p>\\n\\n<p>I've cleared the cache, cleared the browsing history, restarted the iPad, changed the code on the index page of the site... nothing I try seems to work.<br>\\n<strong>Is there a secret cache I can clear somewhere?</strong></p>\\n\\n<p>ps - I'm a designer, not much of a coder, but if I need to add a line of code to my exported php files I could do that.</p>\\n-How to refresh a prototype app (actually a bookmark on the homescreen) in iOS 7?-<php><ios7><tap><fireworks>\",\n",
       " '<p>Is it possible to set TENACITY &amp; SLEEP for TERADATA FASTLOADCSV calls made via JAVA JDBC?<br/>\\nMy jdbc connection string is<br/>\\n<code><br/>\\njdbc:teradata://99.99.99.99/TMODE=ANSI,CHARSET=UTF8,TYPE=FASTLOADCSV,LOG=INFO,SESSIONS=1\\n</code><br/>\\nyou can set these options when executing a FASTLOAD script as shown here:-\\n<code><br/>\\nBy default the Tenacity feature is not turned on. The feature is turned on by the script command:\\nTenacity n;</p>\\n\\n<p>Where n specifies the number of hours FastLoad continues trying to logon. The n specification must be greater than zero. If zero is entered, Teradata FastLoad responds with an error message and terminates. During the Tenacity duration, FastLoad tries to log on every 6 minutes by default. The 6 minute default can be changed by using the script command:\\nSleep m;</p>\\n\\n<p>Where m specifies the number of minutes Teradata FastLoad sleeps before retrying the logon operation. The m specification must be greater than zero. If zero is entered, Teradata FastLoad responds with an error message, and terminates.</p>\\n\\n<p>Below is an example of Tenacity usage. Suppose the commands in the script are:\\nTenacity 1;</p>\\n\\n<p>Sleep 15;</p>\\n\\n<p>The Tenacity duration is 60 minutes and the Sleep interval is 15 minutes.\\n</code><br/>\\n<br/>\\ncan these option sbe set when performing FASTLOADCSV via jdbc?</p>\\n-Setting FASTLOADCSV Tenacity & Sleep using Teradata jdbc driver-<java><jdbc><teradata>',\n",
       " \"<p>Does anybody know a way to convert unix timestamp to date_time attribute?</p>\\n\\n<p>I tried to use R extensions (my operators are mainly written in R) such as as.POSIXct functions to convert timestamps but it seems that rapidminer doesn't like it and keeps ignoring it.</p>\\n\\n<p>Any help is appreciated </p>\\n\\n<p>Thanks</p>\\n-Rapidminer : converting unix timestamp-<r><rapidminer>\",\n",
       " '<p>Have recently upgraded eclipse from Juno to Kepler, and now when I start a tomcat 7 server using WTP (by selecting the server in the \\'Servers\\' view and pressing the run button in the view\\'s button bar) the console view pops up to the front like before, but instead of displaying tomcat\\'s output like it did in Juno it just displays a message that reads \"No consoles to display at this time.\"</p>\\n\\n<p>I have tried checking the server launch configuration (double click on server entry in Servers view, click \\'Open launch configuration\\' link, switch to \\'Common\\' tab) but the checkbox \\'Allocate console\\' is ticked, which is the only control I am aware of that could cause this problem.</p>\\n\\n<p>Any suggestions how I get the console back?</p>\\n-Tomcat output not appearing in console view-<eclipse-wtp><eclipse-kepler>',\n",
       " '<p>I would like to run a simulation that depends upon Linear Regression Model coefficients.</p>\\n\\n<p>In RapidMiner, how could I extract the Linear Regression Model coefficients?</p>\\n\\n<p>I would find it very useful if I could get those coefficients into macro parameters?</p>\\n-Extract RapidMiner Linear Regression Model Coefficients-<rapidminer>',\n",
       " '<p>I\\'m trying to contribute to the same custom toolbar from two different plugin.xml files. Unfortunately I can\\'t find a way to specify the order in which the buttons appear. The one that is supposed to be the last appears as the first button.</p>\\n\\n<p>I already tried to specify the insertion position by using</p>\\n\\n<pre><code>...\\nMenuManager manager = new MenuManager(null, \"my.toolbar.id\");\\nIMenuService menuService = (IMenuService) getEditorSite().getService( IMenuService.class);\\nmanager.add(new GroupMarker(\"testing\"));\\nmenuService.populateContributionManager(manager, \"toolbar:my.toolbar.id?after=testing\");\\n...\\n</code></pre>\\n\\n<p>and in the plugin.xml</p>\\n\\n<pre><code>&lt;extension point=\"org.eclipse.ui.menus\"&gt;\\n  &lt;menuContribution locationURI=\"toolbar:my.toolbar.id?after=testing\"&gt;\\n    &lt;toolbar id=\"my.toolbar.id\"&gt;\\n&lt;command ...\\n</code></pre>\\n\\n<p>Does anybody have an idea what could be wrong?</p>\\n-Order of toolbar buttons in Eclipse Kepler-<eclipse><toolbar><eclipse-kepler>',\n",
       " \"<p>My Eclipse Kepler is installed on Ubuntu. After I select the Eclipse icon either from folder or dash home, it displays a normal splash screen at the centre of my screen for a second, then the splash screen become totally a white (or gray?) rectangle. It's totally white, so I cannot see the progress of loading Eclipse, and no matter how long I wait, the splash screen hangs on and the workbench never appear.</p>\\n\\n<p>NOTE: I incautiously deleted the ~/workspace directory, is this the cause of the problem?</p>\\n\\n<p>Anyone know how to fix this problem?  :)</p>\\n-My Eclipse Kepler cannot start-<eclipse><eclipse-kepler>\",\n",
       " '<p>have the following process :</p>\\n\\n<ol>\\n<li><strong>Process documents from files</strong>  (where I load the  text files with respective 6 classes ) --> this connects to <strong>set Role</strong> (which changes text attribute to REGULAR attribute to allow machine learning)   -> <strong>Process documents from data</strong> ( I dont need the word vectors so I uncheck that, I keep text, within this process I tokenize, stopwords, stemming etc.)  and then I feed this into <strong>validation</strong> operator. (bayes/svm)</li>\\n</ol>\\n\\n<p>What is happening here is in the example set the <strong><em>text column</em></strong> is going back to type \"TEXT\" from <strong><em>regular</em></strong> after running Process documents from Data. And hence I get the error <strong><em>Input ExampleSet has no attributes</em></strong> as there are zero regular attributes. And this is causing the process to fail. I have no idea why. I try to set the role <strong><em>again</em></strong> after this but then the error says \"<strong><em>No examples in example set\"</em></strong></p>\\n\\n<p>PLEASE HELP. I am stuck since two days!!!  </p>\\n\\n<p><strong>EDIT : I think I know the issue - I was applying  a 10-fold X-Validation on a dataset with few examples</strong></p>\\n-Rapidminer - unable to apply learning algorithm as process document is making regular to text-<machine-learning><rapidminer>',\n",
       " '<p>I am having an excel sheet which contains details about issue resolutions and one of the columns contains email conversation for that respective issue. In essence, each row has an email conversation for that issue (issueid being primary key for this data).\\nThe email conversation format is consistent</p>\\n\\n<p>From: \"Name\" \\\\n\\nDate: \"timestamp\" \\\\n\\nSubject:\"Subject\"  \\\\n\\nTo: \"Name\"  \\\\n\\n\"Body\"</p>\\n\\n<p>This pattern repeats again...</p>\\n\\n<p>Now I need to find how many email conversations have happened for every issue using Rapidminer. I have read the excel and data is available for each row in Rapidminer... How can I accomplish this?</p>\\n\\n<p>Any help will be highly appreciated...</p>\\n-Count number of email conversations using Rapidminer-<nlp><rapidminer>',\n",
       " '<p>Can anybody tell me how to create an xml file in Eclipse ?.</p>\\n\\n<p>Project right click->new->other.. does not shows an xml wizard.</p>\\n\\n<p>New->file and renaming it as filename.xml does not give the xml design page.</p>\\n\\n<p>I am using Eclipse -Kepler 4.3. </p>\\n-How to create xml file in Eclipse-Kepler-<xml><eclipse-kepler>',\n",
       " '<p>Ok, this should be easy, but for some reason I can\\'t figure it out.</p>\\n\\n<p>I want to restore the file transfer confirmation dialog that appears after a file has been successfully uploaded.</p>\\n\\n<p>I clicked the \"don\\'t show next time\" checkbox, but realize I really do need it.</p>\\n-Eclipse Kepler: Restore File Transfer confirmation dialog-<eclipse><dialog><transfer><eclipse-kepler>',\n",
       " '<p>Installed Eclipse-Kepler on a Windows 8 box. Clojure and Leiningen already installed. Installed Counterclockwise. Started up, created test Clojure project, ran without debugger, and ran successfully under debugger. Messed with it a bit more, was in and out of the debugger several times. After one debugging session got \"an error\" (yes, I know I should have written it down, but failed to do so. Take pity upon the soul of this poor sinner...). After that every debugging session I tried to start (using \"Debug As...Clojure Application\") failed to start, eventually timing out with a \"Timed out waiting for REPL to start\" error. Messed with it a while, uninstalled CCW, re-installed CCW - no change, REPL still refused to start under the debugger. Could <em>always</em> start a REPL by \"sending\" the text of the project file to the REPL using Ctrl-Alt-S. Finally deleted all of Kepler, re-installed, re-installed CCW, and could then debug again.</p>\\n\\n<p>My questions: has anyone else encountered this, and is there a better workaround than blowing away the entire Eclipse install and re-extracting from the .zip?  I did do a check for updated versions of anything and nothing came up. Is this a Kepler issue, i.e. might this be more stable under Juno?</p>\\n-Eclipse Kepler hangs when trying to debug Clojure project-<eclipse><clojure><eclipse-kepler>',\n",
       " '<p>I get the error, after I set up MonetDB and try to write a large data.frame as a new table in the default database (demo):</p>\\n\\n<pre><code>&gt;dbWriteTable(conn, \"table1\", df)\\nError in .local(conn, statement, ...) : \\n   Unable to execute statement \\'INSERT INTO table1 VALUES([...])\\n</code></pre>\\n\\n<p>The data.frame has dimensions:</p>\\n\\n<pre><code>&gt;dim(df)\\n[1] 148767    618\\n</code></pre>\\n\\n<p>And has all columns formatted as character:</p>\\n\\n<pre><code>&gt;all(lapply(df,class)==\\'character\\')\\n[1] TRUE\\n</code></pre>\\n\\n<p>The error seems to stem from a string value being too long (Thanks @Hannes Mühleisen):</p>\\n\\n<pre><code>&gt;dbGetException(conn)\\n$errNum\\n[1] 22001\\n\\n$errMsg\\n[1] \"value too long for type (var)char(255)\"\\n</code></pre>\\n\\n<p>How does MonetDB set upper bounds of new (VAR)CHAR variables (I did not find any info on upper bounds in the documentation)? Can a global upper bound be set or can the upper bound be set interactively when creating tables from R via MonetDB.R?</p>\\n-What can be reasons for `Error in .local(conn, statement, ...)´ in dbWriteTable from package MonetDB.R?-<r><monetdb>',\n",
       " '<p>I created monetdb database using the python library <a href=\"https://pypi.python.org/pypi/python-monetdb/11.16.0.7\" rel=\"nofollow\">https://pypi.python.org/pypi/python-monetdb/11.16.0.7</a> on my ubuntu 12.04 lts server. Now I want to transfer this database to another ubuntu 12.04 lts server. However, I am unable to understand as to where(directory name) does monetdb create its database (within ubuntu 12.04) and what is the procedure which I must follow to transfer this database to another machine (so that i may query the database on another machine)?</p>\\n-How to transfer monetdb database to another machine-<monetdb>',\n",
       " \"<p>I'm having trouble deploying an EAR project on liberty profile locally integrated in eclipse kepler. It comes back with the following error: </p>\\n\\n<pre><code>Application 'App-Name' requires feature wasJmsClient, \\nwhich is not supported by WebSphere Application Server V8.5 liberty profile. \\n\\nReason:\\nApplication 'App-Name' requires feature wasJmsClient, \\nwhich is not supported by WebSphere Application Server V8.5 liberty profile. \\n</code></pre>\\n\\n<p>I've read up on IBM documentation which indicates that you need to add suppport to this feature in your server.xml. I have done this, to no evail. </p>\\n-wasJmsClient not supported in websphere 8.5 liberty profile-<jms><ear><eclipse-kepler><websphere-liberty>\",\n",
       " '<p>I have a maven project that give the following two errors</p>\\n\\n<p>JAX-RS (REST Web Services) 2.0 can not be installed : One or more constraints have not been satisfied.<br>\\nJAX-RS (REST Web Services) 2.0 requires Java 1.7 or newer.</p>\\n\\n<p>I have JDK 1.6 installed (I cant change this)</p>\\n\\n<p>The project facets does NOT have JAX-RS ticked.</p>\\n\\n<p>The project facets has java 1.6 set.</p>\\n\\n<p>The project facets has Dynamic Web Project 2.4 set.</p>\\n\\n<p>I have following plugins</p>\\n\\n<p>Sonar 3.2.0\\nMercurialEclipse 2.10\\nEclEmma 2.2.1</p>\\n\\n<p>The pom.xml is just this...</p>\\n\\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\\n    xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\"&gt;\\n    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\\n    &lt;groupId&gt;com.fake.company&lt;/groupId&gt;\\n    &lt;artifactId&gt;customerservice-war&lt;/artifactId&gt;\\n    &lt;version&gt;2.0.0-SNAPSHOT&lt;/version&gt;\\n&lt;/project&gt;\\n</code></pre>\\n\\n<p>the web.xml is</p>\\n\\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;web-app version=\"2.4\" xmlns=\"http://java.sun.com/xml/ns/j2ee\"\\n    xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\\n    xsi:schemaLocation=\"http://java.sun.com/xml/ns/j2ee http://java.sun.com/xml/ns/j2ee/web-app_2_4.xsd\"&gt;\\n    &lt;display-name&gt;Customer Service&lt;/display-name&gt;\\n&lt;/web-app&gt;\\n</code></pre>\\n\\n<p>Cleaning or \"Update Maven Project\" makes no difference.</p>\\n\\n<p>Note: This is in eclipse-jee-kepler-SR1-win32-x86_64.\\nNote: Version eclipse-jee-kepler-win32-x86_64 does not give the error.</p>\\n\\n<p>Note: New workspace does not change the error.</p>\\n\\n<p>Note: I\\'m using JDK.1.6.0_43</p>\\n\\n<p>The only error I can see related to this in the \".log\" file is..</p>\\n\\n<p>!ENTRY org.eclipse.osgi 2 1 2013-10-16 15:07:58.816\\n!MESSAGE NLS unused message: JaxrsProjectConfigurator_The_project_does_not_contain_required_facets in: org.eclipse.m2e.wtp.jaxrs.internal.messages</p>\\n\\n<p>Adding the facet, wont let me apply it since it says I need Java 1.7</p>\\n\\n<p>JSR339 (<a href=\"http://download.oracle.com/otn-pub/jcp/jaxrs-2_0-fr-spec/jsr339-jaxrs-2.0-final-spec.pdf?AuthParam=1381932823_3d235d93090d7ad1709113fead897636\" rel=\"nofollow noreferrer\">JSR339</a>) states \"The API will make extensive use of annotations and will\\nrequire J2SE 6.0 or later\"</p>\\n\\n<p>Any ideas?</p>\\n-Why does Eclipse Kepler SR1 error with : JAX-RS 2.0 requires Java 1.7 or newer-<eclipse><jax-rs><eclipse-kepler>',\n",
       " \"<p>I'm trying to find a way to build my web application project to contain dependent projects java classes merged into WEB-INF/classes directory rather creating dependent project jar files and keeping under WEB-INF/lib directory. I have been playing with 'Properties' -> 'Deployment Assembly' but no luck! Any suggestions please?</p>\\n-Eclipse Kepler JavaEE version - How to add dependent project classes-<dependencies><war><eclipse-kepler>\",\n",
       " '<p>I am fighting with a query where I need to retrieve a list of IDs and a value.</p>\\n\\n<p>Then I need to cast a new value for those IDs that have more than one value.</p>\\n\\n<p>For example:</p>\\n\\n<pre><code>ID          COLOR\\n1           BLUE\\n1           GREEN\\n2           ORANGE\\n</code></pre>\\n\\n<p>What I want to return is like this:</p>\\n\\n<pre><code>ID          COLOR\\n1           MULTIPLE\\n2           ORANGE\\n</code></pre>\\n\\n<p>So far, I\\'ve used ROW_NUMBER() and OVER(PARTITION BY) to retrieve all the results and number them.</p>\\n\\n<p>But then I get stuck going in circles.</p>\\n\\n<p>I can limit by those IDs that have multiple occurrences (where the row count >=2) and I get those IDs that need to be cast as \"MULTIPLE\".</p>\\n\\n<p>BUT: Those same values are also included if I limit to IDs that have only one value (=1), because the multiple value IDs also have a row number 1. I wind up with an invalid result because I\\'m counting the multiple-value-IDs twice, kind of like this:</p>\\n\\n<pre><code>ID         COLOR\\n1          BLUE\\n1          MULTIPLE\\n2          ORANGE\\n</code></pre>\\n\\n<p>So the question is: how can I get all the IDs with multiple values, separate from those that only have one value?</p>\\n-teradata sql multiple occurrences as single value-<sql><count><duplicates><teradata>',\n",
       " \"<p>is there a possibility in eclipse to mark multiple lines in different classes of a project for later editing, so i can write some test code and afterwards before i release my software i got the possibility to show all the marked lines and delete or edit them ?</p>\\n\\n<p>I know the Tasks in eclipse just not sure if this is the right way to go, because i have to enter the name of the Task everytime and there isn't the possibility to group them together as far as i know.</p>\\n-Eclipse - Mark line for later editing-<eclipse><ide><tags><eclipse-juno><eclipse-kepler>\",\n",
       " \"<p>I'm trying to go back and retrieve counts for the last 4 full months.  This is an example of what I have so far:</p>\\n\\n<p><code>SELECT datecolumn, Count(datacolumnA) AS CountOfdatacolumnA, datacolumnB</code></p>\\n\\n<pre><code>FROM tableA\\n\\nWHERE datacolumnB='AA' AND datecolumn &gt;= ADD_MONTHS(CURRENT_DATE, -4)\\n</code></pre>\\n\\n<p>My results show the last four months plus the current month, October in this case.  The problem is that June isn't showing the correct count for the entire month.  I'm only getting a partial count for the month.  </p>\\n-Trying to Look back 4 whole months in teradata with ADD_MONTHS function in sql statement-<sql><teradata>\",\n",
       " \"<p>I'm using the Kepler CDT release (4.3.1) of Eclipse. When I click on anything in the Outline view, the corresponding editor view is reduced to showing just that item. If I click on a variable, I get a single line with just that variable. The Edit->Expand Selection options are all dimmed out. Hitting Shift-Alt-Up Arrow just moves me up to the previous item in the outline view. If I change editor tabs and come back then the Expand Selection options enable and I can manually hit Shift-Alt-Up Arrow a number of times to make the entire file visible again but clicking on anything in the outline view again will just reduce the view. Is there some new setting in Kepler that will make outline stop doing this?</p>\\n-Eclipse Outline view is hiding code-<eclipse><eclipse-cdt><eclipse-kepler><outline-view>\",\n",
       " '<p>I have installed PMD in Eclipse Kepler using Eclipse market place.\\nBut I am unable to access it after the installation.</p>\\n\\n<p>When I go to Window &rarr; Preferences and search for PMD, I get no results.</p>\\n\\n<p>Is there any other plugin available for source code cleanup?<br>\\nOr: What needs to be done to install PMD in Kepler?</p>\\n\\n<p>Thanks</p>\\n-Not able to install PMD in Eclipse Kepler-<eclipse><pmd><eclipse-kepler>',\n",
       " '<p>When running my application the very first cuda_malloc takes 40 seconds which is due to the initialization of the GPU. When I build in debug mode this reduces to 5 seconds and when I run the same code on a Fermi device, it takes far less than a second (not even worth measuring in my case).</p>\\n\\n<p>Now the funny thing is that if I compile for this specific architecture, using the flag sm35 instead of sm20, it becomes fast again. As I should not use any new sm35 features just yet, how can I compile for sm20 and not have this huge delay? Also I am curious what is causing this delay? Is the machine code recompiled on the fly into sm35 code?</p>\\n\\n<p>Ps. I run on windows but a colleague of mine encountered the same problem, probably on windows. The device is a Kepler, driver version 320.</p>\\n-Why is the initialization of the GPU taking very long on Kepler achitecture and how to fix this?-<cuda><initialization><kepler>',\n",
       " '<p>As we know Fermi support only single connection to GPU, and as written here: <a href=\"http://on-demand.gputechconf.com/gtc-express/2011/presentations/StreamsAndConcurrencyWebinar.pdf\" rel=\"nofollow\">http://on-demand.gputechconf.com/gtc-express/2011/presentations/StreamsAndConcurrencyWebinar.pdf</a></p>\\n\\n<blockquote>\\n  <p>Fermi architecture can simultaneously support </p>\\n  \\n  <p>Up to 16 CUDA kernels on GPU</p>\\n</blockquote>\\n\\n<p>And as we know Hyper-Q allows for up to 32 simultaneous connections from multiple CUDA streams, MPI processes, or threads within a process: <a href=\"http://www.nvidia.com/content/PDF/kepler/NVIDIA-Kepler-GK110-Architecture-Whitepaper.pdf\" rel=\"nofollow\">http://www.nvidia.com/content/PDF/kepler/NVIDIA-Kepler-GK110-Architecture-Whitepaper.pdf</a></p>\\n\\n<p>But how many kernels simultaneously support on <strong>Kepler CC3.0/3.5, 16 or 32 (STREAMs)</strong>?</p>\\n-How many kernels simultaneously support on Kepler CC3.0/3.5, 16 or 32 (STREAMs)?-<cuda><gpgpu><nvidia><kepler>',\n",
       " \"<p>How can i search for a particular date for eg: '2013-10-22' from teradata timestamp(6) field?</p>\\n\\n<pre><code>sel * from table A\\nwhere date = '2013-10-22';\\n</code></pre>\\n\\n<p>I tried the above query which is throwing error. Please help!</p>\\n-query to return specific date from teradata timestamp(6)-<sql><teradata>\",\n",
       " \"<p>I've been reading about data demographics of teradata and came across with this two terms. It is mentioned that this two goes hand in hand to make good index choice, but I can't seem to understand exactly what is the difference between the two values.\\nCan anyone explain to me the exact difference between the two. Examples on how the values are derived would be really helpful.\\nI'm thinking both values will come from this query:</p>\\n\\n<pre><code> sel &lt;columnname&gt;, count(*) \\n from &lt;tablename&gt;\\n</code></pre>\\n\\n<p>Here are the definition of the two terms, btw.</p>\\n\\n<pre><code>    Maximum Rows/Value –No. of rows for the most-often-occurring value in the column.\\n    Typical Rows/Value –No. of rows for a typical value in the column.\\n</code></pre>\\n\\n<p>Any inputs will be much appreciated.\\nThank you.</p>\\n-maximum rows per value vs typical rows per value [teradata]-<database><performance><indexing><rdbms><teradata>\",\n",
       " '<p>I\\'m creating E4 RCP application in that I have one part. I want to implement \"Save As\" functionality for my Part, as it is implemented for Editors (Like:Java file Editor).</p>\\n\\n<p>Requirements: </p>\\n\\n<ol>\\n<li>When user click on my part \"Save As\" option should be enable.</li>\\n<li>When user click on \"Save As\" option my code should run so that I can do what I want.</li>\\n</ol>\\n\\n<p>So my question is for this what should I do, is I have to implement any extension point or any this else. ?</p>\\n-How to implement \"Save As\" functionality for our Part-<java><eclipse><eclipse-rcp><eclipse-juno><eclipse-kepler>',\n",
       " '<p>Can we delete duplicate records from a multiset table in teradata without using intermediate table.</p>\\n\\n<p>Suppose we have 2 rows with values \\n1, 2, 3 \\nand 1, 2, 3\\nin my multiset table then after delete i should have \\nonly one row i.e. 1, 2, 3.</p>\\n-Can we delete duplicate records from a table in teradata without using intermediate table-<teradata>',\n",
       " '<h2>Concise-ish problem explanation:</h2>\\n\\n<p>I\\'d like to be able to run multiple (we\\'ll say a few hundred) shell commands, each of which starts a long running process and blocks for hours or days with at most a line or two of output (this command is simply a job submission to a cluster). This blocking is helpful so I can know exactly when each finishes, because I\\'d like to investigate each result and possibly re-run each multiple times in case they fail. My program will act as a sort of controller for these programs.</p>\\n\\n<pre><code>for all commands in parallel {\\n    submit_job_and_wait()\\n    tries = 1\\n    while ! job_was_successful and tries &lt; 3{\\n        resubmit_with_extra_memory_and_wait()\\n        tries++\\n    }\\n}\\n</code></pre>\\n\\n<h2>What I\\'ve tried/investigated:</h2>\\n\\n<p>I was so far thinking it would be best to create a thread for each submission which just blocks waiting for input. There is enough memory for quite a few waiting threads. But from what I\\'ve read, perl threads are closer to duplicate processes than in other languages, so creating hundreds of them is not feasible (nor does it feel right).</p>\\n\\n<p>There also seem to be a variety of event-loop-ish cooperative systems like <code>AnyEvent</code> and <code>Coro</code>, but these seem to require you to rely on asynchronous libraries, otherwise you can\\'t really do anything concurrently. I can\\'t figure out how to make multiple shell commands with it. I\\'ve tried using <code>AnyEvent::Util::run_cmd</code>, but after I submit multiple commands, I have to specify the order in which I want to wait for them. I don\\'t know in advance how long each submission will take, so I can\\'t <code>recv</code> without sometimes getting very unlucky. This isn\\'t really parallel.</p>\\n\\n<pre><code>my $cv1 = run_cmd(\"qsub -sync y \\'sleep $RANDOM\\'\");\\nmy $cv2 = run_cmd(\"qsub -sync y \\'sleep $RANDOM\\'\");\\n\\n# Now should I $cv1-&gt;recv first or $cv2-&gt;recv? Who knows!\\n# Out of 100 submissions, I may have to wait on the longest one before processing any.\\n</code></pre>\\n\\n<p>My understanding of AnyEvent and friends may be wrong, so please correct me if so. :)</p>\\n\\n<p>The other option is to run the job submission in its non-blocking form and have it communicate its completion back to my process, but the inter-process communication required to accomplish and coordinate this across different machines daunts me a little. I\\'m hoping to find a local solution before resorting to that.</p>\\n\\n<p>Is there a solution I\\'ve overlooked?</p>\\n-Waiting on many parallel shell commands with Perl-<multithreading><perl><shell><cluster-computing>',\n",
       " '<p>I am working on an enum which contains commands for dealing with pop3 therefore I would like to store the procedure which will be performed at each command in the enum. This is what i builded:</p>\\n\\n<p>Now my Eclipse codecompletion is not working when I try to edit \\none of the        </p>\\n\\n<pre><code> @Override\\n        public void doWork(Socket clientSocket, DataOutputStream outToServer, BufferedReader inFromServer, EmailAccount emailAccount) {\\n            // TODO Auto-generated method stub\\n        }\\n</code></pre>\\n\\n<p>Methods. I already enabled all proposaltypes in preferences -> Java -> Editor -> COntent Assist -> Advanced</p>\\n\\n<p>Do you have any suggestions?</p>\\n\\n<pre><code>public enum CodecompletionTest {\\nCONST1(\"FOO\", new Event() {\\n\\n    @Override\\n    public void doWork(int foo, String bar) {\\n        // no codecompletion in here\\n    }\\n\\n});\\n\\nprivate CodecompletionTest(String fo, Event bar) {\\n    // do smth with fo and bar\\n}\\n\\nprivate interface Event {\\n    /**\\n     * Verarbeitung spezifisch ausführen\\n     * \\n     */\\n    public abstract void doWork(int foo, String bar);\\n}\\n}\\n</code></pre>\\n-Eclipse Kepler Code completion not working-<java><eclipse><enums><code-completion><eclipse-kepler>',\n",
       " '<p>I have a demo data that i need to cluster. The utility is supposed to send the data to rapid miner algorithm and then retrieve the result. I used Rapid Miner API to use the existing algorithms of rapid miner. However I am facing the problem using this API.</p>\\n\\n<p>Java Class:</p>\\n\\n<pre><code>package rajeev.rapidminer.main;\\n\\nimport java.io.File;\\nimport java.io.IOException;\\n\\nimport com.rapidminer.Process;\\nimport com.rapidminer.RapidMiner;\\nimport com.rapidminer.RapidMiner.ExecutionMode;\\nimport com.rapidminer.operator.IOObject;\\nimport com.rapidminer.operator.OperatorException;\\nimport com.rapidminer.repository.IOObjectEntry;\\nimport com.rapidminer.repository.MalformedRepositoryLocationException;\\nimport com.rapidminer.repository.RepositoryException;\\nimport com.rapidminer.repository.RepositoryLocation;\\nimport com.rapidminer.tools.XMLException;\\n\\n\\n\\npublic class Main {\\n\\npublic static void main(String a[]) throws IOException, XMLException, RepositoryException, OperatorException\\n{\\n    String rmpPath=\"C:/Users/rajeev-s/.RapidMiner5/repositories/Local Repository/DemoClustering.rmp\";\\n    RapidMiner.setExecutionMode(ExecutionMode.COMMAND_LINE);\\n    RapidMiner.init();\\n    Process process=new Process(new File(rmpPath));\\n\\n    RepositoryLocation modelLocation=new RepositoryLocation(\"//C:/Users/rajeev-s/.RapidMiner5/repositories/Local Repository/ClusterDemoOutput\");\\n    IOObject model=((IOObjectEntry)modelLocation.locateEntry()).retrieveData(null);\\n\\n    RepositoryLocation testDataLocation=new RepositoryLocation(\"//LocalRepository/TestData.txt\");\\n    IOObject testData=((IOObjectEntry)testDataLocation.locateEntry()).retrieveData(null);\\n\\n    com.rapidminer.operator.IOContainer ioInput = new com.rapidminer.operator.IOContainer(new IOObject[] { model, testData });\\n    process.run(ioInput);\\n    process.run(ioInput);\\n    final long start = System.currentTimeMillis();\\n    final com.rapidminer.operator.IOContainer ioResult = process.run(ioInput);\\n    final long end = System.currentTimeMillis();\\n    System.out.println(\"T:\" + (end - start));\\n}\\n</code></pre>\\n\\n<p>}</p>\\n\\n<p>The error stack trace is:</p>\\n\\n<pre><code>Oct 25, 2013 6:28:53 PM com.rapidminer.tools.ParameterService init\\nINFO: Reading configuration resource com/rapidminer/resources/rapidminerrc.\\nOct 25, 2013 6:28:53 PM com.rapidminer.tools.I18N &lt;clinit&gt;\\nINFO: Set locale to en.\\nOct 25, 2013 6:28:54 PM com.rapid_i.Launcher ensureRapidMinerHomeSet\\nINFO: Property rapidminer.home is not set. Guessing.\\nOct 25, 2013 6:28:54 PM com.rapid_i.Launcher ensureRapidMinerHomeSet\\nINFO: Trying parent directory of \\'C:\\\\Program Files\\\\Rapid-     I\\\\RapidMiner5\\\\lib\\\\launcher.jar\\'...gotcha!\\nOct 25, 2013 6:28:54 PM com.rapid_i.Launcher ensureRapidMinerHomeSet\\nINFO: Trying parent directory of \\'C:\\\\Program Files\\\\Rapid-   I\\\\RapidMiner5\\\\lib\\\\rapidminer.jar\\'...gotcha!\\nOct 25, 2013 6:29:00 PM com.rapidminer.tools.expression.parser.ExpressionParserFactory  &lt;clinit&gt;\\nINFO: Default version of expression parser registered successfully\\nOct 25, 2013 6:29:06 PM com.rapidminer.parameter.ParameterTypePassword decryptPassword\\nWARNING: Password in XML file looks like unencrypted plain text.\\nOct 25, 2013 6:29:10 PM com.rapidminer.tools.OperatorService init\\nINFO: Number of registered operator classes: 443; number of registered operator    descriptions: 444; number of replacements: 403\\nOct 25, 2013 6:29:10 PM com.rapidminer.tools.jdbc.JDBCProperties &lt;init&gt;\\n</code></pre>\\n\\n<p>WARNING: Missing database driver class name for ODBC Bridge (e.g. Access)\\n    Oct 25, 2013 6:29:10 PM com.rapidminer.tools.jdbc.JDBCProperties registerDrivers\\n    INFO: JDBC driver ca.ingres.jdbc.IngresDriver not found. Probably the driver is not   installed.\\n    [Fatal Error] :1:1: Premature end of file.\\n    Exception in thread \"main\" com.rapidminer.repository.RepositoryException: Requested   repository C: does not exist.\\n    at com.rapidminer.repository.RepositoryManager.getRepository(RepositoryManager.java:201)\\n    at com.rapidminer.repository.RepositoryLocation.getRepository(RepositoryLocation.java:139)\\n    at com.rapidminer.repository.RepositoryLocation.locateEntry(RepositoryLocation.java:162)\\n    at rajeev.rapidminer.main.Main.main(Main.java:29)</p>\\n\\n<p>When i copied the \\'Local Repository\\'to my class path and changed the path to:</p>\\n\\n<pre><code>//Local Repository/ClusterDemoOutput\\n</code></pre>\\n\\n<p>then following stack trace is getting generated:</p>\\n\\n<pre><code>Exception in thread \"main\" com.rapidminer.repository.RepositoryException: Requested  repository LocalRepository does not exist.\\n    at     com.rapidminer.repository.RepositoryManager.getRepository(RepositoryManager.java:201)\\n    at  com.rapidminer.repository.RepositoryLocation.getRepository(RepositoryLocation.java:139)\\n    at  com.rapidminer.repository.RepositoryLocation.locateEntry(RepositoryLocation.java:162)\\n    at rajeev.rapidminer.main.Main.main(Main.java:29)\\n</code></pre>\\n\\n<p>Kindly tell me where to place the repository.</p>\\n\\n<p>Also kindly suggest any proper blog or tutorial which explains how to use Rapid Miner algorithms from java code (the stuff i searched on google does not mention all the details)</p>\\n\\n<p>Thanks</p>\\n-Error in accessing Rapid Miner API from java program-<machine-learning><data-mining><text-mining><rapidminer>',\n",
       " \"<p>Google <b>is</b> my best friend, but the information I'm getting is too scattered and very unclear. There isn't a concise tutorial describing my needs.</p>\\n\\n<p>I want to add actions to my RCP app's main toolbar, but I need every way of doing this explained thoroughly. But since this is StackOverflow, I will write down the questions off the top of my head, and hope the answers will do.</p>\\n\\n<ul>\\n<li><p>Which is the <b>new</b> and <b>old</b> way of adding actions? Via <code>ActionBarAdvisor</code> or via <code>plugin.xml</code>?</p></li>\\n<li><p>Can actions be added to the main toolbar <b>ONLY</b> using <code>plugin.xml</code>?</p></li>\\n<li><p>How many steps are there to add an action via extensions? (<code>handlers</code>, <code>commands</code>, <code>menuContribution</code>, etc.)</p></li>\\n<li><p>Which is the best parent for an action implementation? <code>org.eclipse.jface.action.Action</code>? <code>org.eclipse.ui.menus.WorkbenchWindowControlContribution</code>? Or even <code>org.eclipse.ui.commands.AbstractHandler</code>?</p></li>\\n<li><p>What about workbench-specific actions (<b>Save</b>, <b>Undo</b>, <b>Redo</b> etc.)? How are these added? </p></li>\\n<li><p>In Eclipse Kepler IDE, the coolbar looks beautiful. You can even move toolbars around. How's that implemented? Couldn't find out not even with Plug-in Spy.</p></li>\\n<li><p>Since I have several <code>Perspective</code>s, each will come with its own contribution on the main toolbar. Does that mean I'm obliged to use <code>plugin.xml</code> extensions everywhere?</p></li>\\n</ul>\\n\\n<p><hr>\\n<b>TL;DR</b>: Workbench coolbar actions including <b>Save</b> and actions from other <code>Perspective</code>s. How? (<code>actionSets</code> extensions == deprecated).</p>\\n-Eclipse Kepler RCP Main Toolbar Actions-<eclipse-rcp><action><toolbar><eclipse-kepler><workbench>\",\n",
       " '<p>I\\'m trying to use the call cublasIdamax() but I got a similar error like the title. So I write a simple code to verify the version of cublas, to avoid a version mistake in signature of function. But even this simple code result in a compilation error.</p>\\n\\n<p>Here\\'s my code:</p>\\n\\n<pre><code>__global__ void getVersion(cublasHandle_t handle, int *version){\\n   cublasGetVersion(handle,version);\\n}\\n\\nint main( int argc, const char* argv[] )\\n{\\n  int  *d_version;\\n  int  *h_version; \\n  cublasHandle_t handle;\\n  dim3 dimBlock( 2, 2 );\\n  dim3 dimGrid( 1, 1 );\\n\\n  cublasCreate(&amp;handle);\\n\\n  h_version = (int *)malloc(sizeof(int*));\\n  cudaMalloc((void**)&amp;d_version, sizeof(int*));\\n\\n  getVersion&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(handle, d_version);\\n\\n  cudaMemcpy(h_version,d_version,sizeof(int),cudaMemcpyDeviceToHost);//gpu-&gt;cpu\\n  cout &lt;&lt; *h_version &lt;&lt; endl;\\n}\\n</code></pre>\\n\\n<p>I have the following error on line 3: External calls are not supported (found non-inlined call to cublasGetVersion_v2)</p>\\n\\n<p>What I\\'m doing wrong?</p>\\n\\n<p>PS.: I looked this topic \\n<a href=\"https://devtalk.nvidia.com/default/topic/500814/external-calls-are-not-supported-found-non-inlined-call-to-meminit-/\" rel=\"nofollow\">https://devtalk.nvidia.com/default/topic/500814/external-calls-are-not-supported-found-non-inlined-call-to-meminit-/</a>\\nbut i\\'m still with the problem.</p>\\n-Error: External calls are not supported (found non-inlined call to cublasGetVersion_v2)-<cuda><cublas><kepler>',\n",
       " '<p>I have been using teradata for a while. Why would Oracle \"migrate\" into Teradata?\\nWhat are the advantages of <a href=\"http://downloads.teradata.com/download/extensibility/teradata-udfs-for-popular-oracle-functions\" rel=\"nofollow\">Oracle UDFs</a></p>\\n\\n<p>Supposing that you are not creating functions and stored procedures yourself, what is so cool about Oracles UDFs? It appears that almost all functions could be replaced using longer coding lines in Teradata. Is there something that is not supported natively by Teradata (except for regular expressions) that make Oracle UDFs so neccessary?</p>\\n-What are the advantages of Oracle UDFs for Teradata?-<oracle><user-defined-functions><teradata>',\n",
       " '<p>I have a CUDA program in which threads of a block read elements of a long array in several iterations and memory accesses are almost fully coalesced. When I profile, <em>Global Load Efficiency</em> is over 100% (between 119% and 187% depending on the input). Description for <em>Global Load Efficiency</em> is \"<em>Ratio of global memory load throughput to required global memory load throughput.</em>\" Does it mean that I\\'m hitting L2 cache a lot and my memory accesses are benefiting from it?</p>\\n\\n<p>My GPU is GeForce GTX 780 (Kepler architecture).</p>\\n-\"Global Load Efficiency\" over 100%-<cuda><gpu><gpgpu><nvidia><kepler>',\n",
       " '<p>How can I rename a table in monetdb?\\nThe typical SQL statement <code>ALTER TABLE name RENAME TO new_name</code> is not supported.</p>\\n-How can I rename a table in MonetDB?-<monetdb><table-rename>',\n",
       " \"<p>I've code that has to run on JDK 1.5\\nPlus i need some plugins which has not stopped working for Juno...</p>\\n\\n<p>How do i hack around to make my Kepler run on 1.5 JDK and not JDK 1.6. This way i'll have my required plugins and my code.</p>\\n\\n<p>It might be not possible but just wanted to check with community did anyone found similar scenario and got hack around it.</p>\\n\\n<p>Thanks in advance. </p>\\n-Running JDK 1.5 on Eclipse Kepler-<java><eclipse><eclipse-kepler>\",\n",
       " '<p>Using a browser REST client to POST to the activity stream at e.g.</p>\\n\\n<pre><code>https://connectionsww.demos.ibm.com/connections/opensocial/basic/rest/activitystreams/@me/@all\\n</code></pre>\\n\\n<p>...with the settings prescribed in <a href=\"http://www-10.lotus.com/ldd/appdevwiki.nsf/xpDocViewer.xsp?lookupName=IBM+Connections+4.5+API+Documentation#action=openDocument&amp;res_title=POSTing_new_events_ic45&amp;content=pdcontent\" rel=\"nofollow\">IBM Connections OpenSocial API > POSTing new events</a></p>\\n\\n<p>...results in the following response:</p>\\n\\n<pre><code>&lt;error xmlns=\"http://www.ibm.com/xmlns/prod/sn\"&gt;\\n    &lt;code&gt;403&lt;/code&gt;\\n    &lt;message&gt;You are not authorized to perform the requested action.&lt;/message&gt;\\n    &lt;trace&gt;&lt;/trace&gt;\\n&lt;/error&gt;\\n</code></pre>\\n\\n<p>What am I missing? </p>\\n\\n<p>This same approach works nicely on IBM Connections 4.0. </p>\\n\\n<p>Which setting needs \\'switching on\\'?</p>\\n-Error posting to IBM Connections 4.5 activity stream-<ibm-connections><ibm-sbt>',\n",
       " '<p>After updating my Eclipse Kepler to SR1, I can\\'t run my junit tests anymore from Eclipse.</p>\\n\\n<p>After starting up Eclipse, I see this error message in the logs:</p>\\n\\n<pre><code>Variable references empty selection: ${project_loc}\\n</code></pre>\\n\\n<p>When selecting a test case, and Run As, Junit Test, I\\'m getting this error:</p>\\n\\n<pre><code>An internal error occurred during: \"Launching DrillUtilTest\".\\njava.lang.NullPointerException\\n    at org.eclipse.wst.common.componentcore.internal.resources.VirtualResource.getProjectRelativePaths(VirtualResource.java:119)\\n    at org.eclipse.wst.common.componentcore.internal.resources.VirtualFile.getUnderlyingFiles(VirtualFile.java:104)\\n    ...\\n</code></pre>\\n\\n<p>So maybe the ${project_loc} variable is empty somehow, but it is not used in any of my Junit run configurations. I\\'ve tried starting Junit in various ways, but they all give the same result. Before upgrading to SR1, this used to work fine.</p>\\n\\n<p>Any help is greatly appreciated!</p>\\n-Junit error when run from Eclipse after upgrading to Kepler SR1-<java><eclipse><junit><eclipse-kepler>',\n",
       " '<p>The Google plugin for Eclipse adds a button, GDT pulldown, to the Eclipse toolbar. I do not know what I did, but this button appears <strong>seven</strong> times, and it seems to be a bug! </p>\\n\\n<p>Moreover, could you tell me, what should I do to not always have to sign in when I need to deploy a new GAE version (it always ask me for my Google mail and password) ? </p>\\n-Eclipse Kepler - Google Plugin Eclipse BUG-<gwt><eclipse-plugin><eclipse-kepler>',\n",
       " '<p>I am developing a simple Eclipse plug-in. I added a popup as an extension and assigned an Action class to it. Everything works fine except disabling the action at the first time, when popup is opened. </p>\\n\\n<p>I check whether the action can be performed in the selectionChanged method. But it cannot be called before MyAction object is constructed. It is performed only after clicking on menu\\'s button (it should be disabled, if prerequisites are not fullfilled).</p>\\n\\n<p>How to handle such problem? How can MyAction be constructed before opening popup?</p>\\n\\n<p>I attach a sample of essential code in my project (plugin.xml, MyAction.java):</p>\\n\\n<pre class=\"lang-xml prettyprint-override\"><code>&lt;plugin&gt;\\n   &lt;extension\\n         point=\"org.eclipse.ui.popupMenus\"&gt;\\n      &lt;objectContribution\\n            id=\"action.contribution1\"\\n            objectClass=\"myobject\"&gt;\\n         &lt;menu\\n            id=\"action.menu1\"\\n            label=\"Menu\"\\n            path=\"additions\"&gt;\\n         &lt;/menu&gt;\\n         &lt;actionm\\n               class=\"action.popup.actions.MyAction\"\\n               enablesFor=\"1\"\\n               id=\"action.newAction\"\\n               label=\"Play\"\\n               menubarPath=\"action.menu1\"&gt;\\n         &lt;/action&gt;\\n      &lt;/objectContribution&gt;\\n   &lt;/extension&gt;\\n&lt;/plugin&gt;\\n</code></pre>\\n\\n<pre class=\"lang-java prettyprint-override\"><code>public class MyAction implements IObjectActionDelegate {\\n    public MyAction() {\\n        super();\\n    }\\n\\n    public void setActivePart(IAction action, IWorkbenchPart targetPart) {}\\n\\n    public void run(IAction action) {\\n        //some logic\\n    }\\n\\n    @Override\\n    public void selectionChanged(IAction action, ISelection selection) {\\n        boolean enabled = false;\\n\\n        //some logic concerning enabled variable\\n\\n        action.setEnabled(enabled);\\n    }\\n\\n}\\n</code></pre>\\n-Eclipse plug-in popup\\'s action initial availability-<java><eclipse><popup><eclipse-kepler>',\n",
       " '<p>I am trying to connect to a table that is not in the sys schema. The code below works if sys.tablea exists. </p>\\n\\n<pre><code>conn &lt;- dbConnect(dbDriver(\"MonetDB\"), \"monetdb://localhost/demo\")\\nframe &lt;- monet.frame(conn,\"tablea\")\\n</code></pre>\\n\\n<p>If I define tablea in a different schema, e.g. xyz.tablea, then I get the error message</p>\\n\\n<pre><code>Server says \\'SELECT: no such table \\'tablea\\'\\' [#NA]\\n</code></pre>\\n\\n<p>The account used to connect has rights to the table.</p>\\n\\n<p>In a related question, is it possible to use camel-case from MonetDB.R? When I change the table name to TableA, the server again responds with </p>\\n\\n<pre><code>Server says \\'SELECT: no such table \\'tablea\\'\\' [#NA]\\n</code></pre>\\n\\n<p>where the table name is all lower-case.</p>\\n-Having trouble specifying a schema name from MonetDB.R-<r><monetdb>',\n",
       " '<p>I have been trying to connect to Teradata  </p>\\n\\n<pre><code>Class.forName(\"com.teradata.jdbc.TeraDriver\");\\n        String connectionString = \"jdbc:teradata://xxx.xxxxxx.com/database=xxxxxx,  tmode=ANSI,  charset=UTF8\";\\n        String user = \"Rocket512\";\\n        String password = \"aui8mn5\";\\n        Connection conn = DriverManager.getConnection(connectionString, user, password);\\n</code></pre>\\n\\n<p>Got the following </p>\\n\\n<pre><code>  Exception in thread \"main\" com.teradata.jdbc.jdbc_4.util.JDBCException: [Teradata Database] \\n[TeraJDBC 14.10.00.17] [Error 8017] [SQLState 28000] The UserId, Password or Account is invalid.\\n        at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDatabaseSQLException(ErrorFactory.java:300)\\n        at com.teradata.jdbc.jdbc.GenericLogonController.run(GenericLogonController.java:666)\\n        at com.teradata.jdbc.jdbc_4.TDSession.&lt;init&gt;(TDSession.java:216)\\n</code></pre>\\n\\n<p><strong>I know that the host is specified correctly since i did not get <code>UnknownHost Exception.</code>\\nAlso I have double checked my userid and password are correct.</strong></p>\\n\\n<hr>\\n\\n<p>I ran query suggested by @beni23 (thank you)</p>\\n\\n<pre><code>select * \\nfrom dbc.logonoff \\nwhere logdate &gt;= date \\'2013-10-31\\'\\n</code></pre>\\n\\n<p>Here is the result that I got</p>\\n\\n<p><img src=\"https://i.stack.imgur.com/2kChV.png\" alt=\"enter image description here\"></p>\\n\\n<p>What is <code>Bad Password</code>? I used SQL Assistant with this very password and it works great. Why cannot i connect with Java?</p>\\n-Connecting Java and Teradata: The UserId, Password or Account is invalid-<java><teradata>',\n",
       " \"<p>Apparently after aplying some latest updates of Windows 7, Eclipse stopped working properly. It freezes very often for 30 second to 2 minutes. In fact the system also freezes, for example the music that is played in WMP stops, loading web pages stops, etc.</p>\\n\\n<p>I tried it first with the newest Eclipse Kepler and thought it was only the new version but then fell back to Juno and the same thing happens.</p>\\n\\n<p>My JRE is Java <code>7u45</code> but I also tested it with <code>7u7</code> and no changes.</p>\\n\\n<p>It seems that in the time of freeze some I/O operations are taking place as the hard disk light flickers constantly.</p>\\n\\n<p>It totally prevents from doing anything both in Eclipse as well as in the system itself.</p>\\n\\n<p>Has anybody experienced such a problem lately? I have googled it up and all I found were some bug reports for Eclipse and 64-bit version of Windows.</p>\\n\\n<p>EDIT: I tested it on a different machine with almost the same configuration and it works impeccably. Any idea how I can check on my machine, what causes the problem?</p>\\n\\n<p><b>Still</b> it happens only when I run Eclipse I have no other problem with any other application.</p>\\n\\n<p>EDIT: I ran the Eclipse with <code>-consolelog</code> and here is the stack trace of exception thrown during the freeze.</p>\\n\\n<p><pre><code>\\n2013-11-15 12:31:06,480 [main] INFO  c.n.h.c.p.n.NettyAsyncHttpProvider - Number of application's worked threads is 8\\n2013-11-15 12:32:56,300 [Recommenders-Dependency-Info-Service-0] ERROR o.e.r.i.r.p.ClasspathEntryInfoProvider - Extracing jar information failed with exception.</p>\\n\\n<p>java.lang.RuntimeException: java.io.IOException: Die Anforderung konnte wegen eines E/A-Gerõtefehlers nicht ausgef³hrt werden\\n        at org.eclipse.recommenders.utils.Throws.throwUnhandledException(Throws.java:47) ~[na:na]\\n        at org.eclipse.recommenders.utils.Fingerprints.internal_sha1v2(Fingerprints.java:59) ~[na:na]\\n        at org.eclipse.recommenders.utils.Fingerprints.sha1(Fingerprints.java:64) ~[na:na]\\n        at org.eclipse.recommenders.utils.archive.ArchiveDetailsExtractor.createFingerprint(ArchiveDetailsExtractor.java:56) ~[na:na]\\n        at org.eclipse.recommenders.internal.rcp.providers.ClasspathEntryInfoProvider$3.run(ClasspathEntryInfoProvider.java:247) ~[na:na]\\n        at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) [na:1.7.0_45]\\n        at java.util.concurrent.FutureTask.run(Unknown Source) [na:1.7.0_45]\\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [na:1.7.0_45]\\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [na:1.7.0_45]\\n        at java.lang.Thread.run(Unknown Source) [na:1.7.0_45]\\nCaused by: java.io.IOException: Die Anforderung konnte wegen eines E/A-Gerõtefehlers nicht ausgef³hrt werden\\n        at java.io.FileInputStream.readBytes(Native Method) ~[na:1.7.0_45]\\n        at java.io.FileInputStream.read(Unknown Source) ~[na:1.7.0_45]\\n        at com.google.common.io.ByteStreams.readBytes(ByteStreams.java:708) ~[na:na]\\n        at com.google.common.io.ByteStreams.getDigest(ByteStreams.java:760) ~[na:na]\\n        at com.google.common.io.Files.getDigest(Files.java:642) ~[na:na]\\n        at org.eclipse.recommenders.utils.Fingerprints.internal_sha1v2(Fingerprints.java:57) ~[na:na]\\n        ... 8 common frames omitted</p>\\n\\n<p>!ENTRY org.eclipse.m2e.logback.appender 4 0 2013-11-15 12:32:56.310\\n!MESSAGE Extracing jar information failed with exception.\\n2013-11-15 12:34:44,130 [Recommenders-Dependency-Info-Service-0] ERROR o.e.r.i.r.p.ClasspathEntryInfoProvider - Extracing jar information failed with exception.\\n</pre></code></p>\\n-Eclipse freezes in Windows 7-<windows-7><java-7><eclipse-juno><eclipse-kepler>\",\n",
       " '<p>I have a sharepoint list which i have linked to in MS Access.\\nThe information in this table needs to be compared to information in our datawarehouse based on keys both sets of data have.\\nI want to be able to create a query which will upload the ishare data into our datawarehouse under my login run the comparison and then export the details to Excel somewhere. MS Access seems to be the way to go here.</p>\\n\\n<p>I have managed to link the ishare list (with difficulties due to the attachment fields)and then create a local table based on this.\\nI have managed to create the temp table in my Volatile space.\\nHow do i append the newly created table that i created from the list into my temporary space.</p>\\n\\n<p>I am using Access 2010 and sharepoint 2007</p>\\n\\n<p>Thank you for your time</p>\\n-Create a Volatile table in teradata-<sharepoint><ms-access-2010><volatile><teradata>',\n",
       " '<p>I am a total beginner i am using eclipse kepler to create my web application: an eduction portal\\ni did already set the glassfish4 server(its working) and the properties of the project\\ni am using primefaces along with jsf 2.2 \\ni created the first page and i wanted to to try to run it on the server but nothing is showed its a 404 error The requested resource is not available.\\nand i already added the primefaces jar to the classpath \\nthis is the code of the page index.html</p>\\n\\n<pre><code>    &lt;!DOCTYPE html&gt;\\n    &lt;html\\n    xmlns=\"http://www.w3.org/1999/xhtml\"\\n    xmlns:h=\"http://java.sun.com/jsf/html\"\\n    xmlns:f=\"http://java.sun.com/jsf/core\"\\n    xmlns:ui=\"http://java.sun.com/jsf/facelets\"\\n    xmlns:p=\"http://primefaces.org/ui\"\\n    &gt;\\n    &lt;head&gt;\\n    &lt;/head&gt;\\n    &lt;f:view contentType=\"text/html\"&gt;\\n    &lt;h:head /&gt;\\n    &lt;h:body&gt;\\n    &lt;h:form&gt;\\n    &lt;p:spinner /&gt;\\n    &lt;/h:form&gt;\\n    &lt;/h:body&gt;\\n    &lt;/f:view&gt;\\n    &lt;/html&gt;\\n</code></pre>\\n\\n<p>and this is the code in my web.xml</p>\\n\\n<pre><code>    &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n    &lt;web-app xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \\n    xmlns=\"http://xmlns.jcp.org/xml/ns/javaee\" \\n    xsi:schemaLocation=\"http://xmlns.jcp.org/xml/ns/javaee     http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd\" \\n    id=\"WebApp_ID\" version=\"3.1\"\\n    xmlns:p=\"http://primefaces.org/ui\"&gt;\\n    &lt;display-name&gt;Portail&lt;/display-name&gt;\\n    &lt;welcome-file-list&gt;\\n    &lt;welcome-file&gt;index.html&lt;/welcome-file&gt;\\n    &lt;/welcome-file-list&gt;\\n    &lt;servlet&gt;\\n    &lt;servlet-name&gt;Faces Servlet&lt;/servlet-name&gt;\\n    &lt;servlet-class&gt;javax.faces.webapp.FacesServlet&lt;/servlet-class&gt;\\n    &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;\\n    &lt;/servlet&gt;\\n    &lt;servlet-mapping&gt;\\n    &lt;servlet-name&gt;Faces Servlet&lt;/servlet-name&gt;\\n    &lt;url-pattern&gt;*.html&lt;/url-pattern&gt;\\n    &lt;/servlet-mapping&gt;\\n    &lt;/web-app&gt;\\n</code></pre>\\n\\n<p>i followed some steps from this article to create it <a href=\"http://computingat40s.wordpress.com/creating-a-simple-jsf-web-application-from-the-ground/\" rel=\"nofollow\">http://computingat40s.wordpress.com/creating-a-simple-jsf-web-application-from-the-ground/</a>\\nand my lib directory still empty i didn\\'t add anything to it i didn\\'t even started creating the entities</p>\\n-Required files and configuration to create a jsf2.2 web application-<java><primefaces><jsf-2.2><eclipse-kepler>',\n",
       " '<p>I have an excel sheet where every row denotes and issue and its description. One column corresponding to each row denotes details about that issue which is what I want to mine. I want to find out occurrences of each token in each cell and then categorize each row correspondingly(e.g. if a specific token appears more than x times, I will tag that row with that token). When I do it using Rapidminer, it takes all the rows together and tells how many occurrences of each token across all rows rather than taking each row at a time. How can I let Rapidminer take each row at a time and determine for each row the frequency of tokens and apply n grams etc...</p>\\n-Text Mining Using Rapidminer-<nlp><text-mining><rapidminer>',\n",
       " '<p>Is it possible to add the Team options for SVN repository access to a custom context menu on a custom view? If so, how?</p>\\n\\n<p>I have tried </p>\\n\\n<pre><code>&lt;menuContribution locationURI=\"popup:my.view.ID#PopupMenu\"&gt;\\n    &lt;menu id=\"team.main\" label=\"Team\"&gt;\\n        &lt;separator name=\"group1\" visible=\"false\"/&gt;\\n        &lt;separator name=\"group2\" visible=\"false\"/&gt;\\n        &lt;separator name=\"group3\" visible=\"false\"/&gt;\\n    &lt;/menu&gt;\\n</code></pre>\\n\\n<p>in my plugin.xml, but only an EGit menu popped up once with grayed items when the mouse hovered over them.</p>\\n-Add Team options for SVN repository access to custom context menu?-<svn><eclipse-plugin><contextmenu><eclipse-kepler>',\n",
       " '<p>I use Eclipse Kepler on Ubuntu, which I downloaded and installed separately in /usr/local/eclipse</p>\\n\\n<p>Last night I upgraded from Ubuntu 13.04 to 13.10, and today all menus (File, ..., Help) are empty. I don\\'t know whether this is a coincidence or has to do with the upgrade. It should not, because I hadn\\'t used the version from Ubuntu\\'s repositories.</p>\\n\\n<p>$ java -version\\njava version \"1.7.0_25\"\\nOpenJDK Runtime Environment (IcedTea 2.3.12) (7u25-2.3.12-4ubuntu3)\\nOpenJDK 64-Bit Server VM (build 23.7-b01, mixed mode)</p>\\n\\n<p>I already tried using a different (empty) workspace and deleting the $HOME/.eclipse directory, but this doesn\\'t help.</p>\\n\\n<p>Is there any hope I can get the menus back without purging and re-installing the whole thing?</p>\\n-Eclipse: all main menus empty-<eclipse><menu><eclipse-kepler>',\n",
       " \"<p>I recently downloaded eclipse Indigo after abandoning Kepler because of all the bugs. Almost everything is fine except the loading screen for Indigo is still Kepler. The version of Eclipse IS in fact indigo (I checked About Eclipse for that), but I'm concerned if Indigo might be mixed with Kepler for some reason. After noticing some bugs on Indigo that are Kepler-specific, I re-extracted the files of Indigo, and all those bugs went away, except for the Kepler loading screen. Just concerned if Kepler might have passed on more than its loading screen to Indigo. Thanks.</p>\\n-Eclipse Indigo shows Kepler loading screen-<java><eclipse><eclipse-kepler>\",\n",
       " '<p>Further to: <a href=\"https://stackoverflow.com/questions/17724947/escape-hides-complete-ui/19755648#19755648\">Escape hides complete UI</a></p>\\n\\n<p>I have the exact same problem as described here... driving me nuts. Fresh installation on OSX Mavericks.</p>\\n\\n<p>I\\'ve searched on the Eclipse bugs page to no avail.</p>\\n\\n<p>Anybody?</p>\\n-Eclipse escape button binding-<eclipse><eclipse-kepler>',\n",
       " \"<p>I have been using eclipse for about 2 years now and I had been using what ever the newest version was but about 6 moths ago I found out that I could use eclipse indigo and ever since I have been using it. I am wanting to switch to the new version of eclipse Kepler (4.3.1) and I'm wanting to know if i do switch will it mess up my work space from using indigo.</p>\\n\\n<p>If i didn't explain some thing very well or missed spelled some thing please let me know.</p>\\n-Switching from Eclipse indigo to Eclipse Kepler-<eclipse><eclipse-indigo><eclipse-kepler>\",\n",
       " \"<p>I was wondering if you think it's reasonable, using monetdb (or another columnar database) to put all of your data in one big, flat table rather than breaking it up into several related tables.</p>\\n\\n<p>For example, a database of used cars, flat, might look like:</p>\\n\\n<pre><code>Make    Model   Year   Color    Mileage\\nChevy   Malibu  2009   orange   102100   \\nChevy   Malibu  2009   orange   98112\\nChevy   Malibu  2008   orange   210232\\nChevy   Malibu  2009   pink     150100\\n</code></pre>\\n\\n<p>Noticing the redundancy in Make-Model-Year-Color, in a SQL database or excel spreadsheet or whatever, you might have two tables like:</p>\\n\\n<pre><code>mId   Make   Model   Year  Color\\n1     Chevy  Malibu  2009  orange\\n2     Chevy  Malibu  2008  orange\\n3     Chevy  Malibu  2009  pink\\n\\nmId   Mileage\\n1     102100   \\n1     98112\\n2     210232\\n3     150100\\n</code></pre>\\n\\n<p>This helps with the redundancy at the expense of more complex queries and having to think about how to decompose (break up) the tables.</p>\\n\\n<p>I was reading about columnar databases and monetdb in particular. It seems like, since monetdb compresses columns individually that the redundancy doesn't matter and you could just use the flat table expecting same-or-better performance (query time, disk usage) as a well-decomposed set of relational tables would provide. This saves design effort, but even better lets you completely automate schema design -- by avoiding it.</p>\\n\\n<p>What do you think? Is there some hidden cost that I'm not seeing?</p>\\n-Use a columnar database like MonetDB to avoid dimensional modeling?-<sql><database><cassandra><database-schema><monetdb>\",\n",
       " '<p>Suppose i have list of fields i.e, {field1,field2,field3,field4}\\nI performed some operation on field2 say i want to add increment each tuple values by some value say 5,</p>\\n\\n<pre><code>performed this operation in a function which gave me modified field with \"M_field2\" as out field name now i want to write complete tuple in a file but in place of field2 i want \"M_field2\". How i will achieve this.\\n</code></pre>\\n-how to get complete list of original fields along with new Fields which has been modified in trident?-<apache-storm><trident>',\n",
       " '<p>Thank you so much to the answer from jacouh (<a href=\"https://stackoverflow.com/questions/19919586/vba-copy-paste-3000-rows\">VBA Copy &amp; Paste 3000 rows</a>) to my initial problem. With the help of the forum, I now have the below code, which inserts 2999 rows from an Excel file. Each row is <code>INSERT (X, Y, Z, ...) VALUES (X1, Y1, Z1,...) into DBNAME.TABLE</code>. Whilst it works, it is painfully slow. As I understand, I could increase the speed of the inserts by increasing the size of the buffers, as described in <a href=\"http://developer.teradata.com/doc/connectivity/tdnetdp/13.11/webhelp/Teradata.Client.Provider~Teradata.Client.Provider.TdConnectionStringBuilder~ResponseBufferSize.html\" rel=\"nofollow noreferrer\">http://developer.teradata.com/doc/connectivity/tdnetdp/13.11/webhelp/Teradata.Client.Provider~Teradata.Client.Provider.TdConnectionStringBuilder~ResponseBufferSize.html</a>. I\\'ve attempted to incorporate it and failed. Could someone recommend a possible integration to increase the response buffer size. Thank you so much. I\\'ve explored different forums previously, and this is definitely the best one. Definitely. Any other ideas, better than the one I\\'m currently using (inserting approx 3000 rows into about 15 different tables), then please let me know!)</p>\\n\\n<pre class=\"lang-vb prettyprint-override\"><code>Sub Insert_to_TD()\\n\\nDim cn As ADODB.Connection\\nSet cn = New ADODB.Connection\\nDim rs As ADODB.Recordset\\nSet rs = New ADODB.Recordset\\nDim cmdsqldata As ADODB.Command\\nSet cmdsqldata = New ADODB.Command\\n\\nDim i, strSQL\\n\\ncn.Open \"DSN=NNNNNN; Username=XXXXX; Password=YYYYYYY;\"\\n\\n\\nSet cmdsqldata.ActiveConnection = cn\\n\\ncmdsqldata.CommandType = adCmdText\\ncmdsqldata.CommandTimeout = 0\\n\\nFor i = 1 To 2999\\nstrSQL = ActiveSheet.Cells(i, 1).Value\\ncmdsqldata.CommandText = strSQL\\nSet rs = cmdsqldata.Execute()\\nNext\\n\\n\\nEnd Sub\\n</code></pre>\\n-Quicken upload of 3000 inserts into Teradata via VBA-<sql><excel><vba><response><teradata>',\n",
       " '<p>I am trying to evaluate the performance of MonetDB for an analytical workload that contains a large amount of floating point calculations which are used in an aggregation.</p>\\n\\n<p>I am trying to implement a C based UDF in MonetDB to achieve this and am running into an error.  I am unsure of how to implement the function correctly based on my required signature which is </p>\\n\\n<pre><code>double f(double,double);\\n</code></pre>\\n\\n<p>Firstly, I am using MonetDB-11.15.17 built from source on Ubuntu 13.04.</p>\\n\\n<p>I have added to the following files in the ./sql/backends/monet5/UDF directory as shown:</p>\\n\\n<p>udf.c:</p>\\n\\n<pre><code>str UDFtest(flt *ret,flt *_p1,flt *_p2)\\n{\\n    *ret = *_p1+*_p2;\\n    return MAL_SUCCEED;\\n}\\n</code></pre>\\n\\n<p>udf.h:</p>\\n\\n<pre><code>udf_export str UDFtest(flt *,flt*,flt*);\\n</code></pre>\\n\\n<p>udf.mal:</p>\\n\\n<pre><code>module udf;\\ncommand calc_test(one:flt,two:flt):flt\\naddress UDFtest\\ncomment \"udf floating point test\";\\n</code></pre>\\n\\n<p>80_udf.sql:</p>\\n\\n<pre><code>create function calc_test(one double,two double)\\nreturns double external name udf.calc_test;\\n</code></pre>\\n\\n<p>I then ran , bootstrap; make; sudo make install; and typed the following at the mclient prompt:</p>\\n\\n<pre><code>declare f1 float;\\ndeclare f2 float;\\nset f1=0.1;\\nset f2=0.2;\\nselect calc_test(f1,f2);\\n</code></pre>\\n\\n<p>This results in the following error:</p>\\n\\n<pre><code>TypeException:user.s1_1[6]:\\'udf.calc_test\\' undefined in: _9:any := udf.calc_test(_5:dbl, _8:dbl)\\nprogram contains errors\\n</code></pre>\\n\\n<p>I\\'ve tried to piece together what I can from the documentation and source but am now stuck.\\nWhere have I gone wrong in this process?</p>\\n-A MonetDB User Defined Function with two double parameters-<user-defined-functions><monetdb>',\n",
       " '<p>Is there a way to shorten my number of clicks to get to my source code? I know that search can do this but is there a way to create short cut?</p>\\n-Programming java in Eclipse Kepler. I need a quick short cut to my java source code-<java><eclipse><eclipse-kepler>',\n",
       " \"<p>Have the MonetDb's developers tested any other compression algorithm on it before? </p>\\n\\n<p>Perhaps they have tested other compression algorithms ,but it's really had a negative performance impact.</p>\\n\\n<p>So why haven't they improved this database's compression performance?</p>\\n\\n<p>I am a student from China. MonetDb is really interesting me and I want to try to improve its compression performance.</p>\\n\\n<p>So, I should make sure that any body have done this before.</p>\\n\\n<p>It would be my grateful if you could answer my question.</p>\\n\\n<p>That is because i really need this.</p>\\n\\n<p>Thank you So much.</p>\\n-Have the monetdb's developers tested any other compression algorithm on it?-<compression><monetdb>\",\n",
       " \"<p>Regarding eclipse Kepler SR1</p>\\n\\n<p>Hi,</p>\\n\\n<p>I remember that a unused private methods were getting a warning from the compiler.</p>\\n\\n<p>I don't know if it's changed switching to Kepler (or a previous release) or if it is project settings, because I'm working on a new project where the eclipse settings are shared. </p>\\n\\n<p>I even couldn't find the setting in the eclipse compiler settings.</p>\\n-eclipse: no warning for unused private methods-<java><eclipse><compiler-warnings><eclipse-kepler>\",\n",
       " '<p>I have never used the Teradata JDBC driver before, so I may be running into something very obvious...</p>\\n\\n<p>Basically, I am trying to create a volatile table from which I can query.</p>\\n\\n<p><strong>Questions</strong></p>\\n\\n<ol>\\n<li><p>Maybe I just can\\'t run <code>CREATE VOLATILE TABLE</code> statements like this...  What am I doing wrong?</p></li>\\n<li><p>Maybe the driver version is too old? (Currently 12.00.00.110)</p></li>\\n</ol>\\n\\n<p><strong>Java code</strong></p>\\n\\n<pre><code>PreparedStatement  pstmtCreateVT = dbConn.prepareStatement(util.getQuery(\"CREATE_VOLATILE_TABLE_ABC\"));\\n   pstmtCreateVT.setInt(1, 7);\\n   pstmtCreateVT.setInt(2, 11);\\n   pstmtCreateVT.executeQuery();\\n</code></pre>\\n\\n<p><strong>Query.properties</strong></p>\\n\\n<pre><code>CREATE_VOLATILE_TABLE_ABC = \\\\\\nCREATE VOLATILE TABLE ABC \\\\\\nAS ( SELECT DISTINCT t1.NAME \\\\\\n  , lookup.price \\\\\\nFROM SUPERMARKET.FRUITS t1 \\\\\\n  INNER JOIN SUPERMARKET.PRICING lookup \\\\\\n    ON \\n      lookup.price BETWEEN ? AND ? \\\\\\n) \\\\\\nWITH DATA NO PRIMARY INDEX \\\\\\nON COMMIT PRESERVE ROWS ;\\n</code></pre>\\n\\n<p><strong>Error</strong></p>\\n\\n<pre><code>2013-11-15 09:55:19,220 DEBUG [CreatePoolingConnection.createConnection:102] url: jdbc:teradata://&lt;&lt;some_url_here&gt;&gt;/DATABASE=SUPERMARKET,ENCRYPTDATA=ON,TMODE=ANSI\\n2013-11-15 09:55:19,673 ERROR [BOMPartsApplicability.fetch:245] ERROR:\\ncom.teradata.jdbc.jdbc_4.util.JDBCException: [Teradata Database] [TeraJDBC 12.00.00.110] [Error 3585] [SQLState HY000] USING modifier NOT allowed with DDL.\\n   at com.teradata.jdbc.jdbc_4.util.ErrorFactory.makeDatabaseSQLException(ErrorFactory.java:277)\\n   at com.teradata.jdbc.jdbc_4.statemachine.ReceiveInitSubState.action(ReceiveInitSubState.java:102)\\n   at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.subStateMachine(StatementReceiveState.java:285)\\n   at com.teradata.jdbc.jdbc_4.statemachine.StatementReceiveState.action(StatementReceiveState.java:176)\\n   at com.teradata.jdbc.jdbc_4.statemachine.StatementController.runBody(StatementController.java:108)\\n   at com.teradata.jdbc.jdbc_4.statemachine.StatementController.run(StatementController.java:99)\\n   at com.teradata.jdbc.jdbc_4.Statement.executeStatement(Statement.java:309)\\n   at com.teradata.jdbc.jdbc_4.Statement.prepareRequest(Statement.java:467)\\n   at com.teradata.jdbc.jdbc_4.PreparedStatement.&lt;init&gt;(PreparedStatement.java:53)\\n   at com.teradata.jdbc.jdbc_4.TDSession.createPreparedStatement(TDSession.java:506)\\n   at com.teradata.jdbc.jdbc_3.ifjdbc_4.TeraLocalPreparedStatement.&lt;init&gt;(TeraLocalPreparedStatement.java:84)\\n   at com.teradata.jdbc.jdbc_3.ifjdbc_4.TeraLocalConnection.prepareStatement(TeraLocalConnection.java:328)\\n   at com.teradata.jdbc.jdbc_3.ifjdbc_4.TeraLocalConnection.prepareStatement(TeraLocalConnection.java:149)\\n   at GroceryStore.SetupFruits.fetch(BOMPartsApplicability.java:147)\\n   at GroceryStore.SetupFruits.main(BOMPartsApplicability.java:359)\\n</code></pre>\\n-Teradata JDBC Create Volatile Table error 3585-<java><sql><jdbc><teradata>',\n",
       " '<p>I need to create association rules using apriori algorithm in Rapidminer, but I can\\'t seem to make it work. I\\'m using the 5.3.1 weka extension.</p>\\n\\n<p>I\\'ve already created the association rules using built-in FP-Growth and Create Associations operators, and it worked as expected. This is how the process looks like:</p>\\n\\n<p><img src=\"https://i.stack.imgur.com/9vTuI.jpg\" alt=\"FP-Growth\"></p>\\n\\n<p>Because all my attributes are already of binomial type I could use the FP-Growth directly. But if i use the same approach for apriori (confidence=0.1, support=0.1):</p>\\n\\n<p><img src=\"https://i.stack.imgur.com/3Gh2L.jpg\" alt=\"enter image description here\"></p>\\n\\n<p>As a result I\\'m not getting what I was looking for:</p>\\n\\n<pre><code>Minimum support: 0.1 (26 instances)\\nMinimum metric &lt;confidence&gt;: 0.1\\nNumber of cycles performed: 18\\n</code></pre>\\n\\n<p>(...)</p>\\n\\n<pre><code>Best rules found:\\n  1. A=FALSE 53 ==&gt; E=FALSE 26    conf:(0.49)\\n  2. H=FALSE 74 ==&gt; E=FALSE 30    conf:(0.41)\\n  3. E=FALSE 75 ==&gt; H=FALSE 30    conf:(0.4)\\n  4. C=FALSE 68 ==&gt; E=FALSE 27    conf:(0.4)\\n  5. D=FALSE 67 ==&gt; H=FALSE 26    conf:(0.39)\\n  6. E=FALSE 75 ==&gt; C=FALSE 27    conf:(0.36)\\n  7. H=FALSE 74 ==&gt; D=FALSE 26    conf:(0.35)\\n  8. E=FALSE 75 ==&gt; A=FALSE 26    conf:(0.35)\\n</code></pre>\\n-W-apriori in Rapidminer-<rapidminer><apriori>',\n",
       " \"<p>Based on what I know, when threads of a warp access the same address in global memory, requests get serialized so it's better to use constant memory. Does serializing of simultaneous global memory accesses happen when GPU is equipped with L1 and L2 cache levels (in Fermi and Kepler architecture)? In other words, when threads of a warp access the same global memory address, do 31 threads of a warp benefit from cache existence because 1 thread has already requested that address? What happens when the access is a read and also when access is a write?</p>\\n-Does serializing of simultaneous global memory accesses to one address happen when there are L1 and L2 cache levels?-<cuda><gpgpu><nvidia><kepler>\",\n",
       " \"<p>I'm not able to find the zip file of Spring Framework. But on Stackoverflow I found that we can download the JARs using maven repo by configuring POM.xml. But by having the latest version of Eclipse IDE I was only able to get a folder named .m2 into that a repository folder and into that .cache and org, and so on. <strong>No POM.xml</strong>. So due to this I'm not able to create a Maven project or a Spring project. </p>\\n-How to create MVC project in Kepler IDE-<java><spring><maven><eclipse-kepler>\",\n",
       " '<p>I am writing a plug-in for Eclipse Kepler that is aimed to read some data from the debugger. Currently, the action set with the Resume, Step into, etc. buttons is enabled. If a user clicks, for example, the Resume Button, the thread that reads from the debugger will of course fail.</p>\\n\\n<p>Is there any way to disable/enable an action set from the Java code in Eclipse Kepler knowing the action set ID?</p>\\n-How to disable/enable an action set in Eclipse e4?-<java><eclipse><eclipse-kepler>',\n",
       " \"<p>the create table query is :</p>\\n\\n<blockquote>\\n  <p>create table catalog (id int, ra double, decl double , zone int);</p>\\n</blockquote>\\n\\n<p>the zone value is calculated from decl using formula below:</p>\\n\\n<blockquote>\\n  <p>CAST(FLOOR(decl) AS INTEGER),</p>\\n</blockquote>\\n\\n<p>i have insert all id,ra,decl values of the table, then I have to calculate zone values,</p>\\n\\n<p>how to calculate the table's zone value in a sql query?</p>\\n\\n<p>**in another table extractedcatalog, i want to calculate y from ra,decl in sql:</p>\\n\\n<blockquote>\\n  <p>update extractedcatalog set x= (cos(radians(decl))<em>cos(radians(ra)));\\n  but responsed: connection terminated!\\n  is there any problem with my sql?</em>*</p>\\n</blockquote>\\n\\n<p>Thanks very much!</p>\\n-how to calculate a column using another column in sql?-<sql><monetdb>\",\n",
       " '<p>I tried this</p>\\n\\n<pre><code>SELECT * \\nFROM\\n( SELECT *\\nFROM mytable;\\n);\\n</code></pre>\\n\\n<p>and this</p>\\n\\n<pre><code>SELECT * \\nFROM\\n( SELECT *\\nFROM mytable\\n);\\n</code></pre>\\n\\n<p>Why these simple queries do not execute in Teradata? </p>\\n-Why subquery does not work in teradata?-<sql><teradata>',\n",
       " '<p>In monetdb I created a table:</p>\\n\\n<pre><code>create table extractedcatalog(id int, ra double, decl double, x double, y double, z double);\\n</code></pre>\\n\\n<p><code>ra,decl</code> are all inserted into tables already, now I want to calculate <code>x,y,z</code> from <code>ra,decl</code> columns. In sql I executed like this: </p>\\n\\n<pre><code>update extractedcatalog set x = (cos(radians(decl))*cos(radians(ra)));\\n</code></pre>\\n\\n<p>but i got response: </p>\\n\\n<blockquote>\\n  <p>connection terminated</p>\\n</blockquote>\\n\\n<p>Is there any problem with my sql query? </p>\\n\\n<p>Thanks very much!</p>\\n-how to calculate one column from another columns?-<sql><database-connection><create-table><calculated-columns><monetdb>',\n",
       " '<p>Teradata Float datatype truncates decimal value to 2 decimal point.</p>\\n<pre><code>select cast(10.123456789 as float) deci_num\\n</code></pre>\\n<p>Answer:</p>\\n<blockquote>\\n<p>deci_num</p>\\n<p>10.12</p>\\n</blockquote>\\n<p>Could anyone please tell me how to avoid this <strong>without converting to Decimal or Double</strong> because i have a table with float column which is storing only truncated value ?</p>\\n<p>Its working in Oracle but not in Teradata. Why ?</p>\\n-Float getting truncated in teradata sql-<sql><teradata>',\n",
       " '<p>The below link has some xml code of a Decision tree model. Please let me know how it got converted to the java code, which is also there in the below link.</p>\\n\\n<p><a href=\"https://stackoverflow.com/questions/13928769/implementing-rapidminer-as-a-java-application\">implementing rapidminer as a java application</a></p>\\n\\n<p>thanks,\\nRavi Sankar</p>\\n-how to convert the rapidminer generated xml to it\\'s java code (in rapidminer)-<java><xml><rapidminer>',\n",
       " '<p>When using Teradata 14 over JDBC I get the following SQL error for this SQL query bindings pair</p>\\n\\n<p>query</p>\\n\\n<pre><code>\"select regexp_instr(\\'abc\\', \\'a\\' || ?) s\" \\n</code></pre>\\n\\n<p>bindings</p>\\n\\n<pre><code>\"bc\"\\n</code></pre>\\n\\n<p>error</p>\\n\\n<pre><code> com.teradata.jdbc.jdbc_4.util.JDBCException : [Teradata Database] [TeraJDBC 14.10.00.17] [Error 3536] [SQLState HY000] UPPERCASE or CASESPECIFIC specified for non-CHAR data.\\n</code></pre>\\n\\n<p>When I execute the query directly with inline literals it works correctly.</p>\\n\\n<p>Any ideas what does wrong here?</p>\\n-Teradata JDBC prepared statement error-<java><sql><jdbc><teradata>',\n",
       " \"<p>I have one attribute named ‘rental time’ contain 2008, 2009, 2010, 2011, 2012 years of data sets of this format like “Mon Jan 14 07:32:30 CET 2008”. I want to apply filter for exclude 2008 and 2009 data. How should I apply filter in rapid miner to exclude this data.</p>\\n\\n<p>I would be grateful to you.</p>\\n\\n<p>Kind regards,\\nSohaib</p>\\n-How to apply filter of 'date and time' attribute in rapid miner-<rapidminer>\",\n",
       " '<p>Currently I\\'m trying to build my maven project which requires plugins from latest equinox KeplerSR1 release. So I\\'m looking for pluginRepository for eclipse-kepler SR1 release but could not find it. </p>\\n\\n<p>I found this mirror url [1] but did\\'t know how to use it to build my maven module.</p>\\n\\n<p>Appreciate if someone can provide this.</p>\\n\\n<p>[1] <a href=\"http://mirrors.ibiblio.org/eclipse/equinox/drops/R-KeplerSR1-201309111000/\" rel=\"nofollow\">http://mirrors.ibiblio.org/eclipse/equinox/drops/R-KeplerSR1-201309111000/</a></p>\\n-maven plugin repository for eclipse equinox KeplerSR1-<eclipse><maven><maven-2><equinox><eclipse-kepler>',\n",
       " '<p>In CUDA devices, is coalescing in global memory writes as important as coalescing in global memory reads? If yes, how can it be explained? Also are there differences between early generations of CUDA devices and most recent ones regarding this issue?</p>\\n-Memory coalescing in global writes-<cuda><gpu><gpgpu><kepler>',\n",
       " \"<p>In my Eclipse plug-in I have a custom editor which has its own toolbar. If I detach such an editor I still can use this toolbar. But only if there is no other editor (without this toolbar) active on the workbench. In this case the toolbar of the detached window disappears while activating the workbench so I can't use it anymore for the detached window. Is this a bug or a feature?</p>\\n\\n<p>Is it somehow possible to attach the toolbar to the detached editor window to prevent such errors?</p>\\n-Eclipse Kepler: detached window and toolbar-<eclipse-plugin><eclipse-kepler>\",\n",
       " '<p>I am trying to use eclipse kepler for Java EE 7.I already installed JBoss Tools and added JBoss Wildfly successfully as a server. However my changes are not automatically deployed. Is there anyway the app can be deployed automatically just as when using glassfish?</p>\\n-Eclipse Kepler and JBoss Wildfly hot deployment-<eclipse><eclipse-kepler><jboss-tools><wildfly>',\n",
       " '<p>It seems everything is ok if </p>\\n\\n<pre><code>subset(mdf, id %in% c(\"A\",\"B\"))\\n</code></pre>\\n\\n<p>but error if </p>\\n\\n<pre><code>ids = c(\"A\",\"B\")\\nsubset(mdf,id %in% ids)\\n</code></pre>\\n\\n<p>The following is demo codes:</p>\\n\\n<pre><code>con1 = dbConnect(dbDriver(\"MonetDB\"),\"monetdb://go:50000/voc\")\\nd = data.frame(id=base::sample(c(\"A\",\"B\",\"C\",\"D\"),100,replace=T),v=sample(1:10,100,replace=T),stringsAsFactors=F)\\nhead(d)\\nstr(d)\\n\\ndbWriteTable(con1, \"test\", d)\\n\\nmdf &lt;- monet.frame(con1,\"test\")\\nsubset(mdf, id %in% c(\"A\",\"B\"))\\n\\nids = c(\"A\",\"B\")\\nsubset(mdf,id %in% ids)\\n</code></pre>\\n\\n<p>MonetDB.R_0.8.0 DBI_0.2-7   </p>\\n\\n<p>R version 3.0.2 (2013-09-25)\\nPlatform: x86_64-pc-linux-gnu (64-bit)</p>\\n\\n<p>The <code>subset(mdf, id %in% c(\"A\",\"B\"))</code> actually translate to sql as:</p>\\n\\n<pre><code>MonetDB-backed data.frame surrogate\\n2 columns, 44 rows\\nQuery: SELECT * FROM test WHERE ( (id IN (\\'A\\',\\'B\\')) ) \\nColumns: id (character), v (numeric)\\n</code></pre>\\n\\n<p>The error message for \\n    <code>IDS = c(\"A\",\"B\")\\n    subset(mdf,id %in% IDS)</code></p>\\n\\n<p>is something like:</p>\\n\\n<pre><code>Error in .local(conn, statement, ...) : \\n  Unable to execute statement \\'SELECT COUNT(*) FROM test WHERE ( (id IN \\'AB\\') ) \\'.\\nServer says \\'syntax error, unexpected STRING, expecting \\'(\\' in: \"select count(*) from test where ( (id in \\'AB\\'\"\\' [#42000].\\n</code></pre>\\n\\n<p>I guess it is a <a href=\"http://monetr.r-forge.r-project.org/\" rel=\"nofollow\">MonetDB.R</a> specific issue.   Just don\\'t know how to circumvent it.</p>\\n\\n<p>Thanks.</p>\\n-How to subset monet.frame with %in% properly-<r><monetdb>',\n",
       " '<p><img src=\"https://i.stack.imgur.com/vK4Ga.png\" alt=\"\"></p>\\n\\n<p>The <code>Label</code> background color looks terrible on Mac, when it\\'s placed in a <code>Group</code>.</p>\\n\\n<p>From what I\\'ve read, this has been fixed already, a long time ago.</p>\\n\\n<ul>\\n<li><a href=\"https://bugs.eclipse.org/bugs/show_bug.cgi?id=47146\" rel=\"nofollow noreferrer\">Bug #1</a></li>\\n<li><a href=\"https://bugs.eclipse.org/bugs/show_bug.cgi?id=290212\" rel=\"nofollow noreferrer\">Bug #2</a></li>\\n<li><a href=\"https://bugs.eclipse.org/bugs/show_bug.cgi?id=297633\" rel=\"nofollow noreferrer\">Bug #3</a></li>\\n<li><a href=\"https://bugs.eclipse.org/bugs/show_bug.cgi?id=327790\" rel=\"nofollow noreferrer\">Bug #4</a></li>\\n</ul>\\n\\n<p>If these are marked as fixed, then why aren\\'t the changes present in Kepler 4.3.1?</p>\\n\\n<p>I don\\'t want to open ANOTHER bug on Eclipse\\'s Bugzilla. Also, I don\\'t want to implement workarounds in our system. </p>\\n\\n<p>Any suggestions?</p>\\n-SWT Label background color in Group [MacOSX]-<java><macos><widget><swt><eclipse-kepler>',\n",
       " '<p>I just started to use KNIME and it suppose managed a huge mount of data, but isn\\'t, it\\'s slow and often not response. I\\'ll manage more data than that I\\'m using now, What am I doing wrong?.\\nI set in my configuration file \"knime.ini\":</p>\\n\\n<pre><code>-XX:MaxPermSize=1024m\\n-Xmx2048m\\n</code></pre>\\n\\n<p>I also read data from a database node (millions of rows) but I can\\'t limit it by SQL (I don\\'t really mind, I need this data).</p>\\n\\n<pre><code>SELECT * FROM foo LIMIT 1000\\n</code></pre>\\n\\n<p>error:</p>\\n\\n<pre><code>WARN     Database Reader     com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near \\'LIMIT 0\\' at line 1\\n</code></pre>\\n-Knime too slow - performance-<bigdata><knime>',\n",
       " \"<p>I currently am using Eclipse Kepler on Windows 7.  Previously I could get to the workspace screen to apply which workspace I wanted to use but for some reason as I start Eclipse I now get the Kepler's spash screen for maybe a 10th of a second and the application crashes. I search the Eclipse folder for logs to determine what was the problem but I did not find and logs.  Has anyone experienced issue like this?  I would really appreciate any help.</p>\\n-Eclipse Kepler Crashes on Start up only shows splash screen-<eclipse><window><eclipse-kepler>\",\n",
       " '<p>I am trying to train a multilayer neural network in rapidminer. \\nThe key to my problem is that I have three nodes on my output layer that I want predict:\\n(y1,y2,y3) = f(x1,x2,x3) with one hidden node. \\nAs far as I understood, Rapidminer does not allow to assign a \"special role\" (e.g. label, id, prediction) to more than one variable. </p>\\n\\n<p>I am assuming an output layer containing more than one node must be possible to model but how is this done?</p>\\n\\n<p>Several posts recommended using the \"setRole\" operator - can you give me any hints on that if it is helpful?</p>\\n\\n<p>Thank you,</p>\\n\\n<p>Mat</p>\\n-Rapidminer Multilayer Perceptron Modeling output layer-<neural-network><rapidminer>',\n",
       " \"<p>I'm new to monet. When it comes to dump/restore activities I would like to use the same functionality of <code>mysqldump</code>. I tried the following:</p>\\n\\n<pre><code>mclient -u monetdb -lsql --database=my_db --dump &gt; ~/my_db.sql\\n</code></pre>\\n\\n<p>However, when restoring with: </p>\\n\\n<pre><code>mclient -u monetdb  -lsql --database=my_db ~/my_db.sql\\n</code></pre>\\n\\n<p>I came across the following error:</p>\\n\\n<pre><code>CREATE SEQUENCE: name 'seq_6620' already in use\\ncurrent transaction is aborted (please ROLLBACK)\\n</code></pre>\\n\\n<p>What am I missing? Does this mean I can only feed dumped data to an empty db? Thanks in advance for your hints.</p>\\n-Dump and restore in Monetdb-<monetdb>\",\n",
       " '<p>In some of my jsf i have a form. when user fill the form he/she can click on the button. on the following you can see a example of code that i use :  </p>\\n\\n<pre><code>&lt;h:commandButton styleClass=\"btn btn-primary btn-block\" value=\"#{msg[\\'button.add.share\\']}\" action=\"#{AddShareControler.add}\" /&gt;  \\n</code></pre>\\n\\n<p>And in the <code>AddShareControler</code> i have a method called <code>add</code> with following structure :  </p>\\n\\n<pre><code>public void add() throws DaoImplNotFoundException, DataSourceException, IOException{\\n    share.setUserId(id);\\n    ShareDao dao=(ShareDao) Factory.getDao(ConfigReader.getConfig().getProperty(\"dao.share\"));\\n    dao.add(share);\\n    FacesContext.getCurrentInstance().getExternalContext().redirect(\"share.jsf?uid=\"+share.getUserId());\\n}  \\n</code></pre>\\n\\n<p>But eclipse kepler give me a error in my xhtml file. this is the error that kelper give me :  </p>\\n\\n<blockquote>\\n  <p>Method must have signature \"String method()\" but has signature \"void\\n  method()\"</p>\\n</blockquote>\\n\\n<p>I know what is the reason of this Error. Beacuase Add method return void. but why eclipse juno don\\'t give me any error for the same code. And how i can resolve it in the kepler?</p>\\n\\n<p>thanks.</p>\\n-Method must have signature \"String method()\" but has signature \"void method()\"-<eclipse><jsf><eclipse-juno><eclipse-kepler>',\n",
       " '<p>I am working on a set of plugins and fragments on eclipse 4.3 (Kepler) but using the 3.x model.</p>\\n\\n<p>One plugin contains part of the application model and has a fragment which contributes some model editors. (This seems right to me - tell me if it\\'s unnecessarily complicated)</p>\\n\\n<p>In the main app these editor contributions are not listed. I changed the fragment to a plugin and the editors became visible.</p>\\n\\n<p>I could change the fragment to a plugin but this would mean that I have to expose more of the model.</p>\\n\\n<p>I could move the fragment to the host plugin but this would mean mixing model and view (perhaps not a big deal)</p>\\n\\n<p>Is it generally not possible to contribute from a fragment or have I done something wrong?</p>\\n\\n<p>Here is the fragment:</p>\\n\\n<pre class=\"lang-xml prettyprint-override\"><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;?eclipse version=\"3.4\"?&gt;\\n&lt;fragment&gt;\\n   &lt;extension\\n         point=\"org.eclipse.ui.editors\"&gt;\\n      &lt;editor\\n            class=\"com.acme.atf.device.ui.editors.NullConfigEditor\"\\n            default=\"false\"\\n            id=\"827299e9-6039-4a76-bfa6-08ef2d7f8724\"\\n            name=\"VariableEditor\"&gt;\\n      &lt;/editor&gt;\\n      &lt;editor\\n            class=\"com.acme.atf.device.ui.editors.SerialConfigEditor\"\\n            default=\"false\"\\n            id=\"cbaba4b2-8380-4ae5-9896-542cf97ca8cc\"\\n            name=\"SerialEditor\"&gt;\\n      &lt;/editor&gt;\\n   &lt;/extension&gt;\\n</code></pre>\\n\\n<p></p>\\n\\n<p><strong>Further information</strong></p>\\n\\n<p>The plugin hierarchy looks like this: (P=plugin, F=fragment)</p>\\n\\n<pre><code>com.acme.atf.app (P)\\ncom.acme.atf.device (P)\\ncom.acme.atf.device.help (F)\\ncom.acme.atf.device.ui (F)\\ncom.acme.atf.model (P)\\ncom.acme.atf.core.ui (P)\\n</code></pre>\\n\\n<p>An action in <code>com.acme.atf.app</code> attempts to load an editor (which happens to be in <code>device.ui</code>) which cannot be found. If I change <code>device.ui</code>to a plugin then the editor is found.</p>\\n-Cannot contribute extension from a fragment-<eclipse-plugin><fragment><eclipse-kepler>',\n",
       " '<p>Similar to <a href=\"https://stackoverflow.com/questions/18982063/add-toolbar-in-eclipse-rcp-application\">this question</a>\\'s screenshot, my RCP App has those annoying spaces between actions on the coolbar. My guess is that it\\'s because I\\'m using deprecated extensions in the <code>plugin.xml</code> (i.e. <strike><code>viewActions</code></strike> <code>actionSet</code>s).</p>\\n\\n<p>For now, I don\\'t want to begin changing all those action implementations to comply with the new rules (mostly because there are A LOT of those actions). </p>\\n\\n<p><b>Question</b>: Can I somehow apply CSS styling to that <code>CoolBar</code> so that extra spaces are grabbed by the actions?</p>\\n\\n<p><b>The road so far</b>:</p>\\n\\n<ol>\\n<li>NO e4 STUFF! <a href=\"https://stackoverflow.com/questions/18982063/add-toolbar-in-eclipse-rcp-application\">This question does not satisfy mine.</a></li>\\n<li><a href=\"https://stackoverflow.com/questions/19594890/eclipse-kepler-rcp-main-toolbar-actions\">My other (similar) question. Feel free to complete that guy\\'s answer.</a></li>\\n<li><a href=\"http://help.eclipse.org/juno/index.jsp?topic=/org.eclipse.rap.help/help/html/reference/theming/CoolBar.html\" rel=\"nofollow noreferrer\">RAP CSS - does this apply to RCP as well?</a></li>\\n</ol>\\n-Eclipse Kepler RCP CoolBar Actions CSS-<css><eclipse><swt><eclipse-rcp><eclipse-kepler>',\n",
       " \"<p>I have Eclipse Kepler and NodeEclipse plugin installed. For NodeEclipse's performance reasons I have disabled content assistant for JavaScript files.</p>\\n\\n<p><strong>Problem:</strong>\\nWhenever I copy or cut some lines in JS files (ctrl+c / ctrl+x), the CPU usages goes 100% and the eclipse process eats up all memory (>1GB). I think some GC thrashing is happening. Increasing JVM max memory more is an option, but isn't 1GB enough for eclipse?</p>\\n\\n<p>This could be some memory leak. Is this specific problem with Nodeclipse? Are there any workarounds/settings in eclipse to diable? </p>\\n\\n<p><strong>Edited:</strong>\\nWhen I double click a variable, CPU pumps to 100% for 3-4 seconds, comes down. I have mark occurences already disbled. This is happending only for route/*.js files and not for public folder. Investigating more, I find that commenting line containing <code>require('&lt;package-name&gt;')</code> solves this problem, why?</p>\\n-Eclipse hangs on copy/cut for JavaScript files-<javascript><eclipse><node.js><eclipse-kepler><nodeclipse>\",\n",
       " '<p>I build a project just like ODesk Tracker which take snapshots after intervals. The application is working fine in windows and linux. But in Mac when i minimize the application. It gives Error and a popup displays <strong>Java Quit Unexpectedly.</strong></p>\\n\\n<pre><code> A fatal error has been detected by the Java Runtime Environment:\\n#\\n#  SIGSEGV (0xb) at pc=0x00007fff885a20e6, pid=1386, tid=1287\\n#\\n# JRE version: Java(TM) SE Runtime Environment (8.0-b118) (build 1.8.0-ea-b118)\\n# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.0-b60 mixed mode bsd-amd64 compressed oops)\\n# Problematic frame:\\n# C  [libobjc.A.dylib+0x80e6]  objc_release+0x16\\n#\\n# Failed to write core dump. Core dumps have been disabled. To enable core dumping, try \"ulimit -c unlimited\" before starting Java again\\n#\\n# An error report file with more information is saved as:\\n# /Users/babita/Documents/workspace/FBTracker 2/hs_err_pid1386.log\\n</code></pre>\\n-Javafx application gives error when minimized in Mac-<macos><javafx><java-7><eclipse-kepler>',\n",
       " '<p>I have a table like this:</p>\\n\\n<pre><code>id      | animals       | \\n------- |:-------------:| \\n1       | cat, dog, fish| \\n2       | dog           |\\n3       | cat,          | \\n4       |               |\\n5       | fish, cat     | \\n6       | dog           |\\n</code></pre>\\n\\n<p>etc ..\\nThe only possible value in animal  column, is cat, dog, fish or nothing.</p>\\n\\n<p>And i would like to have table like this: </p>\\n\\n<pre><code>id     | cat | dog |fish|\\n------ |:----|-----|----|\\n1      | 1   |1    |1   |  \\n2      |     |1    |    |\\n3      | 1   |     |    | \\n4      |     |     |    |\\n5      | 1   |     |1   | \\n6      |     |1    |    |\\n</code></pre>\\n\\n<p>Is there any way how to do this in RapidMiner. </p>\\n-Rapidminer, how to split column?-<split><rapidminer>',\n",
       " '<p>We are running MonetDB on an OpenIndiana fileserver (Xenon E5-1620 / 16GB RAM, RAM will be ugraded soon). \\nWe merge many small csv files to two big ones.</p>\\n\\n<p>The first is around 25GB but the second is around 125GB.\\nImporting the first one works fine but the second does not work, the server just shuts down.</p>\\n\\n<p>Why does this happen?\\nIs there any way around this, except splitting the file?\\nIf splitting is unavoidable, is there a way to load the csvs using a loop?</p>\\n\\n<p>--</p>\\n\\n<p>How can I do the debugging steps described <a href=\"http://www.monetdb.org/Documentation/UserGuide/Debugging\" rel=\"nofollow\">here</a> when the whole system crashes? Not only the database crashes, the whole system shuts down.</p>\\n\\n<p>--</p>\\n\\n<p>We meanwhile found a way around this problem. We split the big file into five smaller ones à 25GB and imported them using a .sql file which contains five COPY INTO statements.</p>\\n-Importing huge csv into MonetDB-<csv><monetdb>',\n",
       " '<p>I have a table consider A with\\nFields A1, A2 where A1 is the unique/primary key and A2 is type II field.\\nNow i need to find A1 and A2 where the A2 gets updated/changed.\\nCan anyone please help with a query for this?</p>\\n\\n<p>Sample data</p>\\n\\n<pre><code>A1     A2   A3\\nemp1   1     2\\nemp2   1     3\\nemp3   1     4\\nemp3   2     4\\n</code></pre>\\n\\n<p>I should be getting the output as</p>\\n\\n<pre><code>A1     A2   A3\\nemp3   1     4\\nemp3   2     4\\n</code></pre>\\n-To check type II in teradata-<sql><teradata>',\n",
       " '<p>I have to transfer around 5 million rows of data from Teradata to MySQL. Can anyone please suggest me the fastest way to do this over the network, without using the filesystem. I am new to Teradata and MySQL. I want to run this transfer as a batch job on weekly basis, so I am looking for the solution which can be fully automated. Any suggestions or hints will be greatly appreciated.</p>\\n\\n<p>I have already written the code using JDBC to get the records from the Teradata and insert them into the MySQL. But it is very slow, so I am looking to make that code more efficient. I kept in generic because I didn\\'t have the solution to be constrained by my implementation, as along with making existing code more efficient I am open to other alternatives also. But I don\\'t want to use the file system since it\\'s not easier to maintain or update the scripts.</p>\\n\\n<p>My implementation: </p>\\n\\n<p><strong>Getting records from teradata:</strong></p>\\n\\n<pre><code>connection  =   DBConnectionFactory.getDBConnection(SOURCE_DB);\\n\\n    statement = connection.createStatement();\\n    rs = statement.executeQuery(QUERY_SELECT);\\n    while (rs.next()) {\\n\\n        Offer offer = new Offer();\\n        offer.setExternalSourceId(rs.getString(\"EXT_SOURCE_ID\"));\\n        offer.setClientOfferId(rs.getString(\"CLIENT_OFFER_ID\"));\\n        offer.setUpcId(rs.getString(\"UPC_ID\"));\\n\\n        offers.add(offer);\\n    }\\n</code></pre>\\n\\n<p><strong>Inserting the records in mySQL:</strong></p>\\n\\n<pre><code>int count = 0;\\n    if (isUpdated) {\\n        for (Offer offer : offers) {\\n\\n            count++;\\n\\n            stringBuilderUpdate = new StringBuilder();\\n            stringBuilderUpdate = stringBuilderUpdate\\n                    .append(QUERY_INSERT);\\n\\n            stringBuilderUpdate = stringBuilderUpdate.append(\"\\'\"\\n                    + offer.getExternalSourceId() + \"\\'\");\\n\\n            statement.addBatch(stringBuilderUpdate.toString());\\n\\n            queryBuilder = queryBuilder.append(stringBuilderUpdate\\n                    .toString() + SEMI_COLON);\\n\\n            if (count &gt; LIMIT) {\\n                countUpdate = statement.executeBatch();\\n                LOG.info(\"DB update count : \" + countUpdate.length);\\n                count = 0;\\n            }\\n\\n        }\\n        if (count &gt; 0) {\\n            // Execute batch\\n            countUpdate = statement.executeBatch();\\n        }\\n</code></pre>\\n\\n<p>Can anybody please tell me if we can make this code more efficient ??? </p>\\n\\n<p>Thanks</p>\\n\\n<p>PS: Please ignore the syntax error in above code as this code is working fine. Some info might be missing because of copy and paste.</p>\\n-Transferring Millions of rows from teradata to mySQL-<java><mysql><sql><bigdata><teradata>',\n",
       " \"<p>I'm working on eclipse-kepler.\\nwhen I try to add external jars, they are added under the project and not under any folder (that is under the project),\\nIs there a way I can abbreviate (shorten/shrink) them so it wouldn't be so annoying?</p>\\n-adding jars to a project in eclipse is not under any folder-<eclipse><jar><eclipse-kepler>\",\n",
       " '<p>I have a scheduled job, which is mirroring the P2 Kepler repository (<a href=\"http://download.eclipse.org/releases/kepler/\" rel=\"nofollow\">http://download.eclipse.org/releases/kepler/</a>). The mirroring seems successful but i if i open the \"install new software\" dialog in Eclipse and pointing it to the mirrored directory there is nothing to install.</p>\\n\\n<p>I compared the Kepler Repository with the Indigo Repository and recognized that the content.jar has a big difference. While in the indigo repository the size of the content.jar is about 5MByte, the content.jar in the Kepler Repository is only 300Byte. This difference occurs because the content.xml for the Kepler Release is empty!</p>\\n\\n<p>Is this a bug? How can i make it work? I wanna be able to install all kepler components from the \"install new software\" dialog.</p>\\n\\n<p>Thank you for your help</p>\\n-Empty content.jar after mirroring Eclipse Kepler repository-<java><eclipse><repository><eclipse-kepler><p2>',\n",
       " '<p>According to the <a href=\"http://www.nvidia.com/content/PDF/kepler/NVIDIA-Kepler-GK110-Architecture-Whitepaper.pdf\" rel=\"nofollow\">Kepler architecture whitepaper</a>, a SMX has <code>192</code> CUDA cores and <code>64</code> Double Precision Units (DPUs). For a K20Xm there are <code>14</code> SMXs totalling at <code>2688</code> cores, which means that only the CUDA cores are counted. What exactly is then the usage of the DPUs for and how is their usage related to the cores?</p>\\n\\n<p>My thoughts:</p>\\n\\n<p>a) The CUDA cores can\\'t do double precision operations and only the DPUs can. Therefore, the CUDA cores are free for other stuff while the DPUs are busy.</p>\\n\\n<p>b) The CUDA cores somehow need a double precision unit to do double precision operations, therefore only <code>128</code> of the <code>192</code> CUDA cores are available for other stuff.</p>\\n\\n<p>Cheers\\nAndi</p>\\n-On Double Precision Units (DPUs) on Kepler K20Xm-<cuda><double><kepler>',\n",
       " '<p>In the <a href=\"http://www.nvidia.com/content/PDF/kepler/NVIDIA-Kepler-GK110-Architecture-Whitepaper.pdf\" rel=\"noreferrer\">Kepler architecture whitepaper</a>, NVIDIA states that there are <code>32</code> Special Function Units (SFUs) and <code>32</code> Load/Store Units (LD/ST) on a SMX.</p>\\n\\n<p>The SFU are for \"fast approximate transcendental operations\". Unfortunately, I don\\'t understand what this is supposed to mean. On the other hand, at <a href=\"https://stackoverflow.com/questions/11554122/special-cuda-double-precision-trig-functions-for-sfu\">Special CUDA Double Precision trig functions for SFU</a> it is said, that they only work in single precision. Is this still correct on a K20Xm? </p>\\n\\n<p>The LD/ST units are obviously for storing and loading. Is any memory load/write required to go through one of theses? And are they also used as a single warp? In other words, can there be only one warp which is currently writing or reading?</p>\\n\\n<p>Cheers,\\nAndi</p>\\n-Load/Store Units (LD/ST) and Special Function Units (SFUs) for the Kepler architecture-<cuda><nvidia><kepler>',\n",
       " \"<p>I am trying to change the default data directory of MonetDB. I am running out of space, and I would like to migrate the data to another folder.\\nDoes anyone know how to do that? </p>\\n\\n<p>I have installed MonetDB using the ubuntu package, and by default the data is stored in:</p>\\n\\n<pre><code>/var/lib/monetdb\\n</code></pre>\\n\\n<p>I would welcome a solution that doesn't involve compiling MonetDB from source...</p>\\n-How to change default data directory in MonetDB?-<directory><monetdb>\",\n",
       " '<p>I am having trouble adding the JAX-RS 2.0 facet to an Eclipse 4.3. Dynamic Web project with Glassfish 4.0.  I tried the approach noted <a href=\"https://stackoverflow.com/questions/14924925/how-to-add-jax-rs-to-an-existing-project\">here</a> and the values that are filled in for the parameters with \"Disabled Library Configuration\" are:</p>\\n\\n<p>JAX-RS servlet name: JAX-RS Servlet<br/>\\nJAX-RS servlet class name: javax.ws.rs.core.Application<br/>\\nURL mapping patterns: /jaxrs/*</p>\\n\\n<p>That gets saved without any problems.  Then, when I go back to the Project Properties, I get a pop-up that says \"The currently displayed page contains invalid values\".  Acknowledging that, and getting to the JAX-RS facet page, all the fields are now empty, and instead of saying \"Disabled Library Configuration\", the Type says \"Unknown Library Configuration\". Going to the main Project Facets page and trying to uncheck the JAX-RS facet item gives a pop-up that says \"Failed while uninstalling JAX-RS (REST Web Services) 2.0. Reason: Failed while uninstalling JAX-RS (REST Web Services) 2.0.  The details say \"Failed while uninstalling JAX-RS (REST Web Services) 2.0.\\norg.eclipse.jst.javaee.web.internal.impl.WebAppImpl cannot be cast to org.eclipse.jst.j2ee.webapplication.WebApp\"</p>\\n\\n<p>I updated Eclipse to 4.3.1 and got the same behavior.  The fields getting blanked out without any warning when they were initially saved, and not being able to uninstall the JAX-RS facet  leads me to believe there\\'s something wrong with my Eclipse install or project file (although I tried this with several projects and they all behaved the same).  </p>\\n\\n<p>Can anyone point me at a way to fix this?</p>\\n-How to configure JAX-RS 2.0 with Eclipse Kepler 4.3.1?-<jax-rs><glassfish-4><eclipse-kepler>',\n",
       " '<p>I have an E4 RCP application and I need to add a Help system to it. \\nAs I have understood from a post in the same forum, called <strong>Eclipse e4 and org.eclipse.help.ui</strong>, it \"<em>seems that pure Eclipse 4 code currently (as in Eclipse juno 4.2) lacks support for help...</em>\". Is this problem solved in Kepler? I have added org.eclipse.help.ui and org.eclipse.help.webapp but I have no idea of how can I implement a handler to open the help content.</p>\\n-How to implement help in Eclipse 4.3 (Kepler) application?-<java><kepler><help-system>',\n",
       " '<p>I\\'m using Eclipse Java EE IDE for Web Developers (Kepler Service Release 1) and this morning it started throwing the following exception:</p>\\n\\n<pre><code>eclipse.buildId=4.3.0.M20130911-1000\\njava.version=1.6.0_45\\njava.vendor=Sun Microsystems Inc.\\nBootLoader constants: OS=win32, ARCH=x86_64, WS=win32, NL=en_US\\nFramework arguments:  -product org.eclipse.epp.package.jee.product\\nCommand-line arguments:  -os win32 -ws win32 -arch x86_64 -product org.eclipse.epp.package.jee.product -clean -data c:\\\\dev\\\\workspaces\\\\workspace-moneris\\\\\\n\\nError\\nTue Dec 10 10:10:43 AST 2013\\nException while reading /PC_war/.classpath\\n\\njava.io.IOException: Bad format\\n    at org.eclipse.jdt.internal.core.JavaProject.decodeClasspath(JavaProject.java:915)\\n    at org.eclipse.jdt.internal.core.JavaProject.readFileEntriesWithException(JavaProject.java:2497)\\n    at org.eclipse.jdt.internal.core.JavaProject.readFileEntries(JavaProject.java:2507)\\n    at org.eclipse.jdt.internal.core.JavaProject.writeFileEntries(JavaProject.java:2895)\\n    at org.eclipse.jdt.internal.core.JavaModelManager$PerProjectInfo.writeAndCacheClasspath(JavaModelManager.java:1352)\\n    at org.eclipse.jdt.internal.core.JavaModelManager$PerProjectInfo.writeAndCacheClasspath(JavaModelManager.java:1364)\\n    at org.eclipse.jdt.internal.core.SetClasspathOperation.executeOperation(SetClasspathOperation.java:77)\\n    at org.eclipse.jdt.internal.core.JavaModelOperation.run(JavaModelOperation.java:728)\\n    at org.eclipse.core.internal.resources.Workspace.run(Workspace.java:2345)\\n    at org.eclipse.jdt.internal.core.JavaModelOperation.runOperation(JavaModelOperation.java:793)\\n    at org.eclipse.jdt.internal.core.JavaProject.setRawClasspath(JavaProject.java:3096)\\n    at org.eclipse.jdt.internal.core.JavaProject.setRawClasspath(JavaProject.java:3058)\\n    at org.eclipse.jdt.internal.core.JavaProject.setRawClasspath(JavaProject.java:3111)\\n    at org.eclipse.jdt.apt.core.internal.generatedfile.ClasspathUtil.updateProjectClasspath(ClasspathUtil.java:208)\\n    at org.eclipse.jdt.apt.core.internal.generatedfile.GeneratedSourceFolderManager.addToClasspath(GeneratedSourceFolderManager.java:117)\\n    at org.eclipse.jdt.apt.core.internal.generatedfile.GeneratedSourceFolderManager.ensureFolderExists(GeneratedSourceFolderManager.java:192)\\n    at org.eclipse.jdt.apt.core.internal.generatedfile.GeneratedResourceChangeListener.addGeneratedSrcFolderTo(GeneratedResourceChangeListener.java:123)\\n    at org.eclipse.jdt.apt.core.internal.generatedfile.GeneratedResourceChangeListener.resourceChanged(GeneratedResourceChangeListener.java:92)\\n    at org.eclipse.jdt.internal.core.DeltaProcessingState$1.run(DeltaProcessingState.java:465)\\n    at org.eclipse.core.runtime.SafeRunner.run(SafeRunner.java:42)\\n    at org.eclipse.jdt.internal.core.DeltaProcessingState.resourceChanged(DeltaProcessingState.java:460)\\n    at org.eclipse.core.internal.events.NotificationManager$1.run(NotificationManager.java:291)\\n    at org.eclipse.core.runtime.SafeRunner.run(SafeRunner.java:42)\\n    at org.eclipse.core.internal.events.NotificationManager.notify(NotificationManager.java:285)\\n    at org.eclipse.core.internal.events.NotificationManager.broadcastChanges(NotificationManager.java:149)\\n    at org.eclipse.core.internal.resources.Workspace.broadcastBuildEvent(Workspace.java:382)\\n    at org.eclipse.core.internal.events.AutoBuildJob.doBuild(AutoBuildJob.java:139)\\n    at org.eclipse.core.internal.events.AutoBuildJob.run(AutoBuildJob.java:241)\\n    at org.eclipse.core.internal.jobs.Worker.run(Worker.java:53)\\n</code></pre>\\n\\n<p>… and then replacing my .classpath with the following:</p>\\n\\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;classpath&gt;\\n    &lt;classpathentry kind=\"src\" path=\".apt_src\"&gt;\\n        &lt;attributes&gt;\\n            &lt;attribute name=\"optional\" value=\"true\"/&gt;\\n        &lt;/attributes&gt;\\n    &lt;/classpathentry&gt;\\n    &lt;classpathentry kind=\"output\" path=\"bin\"/&gt;\\n&lt;/classpath&gt;\\n</code></pre>\\n\\n<p>Obviously this is causing problems...</p>\\n\\n<p>Any idea why this is happening?  </p>\\n-Eclipse wiping my .classpath-<java><eclipse><eclipse-kepler>',\n",
       " '<p>I\\'m very new to python but I need to simulate kepler\\'s second law through vpython! I\\'ve got the orbit going so far but I don\\'t know how to code the sweeping motion and how to code the r, theta etc. Can anyone help? </p>\\n\\n<p><a href=\"http://en.wikipedia.org/wiki/File:Kepler-second-law.gif\" rel=\"nofollow\">http://en.wikipedia.org/wiki/File:Kepler-second-law.gif</a>\\nthis is the kind of thing I want to make! Thank you for your help!</p>\\n-vpython) How to simulate kepler\\'s 2nd law?-<orbit><vpython><kepler>',\n",
       " '<p>Can someone propose a resolution to the following? </p>\\n\\n<p>There has to be an easy way out by running something with git.</p>\\n\\n<pre><code>$ rm -rf /usr/local/Cellar /usr/local/.git &amp;&amp; brew cleanup\\n$ ruby -e \"$(curl -fsSL https://raw.github.com/mxcl/homebrew/go/install)\"\\n==&gt; This script will install:\\n/usr/local/bin/brew\\n/usr/local/Library/...\\n/usr/local/share/man/man1/brew.1\\n\\nPress ENTER to continue or any other key to abort\\n==&gt; Downloading and installing Homebrew...\\nremote: Counting objects: 142460, done.\\nremote: Compressing objects: 100% (49574/49574), done.\\nremote: Total 142460 (delta 97359), reused 136688 (delta 91829)\\nReceiving objects: 100% (142460/142460), 33.23 MiB | 1.57 MiB/s, done.\\nResolving deltas: 100% (97359/97359), done.\\nFrom https://github.com/mxcl/homebrew\\n * [new branch]      master     -&gt; origin/master\\nHEAD is now at 8cb0f87 brew-test-bot: fix testing job tag numbering.\\n==&gt; Installation successful!\\nYou should run `brew doctor\\' *before* you install anything.\\nNow type: brew help\\n$ brew doctor\\nWarning: Your file-system on / appears to be CaSe SeNsItIvE.\\nHomebrew is less tested with that - don\\'t worry but please report issues.\\n$ brew update\\nerror: insufficient permission for adding an object to repository database .git/objects\\nfatal: failed to write object\\nfatal: unpack-objects failed\\nError: Failed to update tap: homebrew/science\\nAlready up-to-date.\\n$ brew untap homebrew/science\\nError: No such tap!\\n$\\n</code></pre>\\n-Homebrew: insufficient permission for adding an object to repository database .git/objects-<git><homebrew>',\n",
       " '<p>Does MonetDB support online schema changes? For example adding/changing a column on the fly while the tables are loaded in memory. Many in-memory databases have to restarted to get the schema changes reflected. So, I was wondering if MonetDB took care of this issue.</p>\\n-monetdb: Online Schema Alters-<monetdb>',\n",
       " '<p>I\\'m trying to install subclipse on my desktop so i can use subversion.</p>\\n\\n<p>When I was at school with my laptop, the installation worked well and i was able to update and commit on the subversion, but, while i was at home, i tried to update and nothing worked. Then I tried on my desktop to install the subclipse plugin without success.</p>\\n\\n<p>When I go to \"Help>install new software>Work With Kepler - <a href=\"http://download.eclipse.org/releases/kepler\" rel=\"nofollow\">http://download.eclipse.org/releases/kepler</a>\"</p>\\n\\n<p>Nothing appears on the software view except \"pending...\"</p>\\n\\n<p>The thing I don\\'t understand is that i\\'m not using a proxy at home, so i don\\'t understand why it\\'s freezing at this step.</p>\\n\\n<p>Then, I tried to change my eclipse version but it didn\\'t worked\\nI also tried to change my network settings without success</p>\\n\\n<p>Anyone had the same problem before or know how to fix it ? </p>\\n\\n<p>EDIT 1 : While waiting for the pending thing to resolv i had an error :\\n\"Unable to read repository at http: //download.eclipse.org/releases/kepler/content.xml.\\nUnable to read repository at http: //download.eclipse.org/releases/kepler/content.xml.\\nConnection has been shut down\"</p>\\n-Unable to install new software/check for updates eclipse-<java><eclipse><svn><networking><eclipse-kepler>',\n",
       " '<p>I am trying to load a ready rapidminer model through netbeans...The whole application seems to be running fine except when i get an error at this line of code:</p>\\n\\n<pre><code>IOContainer ioResult = process.run();\\n**ExampleSet resultSet = (ExampleSet) ioResult.getElementAt(0);**\\nExampleTable mytable = resultSet.getExampleTable();\\n</code></pre>\\n\\n<p>The error is this one...</p>\\n\\n<pre><code>com.rapidminer.Process run\\nINFO: Process C:\\\\Users\\\\Antonis\\\\.RapidMiner5\\\\repositories\\\\Local Repository\\\\yo.rmp      finished successfully after 0 s\\n**Exception in thread \"AWT-EventQueue-0\" java.lang.ClassCastException:        com.rapidminer.operator.learner.bayes.SimpleDistributionModel cannot be cast to   com.rapidminer.example.ExampleSet**\\n</code></pre>\\n\\n<p>I don\\'t know how to handle this...If you could please help me find a way to make it work...I am trying to fix it and i feel that i am sooo close...!!! thank you in advance for your ideas and time!!!</p>\\n-Rapidminer/Netbeans Integration Error-<java><netbeans><integration><rapidminer>',\n",
       " \"<p>I have to compare two columns say A and B. If they change I need to print both the values as old and new values.<br>\\nBut the problem is that the 2nd column is having some extra spaces between the words, so the names are displayed even though the name didn't change >\\nhelp me out on this. \\nI am looking for a SQL query in teradata </p>\\n-How to replace multiple spaces between words with a single space in teradata SQL?-<sql><teradata>\",\n",
       " '<p>I have a problem with the creation of a new database. \\nAfter that I started MonetDB Server, I insert this command: \\nmserver5 --dbpath=\"C:\\\\database1\"</p>\\n\\n<p>But the server give me this error:\\n!SintaxException:parseError:mserver5 --dbpath=\"C:\\\\database1\"\\n!SintaxException:parseError: ^\\';\\' expected</p>\\n\\n<p>What should i do to resolve this problem? \\nThanks</p>\\n-create a database MonetDB-<database><monetdb>',\n",
       " '<p>below is the oracle SQL and i want to change it in Teradata format.</p>\\n\\n<pre><code>SELECT branch_code,\\n    branch_no,\\n    c_no,\\n    cd_type\\nFROM (\\n    SELECT branch_code,\\n        branch_no,\\n        c_no,\\n        cd_type,\\n        * * RANK() OVER (\\n            PARTITION BY c_no ORDER BY cd_type\\n            ) RANK * *\\n    FROM (\\n        SELECT branch_code,\\n            branch_no,\\n            c_no,\\n            MIN(cd_type) cd_type\\n        FROM EMPLOYEE\\n        WHERE S_CODE = \\'C\\'\\n            AND (branch_no) NOT IN (\\n                SELECT branch_code\\n                FROM DEPARTMENT\\n                WHERE branch_code = \\'ABC\\'\\n                )\\n        )\\n    )\\nWHERE RANK = 1\\n</code></pre>\\n\\n<p>I have used QUALIFY for RANK as below .</p>\\n\\n<pre><code>SELECT branch_code,\\n    branch_no,\\n    c_no,\\n    cd_type\\nFROM (\\n    SELECT branch_code,\\n        branch_no,\\n        c_no,\\n        cd_type,\\n        * * QUALIFY ROW_NUMBER() OVER (\\n            PARTITION BY c_no ORDER BY cd_type\\n            ) * * RANK\\n    FROM (\\n        SELECT branch_code,\\n            branch_no,\\n            c_no,\\n            MIN(cd_type) cd_type\\n        FROM EMPLOYEE\\n        WHERE S_CODE = \\'C\\'\\n            AND (branch_no) NOT IN (\\n                SELECT branch_code\\n                FROM DEPARTMENT\\n                WHERE branch_code = \\'ABC\\'\\n                )\\n        )\\n    )\\nWHERE RANK = 1\\n</code></pre>\\n\\n<p>But getting error that \"Expected something between , and QUALIFY.</p>\\n\\n<p>Can we put QUALIFY in select statement ?</p>\\n-QUALIFY ROW_NUMBER in teradata-<sql><oracle><teradata>',\n",
       " '<p>Eclipse-kepler keeps crashing when I try to open a JavaScript file.</p>\\n\\n<p>Error:</p>\\n\\n<blockquote>\\n  <p>Exception in thread\\n  \"org.eclipse.wst.jsdt.internal.ui.text.JavaReconciler\"\\n  java.lang.OutOfMemoryError: GC overhead limit exceeded    at\\n  org.eclipse.wst.jsdt.internal.compiler.parser.Parser.isErrorState(Parser.java:5558)\\n    at\\n  org.eclipse.wst.jsdt.internal.compiler.parser.Parser.parse(Parser.java:5688)\\n    at\\n  org.eclipse.wst.jsdt.internal.compiler.parser.Parser.parse(Parser.java:6045)\\n    at\\n  org.eclipse.wst.jsdt.internal.compiler.parser.Parser.parse(Parser.java:5997)\\n    at\\n  org.eclipse.wst.jsdt.internal.compiler.parser.Parser.dietParse(Parser.java:4602)\\n    at\\n  org.eclipse.wst.jsdt.internal.core.CompilationUnitProblemFinder.accept(CompilationUnitProblemFinder.java:166)</p>\\n</blockquote>\\n\\n<p>The code has some menu items and few JavaScript functions which works on the menu items. I\\'m working on a Mac</p>\\n-Eclipse Kepler crashes when opening an ext-js javascript file-<javascript><eclipse-kepler>',\n",
       " '<p>I\\'m new in integrating Rapid miner in Java applications and currently i\\'m having an exception in the rapidminer.init(). As far as i can understand its because od the repositories. But i don\\'t know what to do. I did some research but i still cant resolve my problem. The exception is:</p>\\n\\n<pre><code>Dez 17, 2013 5:26:39 PM com.rapidminer.tools.ParameterService init\\nINFO: Reading configuration resource com/rapidminer/resources/rapidminerrc.\\nDec 17, 2013 5:26:39 PM com.rapidminer.tools.I18N &lt;clinit&gt;\\nINFO: Set locale to en.\\nDec 17, 2013 5:26:39 PM com.rapid_i.Launcher ensureRapidMinerHomeSet\\nINFO: Property rapidminer.home is not set. Guessing.\\nDec 17, 2013 5:26:39 PM com.rapid_i.Launcher ensureRapidMinerHomeSet\\nINFO: Trying parent directory of \\'C:\\\\Documents and Settings\\\\Geral\\\\workspace\\\\LinkMiningModule\\\\Resources\\\\RM5.3\\\\launcher.jar\\'...gotcha!\\nDec 17, 2013 5:26:39 PM com.rapid_i.Launcher ensureRapidMinerHomeSet\\nINFO: Trying parent directory of \\'C:\\\\Documents and Settings\\\\Geral\\\\workspace\\\\LinkMiningModule\\\\Resources\\\\RM5.3\\\\rapidminer.jar\\'...gotcha!\\nDec 17, 2013 5:26:39 PM com.rapidminer.repository.RepositoryManager load\\nINFO: Cannot access file system in execution mode UNKNOWN. Not loading repositories.\\nException in thread \"main\" java.lang.NoClassDefFoundError: com/vlsolutions/swing/docking/ui/DockingUISettings\\n    at com.rapidminer.tools.plugin.Plugin.initAll(Plugin.java:945)\\n    at com.rapidminer.RapidMiner.init(RapidMiner.java:550)\\n    at DataMinning.RapidMinnerInteraction.&lt;init&gt;(RapidMinnerInteraction.java:26)\\n    at dataBaseHandling.GatheringInformationFromDB.&lt;init&gt;(GatheringInformationFromDB.java:61)\\n    at General.InitializeEverything.&lt;init&gt;(InitializeEverything.java:22)\\n    at General.Main.main(Main.java:12)\\nCaused by: java.lang.ClassNotFoundException: com.vlsolutions.swing.docking.ui.DockingUISettings\\n    at java.net.URLClassLoader$1.run(Unknown Source)\\n    at java.net.URLClassLoader$1.run(Unknown Source)\\n    at java.security.AccessController.doPrivileged(Native Method)\\n    at java.net.URLClassLoader.findClass(Unknown Source)\\n    at java.lang.ClassLoader.loadClass(Unknown Source)\\n    at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)\\n    at java.lang.ClassLoader.loadClass(Unknown Source)\\n    ... 6 more\\n</code></pre>\\n\\n<p>My code is very simple:</p>\\n\\n<pre><code>public RapidMinnerInteraction()\\n    {\\n\\n             RapidMinerCommandLine.init();\\n\\n              Process process = null;\\n            try {\\n                process = new Process(new File(\"NeuralNetwork/NeuralNetworkProcess.rmp\"));\\n                try {\\n                    process.run();\\n                } catch (OperatorException e) {\\n                    // TODO Auto-generated catch block\\n                    e.printStackTrace();\\n                }\\n            } catch (IOException e1) {\\n                // TODO Auto-generated catch block\\n                e1.printStackTrace();\\n            } catch (XMLException e1) {\\n                // TODO Auto-generated catch block\\n                e1.printStackTrace();\\n            }\\n\\n\\n\\n\\n    }\\n</code></pre>\\n\\n<p>Any help would be awesome</p>\\n\\n<p>Kind regards</p>\\n-Error in rapidminer.init()-<java><rapidminer>',\n",
       " '<p>When trying to connect to my team\\'s svn repo, I\\'m getting this error.</p>\\n<blockquote>\\n<p>SVN: \\'0x0040010b: Obtain Project Name\\' operation finished with error: Selected SVN connector library is not available or cannot be loaded.</p>\\n<p>If you selected native JavaHL connector, please check if binaries are available or install and select pure Java Subversion connector from the plug-in connectors update site.</p>\\n<p>If connectors already installed then you can change the selected one at: Window-&gt;Preferences-&gt;Team-&gt;SVN-&gt;SVN Connector.\\nSelected SVN connector library is not available or cannot be loaded.</p>\\n</blockquote>\\n<p>There are quite a few pages on this exact problem that reference this</p>\\n<p><a href=\"http://community.polarion.com/projects/subversive/download/eclipse/2.0/update-site/\" rel=\"nofollow noreferrer\">http://community.polarion.com/projects/subversive/download/eclipse/2.0/update-site/</a></p>\\n<p>as the solution. However, I am unable to load this page (for the past 40+ hrs). Can anyone either confirm or deny it\\'s status?</p>\\n-Selected SVN connector library is not available-<eclipse><svn><eclipse-kepler>',\n",
       " '<p>My use case is to call a query to fetch records from db with different input parameters. After fetching records, do some processing, and then finally write it into a file.\\nMy input parameter values depend on the complete processing of the previous query.\\nMy problem is, how will i know in a spout that processing of previous query has been completed, i.e. records has been successfully written into the file.</p>\\n\\n<p>I tried implementing <code>ITridentSpout</code> but still not getting any solution. Below is my code for <code>ITridentSpout</code>:</p>\\n\\n<p>TridentCoordinator.java</p>\\n\\n<pre><code>package com.TransactionlTopology;\\n\\nimport java.util.concurrent.ConcurrentHashMap;\\n\\nimport storm.trident.spout.ITridentSpout;\\n\\npublic class TridentCoordinator implements ITridentSpout.BatchCoordinator&lt;ConcurrentHashMap&lt;Long,String&gt;&gt;{\\n\\n    ConcurrentHashMap&lt;Long,String&gt; prevMetadata=new ConcurrentHashMap&lt;Long, String&gt;();\\n    boolean result=true;\\n\\n    @Override\\n    public void success(long txid) {\\n        System.out.println(\"inside success mehod with txid as  \"+txid);\\n        if(prevMetadata.containsKey(txid)){\\n            prevMetadata.replace(txid, \"SUCCESS\");\\n        }\\n    }\\n\\n    @Override\\n    public boolean isReady(long txid) {\\n        if(!prevMetadata.isEmpty()){\\n            result=true;\\n        for(Long txId:prevMetadata.keySet()){\\n            System.out.println(\"txId:---- \"+txId +\"    value\"+prevMetadata.get(txId) );\\n            if(prevMetadata.get(txId).equalsIgnoreCase(\"SUCESS\")){\\n                prevMetadata.put(txid, \"STARTED\");\\n                result= true;\\n            }\\n        }\\n        }\\n        else{\\n            prevMetadata.put(txid, \"STARTED\");\\n            result= true;\\n        }\\n\\n        System.out.println(\"inside isReady function with txid as:---- \"+txid+\"result value:--  \"+result);\\n\\n        return result;\\n    }\\n\\n    @Override\\n    public void close() {\\n        // TODO Auto-generated method stub\\n\\n    }\\n\\n    @Override\\n    public ConcurrentHashMap&lt;Long,String&gt; initializeTransaction(long txid, ConcurrentHashMap&lt;Long,String&gt; prevMetadata, ConcurrentHashMap&lt;Long,String&gt; currMetadata) {\\n        System.out.println(\"inside initialize transaction method with values as:----- \"+txid+\"   \"+prevMetadata+\"   \"+currMetadata);\\n\\n        return prevMetadata;\\n    }\\n}\\n</code></pre>\\n\\n<p>TridentEmitterImpl.java</p>\\n\\n<pre><code>package com.TransactionlTopology;\\n\\nimport java.util.concurrent.ConcurrentHashMap;\\n\\nimport storm.trident.operation.TridentCollector;\\nimport storm.trident.spout.ITridentSpout;\\nimport storm.trident.topology.TransactionAttempt;\\nimport backtype.storm.tuple.Values;\\n\\npublic class TridentEmitterImpl implements ITridentSpout.Emitter&lt;ConcurrentHashMap&lt;Long,String&gt;&gt; {\\n\\n    @Override\\n    public void emitBatch(TransactionAttempt tx, ConcurrentHashMap&lt;Long,String&gt; coordinatorMeta,TridentCollector collector) {\\n        System.out.println(\"inside emitbatch of emitter class with values as:--- \"+coordinatorMeta);\\n        System.out.println(\"tx.getAttemptId()   \"+tx.getAttemptId()+\"tx.getTransactionId()  \"+tx.getTransactionId()+\"tx.getId()  \"+tx.getId().toString());\\n        collector.emit(new Values(\"preeti\"));\\n    }\\n\\n    @Override\\n    public void success(TransactionAttempt tx) {\\n        System.out.println(\"inside success of emitter with tx id as   \"+tx.getTransactionId());\\n\\n    }\\n\\n    @Override\\n    public void close() {\\n        // TODO Auto-generated method stub\\n\\n    }\\n}\\n</code></pre>\\n\\n<p>TridentSpoutImpl.java</p>\\n\\n<pre><code>package com.TransactionlTopology;\\n\\nimport java.util.HashMap;\\nimport java.util.Map;\\nimport java.util.concurrent.ConcurrentHashMap;\\n\\nimport storm.trident.spout.ITridentSpout;\\nimport backtype.storm.task.TopologyContext;\\nimport backtype.storm.tuple.Fields;\\n\\npublic class TridentSpoutImpl implements ITridentSpout&lt;ConcurrentHashMap&lt;Long,String&gt;&gt; {\\n\\n    @Override\\n    public storm.trident.spout.ITridentSpout.BatchCoordinator&lt;ConcurrentHashMap&lt;Long,String&gt;&gt; getCoordinator(String txStateId, Map conf, TopologyContext context) {\\n\\n        return new TridentCoordinator();\\n    }\\n\\n    @Override\\n    public storm.trident.spout.ITridentSpout.Emitter&lt;ConcurrentHashMap&lt;Long,String&gt;&gt; getEmitter(String txStateId, Map conf, TopologyContext context) {\\n\\n        return new TridentEmitterImpl();\\n    }\\n\\n    @Override\\n    public Map getComponentConfiguration() {\\n\\n        Map&lt;String,String&gt; newMap=new HashMap&lt;String, String&gt;();\\n        newMap.put(\"words\",\"preeti\");\\n        return newMap;\\n    }\\n\\n    @Override\\n    public Fields getOutputFields() {\\n\\n        return new Fields(\"word\");\\n    }\\n\\n}\\n</code></pre>\\n\\n<p>Also not able to understand what values will come in <code>initializeTransaction</code> as <code>prevMetaData</code> and <code>curMetada</code>. Please provide some solution</p>\\n-issue on implementing transactional topology in trident-<java><apache-storm><trident>',\n",
       " '<p>Using Eclipse Kepler Running on Windows 7 (64-Bit), my workspace is acting weird.</p>\\n\\n<p>The following is not working (workspace wide):</p>\\n\\n<ul>\\n<li>Type hierarchy of a class / method is showing up empty</li>\\n<li>searching for references</li>\\n</ul>\\n\\n<p>Have tried to </p>\\n\\n<ul>\\n<li>Rebuilding / Cleaning / Closing &amp; Opening Projects</li>\\n<li>Restart Eclipse</li>\\n<li>Reboot</li>\\n<li>Boot Eclipse in Clean mode (-clean)</li>\\n<li>Clear out indexes manually</li>\\n</ul>\\n\\n<p>The <strong>only</strong> thing that has worked so far is switching to a new workspace and checking out one of my projects there. However, this is very suboptimal, since I have a certain amount of projects and settings in my current workspace.</p>\\n\\n<p><strong>EDIT - Stack Trace</strong></p>\\n\\n<p>I\\'m getting the following stack trace when I specifically \"Focus On\" in Type Hierarchy view:</p>\\n\\n<pre><code>!ENTRY org.eclipse.core.jobs 4 2 2013-12-19 15:08:39.156\\n!MESSAGE An internal error occurred during: \"Computing type hierarchy of \\'String - java.lang\\'...\".\\n!STACK 0\\njava.lang.NullPointerException\\n    at org.eclipse.core.runtime.Path.&lt;init&gt;(Path.java:183)\\n    at org.eclipse.core.internal.resources.WorkspaceRoot.getProject(WorkspaceRoot.java:182)\\n    at org.eclipse.jdt.internal.core.JavaModel.getJavaProject(JavaModel.java:189)\\n    at org.eclipse.jdt.internal.core.search.IndexSelector.getJavaProject(IndexSelector.java:286)\\n    at org.eclipse.jdt.internal.core.search.IndexSelector.initializeIndexLocations(IndexSelector.java:217)\\n    at org.eclipse.jdt.internal.core.search.IndexSelector.getIndexLocations(IndexSelector.java:276)\\n    at org.eclipse.jdt.internal.core.search.JavaSearchParticipant.selectIndexURLs(JavaSearchParticipant.java:121)\\n    at org.eclipse.jdt.internal.core.search.PatternSearchJob.getIndexes(PatternSearchJob.java:84)\\n    at org.eclipse.jdt.internal.core.search.SubTypeSearchJob.getIndexes(SubTypeSearchJob.java:33)\\n    at org.eclipse.jdt.internal.core.search.PatternSearchJob.ensureReadyToRun(PatternSearchJob.java:52)\\n    at org.eclipse.jdt.internal.core.search.processing.JobManager.performConcurrentJob(JobManager.java:174)\\n    at org.eclipse.jdt.internal.core.hierarchy.IndexBasedHierarchyBuilder.searchAllPossibleSubTypes(IndexBasedHierarchyBuilder.java:523)\\n    at org.eclipse.jdt.internal.core.hierarchy.IndexBasedHierarchyBuilder.determinePossibleSubTypes(IndexBasedHierarchyBuilder.java:406)\\n    at org.eclipse.jdt.internal.core.hierarchy.IndexBasedHierarchyBuilder.build(IndexBasedHierarchyBuilder.java:120)\\n    at org.eclipse.jdt.internal.core.hierarchy.TypeHierarchy.compute(TypeHierarchy.java:300)\\n    at org.eclipse.jdt.internal.core.hierarchy.TypeHierarchy.refresh(TypeHierarchy.java:1267)\\n    at org.eclipse.jdt.internal.core.CreateTypeHierarchyOperation.executeOperation(CreateTypeHierarchyOperation.java:90)\\n    at org.eclipse.jdt.internal.core.JavaModelOperation.run(JavaModelOperation.java:728)\\n    at org.eclipse.jdt.internal.core.JavaModelOperation.runOperation(JavaModelOperation.java:788)\\n    at org.eclipse.jdt.internal.core.BinaryType.newTypeHierarchy(BinaryType.java:918)\\n    at org.eclipse.jdt.internal.core.BinaryType.newTypeHierarchy(BinaryType.java:876)\\n    at org.eclipse.jdt.internal.ui.typehierarchy.TypeHierarchyLifeCycle.createTypeHierarchy(TypeHierarchyLifeCycle.java:299)\\n    at org.eclipse.jdt.internal.ui.typehierarchy.TypeHierarchyLifeCycle.doHierarchyRefresh(TypeHierarchyLifeCycle.java:330)\\n    at org.eclipse.jdt.internal.ui.typehierarchy.TypeHierarchyLifeCycle.doHierarchyRefreshBackground(TypeHierarchyLifeCycle.java:271)\\n    at org.eclipse.jdt.internal.ui.typehierarchy.TypeHierarchyLifeCycle$2.run(TypeHierarchyLifeCycle.java:224)\\n    at org.eclipse.core.internal.jobs.Worker.run(Worker.java:53)\\n</code></pre>\\n\\n<p>Anyone?</p>\\n-Eclipse (Kepler) Workspace acting weird (type hierarchy, searching for references not working)-<java><eclipse><eclipse-kepler>',\n",
       " '<p>In a C# application, I have a DataTable being populated from a Teradata table.\\nThe teradata table data looks like this:</p>\\n\\n<pre><code>UnitNum UnitName\\n------- --------\\n123456  Location A1 - 123456 \\n123456  Location B1 - 123456\\n</code></pre>\\n\\n<p>I\\'d like to create output that looks like this:</p>\\n\\n<pre><code>UnitNum UnitName\\n------- --------\\n123456  Location A1 / B1 - 123456\\n</code></pre>\\n\\n<p>I have accomplished this in C# with the following code:</p>\\n\\n<pre><code>string compSql2 = \"SELECT UnitNum , \\n                   UPPER(MAX((CASE a.RNK WHEN 1 THEN a.UnitName ELSE NULL END))) Location1,\\n                   UPPER(MAX((CASE a.RNK WHEN 2 THEN a.UnitName ELSE NULL END))) Location2 \\n                   FROM (SELECT UnitNum , UnitName, RANK(UnitName) AS RNK \\n                     FROM idw_app_field_sync.compressor_list GROUP BY UnitNum) AS a \\n                   GROUP BY 1\";\\n        DataTable compdt = TeradataConnector.RunQuery(compSql2);\\n\\n        compdt.Columns.Add(\"UnitName\");\\n        for (int i = 0; i &lt; compdt.Rows.Count; i++)\\n        {\\n            if (compdt.Rows[i].Field&lt;string&gt;(\"Location2\") != null)\\n            {\\n                string loc1 = compdt.Rows[i].Field&lt;string&gt;(\"Location1\");\\n                string loc2 = compdt.Rows[i].Field&lt;string&gt;(\"Location2\");\\n                string unitname;\\n\\n                for (int j = 0; j &lt; loc1.Length - 9; j++)\\n                {\\n                    if (!loc2[j].Equals(loc1[j]))\\n                    {\\n                        unitname = loc2.Substring(0, loc2.Length - 9) + @\" / \" + loc1.Substring(j);\\n                        compdt.Rows[i].SetField&lt;string&gt;(\"UnitName\", unitname);\\n                    }\\n                }\\n            }\\n            else\\n            {\\n                compdt.Rows[i].SetField&lt;string&gt;(\"UnitName\", compdt.Rows[i].Field&lt;string&gt;(\"Location1\"));\\n            }\\n        }\\n</code></pre>\\n\\n<p>I\\'d like to accomplish this in Teradata SQL if at all possible. However, I do not know how to compare characters of strings with SQL like I did with C#.</p>\\n-Teradata Character Compare-<c#><sql><teradata>',\n",
       " \"<p>I have a timestamp column in a monetdb table which I want to occasionally group by hour and occasionally group by day or month. What is the most optimal way of doing this in MonetDB?</p>\\n\\n<p>In say postgres you could do something like:</p>\\n\\n<pre><code>select date_trunc('day', order_time), count(*)\\n    from orders\\n    group by date_trunc('day', order_time);\\n</code></pre>\\n\\n<p>Which I appreciate would not use an index, but is there any way of doing this in MonetDB without creating additional date columns holding day, month and year truncated values?</p>\\n\\n<p>Thanks.</p>\\n-MonetDB: Group by different parts of a timestamp-<monetdb>\",\n",
       " \"<p>I've recently installed Kepler Edition on Ubuntu and I'm trying to get my Glassfish 4.0 (open source edition) server runtime set up.</p>\\n\\n<p>I installed Glassfish separately via the zip file. My directory looks like:</p>\\n\\n<p>/home/matteo/glassfish4/glassfish\\nIn Eclipse, I installed the jboss tools and the kepler tool. \\nI then went into my Preferences | Server | Runtim Environments to add my Glassfish server. When I choose the above path, I get the error message There is no valid GlassFish installlation in the specified directory. I have the Finish button and the Next button locked.</p>\\n\\n<p>Why?Help me!</p>\\n-kepler glassfish on ubuntu-<java><eclipse><glassfish><eclipse-kepler>\",\n",
       " \"<p>I have created a view as below :</p>\\n\\n<pre><code>REPLACE    VIEW EDB_MAN_VWS.EMP AS\\nLOCKING    ROW FOR ACCESS   (\\n  SELECT 'P2' AS REGION_CD , a.* FROM EDB_MAN_WORK.EMP_IND a\\n  UNION \\n  SELECT 'Z2' AS REGION_CD , b.* FROM EDB_MAN_WORK.EMP_US b\\n  UNION \\n  SELECT 'I2' AS REGION_CD , c.* FROM EDB_MAN_WORK.EMP_UK c\\n);\\n</code></pre>\\n\\n<p>I am running below query on the view but this is taking time.</p>\\n\\n<pre><code>SELECT EMP_IND.CUST_NO,EMP_US.REGION_CD,EMP_UK.CUST_TYPE\\nFROM EDB_MAN_VWS.EMP_IND,\\n     EDB_MAN_VWS.EMP_US,\\n     EDB_MAN_VWS.EMP_UK\\nWHERE EMP_UK.CNTL_ENT_no = EMP_US.CNTL_ENT_no\\nAND EMP_US.CUST_no = EMP_IND.CUST_no\\nAND EMP_UK.REGION_CD = EMP_US.REGION_CD\\nAND EMP_US.REGION_CD = EMP_IND.REGION_CD\\nGROUP BY  CUST_NO,REGION_CD,CUST_TYPE\\n</code></pre>\\n\\n<p>I want to tune this query but we cant create SI on the view.\\nPlease help me in optimizing this query.</p>\\n-Performance tuning in teradata for view-<sql><teradata>\",\n",
       " '<p>I am trying to bulk-load a list of objects into a one-column (primary key) db. \\nThe only reason is to remove duplicates. I can\\'t load the list in memory, because the file size is way greater than my memory size (I need around 10^14 insertions!).</p>\\n\\n<p>I use monetdb\\'s <a href=\"http://www.monetdb.org/Documentation/Manuals/SQLreference/CopyInto\" rel=\"nofollow\">COPY-INTO</a> command, but I don\\'t want it to fail when there is a duplicate. I want it to add everything that is not a duplicate and skip the duplicates. </p>\\n\\n<p>Is there any way to do that with monetdb? Any other way?</p>\\n-Bulk-loading in monetdb with primary key constraint-<sql><duplicates><primary-key><bulk-load><monetdb>',\n",
       " '<p>I\\'m trying to update my project from  <strong>Eclipse 3.6 to eclipse 4.3.1</strong> .I downloaded the latest eclipse and followed the \"File->Import->install->from existing installation\" method to \\nget the configuration from my existing eclipse installation.Everything is fine except one thing, Some of the imports having starting with this \"<strong>import org.eclipse.ui.internal.x;</strong>\"\\nare showing errors.</p>\\n\\n<p>When I searched for the package \"org.eclipse.ui.internal\" , I found that it exists in the jar \\n<strong>\"org.eclipse.ui.workbench_3.105.1.v-20130821-1411.jar\"</strong> .\\nAny idea what to do , is there a possibility that I \\'ve to manually update this jar and if that\\'s the case is how to do that?</p>\\n-Upgrading Eclipse 3.6.1 to eclipse 4.3.1-<eclipse><eclipse-plugin><upgrade><eclipse-kepler>',\n",
       " '<p>It is said in note that:</p>\\n\\n<blockquote>\\n  <p>Action required: Clients that rely on the provisional API or\\n  implementation in these bundles should migrate to the functionality\\n  and API provided by Equinox p2. Alternatively, products can install\\n  these bundles from the Eclipse 3.8 repository and they will continue\\n  to function in Eclipse 4.2-based applications.</p>\\n</blockquote>\\n\\n<p>The question is: how to accomplish that? I have added Indigo repository to <code>Install Software</code> list, but software is identified by human friendly names. How to find bundles there?</p>\\n\\n<pre><code>org.eclipse.update.core\\norg.eclipse.update.core.win32\\norg.eclipse.update.scheduler\\norg.eclipse.update.ui\\n</code></pre>\\n-How to install `org.eclipse.update.ui` in Kepler?-<eclipse><eclipse-indigo><eclipse-kepler>',\n",
       " '<p>I went through a small tutorial on creating GUIs with C++.\\n<a href=\"http://www.jose.it-berater.org/smfforum/index.php?PHPSESSID=1f234ca7a3e61aebeeaae8b8f73a7125&amp;topic=3389.msg14548#msg14548\" rel=\"nofollow\">http://www.jose.it-berater.org/smfforum/index.php?PHPSESSID=1f234ca7a3e61aebeeaae8b8f73a7125&amp;topic=3389.msg14548#msg14548</a></p>\\n\\n<p>When I tried it out with Eclipse Kepler CDT I only got lots of Errors shown in the Editor which all sounded same:\\nType \\'CALLBACK\\' could not be resolved.\\nType \\'HWND\\' could not be resolved.\\n[...]</p>\\n\\n<p>What have I done wrong? I have just copied the Sourcecode mentioned in this Tutorial...</p>\\n-Eclipse Kepler CDT - Symbol \\'HWND\\' could not be resolved-<c++><eclipse><eclipse-kepler>',\n",
       " '<p>I am using below self join query as Lookup override in informatica. This is running        fine in teradata.</p>\\n\\n<pre><code>SELECT A.region_cd AS REGION_CODE, \\n       A.enp_no    AS ENP_NBR, \\n       B.sla_cd    AS SLA_CODE \\nFROM   edb_man_work.emp A, \\n       edb_man_work.emp B \\nWHERE  A.company_no = Trim(Cast(B.enp_no AS INTEGER)) \\n       AND A.region_cd = B.region_cd \\n</code></pre>\\n\\n<p>This is running fine in teradata but while running in mapping it is giving error</p>\\n\\n<p>as Column SLA_CD is ambiguous.</p>\\n\\n<p>I am not sure why this is giving this type of error.</p>\\n-Ambiguous error in Lookup Override for teradata query-<sql><teradata><informatica-powercenter>',\n",
       " \"<p>I have created a procedure in different database than the user used to create it, \\nThe user I used to create the procedure is like 'vbhas' and the database it is created is like 'IFSRD', when I login as 'vbhas' and try to execute the proc it gives the error, could you please let me know how to modify the  execute access so that I will be able to execute this procedure successfully?\\nThanks</p>\\n-Permission issues while executing stored procedures in teradata-<sql><stored-procedures><teradata>\",\n",
       " \"<p>I'm trying to start with the <code>MonetDB.R</code> package but I struggle to setup a connection (<code>dbConnect</code>).\\nHere is a reproductible example</p>\\n\\n<pre><code>require(MonetDB.R)\\ndrv &lt;- dbDriver('MonetDB.R')\\ncon &lt;- dbConnect(drv, 'monetdb://localhost/demo')\\n## Process R aborted (core dumped) at Thu Dec 26 11:03:47 2013\\n## R: mapi.c:72: mapiConnect: Assertion `Rf_isInteger(port)' failed.\\n</code></pre>\\n\\n<p>Is there any trick to fix this error ?</p>\\n\\n<p>I'm using the developpement version of MonetDB (<code>Database: MonetDB v11.18.0 (unreleased)</code>) , the latest version of R (beta build, see sessionInfo below) and Arch Linux (64 bit, kernel 3.12.5-1) </p>\\n\\n<pre><code>## R version 3.0.2 Patched (2013-12-23 r64506)\\n## Platform: x86_64-unknown-linux-gnu (64-bit)\\n\\n## locale:\\n##  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \\n##  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \\n##  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \\n##  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \\n##  [9] LC_ADDRESS=C               LC_TELEPHONE=C            \\n## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \\n\\n## attached base packages:\\n## [1] stats     graphics  grDevices utils     datasets  methods  \\n## [7] base     \\n\\n## other attached packages:\\n## [1] MonetDB.R_0.8.1 digest_0.6.4    DBI_0.2-7      \\n\\n## loaded via a namespace (and not attached):\\n## [1] compiler_3.0.2 tools_3.0.2\\n</code></pre>\\n\\n<p>Thanks</p>\\n-Unable to connect R to MonetDB (MonetDB.R)-<r><monetdb>\",\n",
       " '<p>I want to sort my table in Teradata by months in a logical way so that it goes as <code>jan, feb, mar .......</code></p>\\n\\n<p>However when i try</p>\\n\\n<p><code>order by month_event</code></p>\\n\\n<p>I get <code>apr, aug ......</code> that is alphabetical</p>\\n\\n<p><strong>I know I can associate numbers 1 to 12 for each month, then sort and then get rid of that field. Is there any other more efficient way ?</strong></p>\\n\\n<p>How can i do this?</p>\\n-How to force logical sorting in teradata?-<sql><teradata>',\n",
       " '<p>How can I switch to the Web perspective in Eclipse Kepler, so that i can see the Servers tab in the console? Or is there any other way i can access servers tab in Eclipse Kepler?</p>\\n\\n<p>Please note that the Web Perspective is not available in Eclipse Kepler. Please  help me with this</p>\\n-Eclipse Kepler: switching to the Web perspective-<eclipse><eclipse-kepler>',\n",
       " '<p>Till now we are using myeclipse to for development.</p>\\n\\n<p>Now we started development using eclipse keplar.</p>\\n\\n<p>My first problem is jars are not deployed in webapps lib directory\\nwhich we solved by referring [link] <a href=\"https://stackoverflow.com/questions/5467038/adding-3rd-party-jars-to-web-inf-lib-automatically-using-eclipse-tomcat\">Adding 3rd party jars to WEB-INF/lib automatically using Eclipse/Tomcat</a></p>\\n\\n<p>but every time if want to add third party jar\\'s dependency then we have to add in build path and Deployment Assembly</p>\\n\\n<p>so please let me know if there is any workaround to add entry only in build path and this jar is automatically deployed in tomcat directory.</p>\\n\\n<p>Thanks in advance.</p>\\n-jars are not automatically deploy in tomcat using eclipse keplar-<java><eclipse><myeclipse><eclipse-kepler>',\n",
       " '<p>As per the solution provided in this <a href=\"https://stackoverflow.com/questions/16186089/hide-status-bar-or-progress-bar-in-eclipse\">SO thread</a> we can get back some screen real estate from Eclipse.</p>\\n\\n<p>However, when working with multiple editor tabs (e.g. in a quadrant layout) the ever present horizontal scrollbars (even when there is no text extending beyond viewing area!) are a waste of valuable coding space, not to mention, hideous.</p>\\n\\n<p>Have tried various CSS hacks in eclipse default_gtk.css (on Linux here) all to no avail; it appears that the ScrollBar widget is not modifiable via CSS.</p>\\n\\n<p>This <a href=\"https://bugs.eclipse.org/bugs/show_bug.cgi?id=420238\" rel=\"nofollow noreferrer\">bug thread</a> on saving space in Eclipse indicates something is in the works for Luna, but the developers seem split, some not caring about scrollbars or any wasted space (assuming everyone has huge monitors), and others preferring a clean and lean UI.</p>\\n\\n<p>Anyway, if anyone has leads for how to hide scrollbars in Eclipse >= 4.2, do chime in! It\\'s IMO the missing UI \"feature\" of Eclipse as you can strip out everything else, almost have max screen real estate available for code...just these damn scrollbars o_O</p>\\n-Hide Horizontal Scrollbars [Juno/Kepler/Luna]-<eclipse><scrollbar><hide><eclipse-juno><eclipse-kepler>',\n",
       " \"<p>I just migrated an older OSGi project to the current equinox version (Kepler SR1). When using the gogo console I encountered a problem when starting the gogo bundles with start level 1 (that's what I usually do with all relevant framework bundles). The gogo console won't startup though all four bundles are active and running. Typing help would result in a NullPointerException. The solution is to start all gogo bundles with the default start level. Did I miss anything or is this just a case of bad design of the bundle lifecycle? Bundles should not depend on start levels in order to work.</p>\\n\\n<p>Mike </p>\\n-Issues with start levels for gogo console bundles-<osgi><equinox><eclipse-kepler><gogo-shell>\",\n",
       " '<p>Loading the .tbl file I\\'ve got this error:</p>\\n\\n<pre><code>[nicola@localhost ~]$ mclient -d dbmonet  -s  \"COPY  INTO  monet.SUPPLIER  FROM  STDIN  USING  DELIMITERS \\',\\',\\'\\\\\\\\n\\',\\'\\\\\"\\'\" - &lt; /home/nicola/Scrivania/tabellemonetdb/supplier.tbl\\nuser(nicola):monetdb\\npassword:\\nmissing separator \\',\\' line 0 expecting 6 got 1  fields\\nfailed to import table\\ncurrent transaction is aborted (please ROLLBACK)\\nsyntax error, unexpected sqlINT in: \"0201\"\\n</code></pre>\\n\\n<p>Why do I get this error?\\nI\\'m using an ssb schema.</p>\\n-monetdb - error loading tbl-<database><loading><monetdb>',\n",
       " '<p>I have setup a <code>gradle</code> java multi-project in <code>Eclipse Kepler</code>. I have the build working great. The problem is, when I try to re-factor rename a class in one of the projects, the re-factor only occurs in that one project, and does not properly cascade to the other projects. My other projects become filled with compile errors due to the previously named class references.</p>\\n\\n<p>Is there a way to make this re-factoring work across gradle projects?</p>\\n-Re-factoring not working in eclipse Kepler with gradle plugin?-<eclipse-plugin><refactoring><gradle><eclipse-kepler>',\n",
       " \"<p>I am not an Eclipse/RAP developer, but over a year ago I was tasked with getting a particular application to run.  The development environment was Eclipse/RAP using Java.  The application was already almost done -- I just needed to make a few changes to get it to work the way we wanted it to work.  I made the changes, stuck it into the Jboss app, and it worked.  I saved away my source code.</p>\\n\\n<p>Since then they upgraded my PC, so I no longer have access to my old development environment.  We need to move the RAP application to another server, and for some reason it has quit working.  Either I don't understand why it ever worked or I don't understand why it doesn't work -- it's all a bit baffling.</p>\\n\\n<p>So now I'm trying to get this thing working again.</p>\\n\\n<p>The basic problem I haven't been able to resolve is dependencies.  Eclipse reports that the following three bundles can't be found:</p>\\n\\n<p>org.eclipse.rap.draw2d<br>\\norg.eclipse.rap.zest.core<br>\\norg.eclipse.rap.zest.layouts  </p>\\n\\n<p>All three should be in the GEF package.</p>\\n\\n<p>I have tried installing Eclipse Indigo.  When I do that, Eclipse can't find GEF to install it, even though it's given the same URL as I give to Kepler.  I've installed Eclipse Kepler.  I can install GEF, but while Eclipse reports a valid install, and reports that it is installed, I'm still seeing the same missing dependencies.</p>\\n\\n<p>Any ideas?  It's baffled an Eclipse developer here, but then we don't really use RAP except for this one application.</p>\\n\\n<p>Any help at all would be greatly appreciated.</p>\\n\\n<p>Sean.</p>\\n-Eclipse/RAP/GEF Indigo to Eclipse/RAP/GEF Kepler-<eclipse><eclipse-indigo><eclipse-kepler><eclipse-rap>\",\n",
       " '<p><br>\\nWhen I try to install ansible role, I see this exception.</p>\\n\\n<pre><code> $ ansible-galaxy install zzet.postgresql\\n Traceback (most recent call last):\\n File \"/Users/myHomeDir/.homebrew/Cellar/ansible/1.4.3/libexec/bin/ansible-galaxy\", line 34, in &lt;module&gt;\\n import yaml\\n ImportError: No module named yaml\\n</code></pre>\\n\\n<p>OS: Mac Os Maverick<br>\\nAnsible: 1.4.3</p>\\n\\n<p>Does anyone know how to fix it?</p>\\n-Ansible-galaxy throws ImportError: No module named yaml-<macos><ansible><ansible-galaxy>',\n",
       " '<p>I want to be able to recover my queries that I used previously. I was wondering if there is a way for me to get them using <code>sql assistant</code>.</p>\\n\\n<p>Is there any way I could do that?</p>\\n-Recovering queries with teradata?-<teradata>',\n",
       " '<p>Currently I have data in a table as shown below:</p>\\n\\n<pre><code>date        id   value\\n1-Jan-13    1    100\\n2-Jan-13    1    100\\n3-Jan-13    1    100\\n4-Jan-13    1    200\\n5-Jan-13    1    200\\n6-Jan-13    1    100\\n7-Jan-13    1    100\\n</code></pre>\\n\\n<p>I am trying to group the records based on the id and val and version records with startdate and end date .</p>\\n\\n<p>Desired output:</p>\\n\\n<pre><code>start date  end date    id   value\\n1-Jan-13    3-Jan-13    1    100\\n4-Jan-13    5-Jan-13    1    200\\n6-Jan-13    7-Jan-13    1    100\\n</code></pre>\\n-group a set of records by date in teradata-<sql><date><grouping><teradata>',\n",
       " '<p>I´m trying to connect R and Teradata using RJDBC. </p>\\n\\n<p>I´ve found this <a href=\"http://www.rforge.net/RJDBC/\" rel=\"nofollow noreferrer\">link</a> that has an example using mysql, but i´m nos sure how to do the same with teradata.</p>\\n\\n<pre><code>library(RJDBC)\\ndrv &lt;- JDBC(\"com.mysql.jdbc.Driver\",\\n           \"/etc/jdbc/mysql-connector-java-3.1.14-bin.jar\",\\n           identifier.quote=\"`\")\\nconn &lt;- dbConnect(drv, \"jdbc:mysql://localhost/test\", \"user\", \"pwd\")\\n</code></pre>\\n\\n<p>I´ve downloaded this driver: \\n<a href=\"http://downloads.teradata.com/download/connectivity/jdbc-driver\" rel=\"nofollow noreferrer\">http://downloads.teradata.com/download/connectivity/jdbc-driver</a>\\nBut i´m not sure where i should reference the directory.</p>\\n\\n<p>I know there is a teradataR package <a href=\"https://github.com/nonsleepr/teradataR\" rel=\"nofollow noreferrer\">out there</a>, but i don´t know if it really works with the R 3.0.0.</p>\\n\\n<p>For the time being i´m just interesting in pulling data out of the database. Something as simple as <code>SELECT * FROM table</code>. The problem is RODBC is very slow...</p>\\n\\n<p>Are there other options for doing this task?</p>\\n-Connect R and Teradata using JDBC-<r><teradata><rodbc><rjdbc>',\n",
       " '<p>I am trying to import a Teradata table from SAS. The teradata table has 21 digit surrogate keys.\\nWhen i import it from SAS, the surrogate key column gets imported in the form 2.011E12145, however its actual value is a 21 digit number.\\nI tried the following code</p>\\n\\n<pre><code>Data chk;\\nformat p_key $30.;\\nset chk;\\np_key=surrogate;\\nrun;\\n</code></pre>\\n\\n<p>But this did not work?</p>\\n\\n<p>How to avoid this situation and import it in numeric form?</p>\\n-Importing Teradata table with surrogate key in sas-<sas><teradata>',\n",
       " '<p>I am using the recursive function below which will loop for more than 4500 times. In each iteration the code will extract more than 50 records.</p>\\n\\n<pre><code>WITH RECURSIVE RECEMP (\\n    EMP_ID,\\n    EMP_DB,\\n    lvl,\\n    LEVEL\\n    )\\nAS (\\n    SELECT EMP_ID,\\n        EMP_DB,\\n        lvl,\\n        1 (INT)\\n    FROM EDW_MAN_WORK.emp --WHERE EMP_ID = 12\\n\\n    UNION ALL\\n\\n    SELECT E.EMP_ID,\\n        E.EMP_DB,\\n        E.lvl,\\n        R.LEVEL + 1\\n    FROM EDW_MAN_WORK.emp E\\n    INNER JOIN RECEMP R\\n        ON E.EMP_ID = R.EMP_ID\\n            AND R.LEVEL &lt;= r.lvl\\n    )\\nSELECT EMP_ID,\\n    EMP_DB,\\n    Lvl,\\n    LEVEL\\nFROM RECEMP\\n</code></pre>\\n\\n<p>My query is failing because of a SPOOL space error. Can we collect stats on recursive table?</p>\\n\\n<p>I have tried this with a stored procedure as well, but again this is very time consuming.</p>\\n\\n<p>Please suggest an alternative approach.</p>\\n-Alternative for a stored procedure for recursion in teradata-<sql><teradata>',\n",
       " '<p>I want to change the <code>INTEGEr</code> field in my table to <code>VARCHAR</code>.\\nHow can I do this?</p>\\n\\n<p>Do I have to create a new table?</p>\\n-Is there any way to change datatype in teradata?-<teradata>',\n",
       " '<p>I am debugging some Java code and came into a situation where I reached a method declared for simplicity sake as: Method1(Var1 var1, Var2, var2).</p>\\n\\n<p>I am certain that the previous class I was in that called this method passed in no arguments and am struggling to find where these parameters are being created and passed from.</p>\\n\\n<p>Is there a short-cut or way in eclipse for me to go to where an argument is passed from when debugging? I have gone over all the cheat sheets but cannot seem to find anything that does this. </p>\\n\\n<p>The thing I want to be able to find is the values being assigned to the fields in var1 and var2. </p>\\n-Debugging in Eclipse find where argument created-<eclipse><debugging><eclipse-kepler>',\n",
       " \"<p>I have an excel file with data where the columns are the samples and the rows the attributes. I can't transpose the data, because the rows exceed the maximum number of columns. When I load this data into rapidminer, it automatically sets the columns as attributes and the rows as samples. </p>\\n\\n<p>How can I set the columns as samples and the rows as attributes?</p>\\n-How can I set the rows to be the attributes and columns the samples in rapidminer?-<rapidminer>\",\n",
       " '<p>I have a query (that I cannot modify) that starts like this</p>\\n\\n<pre><code>with CodeSet (\\n   code_context_c\\n , bom_index_c\\n , src_qs_c\\n , src_code_set_c \\n , src_code_set_x \\n , src_code_value_c\\n , src_code_value_x\\n , tgt_code_set_c\\n , tgt_code_value_c\\n  ) as (\\nSELECT ...\\n</code></pre>\\n\\n<p>and then goes on. Now I need to use it as a subquery and do something like</p>\\n\\n<pre><code>select * from (with CodeSet (\\n       code_context_c\\n     , bom_index_c\\n     , src_qs_c\\n     , src_code_set_c \\n     , src_code_set_x \\n     , src_code_value_c\\n     , src_code_value_x\\n     , tgt_code_set_c\\n     , tgt_code_value_c\\n      ) as (\\n    SELECT ...\\n</code></pre>\\n\\n<p>but Teradata does not like it... Anyone has seen this before? Changing the query would require some time and I would prefer not to. Anyone can help me out here? </p>\\n\\n<p>Error message is:</p>\\n\\n<p>SELECT Failed.  [3707] Syntax error, expected something like a name or a Unicode delimited identifier or \\'(\\' between the \\'from\\' keyword and the \\'as\\' keyword.</p>\\n\\n<p>Thanks in advance,\\nUmberto</p>\\n-subquery that starts with \"with\" in teradata is not working-<teradata>',\n",
       " \"<p>Using Eclipse 4.3.1 (Kepler) on Ubuntu, I find that each time I exit and reenter Eclipse, I have to respecify my external source attachments. What is the key to making Eclipse remember them?</p>\\n<ol>\\n<li><p>Open the Source Attachment Configuration dialog by clicking on the &quot;Change Attached Source&quot; button in the Class File Editor; or open the similar Java Source Attachment dialog by right-clicking on the containing jar in Package Explorer and selecting &quot;Properties&quot;.</p>\\n<p>I've tried both.</p>\\n</li>\\n<li><p>In the dialog, click on the &quot;External File&quot; button and navigate to a jar containing sources, or click on the &quot;External Folder&quot; button and navigate to a directory containing .java files in subdirectories corresponding to the package hierarchy.</p>\\n</li>\\n</ol>\\n<p>All these ways work until I quit Eclipse and come back in.</p>\\n-Eclipse does not remember external source attachments-<eclipse><ubuntu><eclipse-kepler>\",\n",
       " '<p>I am a newbie starting with <strong>Java EE</strong> and <strong>GlassFish</strong>, in <strong>GNU/Linux Debian 7</strong>, using <strong>Eclipse Kepler</strong> as IDE.</p>\\n\\n<p>I have a problem when I try to install a new server in Eclipse. The steps I follow are:</p>\\n\\n<ul>\\n<li>Click on Eclipse\\'s link \"No server are available. Click this link to\\ncreate a new server...\" on Servers tab.</li>\\n<li>Select Glassfish>Glassfish 4.0 and click on next button. </li>\\n<li>Select JDK and Glassfish Server Directory, then click on next button\\nonce more.</li>\\n<li><p>Then an error message is appeared on the top of the context window:</p>\\n\\n<p>\"<strong><em>/opt/glassfish4/glassfish/domains/domain1/config/domain.xml is not a valid domain (no domain.xml)</em></strong>\"</p></li>\\n</ul>\\n\\n<p><a href=\"http://oi43.tinypic.com/eivakp.jpg\" rel=\"nofollow\">http://oi43.tinypic.com/eivakp.jpg</a></p>\\n\\n<p>I do not have much experience, specially in GNU/Linux but this is for auto-educational purpose, so I will be glad if your are so kind to help me to solve this situation :S</p>\\n\\n<p>Any doubt I will try to clear it as soon as posible.</p>\\n\\n<p>Thank you very much :)</p>\\n-glassfish4/glassfish/domains/domain1/config/domain.xml is not a valid domain (no domain.xml)-<eclipse><jakarta-ee><glassfish><debian><eclipse-kepler>',\n",
       " '<p>Essentially, what i want to do is set up the initial conditions for an n-body simulation of a galaxy. The paper i tried to go by is found here <a href=\"http://arxiv.org/abs/1204.0513\" rel=\"nofollow\">http://arxiv.org/abs/1204.0513</a> .</p>\\n\\n<p>The paper describes two density functions:</p>\\n\\n<ul>\\n<li><p>a surface density function of form rho(r) for density at distance r from the center</p></li>\\n<li><p>and a density function of form rho(r, z) with z being the z component in (x,y,z) coordinates.</p></li>\\n</ul>\\n\\n<p>What i did so far for each particle (star) was:</p>\\n\\n<ul>\\n<li>use a monte carlo method with the first function to get a distance from the galaxy center</li>\\n<li>use a monte carlo method with the second function to get a z component</li>\\n<li>generate x,y coords randomly on a circle of radius r, at height z</li>\\n</ul>\\n\\n<p>But perhaps the code for this would be more helpful:</p>\\n\\n<pre><code>/* generate 3 random numbers x,y,z having a domain from 0 to 1 (not including 0.0) */\\n\\n// The distance from the galaxy center for the current star.\\n// Rs is a constant parameter in the surface density function.\\nfloat r = -Rs * log(1.0f - x);\\n\\n// The z coordinate for the current star\\nfloat Sz = -0.5f * (0.1f * Rs) * log(-((z-1)/z));\\n\\n// The x,y coordinates for the current star\\nfloat Sx = sqrt(r*r - Sz*Sz) * cos(2.0f * PI * y);\\nfloat Sy = sqrt(r*r - Sz*Sz) * sin(2.0f * PI * y);\\n</code></pre>\\n\\n<p>Visually, this produces the expected shape, until the obvious situation arises, namely if the generated Sz value is higher than the generated r value.</p>\\n\\n<p>So my question is firstly if i\\'m even on the right track to begin with, and if so if anyone can either suggest a correction mechanism for the case i described above or an alternate method of generating these coordinates.</p>\\n-Distributing particles over a volume starting from a couple of density functions-<distribution><volume><galaxy><montecarlo>',\n",
       " '<p>I have been trying to establish connection to teradata. </p>\\n\\n<p>Since there is already a <code>odbc</code> on my computer, for Java the string looks like</p>\\n\\n<p><code>String connURL = \"jdbc:odbc:DatabaseName\" // and it works!</code></p>\\n\\n<hr>\\n\\n<p>I need to establish the connection in <strong>c#</strong>.\\nSo I tried</p>\\n\\n<p><code>TdConnection cn = new TdConnection (\"Data Source = odbc:DatabaseName; User ID = xxx; Password = ooo;\")</code></p>\\n\\n<p>But it did not accept this string.\\nWhat is the correct way to write the <code>Data Source</code> ? What is the <code>jdbc</code> counterpart in <strong>c#</strong> ?</p>\\n-How to get the correct connection string for c# to teradata?-<c#><teradata>',\n",
       " \"<p>I'm trying to use shared_ptr and for some reason it dosen't work, obviously I wrote -std=c++0x and <strong>__GXX_EXPERIMENTAL_CXX0X__</strong> in the right places,\\nand yet I get the error:</p>\\n\\n<blockquote>\\n  <p>Description   Resource    Path    Location    Type</p>\\n  \\n  <p>Symbol 'shared_ptr' could not be resolved server.h    \\u202a/ex4\\u202c  line 16 Semantic Error</p>\\n</blockquote>\\n\\n<p>Does anyone have an idea what I'm doing wrong?</p>\\n-I can't use std::shared_ptr-<c++><eclipse><c++11><eclipse-kepler>\",\n",
       " \"<p>Since using Eclipse Kepler (with PDT) whenever I create a new PHP project, using an existing source, my 'public' folder is excluded from the Explorer view. Its not that the folder wasn't included during the Project creation though, because if I try to add it later , as a Source folder, it says it already exists!?</p>\\n\\n<p>I've used Eclipse with PDT since E3.7 and can't work out whats going on here. Any ideas fellow Eclipse users?</p>\\n-Eclipse Kepler with PDT hides my public folder-<php><eclipse><eclipse-kepler><eclipse-pdt>\",\n",
       " '<p>I used to share Eclipse CDT perspectives across machines and workspaces of the same machine by exporting preferences to a file and then importing it where needed.</p>\\n\\n<p>It worked in Juno version but now when I am doing the same in Kepler the perspective simply is not listed though it appears in:</p>\\n\\n<pre><code>org.eclipse.e4.workbench/workbench.xmi\\norg.eclipse.core.runtime/.settings/org.eclipse.ui.workbench.prefs\\norg.eclipse.core.runtime/.settings/org.eclipse.ui.prefs\\norg.eclipse.core.runtime/.settings/org.eclipse.debug.ui.prefs\\n</code></pre>\\n\\n<p>How can I debug this? Any logs to check?</p>\\n-Cannot import perspective in Eclipse Kepler-<eclipse><ide><eclipse-juno><eclipse-cdt><eclipse-kepler>',\n",
       " '<p>I\\'ve been having issues inserting / updating a period column.</p>\\n\\n<p>According to a sample from the Teradata website located <a href=\"http://developer.teradata.com/doc/connectivity/jdbc/reference/current/samp/T22007JD.java.txt\" rel=\"nofollow\">here</a>, you should be able to create your own java.sql.Struct implmentation which you then provide as a parameter.  I\\'ve done this, and once I actually hit the database, Weblogic throws an error stating my implementation of the Struct failed to be loaded (<code>ClassNotFoundException</code>)</p>\\n\\n<p>Do I have to I have to inject this into the Weblogic class loader somehow?</p>\\n\\n<p>Any help would be much appreciated!</p>\\n-Weblogic Inserting Teradata PERIOD datatype-<java><weblogic><teradata>',\n",
       " '<p>I am trying to connect to Teradata with c#. I am using the sample code from <a href=\"http://developer.teradata.com/doc/connectivity/tdnetdp/13.11/webhelp/DevelopingNetDataProviderforTeradataApplications.html\">this website</a> </p>\\n\\n<pre><code>using System;\\nusing System.Collections.Generic;\\nusing System.Text;\\nusing Teradata.Client.Provider;\\n\\nnamespace Teradata.Client.Provider.HelloWorld\\n{\\n    class HelloWorld\\n    {\\n        static void Main(string[] args)\\n        {\\n            using (TdConnection cn = new TdConnection(\"Data Source = x;User ID = y;Password = z;\"))\\n            {\\n                cn.Open();\\n                TdCommand cmd = cn.CreateCommand();\\n                cmd.CommandText = \"SELECT DATE\";\\n                using (TdDataReader reader = cmd.ExecuteReader())\\n                {\\n                    reader.Read();\\n                    DateTime date = reader.GetDate(0);\\n                    Console.WriteLine(\"Teradata Database DATE is {0}\", date);\\n                 }\\n             }\\n         }\\n    }\\n}\\n</code></pre>\\n\\n<p>(I have also tried <code>DSN , UID , PWD</code> \\nHowever, I am getting exception that either my <strong>userid , account or password not correct</strong> ...\\nBut I am able to login using SQL Assistant easily. So , I rule out incorrect userid or password </p>\\n\\n<p><a href=\"http://forums.teradata.com/forum/connectivity/net-connection-string-with-ldap\">Here I found a possible solution for my problem</a>\\nBut I do not know what exactly I need to change in my sample code.</p>\\n\\n<p>So, I have no idea how to implement that solution.</p>\\n\\n<p>Can anybody give me a working sample code?</p>\\n-How to set up .net teradata connection in c#?-<c#><teradata>',\n",
       " '<p>I installed Eclipse kepler recently and I am having trouble installing new softwear. When I try to reload the repository site <a href=\"http://download.eclipse.org/releases/kepler\" rel=\"nofollow\">http://download.eclipse.org/releases/kepler</a> I get an error every time:</p>\\n\\n<pre><code>Unable to read repository at http://download.eclipse.org/releases/kepler.\\nUnable to read repository at http://download.eclipse.org/releases/kepler.\\nUnable to read repository at http://download.eclipse.org/releases/kepler/201309270900/content.jar.\\nRead timed out\\n</code></pre>\\n\\n<p>I have tried every solution mentioned on this site, but nothing works for me.</p>\\n\\n<p>Help me please, I really need this to work!!</p>\\n-Error eclipse kepler when trying to install new software-<eclipse><eclipse-kepler>',\n",
       " '<p>I made a process which reads a file, then makes document of it, then tokenizes it by regular expressions. Preview mode of tokenizer highlights found patterns correctly.</p>\\n\\n<p>Now can I just get a list of found patterns on my screen?</p>\\n\\n<p><img src=\"https://i.stack.imgur.com/qu0q3.jpg\" alt=\"enter image description here\"></p>\\n-Can I just show the list of found tokens in RapidMiner?-<tokenize><rapidminer>',\n",
       " '<p>I\\'m new to ANTLR and trying to write grammar in ANTLR4 without any prior brush with the previous version. I\\'m following the book \\'<em>The Definitive ANTLR 4 Reference</em>\\'. I use Eclipse and installed ANTLR4 IDE as given in <a href=\"https://github.com/jknack/antlr4ide\" rel=\"noreferrer\">here</a>. I wrote the following grammar in Expr.g4:</p>\\n\\n<pre><code>grammar Expr;\\n\\nimport Common;\\n\\noptions{\\nlanguage = Java;\\n}\\nprog: stat+;\\n\\nstat: expr NEWLINE\\n    | ID \\'=\\' expr NEWLINE\\n    | NEWLINE;\\n\\nexpr: expr (\\'/\\'|\\'*\\') expr\\n    | expr (\\'+\\'|\\'-\\') expr\\n    | INT\\n    | ID\\n    | \\'(\\'expr\\')\\';\\n</code></pre>\\n\\n<p>The Common.g4 contains the following:</p>\\n\\n<pre><code>lexer grammar Common;\\n\\nID: [A-Za-z]+;\\nINT: [0-9]+;\\nNEWLINE: \\'\\\\r\\'?\\'\\\\n\\';\\nWS: [\\\\t]+ -&gt; skip;\\n</code></pre>\\n\\n<p>The lexer.java was created but not parser.java and visitor.java and other base file. Please help me fix the problem. Thanks in advance.</p>\\n-ANTLR4 Parser, Visitor not created-<java><parsing><antlr4><eclipse-kepler>',\n",
       " '<p>I tried to use <code>Tokenize</code> to search for IP addresses with port number</p>\\n\\n<p><img src=\"https://i.stack.imgur.com/t4HEW.jpg\" alt=\"enter image description here\"></p>\\n\\n<p>but apparently it uses regexp to define token delimiter, i.e. it returns text BETWEEN ip addresses. Is it possible to retrieve addresses themselves?</p>\\n\\n<p><strong>UPDATE</strong></p>\\n\\n<p>I have one big textual log file as input. It has rows, rows contain some messages about IP addresses. I would like to take all addresses an group them counting.</p>\\n\\n<p>Is it possible with RapidMiner?</p>\\n-How to grab regex matches in RapidMiner?-<regex><tokenize><rapidminer>',\n",
       " '<p>When I try to compare a current file in Eclipse Kepler with an older revision I always get the \\nmessage that there are no difference (which is not true) followed by the error reporting screen saying:</p>\\n\\n<pre><code>&gt; SVN: \\'0x00400103: Compare with Revision\\' operation finished with\\n&gt; error: null java.lang.NullPointerException\\n</code></pre>\\n\\n<p>I actually switched from Subclipse to Subversive as Comparing was much more advanced there but after migrating eclipse from on computer to the other (which involed relinking my projects to there repos) I cannot use Compare anymore.</p>\\n\\n<p>The section in the <code>.metadata/.logs</code> says:</p>\\n\\n<pre><code>!ENTRY org.eclipse.team.svn.core.svnnature 4 0 2014-01-17 11:09:58.870\\n!MESSAGE SVN: \\'0x00400103: Compare with Revision\\' operation finished with error\\n!SUBENTRY 1 org.eclipse.team.svn.core.svnnature 4 0 2014-01-17 11:09:58.870\\n!MESSAGE SVN: \\'0x00400103: Compare with Revision\\' operation finished with error: null\\n!STACK 0\\njava.lang.NullPointerException\\n    at org.tmatesoft.svn.core.internal.wc2.ng.SvnNgDiffSummarize.doDiffReposRepos(SvnNgDiffSummarize.java:229)\\n    at org.tmatesoft.svn.core.internal.wc2.ng.SvnNgDiffSummarize.doDiff(SvnNgDiffSummarize.java:86)\\n    at org.tmatesoft.svn.core.internal.wc2.ng.SvnNgDiffSummarize.run(SvnNgDiffSummarize.java:61)\\n    at org.tmatesoft.svn.core.internal.wc2.ng.SvnNgDiffSummarize.run(SvnNgDiffSummarize.java:1)\\n    at org.tmatesoft.svn.core.internal.wc2.ng.SvnNgOperationRunner.run(SvnNgOperationRunner.java:20)\\n    at org.tmatesoft.svn.core.internal.wc2.SvnOperationRunner.run(SvnOperationRunner.java:20)\\n    at org.tmatesoft.svn.core.wc2.SvnOperationFactory.run(SvnOperationFactory.java:1149)\\n    at org.tmatesoft.svn.core.wc2.SvnOperation.run(SvnOperation.java:294)\\n    at org.tmatesoft.svn.core.javahl17.SVNClientImpl.diffSummarize(SVNClientImpl.java:1036)\\n    at org.polarion.team.svn.connector.svnkit.SVNKitConnector.diffStatus(SVNKitConnector.java:1599)\\n    at org.eclipse.team.svn.core.extension.factory.ThreadNameModifier.diffStatus(ThreadNameModifier.java:158)\\n    at org.eclipse.team.svn.core.utility.SVNUtility.diffStatus(SVNUtility.java:318)\\n    at org.eclipse.team.svn.ui.operation.CompareResourcesInternalOperation$3.run(CompareResourcesInternalOperation.java:148)\\n    at org.eclipse.team.svn.core.utility.ProgressMonitorUtility.doSubTask(ProgressMonitorUtility.java:118)\\n    at org.eclipse.team.svn.core.operation.AbstractActionOperation.protectStep(AbstractActionOperation.java:156)\\n    at org.eclipse.team.svn.ui.operation.CompareResourcesInternalOperation.runImpl(CompareResourcesInternalOperation.java:138)\\n    at org.eclipse.team.svn.ui.operation.CompareResourcesOperation$1.runImpl(CompareResourcesOperation.java:64)\\n    at org.eclipse.team.svn.core.operation.AbstractActionOperation.run(AbstractActionOperation.java:82)\\n    at org.eclipse.team.svn.core.utility.ProgressMonitorUtility.doTask(ProgressMonitorUtility.java:104)\\n    at org.eclipse.team.svn.core.operation.CompositeOperation.runImpl(CompositeOperation.java:99)\\n    at org.eclipse.team.svn.core.operation.AbstractActionOperation.run(AbstractActionOperation.java:82)\\n    at org.eclipse.team.svn.core.utility.ProgressMonitorUtility.doTask(ProgressMonitorUtility.java:104)\\n    at org.eclipse.team.svn.core.operation.CompositeOperation.runImpl(CompositeOperation.java:99)\\n    at org.eclipse.team.svn.core.operation.AbstractActionOperation.run(AbstractActionOperation.java:82)\\n    at org.eclipse.team.svn.core.operation.LoggedOperation.run(LoggedOperation.java:40)\\n    at org.eclipse.team.svn.core.utility.ProgressMonitorUtility.doTask(ProgressMonitorUtility.java:104)\\n    at org.eclipse.team.svn.core.utility.ProgressMonitorUtility.doTaskExternal(ProgressMonitorUtility.java:90)\\n    at org.eclipse.team.svn.ui.utility.DefaultCancellableOperationWrapper.run(DefaultCancellableOperationWrapper.java:55)\\n    at org.eclipse.team.svn.ui.utility.SVNTeamOperationWrapper.run(SVNTeamOperationWrapper.java:35)\\n    at org.eclipse.team.internal.ui.actions.JobRunnableContext.run(JobRunnableContext.java:144)\\n    at org.eclipse.team.internal.ui.actions.JobRunnableContext$ResourceJob.runInWorkspace(JobRunnableContext.java:72)\\n    at org.eclipse.core.internal.resources.InternalWorkspaceJob.run(InternalWorkspaceJob.java:38)\\n    at org.eclipse.core.internal.jobs.Worker.run(Worker.java:53)\\n</code></pre>\\n-Subversive \"Compare with Revision \"always fails in Eclipse Kepler-<eclipse><svn><subversive><eclipse-kepler>',\n",
       " '<p>I tried searching online but was unable to find anything pertaining to my requirements.\\nI am new to Teradata.</p>\\n\\n<p>In our team Teradata jobs are used to call the ksh which in turn calls the procedure to run at a scheduled time.</p>\\n\\n<p>I want to understand how exactly does this calling works? How does a job call a KSH and then how does a KSH call a procedure in turn.</p>\\n\\n<p>Your help would be much appreciated.</p>\\n-Teradata Jobs and KSH-<ksh><teradata><jobs>',\n",
       " \"<p>I have RCP Application based on compatibility layer.\\nIn that i have one <strong>Part Stack</strong>.\\nWhen I try to minimize that <strong>PartStack</strong> with out adding any Part in that I'm getting following Exception.</p>\\n\\n<pre><code>!ENTRY org.eclipse.equinox.event 4 0 2014-01-21 16:47:09.020\\n!MESSAGE Exception while dispatching event org.osgi.service.event.Event [topic=org/eclipse/e4/ui/model/application/ApplicationElement/tags/REMOVE] to handler org.eclipse.e4.ui.services.internal.events.UIEventHandler@3b6297\\n!STACK 0\\njava.lang.NullPointerException\\n    at org.eclipse.e4.ui.workbench.addons.minmax.MinMaxAddon.restore(MinMaxAddon.java:580)\\n    at org.eclipse.e4.ui.workbench.addons.minmax.MinMaxAddon$8.handleEvent(MinMaxAddon.java:395)\\n    at org.eclipse.e4.ui.services.internal.events.UIEventHandler$1.run(UIEventHandler.java:41)\\n    at org.eclipse.swt.widgets.Synchronizer.syncExec(Synchronizer.java:180)\\n    at org.eclipse.ui.internal.UISynchronizer.syncExec(UISynchronizer.java:150)\\n    at org.eclipse.swt.widgets.Display.syncExec(Display.java:4688)\\n    at org.eclipse.e4.ui.internal.workbench.swt.E4Application$1.syncExec(E4Application.java:205)\\n    at org.eclipse.e4.ui.services.internal.events.UIEventHandler.handleEvent(UIEventHandler.java:38)\\n    at org.eclipse.equinox.internal.event.EventHandlerWrapper.handleEvent(EventHandlerWrapper.java:197)\\n    at org.eclipse.equinox.internal.event.EventHandlerTracker.dispatchEvent(EventHandlerTracker.java:197)\\n    at org.eclipse.equinox.internal.event.EventHandlerTracker.dispatchEvent(EventHandlerTracker.java:1)\\n    at org.eclipse.osgi.framework.eventmgr.EventManager.dispatchEvent(EventManager.java:230)\\n    at org.eclipse.osgi.framework.eventmgr.ListenerQueue.dispatchEventSynchronous(ListenerQueue.java:148)\\n    at org.eclipse.equinox.internal.event.EventAdminImpl.dispatchEvent(EventAdminImpl.java:135)\\n    at org.eclipse.equinox.internal.event.EventAdminImpl.sendEvent(EventAdminImpl.java:78)\\n    at org.eclipse.equinox.internal.event.EventComponent.sendEvent(EventComponent.java:39)\\n    at org.eclipse.e4.ui.services.internal.events.EventBroker.send(EventBroker.java:80)\\n    at org.eclipse.e4.ui.internal.workbench.UIEventPublisher.notifyChanged(UIEventPublisher.java:58)\\n    at org.eclipse.emf.common.notify.impl.BasicNotifierImpl.eNotify(BasicNotifierImpl.java:374)\\n    at org.eclipse.emf.ecore.util.EcoreEList.dispatchNotification(EcoreEList.java:249)\\n    at org.eclipse.emf.common.notify.impl.NotifyingListImpl.remove(NotifyingListImpl.java:725)\\n    at org.eclipse.emf.common.util.AbstractEList.remove(AbstractEList.java:460)\\n    at org.eclipse.e4.ui.workbench.addons.minmax.TrimStack.restoreStack(TrimStack.java:778)\\n    at org.eclipse.e4.ui.workbench.addons.minmax.TrimStack.updateTrimStackItems(TrimStack.java:754)\\n    at org.eclipse.e4.ui.workbench.addons.minmax.TrimStack.createWidget(TrimStack.java:538)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\\n    at java.lang.reflect.Method.invoke(Method.java:597)\\n    at org.eclipse.e4.core.internal.di.MethodRequestor.execute(MethodRequestor.java:56)\\n    at org.eclipse.e4.core.internal.di.InjectorImpl.processAnnotated(InjectorImpl.java:877)\\n    at org.eclipse.e4.core.internal.di.InjectorImpl.inject(InjectorImpl.java:119)\\n    at org.eclipse.e4.core.internal.di.InjectorImpl.internalMake(InjectorImpl.java:333)\\n    at org.eclipse.e4.core.internal.di.InjectorImpl.make(InjectorImpl.java:267)\\n    at org.eclipse.e4.core.contexts.ContextInjectionFactory.make(ContextInjectionFactory.java:186)\\n    at org.eclipse.e4.ui.internal.workbench.ReflectionContributionFactory.createFromBundle(ReflectionContributionFactory.java:105)\\n    at org.eclipse.e4.ui.internal.workbench.ReflectionContributionFactory.doCreate(ReflectionContributionFactory.java:71)\\n    at org.eclipse.e4.ui.internal.workbench.ReflectionContributionFactory.create(ReflectionContributionFactory.java:49)\\n    at org.eclipse.e4.ui.workbench.renderers.swt.ToolControlRenderer.createWidget(ToolControlRenderer.java:75)\\n    at org.eclipse.e4.ui.internal.workbench.swt.PartRenderingEngine.createWidget(PartRenderingEngine.java:949)\\n    at org.eclipse.e4.ui.internal.workbench.swt.PartRenderingEngine.safeCreateGui(PartRenderingEngine.java:633)\\n    at org.eclipse.e4.ui.internal.workbench.swt.PartRenderingEngine.safeCreateGui(PartRenderingEngine.java:735)\\n    at org.eclipse.e4.ui.internal.workbench.swt.PartRenderingEngine.access$2(PartRenderingEngine.java:706)\\n    at org.eclipse.e4.ui.internal.workbench.swt.PartRenderingEngine$7.run(PartRenderingEngine.java:700)\\n    at org.eclipse.core.runtime.SafeRunner.run(SafeRunner.java:42)\\n    at org.eclipse.e4.ui.internal.workbench.swt.PartRenderingEngine.createGui(PartRenderingEngine.java:685)\\n    at org.eclipse.e4.ui.internal.workbench.swt.PartRenderingEngine$4.handleEvent(PartRenderingEngine.java:279)\\n    at org.eclipse.e4.ui.services.internal.events.UIEventHandler$1.run(UIEventHandler.java:41)\\n    at org.eclipse.swt.widgets.Synchronizer.syncExec(Synchronizer.java:180)\\n    at org.eclipse.ui.internal.UISynchronizer.syncExec(UISynchronizer.java:150)\\n    at org.eclipse.swt.widgets.Display.syncExec(Display.java:4688)\\n    at org.eclipse.e4.ui.internal.workbench.swt.E4Application$1.syncExec(E4Application.java:205)\\n    at org.eclipse.e4.ui.services.internal.events.UIEventHandler.handleEvent(UIEventHandler.java:38)\\n    at org.eclipse.equinox.internal.event.EventHandlerWrapper.handleEvent(EventHandlerWrapper.java:197)\\n    at org.eclipse.equinox.internal.event.EventHandlerTracker.dispatchEvent(EventHandlerTracker.java:197)\\n    at org.eclipse.equinox.internal.event.EventHandlerTracker.dispatchEvent(EventHandlerTracker.java:1)\\n    at org.eclipse.osgi.framework.eventmgr.EventManager.dispatchEvent(EventManager.java:230)\\n    at org.eclipse.osgi.framework.eventmgr.ListenerQueue.dispatchEventSynchronous(ListenerQueue.java:148)\\n    at org.eclipse.equinox.internal.event.EventAdminImpl.dispatchEvent(EventAdminImpl.java:135)\\n    at org.eclipse.equinox.internal.event.EventAdminImpl.sendEvent(EventAdminImpl.java:78)\\n    at org.eclipse.equinox.internal.event.EventComponent.sendEvent(EventComponent.java:39)\\n    at org.eclipse.e4.ui.services.internal.events.EventBroker.send(EventBroker.java:80)\\n    at org.eclipse.e4.ui.internal.workbench.UIEventPublisher.notifyChanged(UIEventPublisher.java:58)\\n    at org.eclipse.emf.common.notify.impl.BasicNotifierImpl.eNotify(BasicNotifierImpl.java:374)\\n    at org.eclipse.emf.ecore.util.EcoreEList.dispatchNotification(EcoreEList.java:249)\\n    at org.eclipse.emf.common.notify.impl.NotifyingListImpl.addUnique(NotifyingListImpl.java:294)\\n    at org.eclipse.emf.common.util.AbstractEList.add(AbstractEList.java:301)\\n    at org.eclipse.e4.ui.workbench.addons.minmax.MinMaxAddon.createTrim(MinMaxAddon.java:782)\\n    at org.eclipse.e4.ui.workbench.addons.minmax.MinMaxAddon.minimize(MinMaxAddon.java:558)\\n    at org.eclipse.e4.ui.workbench.addons.minmax.MinMaxAddon$8.handleEvent(MinMaxAddon.java:389)\\n    at org.eclipse.e4.ui.services.internal.events.UIEventHandler$1.run(UIEventHandler.java:41)\\n    at org.eclipse.swt.widgets.Synchronizer.syncExec(Synchronizer.java:180)\\n    at org.eclipse.ui.internal.UISynchronizer.syncExec(UISynchronizer.java:150)\\n    at org.eclipse.swt.widgets.Display.syncExec(Display.java:4688)\\n    at org.eclipse.e4.ui.internal.workbench.swt.E4Application$1.syncExec(E4Application.java:205)\\n    at org.eclipse.e4.ui.services.internal.events.UIEventHandler.handleEvent(UIEventHandler.java:38)\\n    at org.eclipse.equinox.internal.event.EventHandlerWrapper.handleEvent(EventHandlerWrapper.java:197)\\n    at org.eclipse.equinox.internal.event.EventHandlerTracker.dispatchEvent(EventHandlerTracker.java:197)\\n    at org.eclipse.equinox.internal.event.EventHandlerTracker.dispatchEvent(EventHandlerTracker.java:1)\\n    at org.eclipse.osgi.framework.eventmgr.EventManager.dispatchEvent(EventManager.java:230)\\n    at org.eclipse.osgi.framework.eventmgr.ListenerQueue.dispatchEventSynchronous(ListenerQueue.java:148)\\n    at org.eclipse.equinox.internal.event.EventAdminImpl.dispatchEvent(EventAdminImpl.java:135)\\n    at org.eclipse.equinox.internal.event.EventAdminImpl.sendEvent(EventAdminImpl.java:78)\\n    at org.eclipse.equinox.internal.event.EventComponent.sendEvent(EventComponent.java:39)\\n    at org.eclipse.e4.ui.services.internal.events.EventBroker.send(EventBroker.java:80)\\n    at org.eclipse.e4.ui.internal.workbench.UIEventPublisher.notifyChanged(UIEventPublisher.java:58)\\n    at org.eclipse.emf.common.notify.impl.BasicNotifierImpl.eNotify(BasicNotifierImpl.java:374)\\n    at org.eclipse.emf.ecore.util.EcoreEList.dispatchNotification(EcoreEList.java:249)\\n    at org.eclipse.emf.common.notify.impl.NotifyingListImpl.addUnique(NotifyingListImpl.java:304)\\n    at org.eclipse.emf.common.util.AbstractEList.add(AbstractEList.java:301)\\n    at org.eclipse.e4.ui.workbench.addons.minmax.MinMaxAddon.setState(MinMaxAddon.java:274)\\n    at org.eclipse.e4.ui.workbench.addons.minmax.MinMaxAddon.access$2(MinMaxAddon.java:270)\\n    at org.eclipse.e4.ui.workbench.addons.minmax.MinMaxAddon$3.minimize(MinMaxAddon.java:173)\\n    at org.eclipse.swt.custom.CTabFolder.onSelection(CTabFolder.java:2044)\\n    at org.eclipse.swt.custom.CTabFolder$1.handleEvent(CTabFolder.java:287)\\n    at org.eclipse.swt.widgets.EventTable.sendEvent(EventTable.java:84)\\n    at org.eclipse.swt.widgets.Widget.sendEvent(Widget.java:1057)\\n    at org.eclipse.swt.widgets.Display.runDeferredEvents(Display.java:4170)\\n    at org.eclipse.swt.widgets.Display.readAndDispatch(Display.java:3759)\\n    at org.eclipse.e4.ui.internal.workbench.swt.PartRenderingEngine$9.run(PartRenderingEngine.java:1113)\\n    at org.eclipse.core.databinding.observable.Realm.runWithDefault(Realm.java:332)\\n    at org.eclipse.e4.ui.internal.workbench.swt.PartRenderingEngine.run(PartRenderingEngine.java:997)\\n    at org.eclipse.e4.ui.internal.workbench.E4Workbench.createAndRunUI(E4Workbench.java:138)\\n    at org.eclipse.ui.internal.Workbench$5.run(Workbench.java:610)\\n    at org.eclipse.core.databinding.observable.Realm.runWithDefault(Realm.java:332)\\n    at org.eclipse.ui.internal.Workbench.createAndRunWorkbench(Workbench.java:567)\\n    at org.eclipse.ui.PlatformUI.createAndRunWorkbench(PlatformUI.java:150)\\n    at com.infineon.scm.Application.start(Application.java:22)\\n    at org.eclipse.equinox.internal.app.EclipseAppHandle.run(EclipseAppHandle.java:196)\\n    at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.runApplication(EclipseAppLauncher.java:110)\\n    at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.start(EclipseAppLauncher.java:79)\\n    at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:354)\\n    at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:181)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\\n    at java.lang.reflect.Method.invoke(Method.java:597)\\n    at org.eclipse.equinox.launcher.Main.invokeFramework(Main.java:636)\\n    at org.eclipse.equinox.launcher.Main.basicRun(Main.java:591)\\n    at org.eclipse.equinox.launcher.Main.run(Main.java:1450)\\n    at org.eclipse.equinox.launcher.Main.main(Main.java:1426)\\n</code></pre>\\n\\n<p>It work perfectly if I add any Part in this PartStack<br>\\nI already added <strong>MinMax Addon</strong>.</p>\\n\\n<p>What is going wrong if there is no Part.?</p>\\n-Minimize empty PartStack throwing Exception-<java><eclipse><eclipse-plugin><eclipse-rcp><eclipse-kepler>\",\n",
       " \"<p>I am using <strong>ECLIPSE KEPLER</strong> for java programing. When I press (<kbd>Ctrl</kbd>+<kbd>Z</kbd>) it working fine and easily undo the history but the problem is when I press (<kbd>Ctrl</kbd>+<kbd>Y</kbd>) , this is not working to re-undo the history. Before KEPLER I use INDIGO it's accept above tags but how I reach it in KEPLER.</p>\\n-shortcut not working in eclipse-<eclipse><keyboard-shortcuts><eclipse-kepler>\",\n",
       " '<p>I´m trying to run the following query. The size is probably on the limit of the database, however similar sized tables were working. </p>\\n\\n<p>I know there is a way to partition the query using HASHAMP, HASHBUCKET, HASHROW functions but i have no idea how to do this.</p>\\n\\n<p>The query is simple, i´m just checking if the main_acct_product_id variable is on b table.</p>\\n\\n<p>Some info about the tables inside the query:</p>\\n\\n<pre><code>sel count(*) from graph_total_3\\n678.336.354\\n\\ntop 5 of graph_total_3\\nid_phone    destino WEIGHT  DIR access_method_id    access_destino  operador    producto    operador_destino\\n2615071884  2615628271  0,42800 0,417000    T2615071884 T2615628271 A   aa  II\\n1150421872  1159393065  343,200 0,424000    T1150421872 T1159393065 B   bb  LI\\n2914076292  2914735291  0,16500 1,003,000   T2914076292 T2914735291 C   ar  OJ\\n2914735291  2914076292  0,16500 -0,003000   T2914735291 T2914076292 A   tm  JA\\n2804535124  2804454795  0,39600 1,000,000   T2804535124 T2804454795 B   ma  UE\\n\\nprimary key(id_phone, destino);\\n\\nsel count(*) from producto\\n26.473.287\\n\\ntop 5 of producto\\n    Access_Method_Id    Main_Acct_Product_Id\\n    T2974002818         PR_PPAL_AHORRO  \\n    T3875943432         PR_PPAL_ACTIVA  \\n    T2616294339         PR_PPAL_ACTIVA  \\n    T3516468805         PR_PPAL_ACTIVA  \\n    T2616818855         PR_PPAL_ACTIVA  \\n\\nprimary key(Access_Method_Id);\\n</code></pre>\\n\\n<p><strong>SHOW TABLE</strong></p>\\n\\n<pre><code>show table producto\\n\\nCREATE MULTISET VOLATILE TABLE MARBEL.producto ,NO FALLBACK ,\\n     CHECKSUM = DEFAULT,\\n     LOG\\n     (\\n      Access_Method_Id VARCHAR(50) CHARACTER SET LATIN NOT CASESPECIFIC,\\n      Main_Acct_Product_Id CHAR(16) CHARACTER SET LATIN NOT CASESPECIFIC)\\nPRIMARY INDEX ( Access_Method_Id )\\nON COMMIT PRESERVE ROWS;\\n\\nshow table graph_total_3\\n\\nCREATE MULTISET VOLATILE TABLE MARBEL.graph_total_3 ,NO FALLBACK ,\\n     CHECKSUM = DEFAULT,\\n     LOG\\n     (\\n      id_phone VARCHAR(21) CHARACTER SET LATIN NOT CASESPECIFIC,\\n      destino VARCHAR(21) CHARACTER SET LATIN NOT CASESPECIFIC,\\n      WEIGHT DECIMAL(10,5),\\n      DIR DECIMAL(7,6),\\n      access_method_id VARCHAR(22) CHARACTER SET LATIN NOT CASESPECIFIC,\\n      access_destino VARCHAR(22) CHARACTER SET LATIN NOT CASESPECIFIC,\\n      operador VARCHAR(8) CHARACTER SET UNICODE NOT CASESPECIFIC,\\n      producto VARCHAR(16) CHARACTER SET LATIN NOT CASESPECIFIC,\\n      operador_destino VARCHAR(8) CHARACTER SET UNICODE NOT CASESPECIFIC)\\nPRIMARY INDEX ( id_phone ,destino )\\nON COMMIT PRESERVE ROWS;\\n</code></pre>\\n\\n<p><strong>QUERY</strong></p>\\n\\n<pre><code>create multiset volatile table graph_total_final as\\n(\\nselect  a.* ,  coalesce(b.main_acct_product_id,\\'NO MOV\\') as producto_destino\\nfrom graph_total_3 a\\nleft join producto b on a.access_destino=b.access_method_id\\n)\\nwith data primary index (id_phone, destino)\\non commit preserve rows;\\n</code></pre>\\n\\n<p><strong>Explain</strong></p>\\n\\n<pre><code>     This query is optimized using type 1 profile bootstrap, profileid -/. \\n      1) First, we create the table header. \\n      2) Next, we do an all-AMPs RETRIEVE step from MARBEL.a by way of an\\n         all-rows scan with no residual conditions into Spool 2 (all_amps),\\n         which is redistributed by the hash code of (\\n         MARBEL.a.access_destino) to all AMPs.  Then we do a SORT to order\\n         Spool 2 by row hash.  The result spool file will not be cached in\\n         memory.  The size of Spool 2 is estimated with high confidence to\\n         be 678,343,248 rows (55,624,146,336 bytes).  The estimated time\\n         for this step is 2 minutes and 41 seconds. \\n      3) We do an all-AMPs JOIN step from Spool 2 (Last Use) by way of a\\n         RowHash match scan, which is joined to MARBEL.b by way of a\\n         RowHash match scan.  Spool 2 and MARBEL.b are left outer joined\\n         using a merge join, with condition(s) used for non-matching on\\n         left table (\"NOT (access_destino IS NULL)\"), with a join condition\\n         of (\"access_destino = MARBEL.b.Access_Method_Id\").  The result\\n         goes into Spool 1 (all_amps), which is redistributed by the hash\\n         code of (MARBEL.a.id_phone, MARBEL.a.destino) to all AMPs.  Then\\n         we do a SORT to order Spool 1 by row hash.  The result spool file\\n         will not be cached in memory.  The size of Spool 1 is estimated\\n         with index join confidence to be 25,085,452,093 rows (\\n         2,232,605,236,277 bytes).  The estimated time for this step is 1\\n         hour and 45 minutes. \\n      4) We do an all-AMPs MERGE into MARBEL.graph_total_final from Spool 1\\n         (Last Use). \\n      5) Finally, we send out an END TRANSACTION step to all AMPs involved\\n         in processing the request.\\n      -&gt; No rows are returned to the user as the result of statement 1. \\n</code></pre>\\n\\n<p><strong>EXPLAIN 2</strong></p>\\n\\n<p>After running:</p>\\n\\n<pre><code>DIAGNOSTIC HELPSTATS ON FOR SESSION;\\nEXPLAIN\\ncreate multiset volatile table graph_total_final as\\n(\\nselect  a.* ,  coalesce(b.main_acct_product_id,\\'NO MOVISTAR\\') as producto_destino\\nfrom graph_total_3 a\\nleft join producto b on a.access_destino=b.access_method_id\\n)\\nwith data primary index (id_phone, destino, access_destino)\\non commit preserve rows;\\n\\n  EXPLAIN\\ncreate multiset volatile table graph_total_final as\\n(\\nselect  a.* ,  coalesce(b.main_acct_product_id,\\'NO MOVISTAR\\') as producto_destino\\nfrom graph_total_3 a\\nleft join producto b on a.access_destino=b.access_method_id\\n)\\nwith data primary index (id_phone, destino, access_destino)\\non commit preserve rows;\\n\\n This query is optimized using type 1 profile bootstrap, profileid -/. \\n  1) First, we create the table header. \\n  2) Next, we do an all-AMPs RETRIEVE step from MARBEL.a by way of an\\n     all-rows scan with no residual conditions into Spool 2 (all_amps),\\n     which is redistributed by the hash code of (\\n     MARBEL.a.access_destino) to all AMPs.  Then we do a SORT to order\\n     Spool 2 by row hash.  The result spool file will not be cached in\\n     memory.  The size of Spool 2 is estimated with high confidence to\\n     be 678,343,248 rows (55,624,146,336 bytes).  The estimated time\\n     for this step is 2 minutes and 41 seconds. \\n  3) We do an all-AMPs JOIN step from Spool 2 (Last Use) by way of a\\n     RowHash match scan, which is joined to MARBEL.b by way of a\\n     RowHash match scan.  Spool 2 and MARBEL.b are left outer joined\\n     using a merge join, with condition(s) used for non-matching on\\n     left table (\"NOT (access_destino IS NULL)\"), with a join condition\\n     of (\"access_destino = MARBEL.b.Access_Method_Id\").  The result\\n     goes into Spool 1 (all_amps), which is redistributed by the hash\\n     code of (MARBEL.a.id_phone, MARBEL.a.destino,\\n     MARBEL.a.access_destino) to all AMPs.  Then we do a SORT to order\\n     Spool 1 by row hash.  The result spool file will not be cached in\\n     memory.  The size of Spool 1 is estimated with index join\\n     confidence to be 25,085,452,093 rows (2,232,605,236,277 bytes). \\n     The estimated time for this step is 1 hour and 45 minutes. \\n  4) We do an all-AMPs MERGE into MARBEL.graph_total_final from Spool 1\\n     (Last Use). \\n  5) Finally, we send out an END TRANSACTION step to all AMPs involved\\n     in processing the request.\\n  -&gt; No rows are returned to the user as the result of statement 1. \\n     BEGIN RECOMMENDED STATS -&gt;\\n  6) \"COLLECT STATISTICS MARBEL.producto COLUMN ACCESS_METHOD_ID\". \\n     (HighConf)\\n  7) \"COLLECT STATISTICS MARBEL.graph_total_3 COLUMN ACCESS_DESTINO\". \\n     (HighConf)\\n     &lt;- END RECOMMENDED STATS\\n</code></pre>\\n-Running into spool error in Teradata-<sql><teradata>',\n",
       " \"<p>I would like to know the throughout, latency, and the number of banks in Kepler's L1 cache (read only 'texture' and normal cache).</p>\\n\\n<p>in a CUDA program, I'm reading the same data multiple times by different threads, I need to know if i'm bound by the L1 throughput, I couldn't find this information in any of Nvidia's documents, any help would be appreciated.</p>\\n\\n<p>Edit: I'm using the K20 card.</p>\\n-what is the L1 cache throughput in Nvidia's Kepler?-<cuda><nvidia><kepler>\",\n",
       " '<pre><code>sel cast(9.1 as integer)as inttt;\\n\\nResult:\\n9\\n\\nsel cast(9.9 as integer)as inttt;\\nResult:\\n9\\n</code></pre>\\n\\n<p>I executed the above queries on Teradata, and both of the queries resulted in the floor value.</p>\\n\\n<p>Is this a hard rule for casting from DEC to INT? That is, does it always return the floor value?</p>\\n-converting decimal to integer in teradata-<sql><teradata>',\n",
       " '<p>Is it possible to add placeholders in the editor folder stack?</p>\\n\\n<p>I can currently move my views (drag-n-drop) next to an editor, but I want them to be opened directly there.</p>\\n-Eclipse RCP -- Open View in Editor folder-<java><swt><eclipse-rcp><eclipse-kepler>',\n",
       " '<p>The title summarizes my problem, I think. I\\'d appreciate any pointers anybody might have on what else I could try. I\\'ve pasted the console output with comments in bold below. Theinstalled version of MonetDB.R is the latest from R-forge (0.8.5).</p>\\n\\n<pre><code>fabians@wap27:~$ R\\n\\n[...]\\n\\n&gt; library(MonetDB.R)   \\nLoading required package: DBI\\nLoading required package: digest\\n\\nAttaching package: ‘MonetDB.R’\\n\\nThe following objects are masked from ‘package:stats’:\\n\\n    sd, var\\n\\nThe following objects are masked from ‘package:base’:\\n\\n    sample, tabulate\\n\\n&gt; sessionInfo()\\nR version 3.0.2 (2013-09-25)\\nPlatform: x86_64-pc-linux-gnu (64-bit)\\n\\nlocale:\\n [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              \\n [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    \\n [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   \\n [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 \\n [9] LC_ADDRESS=C               LC_TELEPHONE=C            \\n[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       \\n\\nattached base packages:\\n[1] stats     graphics  grDevices utils     datasets  methods   base     \\n\\nother attached packages:\\n[1] MonetDB.R_0.8.5 digest_0.6.3    DBI_0.2-7      \\n&gt; \\n&gt; # as @ https://github.com/ajdamico/usgsd/tree/master/MonetDB:\\n&gt; batfile &lt;- monetdb.server.setup(\\n+         database.directory = \"~/monetdb\",\\n+         monetdb.program.path = \"/usr\",\\n+         dbname = \"test\",\\n+         dbport = 50000L)\\n/home/fabians/monetdb did not exist.  now it does\\n</code></pre>\\n\\n<p>seems to have worked...</p>\\n\\n<pre><code>&gt;\\n&gt; batfile &lt;- \"/home/fabians/monetdb/test.sh\"\\n\\n&gt; system(paste(\"cat \", batfile))\\n#!/bin/sh\\n/usr/bin/mserver5 --set prefix=/usr --set exec_prefix=/usr --dbpath /home/fabians/monetdb/test --set mapi_port=50000 --daemon yes &gt; /dev/null &amp;\\necho $! &gt; /home/fabians/monetdb/mserver5.started.from.R.pid\\n</code></pre>\\n\\n<p><strong>Can anybody tell me whether that shell script looks like it\\'s supposed to?</strong></p>\\n\\n<pre><code>&gt; system(paste(\"ls ~/monetdb\"))\\ntest  test.sh\\n</code></pre>\\n\\n<p>OK, so an (empty) \\'test\\'-directory was created along with the .sh</p>\\n\\n<pre><code>&gt; monetpid &lt;- monetdb.server.start(batfile)\\nRead 1 item\\n&gt; system(paste(\"ls ~/monetdb/test\"))\\n00e3bc31-ca59-43e6-ace8-a96aac37bddd  bat  box\\n</code></pre>\\n\\n<p>Now after starting the monetdb server the test directory contains the data base info, I guess?</p>\\n\\n<pre><code>&gt; dbname &lt;- \"test\"\\n&gt; dbport &lt;- 50000L\\n&gt; monet.url &lt;- paste0(\"monetdb://localhost/\", dbname)\\n&gt; db &lt;- dbConnect( MonetDB.R(), monet.url, \\n+                  port=as.integer(dbport), timeout=as.integer(86400))\\nError in .monetAuthenticate(socket, dbname, user, password) : \\n  Authentication error: !monetdbd: no such database \\'test\\', please create it first\\n</code></pre>\\n\\n<p><strong>What could be the reason that the test-directory is not found?</strong></p>\\n\\n<p>Slightly different command, as in the help for <code>monetdb.server.start</code>and  also at <code>https://github.com/ajdamico/usgsd/tree/master/MonetDB</code>:</p>\\n\\n<pre><code>&gt; monet.url &lt;- paste0(\"monetdb://localhost:\", dbport, \"/\", dbname)\\n&gt; db &lt;- dbConnect( MonetDB.R(), monet.url, wait = TRUE )\\nR: mapi.c:72: mapiConnect: Assertion `Rf_isInteger(port)\\' failed.\\nAborted\\n</code></pre>\\n\\n<p>This bug is supposed to have been fixed, see <a href=\"https://stackoverflow.com/q/20783829/295025\">here</a> ...</p>\\n-Cannot set up connection to local database in MonetDB.R-<r><monetdb>',\n",
       " '<p>I\\'m using Eclipse Kepler and I\\'m unable to 1) hide the target folder and 2) to hide the duplicate src folder in project explorer.</p>\\n\\n<p><img src=\"https://i.stack.imgur.com/NHyP5.png\" alt=\"Project Explorer\"></p>\\n\\n<p>In previous versions it was possible to filter the folders with a regex, but in Kepler I don\\'t see an option to enter a regex in Customize View->Filters.\\nI checked \"Java output folders\" and \"Maven build folder\" but they are still visible.</p>\\n\\n<p>For the duplicate src folder:\\nI would like to only see the top one (the one with the flattened layout)</p>\\n\\n<p>Here\\'s my build path:</p>\\n\\n<p><img src=\"https://i.stack.imgur.com/MRYpK.png\" alt=\"Build Path\"></p>\\n-Eclipse Kepler: Hide specific items in Project Explorer-<eclipse><eclipse-kepler><buildpath><package-explorer>',\n",
       " \"<p>Assume we have a class <code>FooCollection</code> which contains a somewhat long list of static nested classes*:</p>\\n\\n<pre><code>public class FooCollection {\\n    public static class FooA implements Foo {\\n        // ...\\n    }\\n\\n    public static class FooB implements Foo {\\n        // ...\\n    }\\n\\n    // ...\\n}\\n</code></pre>\\n\\n<p>Assume now we have another class using all of these classes. Currently, Eclipse will auto-format this to import each class separately if we reference the class itself</p>\\n\\n<pre><code>import com.me.FooCollection.FooA;\\nimport com.me.FooCollection.FooB;\\nimport com.me.FooCollection.FooC;\\nimport com.me.FooCollection.FooD;\\n\\n// and then later something like\\ncallBaz( FooA.class );\\n</code></pre>\\n\\n<p>What I would prefer to avoid bloating imports and constant commits changing imports due to colleagues using IntelliJ, is having it imported as</p>\\n\\n<pre><code>import static com.me.FooCollection.*;\\n</code></pre>\\n\\n<p>However, I can't seem to find anything to get Eclipse to do this. Is there something I am missing or any way to get Eclipse to do it this way?</p>\\n\\n<p><em>Edit:</em> I actually just checked and even <code>new FooA()</code> will still cause the imports to switch back to this, despite setting the start imports threshold.</p>\\n\\n<p><sub>*) I realize that this is not exactly a good design, but it's a legacy application and for the sake of it let's assume that the code cannot be changed.</sub></p>\\n-Get Eclipse to prefer static imports of nested classes-<java><eclipse><eclipse-kepler>\",\n",
       " '<p>I need some help from Eclipse/Maven experts out there - I am unable to see my project as a resource to deploy to Tomcat within Eclipse.  So I did a search and found that if I run Maven-->Update Project, that would alleviate the problem.  However, updating resulted in the following error:</p>\\n\\n<pre><code>An internal error occurred during: \"Updating Maven Project\".\\norg.eclipse.wst.common.componentcore.internal.impl.ReferencedComponentXMIResourceImpl cannot be cast to org.eclipse.jpt.common.core.resource.xml.JptXmlResource\\n</code></pre>\\n\\n<p>I have no idea what the problem could be.</p>\\n-Update Maven project internal error - ReferencedComponentXMIResourceImpl cannot be cast to JptXmlResource-<java><eclipse><maven><eclipse-kepler>',\n",
       " '<p>I am  using  python API to connect with tera data, when I use select * query on any pre-existing table I can get column info by Help Table Query. But Now I am using joins and other queries which includes multiple tables. i am receiving data correct but I do not know how to access column names of respective fields to replicate those generated data in other format using API. Please help Me. </p>\\n-get column names after join in teradata-<python><sql><teradata><pyodbc>',\n",
       " \"<p>I'm developing an Installer for a project team to reduce the amount of work they have to do installing it manually.</p>\\n\\n<p>We are using:\\nWindows 7 x64,\\nEclipse Kepler</p>\\n\\n<p>Right now I'm looking for a way to import a .war file using the commandline.</p>\\n\\n<p>Is there any way to do this via cmd? </p>\\n-Importing .war file using cmd-<java><cmd><eclipse-kepler><war>\",\n",
       " '<p>I\\'m new using <code>MonetDB</code>. I\\'ve follow this installation guide <a href=\"http://www.monetdb.org/Documentation/Guide/Installation\" rel=\"nofollow\">http://www.monetdb.org/Documentation/Guide/Installation</a>\\nand I\\'m facing a problem to just create a database. Here is the error</p>\\n\\n<pre><code>monetdb create /tmp/c\\nmonetdb: cannot find a control socket, use -h and/or -p\\n</code></pre>\\n\\n<p>Ruuning the command with option <code>-p</code>or <code>-h</code> didn\\'t change anything</p>\\n\\n<pre><code>monetdb -p create /tmp/c\\nmonetdb: cannot find a control socket, use -h and/or -p\\n</code></pre>\\n\\n<p>can come one help me ?</p>\\n-Cannot create a database on a fresh installed monetdb-<database><monetdb>',\n",
       " '<p>How do I add columns to a <code>monet.frame</code>-object?</p>\\n\\n<p>E.g. if I want to add a simple row ID to a <code>monet.frame</code> called  <code>data</code>, I could achieve that by doing something like:</p>\\n\\n<pre><code>from &lt;- gsub(\"^SELECT(.)+FROM\", \"\", data$query)\\naddid_query &lt;- gsub(\"data\", from, \\n                    \"SELECT  *, ROW_NUMBER() OVER () AS id FROM data\"\\nattr(data, \"query\") &lt;- addidquery\\n</code></pre>\\n\\n<p>but there must be a less hacky way. \\nIf you do it in the way given above, the newly created <code>id</code>-column does not show up when you do <code>str(data)</code> or <code>names(data)</code>, for example, because that info is not updated when the underlying database changes --- it is generated when <code>data</code> is initialized, not at the point where these acessor functions are called. It does show up when accessing rows of <code>data</code> directly because then the query gets executed as written.</p>\\n\\n<p>Is stuff like the above still safe to do? </p>\\n\\n<p>More generally, I need to do things with my <code>monet.frame</code> that are beyond the functionality that\\'s available in the package and would be interested to know how to run more general SQL-queries on a <code>monet.frame</code>, similar to what <code>dplyr</code>\\'s <code>sql()</code>-function allows. I\\'d appreciate any pointers in that regard -- would I have to use <code>dbSendQuery</code> etc. and convert the result of that into a new <code>monet.frame</code>, for example? How?</p>\\n\\n<hr>\\n\\n<p>EDIT: </p>\\n\\n<p>NEVERMIND, should have read the manual more closely, I didn\\'t see that <code>monet.frame</code> accepts strings that can be interpreted as SQL queries as well as table names that are already in the DB. </p>\\n\\n<p>This is how I would do it now:</p>\\n\\n<pre><code>#\\' Apply general SQL queries to a monet.frame object and return the \\n#\\' result in a new monet.frame.\\n#\\' \\n#\\' @param data a \\\\code{monet.frame} object\\n#\\' @param query an SQL query, using \"_DATA_\" as the placeholder for the\\n#\\'     name of the table underlying the \\\\code{data}-object.\\ntransform.monet.frame &lt;- function(data, query){\\n    nm &lt;- paste(sample(letters, 15, rep=TRUE), collapse=\"\")\\n    query &lt;- gsub(\"_DATA_\", paste(\"(\", attr(data, \"query\"), \") AS\", nm), query) \\n    monet.frame(attr(data, \"conn\"), query)\\n}\\n\\nnewdata &lt;- transform(data, \\n                 \"SELECT  *, ROW_NUMBER() OVER () AS id FROM _DATA_\")\\n</code></pre>\\n\\n<hr>\\n\\n<p>EDIT 2: This will break if applied to a <code>monet.frame</code> that was created by <code>transform.monet.frame</code> with a query involving ORDER, LIMIT or OFFSET statements.</p>\\n-How to add columns to a monet.frame?-<r><monetdb>',\n",
       " '<p>I have changed my eclipse version from Indigo to Kepler since it is 64bit. Now, when I clone a project, it shows me like there are some changes in my local project. When I synchronize, I can see the files that looks changed but there are no changes inside the files. I think this is because of eGit.</p>\\n\\n<p>Any suggestions?</p>\\n-Eclipse Kepler > EGit Shows Differences in non Changed Files-<eclipse><egit><tortoisegit><eclipse-kepler>',\n",
       " \"<p>I would like to install MonetDB on Centos 6.5.</p>\\n\\n<p>MonetDB website describes how to install for Debian / Ubuntu / Fedora distributions.</p>\\n\\n<p>I've a server on Centos 6.5 (and I'm not entirely sure to which Fedora distribution this should compare with).</p>\\n\\n<p>Obviously I would like to install the latest binaries of MonetDB: I would rather avoid to install from source (unless strictly necessary).</p>\\n\\n<p>How do I do it?</p>\\n-how do I install MonetDB on Centos 6.5?-<centos><monetdb>\",\n",
       " \"<p>Kepler's ini file (config.ini) looks like this:</p>\\n\\n<pre><code> org.eclipse.update.reconcile=false\\n eclipse.p2.profile=epp.package.standard\\nosgi.instance.area.default=@user.home/Documents/workspace\\n osgi.framework=file\\\\:plugins/org.eclipse.osgi_3.9.1.v20130814-1242.jar\\n equinox.use.ds=true\\n eclipse.buildId=4.3.0.M20130911-1000\\n osgi.bundles=reference\\\\:file\\\\:org.eclipse.equinox.simpleconfigurator_1.0.400.v20130327-    \\n2119.jar@1\\\\:start\\n org.eclipse.equinox.simpleconfigurator.configUrl=file\\\\:org.eclipse.equinox.simpleconfigura\\ntor/bundles.info\\neclipse.product=org.eclipse.platform.ide\\nosgi.splashPath=platform\\\\:/base/plugins/org.eclipse.platform\\nosgi.framework.extensions=\\nosgi.bundles.defaultStartLevel=4\\neclipse.application=org.eclipse.ui.ide.workbench\\neclipse.p2.data.area=@config.dir/../p2/\\n</code></pre>\\n\\n<p>Is this how it is supposed to look? B/c it looks awfully different from others.</p>\\n-What should my ini file for Eclipse look like?-<eclipse><ini><eclipse-kepler>\",\n",
       " '<p>I have recently installed Ubuntu 13.10 on my Lenovo intel core2 duo 32 bit laptop with 15\\'\\' screen. Some days back I downloaded <code>eclipse-standard-kepler-SR1-linux-gtk.tar.gz</code> from the eclipse web site. But after starting eclipse I see the menu distorted.\\n<img src=\"https://i.stack.imgur.com/DumvH.png\" alt=\"The screenshot is here\"></p>\\n\\n<p>You can refer the video here :\\n<a href=\"http://youtu.be/fcBHLAlFj4Y\" rel=\"nofollow noreferrer\">http://youtu.be/fcBHLAlFj4Y</a></p>\\n-Eclipse Menu distorted in Ubuntu-13.10-<eclipse><ubuntu><eclipse-kepler><ubuntu-13.10>',\n",
       " '<p>I\\'m using Eclipse Kepler (4.3) (with pydev, in case that matters). If I collapse my code (using Ctrl-9 or the Source menu) and then selectively uncollapse only the part I\\'m working on, it works fine for a while. But at some point in the session, I\\'ll notice that Eclipse has randomly (?) decided to uncollapse all the code. Am I doing something wrong, or is this a known \"feature\" of Eclipse? And is there any way to prevent it?</p>\\n\\n<p>And in case there\\'s no way to prevent this, I have a follow-up question: Is there any way to collapse just the code I have selected? I.e., can I select a set of functions and collapse them all at once rather than going through and collapsing them one at a time?</p>\\n-How do I prevent Eclipse (with pydev) from uncollapsing code?-<eclipse><eclipse-kepler>',\n",
       " '<p>I installed JBoss Tools for Eclipse Kepler but I cannot find Struts plugins (there are spring and JSF only)</p>\\n\\n<p>I absolutely need Eclipse Kepler but how to install the missing Struts plugins?</p>\\n\\n<p>Thanks</p>\\n-Struts Plugin from JBoss Tools in Eclipse Kepler-<eclipse><struts><eclipse-kepler><jboss-tools>',\n",
       " '<p>I\\'m working on an Eclipse plug-in.</p>\\n\\n<p>With Indigo, a drop-down list in the toolbar like the one below works perfectly.</p>\\n\\n<p><a href=\"http://i.stack.imgur.com/y1Hru.png\" rel=\"nofollow\">Drop-down list in the toolbar, the currently selected item is shown when the list isn\\'t open</a></p>\\n\\n<p>It is built like this (where the <code>MyList</code> instance is dynamically constructed):</p>\\n\\n<pre><code>org.eclipse.ui.menus\\n  (menuContribution) locationURI : toolbar:org.eclipse.ui.main.toolbar\\n    Functions (menu) id : xxxx\\n      (dynamic) id : yyyyyyy\\n        class MyList extends ContributionItem\\n</code></pre>\\n\\n<p>However, this doesn\\'t work on Kepler, and it\\'s difficult to understand why.\\nWhile I can write a dynamic drop-down list which work in main menu (<code>menu:org.eclipse.ui.main.menu</code>),\\nmy list with the same code doesn\\'t appear if locate it in toolbar (<code>toolbar:org.eclipse.ui.main.toolbar?additions</code>).</p>\\n\\n<p>How can I create a drop-down list like the one in the screenshot above in Eclipse Kepler\\'s toolbar, and handle events when items are clicked?</p>\\n-How to put a dynamic drop-down list in Eclipse Kepler toolbar?-<java><eclipse><dynamic><drop-down-menu><eclipse-kepler>',\n",
       " '<p>I am trying to perform a build on Eclipse Kepler but when I do so I get the following error   <code>javac: target release 1.6 conflicts with default source release 1.7</code></p>\\n\\n<p>I have changed the Java Compiler to use 1.6 but this warned me that the default JRE was 1.7. I then went to Installed JREs removed 1.7 and created a new one for 1.6.</p>\\n\\n<p>This still gives the sames error. I also saw something about adding -vm and the java path to eclipse.ini but this didnt help either.</p>\\n\\n<p>Does anyone know what to do?</p>\\n-Eclipse: javac: target release 1.6 conflicts with default source release 1.7-<java><eclipse><eclipse-kepler>',\n",
       " '<p>I think there\\'s a bug in how <code>MonetDB.R</code> filters NAs, see example code below:</p>\\n\\n<p>Handy utility function for doing general SQL-queries on <code>monet.frame</code> objects:</p>\\n\\n<pre><code>#\\' Apply general SQL queries to a monet.frame object and return the \\n#\\' result in a new monet.frame.\\n#\\' \\n#\\' @note Likely to break if  \\\\code{attr(data, \"query\")} contains \\n#\\'      LIMIT or OFFSET statements.\\n#\\' \\n#\\' @param _data a monet.frame object\\n#\\' @param query an SQL query, using \"_DATA_\" as the placeholder for the\\n#\\'     name of the table underlying the \\\\code{_data}-object.\\n#\\' @param keep_order should ORDER BY statements in the original query be kept? \\n#\\'     Will break if columns in the ORDER BY statement are not in the returned \\n#\\'     table.\\n#\\' @importFrom stringr str_extract_all \\n#\\' @export   \\ntransform.monet.frame &lt;- function(`_data`, query, keep_order=TRUE, ...){\\n    stopifnot(require(stringr))\\n    nm &lt;- paste(sample(letters, 15, rep=TRUE), collapse=\"\")\\n    oldquery &lt;- attr(`_data`, \"query\")\\n    if(has_order &lt;- grepl(\"(ORDER BY)\", attr(`_data`, \"query\"))){\\n        pattern &lt;- \"(ORDER BY[[:space:]]+[[:alnum:]]+((,[[:space:]]*[[:alnum:]]+)*))\"\\n        pattern &lt;- ignore.case(pattern)\\n        orderby &lt;- str_extract_all(oldquery, pattern)[[1]]\\n        oldquery &lt;- gsub(pattern, \"\", oldquery, ignore.case = TRUE)\\n    } \\n    query &lt;- gsub(\"_DATA_\", paste(\"(\", oldquery, \") AS\", nm), query)\\n    if(has_order &amp; keep_order) query &lt;- paste(query, orderby)\\n    monet.frame(attr(`_data`, \"conn\"), query)\\n}\\n</code></pre>\\n\\n<p>Example:</p>\\n\\n<pre><code># library(MonetDB.R); monetdb &lt;- dbConnect( MonetDB.R(), ... etc\\nset.seed(1212)\\ntablename &lt;- paste(sample(letters, 10), collapse=\"\")\\ndata  &lt;- data.frame(x=rnorm(100), f=gl(2, 50))\\n\\n# introduce some NAs ...\\ndata$xna &lt;- data$x\\ndata$xna[1:10] &lt;- NA\\n\\ndbWriteTable(monetdb, tablename, data)\\ndm &lt;- monet.frame(monetdb, tablename)   \\n\\nstr(na.omit(dm$xna))\\n# MonetDB-backed data.frame surrogate\\n# 1 column, 100 rows\\n# Query: SELECT xna FROM gcxinabtme WHERE (  NOT ((\\'xna\\') IS NULL) ) \\n# Columns: xna (numeric)\\n</code></pre>\\n\\n<p>100 rows !?! should be 90...</p>\\n\\n<pre><code>nrow(transform(dm, \"SELECT xna FROM _DATA_ WHERE (xna IS NOT NULL)\"))\\n# 90 \\n## as it should be\\nnrow(transform(dm, \"SELECT xna FROM _DATA_ WHERE (\\'xna\\' IS NOT NULL)\"))\\n# 100\\n## so quoting the column name seems to mess this up..   \\n</code></pre>\\n\\n<p>I think I understand why quoting the column name is necessary (so this works for non-standard column names as well, right?), but why would this mess up the query result? Shouldn\\'t these two be perfectly equivalent queries? Also, if it\\'s really necessary to quote the column names, why is the first occurence of <code>xna</code> not quoted in </p>\\n\\n<pre><code># Query: SELECT xna FROM gcxinabtme WHERE (  NOT ((\\'xna\\') IS NULL) ) \\n</code></pre>\\n\\n<p>I noticed this because it also makes other <code>monet.frame</code>-methods behave unexpectedly, e.g.:</p>\\n\\n<pre><code> quantile(dm$xna, na.rm=TRUE)\\n # 0%        25%        50%        75%       100% \\n # NA -0.9974738 -0.3033412  0.4272321  2.6715264 \\n</code></pre>\\n\\n<hr>\\n\\n<p>EDITED to add:</p>\\n\\n<p><code>na.fail</code> seems to be broken as well: </p>\\n\\n<p>It does not raise an error, but instead returns NULL when applied to a column holding NAs, with a  cryptic warning that would indicate at first glance that there are, in fact, no NAs:</p>\\n\\n<pre><code>str(na.fail(dm$xna))\\n# NULL\\n# Warning message:\\n# In monet.frame.internal(attr(x, \"conn\"), nquery, .is.debug(x), nrow.hint = NA,  :\\n#   SELECT xna FROM gcxinabtme WHERE ( (\\'xna\\') IS NULL )  has zero-row result set.\\n</code></pre>\\n\\n<p>If there are no NAs, <code>na.fail()</code> should return its argument unchanged according to the generic\\'s documentation, but it doesn\\'t do that either:</p>\\n\\n<pre><code>str(na.fail(dm$x))\\n# NULL\\n# Warning message:\\n# In monet.frame.internal(attr(x, \"conn\"), nquery, .is.debug(x), nrow.hint = NA,  :\\n#   SELECT x FROM gcxinabtme WHERE ( (\\'x\\') IS NULL )  has zero-row result set.\\n</code></pre>\\n-Is MonetDB.R\\'s `na.omit` broken?-<sql><r><monetdb>',\n",
       " '<p>in the example below, I want to make 2 groups in a regex:</p>\\n\\n<p>Name FirtSurname SecondSurname ..</p>\\n\\n<p>The first group would be\\nName </p>\\n\\n<p>The second\\nFirtSurname SecondSurname ...</p>\\n\\n<pre><code>^(\\\\w+)(.*)$   - would capture all\\n\\\\w+           - would make n groups (number of words). \\n</code></pre>\\n\\n<p>I want only 2 groups. First name and anything that follows on another.</p>\\n\\n<p>Any help?</p>\\n-REGEX Name and any surname-<regex><regex-group><knime>',\n",
       " '<p>I want to get the actual column type from teradata system tables like dbc.columns. </p>\\n\\n<p>This table have column columntype but it does not give the actual datatype.</p>\\n\\n<p>I can get output with </p>\\n\\n<pre><code>select type(columnname) from table\\noutput: varchar2(20) \\n</code></pre>\\n\\n<p>but there are 1000 tables and 50000 columns. Please suggest some query that can give me actual column type of column from metadata itself </p>\\n-Get column type using teradata system tables-<sql><metadata><teradata>',\n",
       " '<p>For example a query : <code>create table ; select xxx  ; delete  ;</code></p>\\n\\n<p>How to execute it in one session ? </p>\\n\\n<p>I saw one answer to a <a href=\"https://stackoverflow.com/questions/21592224/how-to-get-resultset-from-executebatch\">similar question</a> about mysql. The trick is to turn on <code>allow multiple queries</code> </p>\\n\\n<pre><code>String dbUrl = \"jdbc:mysql:///test?allowMultiQueries=true\";\\n</code></pre>\\n\\n<hr>\\n\\n<p><strong>For teradata specifically,\\nwhat is the solution ?</strong></p>\\n\\n<p>I tried </p>\\n\\n<pre><code>String dbUrl = \"jdbc:odbc:dsn?allowMultiQueries=true\";\\n</code></pre>\\n\\n<p>It is not properly working ?</p>\\n-How to execute multiple queries in teradata?-<java><sql><teradata>',\n",
       " '<p>Is it possible to <strong>\"nest\"</strong> qualify statements in Teradata?</p>\\n\\n<p>I have some data that looks like this:</p>\\n\\n<pre><code>event_id = 1:\\n\\n user_id        action_timestamp\\n 971,134,265   17mar2010 20:16:56\\n 739,071,748   17mar2010 22:19:59\\n 919,853,934   18mar2010 15:47:49\\n 919,853,934   18mar2010 15:55:21\\n 919,853,934   18mar2010 16:01:20\\n 919,853,934   18mar2010 16:01:48\\n 919,853,934   18mar2010 16:04:52\\n 472,665,603   20mar2010 18:23:58\\n 472,665,603   20mar2010 18:24:07\\n 472,665,603   20mar2010 18:24:26\\n  ....\\n event_id = 2:     \\n 971,134,265   17mar2069 20:16:56\\n 739,071,748   17mar2069 22:19:59\\n 919,853,934   18mar2069 15:47:49\\n 919,853,934   18mar2069 15:55:21\\n 919,853,934   18mar2069 16:01:20\\n 919,853,934   18mar2069 16:01:48\\n 919,853,934   18mar2069 16:04:52\\n 472,665,603   20mar2069 18:23:58\\n 472,665,603   20mar2069 18:24:07\\n 472,665,603   20mar2069 18:24:26\\n</code></pre>\\n\\n<p>For user 919,853,934, I would like to grab \"18mar2010 16:04:52\" action (the last one in the first cluster of events).</p>\\n\\n<p>I tried this, which does not grab the right date:</p>\\n\\n<pre><code>SELECT action_timestamp\\n       ,user_id\\n       ,event_id\\nFROM table\\nWHERE ...\\nQUALIFY (\\n    MAX(action_timestampt) OVER (PARTITION BY user_id, event_id) = action_timestamp\\n    AND MIN(action_timestamp) OVER (PARTITION BY user_id) = action_timestamp\\n) \\n</code></pre>\\n\\n<p>This actually makes sense since the MAX and MIN apply to the whole data, rather than sequentially.</p>\\n\\n<p>I also tried 2 separate qualify statements to get the MIN() part to apply to the subset of the data created by the MAX() part, but that errors.</p>\\n-Nesting qualify statements in Teradata-<sql><teradata>',\n",
       " '<p>Does anyone know how I can use the Teradata .NET provider in a VBA project? (excel).  ODBC is not an option.  </p>\\n-Using Teradata .net provider from VBA?-<.net><vba><excel><teradata>',\n",
       " \"<p>We've an RCP application based on 3.x api we are trying to migrate it to eclipse 4.x. \\nThe problem is <strong>some part of the code was using eclipse internal classes</strong> present in the workbench.jar . SO i added the workbench.jar JAR from the previous eclipse(HELIOS) to my new eclipse(KEPLER) this resolved the errors .But my application is not able to start.So just wanted to know is it the correct approach </p>\\n\\n<p>1.<strong>Can I have two workbench.jar JARS(3.105 and 3.6) in my application</strong>.</p>\\n\\n<p>2.If no then is there a way to search for the internal classes which I was using previously    in the new jars I was mainly  using the internal classes related to layout and prespectives(like : <strong>org.eclise.ui.internal.layyoutPart ,org.eclipse.internal.ui.perspectives</strong>)</p>\\n\\n<p>3.<strong>Is there a way using which I can avoid rewriting the code</strong>.</p>\\n-Not ableto migrate from eclipse 3.x to eclipse 4.x-<eclipse-plugin><eclipse-rcp><eclipse-kepler>\",\n",
       " '<pre><code>   &lt;/extension&gt;\\n       &lt;extension\\n             point=\"org.eclipse.ui.menus\"&gt;\\n          &lt;menuContribution\\n                allPopups=\"false\"\\n                locationURI=\"toolbar:com.*****.*****.ui.main.toolbar?after=workbenchActions\"&gt;\\n             &lt;command\\n                   commandId=\"org.eclipse.ui.file.save\"\\n                   disabledIcon=\"icons/save24x24_disabled.png\"\\n                   icon=\"icons/save24x24.png\"\\n                   label=\"Save\"\\n                   mode=\"FORCE_TEXT\"\\n                   style=\"push\"&gt;\\n             &lt;/command&gt;\\n          &lt;/menuContribution&gt;\\n   &lt;/extension&gt; \\n</code></pre>\\n\\n<p>I\\'m pointing to the workbench\\'s <code>Save</code> command using a menu command, because I want to change the icon and force the text underneath it.</p>\\n\\n<p>Icon works fine, but the text won\\'t show (it should say <i>Save</i> below the icon, because I set the mode to <code>FORCE_TEXT</code> and have a label present).</p>\\n\\n<p>Is it a bug, or a limitation of the framework?</p>\\n-Eclipse RCP CoolBar Action FORCE_TEXT-<java><swt><eclipse-rcp><eclipse-kepler>',\n",
       " \"<p>I'm working on a product which is an RCP application based on Eclipse 3.x api. Now we are trying to move it to Eclipse 4.x. We are using some internal classes in our code. I've already read the tutorial provided by Vogella about migrating to Eclipse 4.x from 3.x, but I'm still not getting how to start.</p>\\n\\n<p><strong>We want to take advantage of the new Eclipse features, my main question is that we have good number of views, layouts in our old code, so what should be the approach I should follow. Also is there a way to create an application model from my older application using 3.x API.</strong> </p>\\n\\n<p>I'm stuck and not getting how to proceed.</p>\\n-Migrating eclipse 3.x application application to eclipse 4.x-<eclipse><eclipse-rcp><eclipse-kepler><e4><eclipse-api>\",\n",
       " '<p>I have just downloaded the \"<strong>Eclipse IDE for Java Developers</strong>\" (version: <strong>Kepler Service Release 1 for Windows 64bit</strong>) and extract it to a folder.</p>\\n\\n<p>The <a href=\"http://www.eclipse.org/downloads/packages/eclipse-ide-java-developers/keplersr1\">Package Description</a> says that the IDE includes WindowBuilder Core.</p>\\n\\n<p>So I created a <kbd>New</kbd> -> <kbd>Java Project</kbd> using the default settings and pressing <kbd>Finish</kbd>. Then I created a new class file with a <code>public static void main()</code> inside it.</p>\\n\\n<p>Then I search all the toolbars but I can\\'t find the WindowBuilder Toolbar anywhere so that to be able to create a new window..</p>\\n\\n<p><strong>How can I access the WindowBuilder Core from my Eclipse and show up the toolbar with buttons and textboxes e.t.c.?</strong></p>\\n\\n<p><strong>How can I make and run a simple window with WindowBuilder?</strong></p>\\n-How can I use WindowBuilder core in Eclipse Kepler to make a simple window?-<java><eclipse><windowbuilder><eclipse-kepler>',\n",
       " '<p>I\\'m trying to access a monetdb from a .net application - to see if it\\'s feasible for me to use.  I have the ODBC Driver downloaded and installed on my machine (from monetDB.org).  I can\\'t add a reference to it.  My assumption is that the correct file to reference is libMonetODBC.dll and that doesn\\'t work - none of the others work either.  If I try to add a reference to it I get a message - \"A reference to \\'C:\\\\Program Files\\\\MonetDB\\\\MonetDB ODBC Driver\\\\lib\\\\libMonetODBC.dll\\' could not be added.  Please make sure that the file is accessible, and that it is a valid assembly or COM component.\"</p>\\n\\n<p>Has anyone tried this and how did you go about it?  Again, my assumption is that I should be able to reference the dll, like all others, and then add a using/imports statement to use it in the app.  I haven\\'t seen a lot of documentation or other people using it from .net.  MonetDB.org doesn\\'t seem to have steps for it either other than to use it from excel or something.  What am I missing?</p>\\n-How do you reference monetdb odbc driver for .net-<.net><visual-studio><monetdb><add-references-dialog>',\n",
       " '<p>I have just started using rapid miner for text classification. I have created a process in which i used \"Process Document from Files\" operator for tf-idf conversion. I want to ask how to use this operator in Java code ? I search on internet but all are using the already created process or word list generated from documents ? I want to start it from scratch i.e. </p>\\n\\n<p>1 ) Process Documents From File </p>\\n\\n<p>1.1) Tokenization</p>\\n\\n<p>1.2) Filtering</p>\\n\\n<p>1.3) Stemming</p>\\n\\n<p>1.4) N-Gram</p>\\n\\n<p>2) Validation</p>\\n\\n<p>2.1) Training (K-NN)</p>\\n\\n<p>2.2) Apply Model</p>\\n-How to use \"Process Document From File\" operator of RapidMiner in Java Code-<rapidminer>',\n",
       " '<p>i am working on text classification in weka. I want to use a classifier from rapidminer. I just saw the \"weka.jar\" in rapidminer lib directory which may mean that we can use some cross functionality.</p>\\n\\n<p>Can we use a classifier or functionality from rapid miner whereas some other functionalities from weka ???</p>\\n-Can we used a classifier from rapid miner in weka?-<weka><rapidminer><text-classification>',\n",
       " \"<p>I'm having trouble (maybe caching issues) with JD-Eclipse plug-in.  In a .jar I can open a .class file Foo and it gets decompiled correctly.  In the same .jar I open a .class file Bar and a new tab gets opened called Bar, but it has the decompiled contents of Foo.</p>\\n\\n<p>If I go to a different .jar I can successfully decompile one .class file until I get the same behavior where every file from that .jar gets displayed as the first file.</p>\\n\\n<p>Any thoughts on how to fix this are much appreciated.</p>\\n\\n<pre><code>JDK: 1.7.51\\nEclipse Version: Kepler\\nEclipse Location: D:\\nJD-Eclipse Version: 0.1.4\\n</code></pre>\\n-Unable to open more than one .class file per .jar with JD-Eclipse-<eclipse><eclipse-kepler><jd-eclipse>\",\n",
       " '<p>While connecting Teradata DB,It is taking saved password from local Data sources(User DSN) in Control panel. </p>\\n\\n<p>Connection string used :    \"Provider=MSDASQL.1;Password=\" &amp; pwd &amp; \";Persist Security Info=False;Integrated Security=False;User ID=\" &amp; user &amp; \";Data Source=DCMQA;DSN=DCMQA\"</p>\\n\\n<p>If i give wrong password in UI also,it is taking the saved passed for user data sources(DCMQA) created in ODBC Data sources in control panel.</p>\\n\\n<p>How to change the connection string to take the user,name password given in UI?</p>\\n\\n<p>Can anyone plz help on this.</p>\\n-While connecting Teradata DB,It is taking saved password from local Data sources.-<connection-string><teradata>',\n",
       " '<p>I am using tableviewer with Check box style in the following way </p>\\n\\n<p><code>tableViewer = new TableViewer(parent, SWT.MULTI | SWT.FULL_SELECTION | SWT.BORDER| SWT.CHECK);</code></p>\\n\\n<p>I used the following code to listen to tableviewer. I know it is wrong because it listens to the selection and not to check box selection. </p>\\n\\n<pre><code> tableViewer.addSelectionChangedListener(new ISelectionChangedListener() {\\n           @Override\\n           public void selectionChanged(SelectionChangedEvent event) {\\n             IStructuredSelection selection = (IStructuredSelection)tableViewer.getSelection();\\n             Object firstElement = selection.getFirstElement();\\n             System.out.println(\"firstElement\"+firstElement);\\n             // Do something with it\\n           }\\n         }); \\n</code></pre>\\n\\n<p>I need to listen to checkbox selection in JFace TableViewer. </p>\\n\\n<p>Thanks in advance</p>\\n-JFace tableViewer checkbox selection returns null in eclipse e4-<java><eclipse-rcp><jface><eclipse-kepler><e4>',\n",
       " '<p>I want to analyze a large dataset (2,000,000 records, 20,000 customer IDs, 6 nominal attributes) using the Generalized Sequential Pattern algorithm. </p>\\n\\n<p>This requires all attributes, aside from the time and customer ID attribute, to be binominal. Having 6 nominal attributes which I want to analyze for patterns, I need to transform those into binominal attributes, using the \"Nominal to Binominal\" Function. This is causing memory problems on my workstation (with 16GB RAM, of which I allocated 12 to the Java instance running rapidminer). </p>\\n\\n<p>Ideally I would like to set up my project in a way, that it writes temporarily to the disc or using temporary tables in my oracle database, from which my model also reads the data directly. In order to use the \"write database\" or \"update database\" function, I need to have an existing table already in my database with boolean columns already (if I\\'m not mistaken).</p>\\n\\n<p>I tried to write step by step the results of the binominal conversion into csv files onto my local disk. I started using the nominal attribute with the least distinct values, resulting in a csv file containing my dataset ID and now 7 binominal attributes. I was seriously surprised seeing the filesize being >200MB already. This is cause by rapidminer writing strings for the binominal values \"true\"/\"false\". Wouldn\\'t it be way more memory efficient just writing 0/1?</p>\\n\\n<p>Is there a way to either use the oracle database directly or working with 0/1 values instead of \"true\"/\"false\"? My next column would have 3000 distinct values to be transformed which would end in a nightmare...</p>\\n\\n<p>I\\'d highly appreciate recommendations on how to use the memory more efficient or work directly in the database. If anyone knows how to easily transform a varchar2-column in Oracle into boolean columns for each distinct value that would also be appreciated!</p>\\n\\n<p>Thanks a lot,\\nHolger</p>\\n\\n<p><strong>edit:</strong> </p>\\n\\n<p>My goal is to get from such a structure:</p>\\n\\n<pre><code>column_a; column_b; customer_ID; timestamp\\n\\nvalue_aa; value_ba; 1; 1\\n\\nvalue_ab; value_ba; 1; 2\\n\\nvalue_ab; value_bb; 1; 3\\n</code></pre>\\n\\n<p>to this structure:</p>\\n\\n<pre><code>customer_ID; timestamp; column_a_value_aa; column_a_value_ab; column_b_value_ba; column_b_value_bb\\n\\n1; 1; 1; 0; 1; 0\\n\\n1; 2; 0; 1; 1; 0\\n\\n1; 3; 0; 1; 0; 1\\n</code></pre>\\n-Rapidminer: Memory issues transforming nominal to binominal attributes-<oracle><memory><transformation><gsp><rapidminer>',\n",
       " '<p>I cloned a Java project to my local repo. When importing the project to Eclipse it gives me an error saying that No projects are found to import. \\nAm using a Mac and the Eclipse package I am using is Kepler 64bit version. </p>\\n-Import a java project in Eclipse Kepler without .project file-<java><eclipse><eclipse-kepler>',\n",
       " '<p>When i am creating a new maven project in eclipse kepler, eclipse automatically adds junit 3.8 dependency in pom.xml like</p>\\n\\n<pre><code>&lt;dependency&gt;\\n    &lt;groupId&gt;junit&lt;/groupId&gt;\\n    &lt;artifactId&gt;junit&lt;/artifactId&gt;\\n    &lt;version&gt;3.8&lt;/version&gt;\\n    &lt;scope&gt;test&lt;/scope&gt;\\n&lt;/dependency&gt;\\n</code></pre>\\n\\n<p>I want to be available newer version like: 4.10, </p>\\n\\n<p><strong>Where can i configure to use newer version instead of old?</strong></p>\\n-how to configure maven to add junit 4.10 instead of 3.8 dependency in new project-<java><eclipse><junit><maven-3><eclipse-kepler>',\n",
       " \"<p>I'm trying to create a monetdb db with python. The db doesn't exist at the beginning: the code should create it specifying the port, the folder were it will reside &amp; the db name.\\nAll the examples I could fine clearly assume that the db exists already. \\nIn some way this should be similar to the operations typically managed by the moneddbd deamon. \\nHow do I setup a (new) monetdb db in python from scratch?</p>\\n-How to setup a (new) monetdb db from scratch in python?-<python><monetdb>\",\n",
       " '<p>I\\'m trying to load various CSVs into monetdb, using R and RStudio but I get the error below.\\nThis is the code (code tested with the iris dataset &amp; generally working fine) and the error message:</p>\\n\\n<pre><code>&gt; monetdb.read.csv(conn,fnamed,nrows=nrows,header=TRUE,tablename=tabled,delim=\",\")\\n\\n/Users/Enzo/NHS/data/sha_lookup.csv /Users/Enzo/NHS/data/sha_lookup.csv \\nError in .local(conn, statement, ...) : \\n  Unable to execute statement \\'copy 12 offset 2 records into sha_lookup from \\'/Users/Enzo/NHS/data/sh...\\'.\\nServer says \\'!failed to import table\\'.\\n</code></pre>\\n\\n<p>The file I\\'m trying to load is a short and simple csv:</p>\\n\\n<pre><code>    SHA,SHAname\\n    Q30,NORTH EAST STRATEGIC HEALTH AUTHORITY\\n    Q31,NORTH WEST STRATEGIC HEALTH AUTHORITY\\n    Q32,YORKSHIRE AND THE HUMBER STRATEGIC HEALTH AUTHORITY\\n    Q33,EAST MIDLANDS STRATEGIC HEALTH AUTHORITY\\n    Q34,WEST MIDLANDS STRATEGIC HEALTH AUTHORITY\\n    Q35,EAST OF ENGLAND STRATEGIC HEALTH AUTHORITY\\n    Q36,LONDON STRATEGIC HEALTH AUTHORITY\\n    Q37,SOUTH EAST COAST STRATEGIC HEALTH AUTHORITY\\n    Q38,SOUTH CENTRAL STRATEGIC HEALTH AUTHORITY\\n    Q39,SOUTH WEST STRATEGIC HEALTH AUTHORITY\\n</code></pre>\\n\\n<p>RStudio reads the above CSV file with no issues.  More importantly I can read this CSV into a R dataframe with read.csv, then write a CSV file with write.table.  Now I can load this \"new\" CSV into monetdb using the code above with no problem.</p>\\n\\n<p>Please note that I tried to edit the file using quotes \"\" everywhere to see if that was the problem, but I still got the error.</p>\\n-CSV load error with monetdb & R-<r><monetdb>',\n",
       " '<p>I am working in text classification on 20NewsGroup dataset with 100 documents in each category. I classify text documents via Naive Bayes using 10-fold cross validation, It runs successfully and give me results at the end.</p>\\n\\n<p>I tried same with KNN with 10-fold cross validation but it always ends in \"Process Failure\" which shows that it requires more memory than available. I increase heap space for rapidminer from 1GB to 2.5G in build.xml as well as rapidminerGUI.bat but nothing improves(so i assume heap space is not a problem) and it always ends up in demanding more memory.</p>\\n\\n<p>Kindly help, i am stuck out at it and tried every possible option i could think about</p>\\n\\n<p>Platform Details:</p>\\n\\n<p>OS: Windows 7(64 bit)\\nSoftware Version: Rapidminer 5.3 (64 bit)\\nJava: Java 1.7 (64 bit)</p>\\n-Why KNN in rapidminer is giving memory problems?-<rapidminer><text-classification>',\n",
       " '<p>I have a problem profiling the L2 cache on my CUDA card of compute capability 3.5.\\nIn Kepler (3.x) the loads from global memory are cached only in L2 and never in L1.\\nMy question is how do I use nvprof (command-line profiler) to find the hit rate my global loads achieve in the L2 cache? I have queried for all the metrics I can collect and the ones involving L2 ache are:</p>\\n\\n<pre><code>         l2_l1_read_hit_rate:  Hit rate at L2 cache for all read requests from L1 cache\\n    l2_texture_read_hit_rate:  Hit rate at L2 cache for all read requests from texture cache\\n       l2_l1_read_throughput:  Memory read throughput seen at L2 cache for read requests from L1 cache\\n  l2_texture_read_throughput:  Memory read throughput seen at L2 cache for read requests from the texture cache\\n        l2_read_transactions:  Memory read transactions seen at L2 cache for all read requests\\n       l2_write_transactions:  Memory write transactions seen at L2 cache for all write requests\\n          l2_read_throughput:  Memory read throughput seen at L2 cache for all read requests\\n         l2_write_throughput:  Memory write throughput seen at L2 cache for all write requests\\n              l2_utilization:  The utilization level of the L2 cache relative to the peak utilization\\n</code></pre>\\n\\n<p>The only hit rate I get is for reads coming from L1. But the reads to global memory would never come from L1 as they are not cached there! Or am I wrong here and that is exactly the metric I want?</p>\\n\\n<p>Surprisingly (or not) there is a metric giving the L1 hit rate for global memory reads.</p>\\n\\n<pre><code>    l1_cache_global_hit_rate:  Hit rate in L1 cache for global loads\\n</code></pre>\\n\\n<p>Can this ever be non-zero for Kepler?</p>\\n\\n<p>Cheers!</p>\\n-Profiling L2 cache on CUDA compute capability 3.x with nvprof-<caching><cuda><kepler>',\n",
       " '<p>I am working on parallel classifier combination and have a requirement in rapid miner to give output of \"Process Documents from Files\" operator to more than one classifier (L1, L2, L3). One way of doing it is to create three different processes and give each of them documents separately but that could be a performance bottleneck and i want to avoid it.</p>\\n\\n<p>Is there any way to provide example set copies to classifiers (L1, L2, L3) ??</p>\\n-How can i give output exampleset of \"Process Documents From Files\" to Multiple Classifiers in rapid miner?-<machine-learning><rapidminer><text-classification>',\n",
       " \"<p>I am trying to install the plugin for Spring IDE (3.4.0) using Eclipse Marketplace for Eclipse Kepler (4.3). It is giving the error 'Will not be installed (Spring IDE Roo Support)'.</p>\\n\\n<p>I also checked the compatibility of Spring IDE version 3.4.0 with Eclipse Kepler 4.3, and it is compatible. Any ideas on how to complete the installation? </p>\\n\\n<p>Thank you\\nPrachi</p>\\n-Error installing Spring IDE 3.4.0 plugin with Eclipse Kepler: Will not be installed (Spring IDE Roo Support)-<eclipse-plugin><spring-roo><eclipse-kepler>\",\n",
       " '<p>I need to dynamically input files into my java application and use rapidminer for classification. I have the following code:</p>\\n\\n<pre><code>RapidMiner.setExecutionMode(RapidMiner.ExecutionMode.COMMAND_LINE);\\n          RapidMiner.init();\\n\\n          Process process = new Process(new File(\"C:\\\\\\\\Users\\\\\\\\nelze\\\\\\\\.RapidMiner5      \\\\\\\\repositories\\\\\\\\WalkingRepo\\\\\\\\NaiveClassify.rmp\"));\\n          Operator op = process.getOperator(\"Read CSV\");\\n          op.setParameter(com.rapidminer.operator.nio.CSVExampleSource.PARAMETER_CSV_FILE, \"C:\\\\\\\\Users\\\\\\\\nelze\\\\\\\\unlabeled (2).csv\");\\n          //ioInput.append(op);\\n          IOContainer ioResult = process.run();\\n          ExampleSet resultSet1 = (ExampleSet)ioResult.getElementAt(0);\\n          Iterator&lt;Attribute&gt; allAttributes = resultSet1.getAttributes().allAttributes();\\n</code></pre>\\n\\n<p>However, when I run it it always says \"input file not defined.\"</p>\\n\\n<p>In my RapidMiner process, the input port is connected to the READ CSV operator.</p>\\n\\n<p>Any help will be appreciated</p>\\n-Dynamically add input to RapidMiner through Java-<java><rapidminer>',\n",
       " '<p>I have some SQL table, say with a single column c1:</p>\\n<div class=\"s-table-container\">\\n<table class=\"s-table\">\\n<thead>\\n<tr>\\n<th>c1</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td>10</td>\\n</tr>\\n<tr>\\n<td>3</td>\\n</tr>\\n<tr>\\n<td>1</td>\\n</tr>\\n<tr>\\n<td>10</td>\\n</tr>\\n<tr>\\n<td>5</td>\\n</tr>\\n</tbody>\\n</table>\\n</div>\\n<p>Now, I\\'d like to issue an SQL command (not some operation of my DBMS, which I have intentionally not mentioned) which causes my table to be:</p>\\n<div class=\"s-table-container\">\\n<table class=\"s-table\">\\n<thead>\\n<tr>\\n<th>c1</th>\\n<th>record_index</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td>10</td>\\n<td>0</td>\\n</tr>\\n<tr>\\n<td>3</td>\\n<td>1</td>\\n</tr>\\n<tr>\\n<td>1</td>\\n<td>2</td>\\n</tr>\\n<tr>\\n<td>10</td>\\n<td>3</td>\\n</tr>\\n<tr>\\n<td>5</td>\\n<td>4</td>\\n</tr>\\n</tbody>\\n</table>\\n</div>\\n<p>Very simple... can this be done? Obviously, you don\\'t know in advance the length of the table, so no SQL insertion or similar tricks.</p>\\n<p><strong>Note:</strong> I\\'d like a general answer, but to be specific - I\\'m working with <a href=\"http://www.monetdb.org/\" rel=\"nofollow noreferrer\">MonetDB</a>.</p>\\n-SQL statement to generate a column whose value is the record index-<sql><range><idioms><monetdb><iota>',\n",
       " '<p>I am dealing with text classification in rapidminer. I have seperate test and training splits. I applied Information Gain to a dataset using n-fold cross validation but i am confused on how to apply it on seperate test set ? Below is attached image <img src=\"https://i.stack.imgur.com/RETm7.png\" alt=\"enter image description here\"></p>\\n\\n<p>In figure i have connected the word list output from first \"Process Documents From Files\" which is used for training to second \"Processed Documents From Files\" which is used for testing but i want to apply the reduced feature to the second \"Process Documents From Files\" which perhaps should be the one returned from \"Select By Weight\" (reduced dimensions) operator but it returns weights which i cannot provide to second \"Process Documents From Files\". I searched alot but did\\'nt managed to find anything which can satisfy my need ?</p>\\n\\n<p>Is it really possible for Rapidminer to have seperate test/train splits and apply feature selection ?</p>\\n\\n<p>Is there any way to convert these weights into word list ? Please don\\'t say write in repository (i can\\'t do this) ?</p>\\n\\n<p>In such scenario when i have different test/train splits and needs to apply feature selection, how would i make sure that test/train splits have same dimension vectors ?</p>\\n\\n<p>I am really trapped out at it, kindly help ...</p>\\n-How to apply InformationGain in rapidminer with seperate test set ?-<machine-learning><rapidminer><text-classification>',\n",
       " \"<p>on my Mac debugging in Eclipse is extremly slow. With slow I mean stepping through the code. Every step (even if it is only multiline string-concatenation) takes about 1-2 seconds. When I compare that to eclipse running on Windows, that is just painful!</p>\\n\\n<p>I'm using Eclipse Kepler with Sun JDK 1.7.0_45 (as Eclipse Runtime and Program Runtime). Switching to Apple JDK 6 did not change that much. Is there anything that I can do about that?</p>\\n\\n<p><strong>Edit:</strong></p>\\n\\n<p>What I'm trying to do is to debug a local application using Debug As > Java Application.</p>\\n\\n<p>My eclipse.ini is looking like this:</p>\\n\\n<pre><code>-startup\\n../../../plugins/org.eclipse.equinox.launcher_1.3.0.v20130327-1440.jar\\n-vm\\n/Library/Java/JavaVirtualMachines/jdk1.7.0_45.jdk/Contents/Home/bin/java\\n--launcher.library\\n../../../plugins/org.eclipse.equinox.launcher.cocoa.macosx.x86_64_1.1.200.v20130807-1835\\n-product\\norg.eclipse.epp.package.java.product\\n--launcher.defaultAction\\nopenFile\\n-showsplash\\norg.eclipse.platform\\n--launcher.XXMaxPermSize\\n256m\\n--launcher.defaultAction\\nopenFile\\n--launcher.appendVmargs\\n-vmargs\\n-Dorg.eclipse.swt.internal.carbon.smallFonts\\n-Dorg.eclipse.swt.browser.IEVersion=10001\\n-Dosgi.requiredJavaVersion=1.6\\n-XstartOnFirstThread\\n-Dorg.eclipse.swt.internal.carbon.smallFonts\\n-XX:MaxPermSize=512m\\n-Xms40m\\n-Xmx2048m\\n-Xdock:icon=../Resources/Eclipse.icns\\n-XstartOnFirstThread\\n-Dorg.eclipse.swt.internal.carbon.smallFonts\\n</code></pre>\\n-Debugging in Eclipse on MacOS X slow - anything that I can do?-<java><eclipse><macos><debugging><eclipse-kepler>\",\n",
       " '<p>i have install spring STS and Spring IDE (Eclipse Plug-in) in eclipse Kepler from Eclipse Marketplace ,\\nThe installation process has successfully , But did not change anything in the interface, and I did not find spring project in project wizard</p>\\n\\n<p>OS:Linux Fedora\\nIDE:Eclipse kepler</p>\\n-installation spring tools in eclipse kepler-<spring><eclipse-kepler>',\n",
       " '<p>I need some functionality to be used from weka and some functionality to be used from rapidminer. How can i convert into rapid miner exampleset into weka instances perform some operations and then convert back into weka instances ?</p>\\n\\n<p>I found a class on internet mentioned as \"WekaTools\"  but it does not exist in rapidminer(5.3.013)</p>\\n\\n<p>Can any one tell me where this functionality is moved in rapid miner 5.3.013? or how can i convert from Rapid miner example set into weka Instances ?</p>\\n-How can i convert from Rapid Miner exampleset into weka instances?-<machine-learning><rapidminer><text-classification>',\n",
       " '<p>Plugin execution not covered by lifecycle configuration: org.apache.maven.plugins:maven-resources-plugin:2.5:resources (execution: default-resources, phase: process-resources) pom.xml /Project line 1 Maven Project Build Lifecycle Mapping Problem</p>\\n\\n<p>Here is the POM.xml</p>\\n\\n<pre><code>&lt;project xmlns=\"maven.apache.org/POM/4.0.0\"; xmlns:xsi=\"w3.org/2001/XMLSchema-instance\"; xsi:schemaLocation=\"maven.apache.org/POM/4.0.0 maven.apache.org/xsd/maven-4.0.0.xsd\"&gt;;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; \\n &lt;groupId&gt;Assignment&lt;/groupId&gt;\\n &lt;artifactId&gt;ApsalarAssignment&lt;/artifactId&gt;\\n &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;\\n&lt;/project&gt;\\n</code></pre>\\n\\n<p>I am getting this error on Kepler, Eclipse while making a new Maven Project. Don\\'t know how to figure it out as I am new to Maven as well as Eclipse. Any help would be highly appreciated.</p>\\n-Eclipse Kepler Project Configuration-<java><eclipse><maven><configuration><eclipse-kepler>',\n",
       " '<p>I am readind from kepler whitepaper  <a href=\"http://docs.nvidia.com/cuda/kepler-tuning-guide/\" rel=\"nofollow\"> here</a></p>\\n\\n<p>that kepler supports up to  16 blocks / mp.</p>\\n\\n<p>But threads/blocks = 1024  and threads/mp = 2048   ,hence  blocks/mp = 2 .</p>\\n\\n<p>Am I missing something here? </p>\\n-kepler blocks per mp?-<cuda><kepler>',\n",
       " \"<p>everyone</p>\\n\\n<p>Thanks for reading my question. I am doing a project with Rapidminer, I have to add a new algorithm as a operator in Rapidminer, I'm just a beginner, I have been read some docs about this, but I still don't know how to do it, so I need your help to give some solutions.</p>\\n\\n<p>What I know now, we have two solutions to add or change a operator</p>\\n\\n<ol>\\n<li>click right button on a operator which exist already in Rapidminer's operators.</li>\\n<li>import Rapidminer project in Eclipse</li>\\n</ol>\\n\\n<p>But, I just download Rapidminer, I didn't found the operation 'edit source code' when I clicked right button on a operator. </p>\\n\\n<p>And if I import the project, whether I have to import the whole project on github? Or I just need import some jars. </p>\\n\\n<p>Once I finish my 'operator', I just need to export the jar of my 'operator' and put it in the folder of 'plugin' in the location of installation Rapidminer? If just like what I say, It will be not easy for debug...?</p>\\n\\n<p>Please give me some suggestions about my project as you can.\\nThank you very much.</p>\\n\\n<p>Jiang</p>\\n-How to add operator for rapidminer-<plugins><jar><operators><data-mining><rapidminer>\",\n",
       " '<p>I have some database tables that I want to use in RapidMiner. Is there going to be a performance difference between writing sql to join the tables or importing the tables separately and performing a join after?</p>\\n-Rapidminer - importing tables that need to be joined-<join><rapidminer>',\n",
       " '<p>Running MonetDB Database Server Toolkit v1.1 (Feb2013-SP6)</p>\\n\\n<p>This query </p>\\n\\n<pre><code>select rowtype, min(zdate), max(zdate) , count(*) \\nfrom fdhista \\ngroup by rowtype \\n;\\n</code></pre>\\n\\n<p>returns correct minimum and maximum dates for each rowtype.</p>\\n\\n<pre><code>rowtype L1  L2  L3\\n3   1970-12-31  2009-07-31  1664186\\n1   2003-02-24  2013-09-13  11649306\\n</code></pre>\\n\\n<p>This query, over the same table</p>\\n\\n<pre><code>select min(zdate), max(zdate), count(*) from fdhista where rowtype=3;\\n</code></pre>\\n\\n<p>seems to \"ignore\" the where clause, returning</p>\\n\\n<pre><code>L1  L2  L3\\n1970-12-31  2013-09-13  13313492\\n</code></pre>\\n\\n<p>I haven\\'t found a general sql precedent (yet) for that answer. Is this the expected response? </p>\\n\\n<p>I was expecting this</p>\\n\\n<pre><code>L1  L2  L3\\n1970-12-31  2009-07-31  1664186\\n</code></pre>\\n\\n<p>I tried similiar queries in Oracle and SQL Server and get back my expected response. Yet I find <a href=\"https://stackoverflow.com/a/6360290/2013743\">generic sql comments</a> that support \"ignoring\" the where clause. Maybe this is a case of MonetDB\\'s use of a specific SQL standard?</p>\\n-min/max query with a where clause versus group by-<sql><monetdb>',\n",
       " '<p>I am using django as a web framework. I need a workflow engine that can do synchronous as well as asynchronous(batch tasks) chain of tasks. I found celery and luigi as batch processing workflow. My first question is what is the difference between these two modules. </p>\\n\\n<p>Luigi allows us to rerun failed chain of task and only failed sub-tasks get re-executed. What about celery: if we rerun the chain (after fixing failed sub-task code), will it rerun the already succeed sub-tasks?</p>\\n\\n<p>Suppose I have two sub-tasks. The first one creates some files and the second one reads those files. When I put these into chain in celery, the whole chain fails due to buggy code in second task. What happens when I rerun the chain after fixing the code in second task? Will the first task try to recreate those files?</p>\\n-Python based asynchronous workflow modules : What is difference between celery workflow and luigi workflow?-<python><celery><luigi>',\n",
       " '<p>Hi I just installed Kepler and started using EGit. In the history view, changes checked in in the past are showing as xx days/weeks/months ago. Instead of these notations, I want to see the datetime details (or best to have both along side each other).</p>\\n\\n<p>How do you config EGit to do that?</p>\\n\\n<p>If it is not possible in the current config, I dont mind compiling my own patched version of EGit, please provide where the line of code lies.</p>\\n\\n<p>Thank you</p>\\n-Show date details in EGit history-<eclipse><config><egit><eclipse-kepler>',\n",
       " \"<p>I want to generate AUTOMATIC Number to use TD SQL, for example as follows,</p>\\n\\n<pre><code>CREATE MULTISET TABLE TEST_TABLE\\n(\\n  AUTO_NUMBER INT,\\n  NAME VARCHAR(10)\\n)\\nPRIMARY INDEX (AUTO_NUMBER);\\n\\nINSERT INTO TEST_TABLE\\nVALUES('TOM');\\nINSERT INTO TEST_TABLE\\nVALUES('JIM');\\nINSERT INTO TEST_TABLE\\nVALUES('JAN');\\n\\nSELECT * FROM TEST_TABLE;\\n</code></pre>\\n\\n<p>The result above will be ,</p>\\n\\n<pre><code>1 TOM\\n2 JIM\\n3 JAN\\n</code></pre>\\n-How to generate AUTOMATIC Number in Teradata SQL-<sql><auto-increment><teradata>\",\n",
       " '<p>I am trying to plot two data columns coming from different tables using KNIME. I want to plot them in a single plot in order to be easier to compare them. In R this effect can be achieved by the following:</p>\\n\\n<pre><code>boxplot(df$Delay, df2$Delay,names=c(\"Column from Table1\",\"Column from Table2\"), outline=FALSE)\\n</code></pre>\\n\\n<p>However, by using KNIME, I cannot think of a way that you can use data coming from two different tables. Have you ever faced this issue in KNIME?</p>\\n-Creating a boxplot from two tables in KNIME-<r><knime>',\n",
       " \"<p>i'm running into a problem with the fp-growth operator in rapidminer. i'm processing about 20 text files that are all in all &lt;1MB in size. i used the process documents operator and within that tokenize, filter stop words, transform cases, generate n-grams, and filter tokens. from there i used the numerical to binominal operator. up until this point everything works fine, but when i run the fp-growth operator it just processes indefinitely with no result. i tried tweaking the min support parameter, but to no avail. would you have any suggestions on how to troubleshoot this? i'd really appreciate any guidance.</p>\\n-RapidMiner FP-growth operator not returning any results-<text-mining><rapidminer>\",\n",
       " \"<p>Once in a while my mac gets stuck and then I need to hard reset it using the power button. A short time (but not immediately) after reboot, the <code>.svn</code> directories in my eclipse project directories disappear.</p>\\n\\n<p>The call to <code>svn info</code> returns:</p>\\n\\n<blockquote>\\n  <p>svn: E155007: '../workspaces/project_name' is not a working copy</p>\\n</blockquote>\\n\\n<p>It means I can't work with svn and need to checkout all the stuff all over again. This happened to me few times now and I really don't know why.\\nThings I've tried and did not solve the problem:</p>\\n\\n<ul>\\n<li>Replacing Eclipse</li>\\n<li>Upgrading svn version</li>\\n<li>Upgrading Maven version</li>\\n</ul>\\n\\n<p>relevant version data:</p>\\n\\n<ul>\\n<li>Apache Maven 3.1.1</li>\\n<li>Java version: 1.7.0_51</li>\\n<li>mac os X 10.9.1</li>\\n<li>Eclipse Kepler Service Release 1</li>\\n<li>svn, version 1.8.5 (r1542147)</li>\\n</ul>\\n\\n<p>I'm quite clueless about this issue and would like to hear any ideas...</p>\\n-local .svn directory disappears after crash-<eclipse><macos><maven><svn><eclipse-kepler>\",\n",
       " '<p>We have an situation where in  we need to load rows of data from teradata tables to mysql tables\\nany hint on how i could start with it?\\ncanfastexport(or any other utility) help?</p>\\n\\n<p>Thank you in advance :)</p>\\n-Load rows of data from teradata tables to mysql tables-<mysql><teradata>',\n",
       " '<p>I am using pmd-eclipse plugin 4.0.0.201211080927. In Indigo i am able to run the pmd rules successfully. But when i run it in Kepler, it is not showing anything. But in the eclipse log i can see the following Info message,</p>\\n\\n<pre><code>Review code command terminated. 188 rules were executed against 1 files.\\n</code></pre>\\n\\n<p>Is there any configuration should be done in kepler to work with pmd?</p>\\n-PMD Eclipse Plugin is not working in Kepler-<eclipse><pmd><eclipse-kepler>',\n",
       " '<p>I need help about the following question: i would process each row from a data table (Example set structure (<code>label[datatype]): mid[int], body[text]</code>), obtained by Read Database, as a document. In this way i can apply some filters to each document (stop-words, filters, and so on). Can anyone help me?</p>\\n\\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?&gt;\\n&lt;process version=\"5.3.015\"&gt;\\n  &lt;context&gt;\\n    &lt;input/&gt;\\n    &lt;output/&gt;\\n    &lt;macros/&gt;\\n  &lt;/context&gt;\\n  &lt;operator activated=\"true\" class=\"process\" compatibility=\"5.3.015\" expanded=\"true\" name=\"Process\"&gt;\\n    &lt;process expanded=\"true\"&gt;\\n      &lt;operator activated=\"true\" class=\"read_database\" compatibility=\"5.3.015\" expanded=\"true\" height=\"60\" name=\"Server Connection\" width=\"90\" x=\"112\" y=\"120\"&gt;\\n        &lt;parameter key=\"connection\" value=\"Server\"/&gt;\\n        &lt;parameter key=\"query\" value=\"SELECT `mid`, `body` FROM `message` WHERE `mid` &amp;lt; 10 ORDER BY `mid`\"/&gt;\\n        &lt;enumeration key=\"parameters\"/&gt;\\n      &lt;/operator&gt;\\n      &lt;operator activated=\"true\" class=\"loop_data_sets\" compatibility=\"5.3.015\" expanded=\"true\" height=\"76\" name=\"Loop Data Sets\" width=\"90\" x=\"111\" y=\"210\"&gt;\\n        &lt;process expanded=\"true\"&gt;\\n          &lt;operator activated=\"true\" class=\"text:extract_document\" compatibility=\"5.3.002\" expanded=\"true\" name=\"Extract Document (2)\"&gt;\\n            &lt;parameter key=\"attribute_name\" value=\"body\"/&gt;\\n            &lt;parameter key=\"example_index\" value=\"1\"/&gt;\\n          &lt;/operator&gt;\\n          &lt;operator activated=\"true\" class=\"text:transform_cases\" compatibility=\"5.3.002\" expanded=\"true\" name=\"Transform Cases (2)\"/&gt;\\n          &lt;operator activated=\"true\" class=\"text:tokenize\" compatibility=\"5.3.002\" expanded=\"true\" name=\"Tokenize (2)\"/&gt;\\n          &lt;operator activated=\"true\" class=\"text:filter_stopwords_dictionary\" compatibility=\"5.3.002\" expanded=\"true\" name=\"Filter Stopwords (2)\"&gt;\\n            &lt;parameter key=\"file\" value=\"C:\\\\User\\\\stopwords.txt\"/&gt;\\n          &lt;/operator&gt;\\n          &lt;operator activated=\"true\" class=\"text:filter_by_length\" compatibility=\"5.3.002\" expanded=\"true\" name=\"Filter Tokens (2)\"&gt;\\n            &lt;parameter key=\"min_chars\" value=\"2\"/&gt;\\n          &lt;/operator&gt;\\n          &lt;connect from_port=\"example set\" to_op=\"Extract Document (2)\" to_port=\"example set\"/&gt;\\n          &lt;connect from_op=\"Extract Document (2)\" from_port=\"document\" to_op=\"Transform Cases (2)\" to_port=\"document\"/&gt;\\n          &lt;connect from_op=\"Transform Cases (2)\" from_port=\"document\" to_op=\"Tokenize (2)\" to_port=\"document\"/&gt;\\n          &lt;connect from_op=\"Tokenize (2)\" from_port=\"document\" to_op=\"Filter Stopwords (2)\" to_port=\"document\"/&gt;\\n          &lt;connect from_op=\"Filter Stopwords (2)\" from_port=\"document\" to_op=\"Filter Tokens (2)\" to_port=\"document\"/&gt;\\n          &lt;connect from_op=\"Filter Tokens (2)\" from_port=\"document\" to_port=\"output 1\"/&gt;\\n          &lt;portSpacing port=\"source_example set\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"sink_performance\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"sink_output 1\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"sink_output 2\" spacing=\"0\"/&gt;\\n        &lt;/process&gt;\\n      &lt;/operator&gt;\\n      &lt;connect from_op=\"Server Connection\" from_port=\"output\" to_op=\"Loop Data Sets\" to_port=\"example set 1\"/&gt;\\n      &lt;connect from_op=\"Loop Data Sets\" from_port=\"output 1\" to_port=\"result 1\"/&gt;\\n      &lt;portSpacing port=\"source_input 1\" spacing=\"0\"/&gt;\\n      &lt;portSpacing port=\"sink_result 1\" spacing=\"0\"/&gt;\\n      &lt;portSpacing port=\"sink_result 2\" spacing=\"0\"/&gt;\\n    &lt;/process&gt;\\n  &lt;/operator&gt;\\n&lt;/process&gt;\\n</code></pre>\\n-RapidMiner - process data table rows as documents-<mysql><rapidminer>',\n",
       " '<p>I\\'m using Eclipse Kepler on OS X 10.9.2, and editing non-Java (HTML/JS/CSS) files in Sublime Text. I\\'ve checked \"Refresh using native hooks or polling\" from Window > Preferences > General > Workspace , but nothing is actually refreshed until I switch focus to Eclipse. So I have to save in ST, switch to Eclipse, then switch to the browser to refresh. I\\'d like to be able to skip switching to Eclipse, and thought that was what this option was supposed to allow. It\\'s not a matter of just refreshing too quickly when I don\\'t go via Eclipse - it never rebuilds until I switch to Eclipse. Is there any way to get this to work, or a workaround?</p>\\n-Eclipse not refreshing files using native hooks or polling-<eclipse><eclipse-kepler>',\n",
       " \"<p>I get this error with Monetdb when I try to load .tbl data in tables where there are primary key and foreign key, what's wrong?\\nThis is the command:</p>\\n\\n<pre><code>COPY INTO monet.CUSTOMER FROM '/home/nicola/Scrivania/ssb-dbgen-master/1gb/customer.tbl' USING DELIMITERS '|', '|\\\\n' LOCKED;\\n</code></pre>\\n-monetdb - copy into from...requires tables without indices-<indices><monetdb>\",\n",
       " '<p>I want to split strings (deleting cointain of ()) in form </p>\\n\\n<pre><code>eye of vichy, the (l\\'oeil de vichy)\\n</code></pre>\\n\\n<p>to</p>\\n\\n<pre><code>eye of vichy, the\\n</code></pre>\\n\\n<p>I need to use regular expression operator \"Replace\".</p>\\n-Rapidminer, splitting string-<rapidminer>',\n",
       " '<p>Firstly, I\\'m using MonetDB branch from MonetDB Database Server Toolkit v1.1 (Feb2013-SP1)</p>\\n\\n<p>I have two tables, and want to complete the following two functions without creating a temporary table:</p>\\n\\n<p>1) update one table from another table; and\\n2) update one table from aggregation of another table.</p>\\n\\n<p>I figured out how to do them in MySQL and Postgresql but similar queries failed in MonetDB.\\xa0</p>\\n\\n<p>For 1), Suppose I have the following two tables:</p>\\n\\n<pre><code>drop table t1; create table t1(id int, data int);\\ndrop table t2; create table t2(id int, data int);\\ndelete from t1; insert into t1 values (0, 0), (1,1), (2,2); select*from t1;\\ndelete from t2; insert into t2 values (1,2), (2, 3), (3,4); select*from t2;\\n</code></pre>\\n\\n<p>We need to set t1.data to t1.data+t2.data when their ids match, t1.id=t2.id.</p>\\n\\n<p>In MySQL, we can use:</p>\\n\\n<pre><code>update t1 inner join t2 on t1.id=t2.id set t1.data=t1.data+t2.data;\\n</code></pre>\\n\\n<p>In Postgresql, we can use:</p>\\n\\n<pre><code>with tmp as (select t1.id, t1.data+t2.data as data from t1,t2 where t1.id=t2.id) update t1 set data = tmp.data from tmp where t1.id=tmp.id;select*from t1;\\n</code></pre>\\n\\n<p>Or for both, we can use:</p>\\n\\n<pre><code>update t1 set data=(select t1.data+t2.data from t2 where t2.id=t1.id) where exists (select * from t2 where t1.id=t2.id);\\n</code></pre>\\n\\n<p>But none of them work in MonetDB. </p>\\n\\n<p>This question is similar to question asked before (<a href=\"https://www.monetdb.org/pipermail/users-list/2011-August/005072.html\" rel=\"nofollow\">https://www.monetdb.org/pipermail/users-list/2011-August/005072.html</a>), but the answered suggested sees to be incorrect.\\xa0There the two tables having the same value for tuple id=2.</p>\\n\\n<pre><code>update t1 set data=(select data+t2.data from t2 where t2.id=t1.id) where exists (select * from t2 where t1.id=t2.id);\\n</code></pre>\\n\\n<p>Thus in this query, the data field in 1st subquery of select actually comes from t2 not t1. However, if we changed to t1.data, the parser will not recognize name t1. It is somehow wired that it can recognize t1.id in condition clause.</p>\\n\\n<p>Not that the following query is also not correct:</p>\\n\\n<pre><code>update t1 set data=(select t1.data + t2.data from t1, t2 where t2.id = t1.id) where exists (select t2.id from t2 where t2.id = t1.id);\\n</code></pre>\\n\\n<p>As the subquery becomes independent from external query, and might result in multiple values which violates cardinality of assignment.</p>\\n\\n<p>Also changing field names in t1 and t2 will not be helpful either.</p>\\n\\n<p>For 2), Suppose we have the following two tables:</p>\\n\\n<pre><code>delete from t1; insert into t1 values (0, 0), (1,1), (2,2); select * from t1;\\ndelete from t2; insert into t2 values (1,2), (2, 3), (3,4), (2,5); select * from t2;\\n</code></pre>\\n\\n<p>We want to aggregate values in t2 and assign results to t1.</p>\\n\\n<p>In MySQL, we can use:</p>\\n\\n<pre><code>update t1 inner join t2 on t1.id=t2.id set t1.data=(select sum(t2.data) from t2 where t2.id=t1.id group by t2.id);\\n</code></pre>\\n\\n<p>Or in Postgresql, we can use:</p>\\n\\n<pre><code>with tmp as (select t2.id, sum(t2.data) as data from t2 group by t2.id) update t1 set data=tmp.data from tmp where t1.id=tmp.id;\\n</code></pre>\\n\\n<p>In MonetDB, how to correctly write such subquery in update statement?</p>\\n\\n<p>It looks like that MonetDB supports with clause in select, but not in update.</p>\\n\\n<pre><code>with tmp as (select t2.id, sum(t2.data) as data from t2 group by t2.id) select * from tmp; \\n</code></pre>\\n\\n<p>I\\'m also interested to know the performance difference between using subqueries and creating temporary tables.</p>\\n\\n<p>Thanks!</p>\\n-How to update a table using another table or aggregation results from subquery?-<sql-update><subquery><aggregate-functions><monetdb>',\n",
       " '<p>I am using Rapidminer to do a Naive Bayes text classification.\\nMy training set in an excel sheet with 2 columns: the first column is the LABEL and the  second is the TEXT.</p>\\n\\n<p>I used the the \"Read Excel\" operator to read the excel sheet (I use the \"Set Role\" operator to make sure that the LABEL column assumes the role of label and the TEXT column the text). \\nI then used the \"Data to Documents\" operator and the \"Process Documents\" operator (token, stopword, stem, case, etc.) to process the data. However, when I tried to port the data to \"Naive Bayes\" operator, an error msg told me that the data is not labelled and asked me to use \"Set Role\" operator. So I added another \"Set Role\" after the \"Process Documents\" operator, and only \"text\" in the \"attribute name\", the LABEL disappeared.\\nI have no idea what went wrong.</p>\\n-Text classification using Rapidminer with data stored in Excelsheet-<excel><document><rapidminer>',\n",
       " \"<p>How can I view the content of a MONETDB tables ?<br>\\nIn WAMP for example I just type localhost in the browser and there i can see \\nAll the tables and databases with their content .<br>\\nBut I'm unable to do so in MONETDB , Or to be more accurate I don't know how .<br>\\nPlus the documentation doesn't provide info on the matter . </p>\\n-View monetdb table content-<monetdb>\",\n",
       " '<p>I have a table with dates and val that I trying to expand and fill in missing dates in order. Not shown is that I am doing this by group and location, but the crux of what I need to do is below. Say I have the following table </p>\\n\\n<pre><code>dt | val\\n2014-01-01 | 10\\n2014-02-17  | 9 \\n2014-04-21  | 5\\n</code></pre>\\n\\n<p>I have expanded to this is a week table filling in missing with zeros</p>\\n\\n<pre><code>week_bgn_dt| week_end_dt|  val\\n2014-01-01 | 2014-01-08 |  10\\n2014-01-09 | 2014-01-16 |  0\\n2014-01-17 | 2014-01-24 |  0\\n...\\n2014-02-10 | 2014-02-17 |  0\\n2014-02-18 | 2014-02-25 |  9\\n2014-02-26 | 2014-03-05 |  0\\n2014-03-06 | 2014-03-13 |  0\\n...\\n2014-03-30 | 2014-04-06 |  0\\n2014-04-07 | 2014-04-14 |  0\\n2014-04-15 | 2014-04-22 |  5\\n</code></pre>\\n\\n<p>what I want is fill in with the last value until a change, so the output would looks like</p>\\n\\n<pre><code>week_bgn_dt| week_end_dt|  val\\n2014-01-01 | 2014-01-08 |  10\\n2014-01-09 | 2014-01-16 |  10\\n2014-01-17 | 2014-01-24 |  10\\n...\\n2014-02-10 | 2014-02-17 |  10\\n2014-02-18 | 2014-02-25 |  9\\n2014-02-26 | 2014-03-05 |  9\\n2014-03-06 | 2014-03-13 |  9\\n...\\n2014-03-30 | 2014-04-06 |  9\\n2014-04-07 | 2014-04-14 |  9\\n2014-04-15 | 2014-04-22 |  5\\n</code></pre>\\n\\n<p>In teradata I have tried this </p>\\n\\n<pre><code>case when val &lt;&gt; 0 then val \\n   else sum(val) over (partition by group, location order by group, store, week_bgn_dt 1 preceding  to current row) as val2\\n</code></pre>\\n\\n<p>but this only give the last value once, like so, </p>\\n\\n<pre><code>week_bgn_dt| week_end_dt|  val | val2\\n2014-01-01 | 2014-01-08 |  10  | 10\\n2014-01-09 | 2014-01-16 |  0   | 10\\n2014-01-17 | 2014-01-24 |  0   | 0\\n...\\n2014-02-10 | 2014-02-17 |  0   | 0 \\n2014-02-18 | 2014-02-25 |  9   | 9\\n2014-02-26 | 2014-03-05 |  0   | 9\\n2014-03-06 | 2014-03-13 |  0   | 0\\n...\\n2014-03-30 | 2014-04-06 |  0   | 0\\n2014-04-07 | 2014-04-14 |  0   | 0\\n2014-04-15 | 2014-04-22 |  5   | 5\\n</code></pre>\\n\\n<p>If I make the window unbounded, it sums when I hit a new value </p>\\n\\n<pre><code>case when val &lt;&gt; 0 then val \\n   else sum(val) over (partition by group, location order by group, store, week_bgn_dt unbounded preceding  to current row) as val2\\n\\nweek_bgn_dt| week_end_dt|  val | val2\\n2014-01-01 | 2014-01-08 |  10  | 10\\n2014-01-09 | 2014-01-16 |  0   | 10\\n2014-01-17 | 2014-01-24 |  0   | 10\\n...\\n2014-02-10 | 2014-02-17 |  0   | 10 \\n2014-02-18 | 2014-02-25 |  9   | 9\\n2014-02-26 | 2014-03-05 |  0   | 19\\n2014-03-06 | 2014-03-13 |  0   | 19\\n...\\n2014-03-30 | 2014-04-06 |  0   | 19\\n2014-04-07 | 2014-04-14 |  0   | 19\\n2014-04-15 | 2014-04-22 |  5   | 5\\n</code></pre>\\n\\n<p>I have tried with max() and min(), but to similar results. Thank you for any assistance. </p>\\n-Teradata conditional expand-<sql><teradata>',\n",
       " \"<p>Now I know this means I need to add my include path somewhere. So I have gone to properties, C/C++ General, Paths and Symbols, GNU C++ and then I have added /usr/include/c++/4.8 (to debug and release), but intellisense still cant detect  and it the project doesn't build.</p>\\n\\n<p>Is my include path the correct one for a standard default installation on Linux and did I enter it in the correct Eclipse setting?</p>\\n\\n<p>This is on Linux Mint 16 and Eclipse Kepler.</p>\\n-Eclipse CDT not recognizing <iostream>-<c++><linux><eclipse><include-path><eclipse-kepler>\",\n",
       " \"<p>I'm running into issues when running the following query on a monetdb database:</p>\\n\\n<pre><code>SELECT\\n    tpuc.tbl1_col1,\\n    s.tbl2_col1,\\n    COUNT(s.tbl2_col2)\\nFROM\\n    tbl2 AS s INNER JOIN\\n    tbl AS tpuc ON (s.tbl2_col2=tpuc.tbl1_col2)\\nGROUP BY\\n    tpuc.tbl1_col1,\\n    s.tbl2_col1\\n</code></pre>\\n\\n<blockquote>\\n  <p>Can not create object BATproject: does not match always</p>\\n</blockquote>\\n\\n<p>tbl2 has just a little over 35mil rows and tbl1 has around 300k rows. The query runs flawlessly on a database with an identical structure but considerably less data (about a tenth) so I assume this is related to the amount of data. Does anyone have experience with this error? </p>\\n\\n<p>I've ran queries on larger databases than this and monetdb seems to be designed to handle more data so I'm thinking that this is maybe some sort of error on my part / data corruption. However, the error doesn't seem to be documented anywhere so every little bit of insight is appreciated.</p>\\n\\n<p>later edit:\\nrunning into the same error when running simpler queries such as select group by</p>\\n\\n<p>Thanks</p>\\n-BATproject error when querying monetdb database-<sql><database><monetdb>\",\n",
       " '<p>What is the difference between <em>eclipse juno</em> and <em>eclipse kepler</em>, I have done a web project in <em>juno</em>. can I import it in to <em>kepler</em> ?</p>\\n-Difference Between Eclipse Juno and Eclipse Kepler-<java><eclipse><eclipse-kepler>',\n",
       " '<p>Can some tell me how to use connection pooling with MonetDb? Do I just append these args to the URL \"poolminimum=1;poolmaximum=5;\"?</p>\\n-How to use connection pooling for Monetdb?-<connection-pooling><monetdb>',\n",
       " \"<p>I am trying to perform some simple text analysis using KNIME. My process begins by using a normal 'XLS reader' or a 'text reader', followed by 'row filter' which executes fine. This step is followed by 'Strings to document' to convert every string into a document. Following this step, no steps execute. I have tried using 'POS Tagger', 'BoW Creator', 'Row Filter', 'Number Filter' etc, but each one of these gives me a <code>'NULLPointerException'</code>. Why is this the case? \\nMy input document is a text file with about 300,000 rows coming from the database. I have checked that none of the rows in the file is NULL. \\nHow can I get rid of this error? \\nAny help would be highly appreciated. </p>\\n-NULLPointerException while using KNIME-<text-processing><knime>\",\n",
       " \"<p>I ran into a strange problem while calculating the difference between two dates in Oracle(11.2.0.3.0) and Teradata(13.10.07.15).\\nNLS_CALENDAR parameter in Oracle is set to GREGORIAN.</p>\\n\\n<pre><code>Oracle                                                              Teradata\\n------------------------------------------------------------------------------------------------------------\\nselect trunc(sysdate) - date'1800-01-01' diff from dual;            select date - date'1800-01-01' diff;    \\n\\nDIFF                                                                diff    \\n----------                                                          ----------- \\n78224                                                               78224\\n</code></pre>\\n\\n<p>No difference so far.</p>\\n\\n<pre><code>select trunc(sysdate) - date'1500-01-01' diff from dual;            select date - date'1500-01-01' diff;\\n\\nDIFF                                                                diff\\n----------                                                          -----------\\n187788                                                              187797\\n</code></pre>\\n\\n<p>Difference of 9 days.</p>\\n\\n<pre><code>select trunc(sysdate) - date'1000-01-01' diff from dual;            select date - date'1000-01-01' diff;\\n\\n      DIFF                                                          diff\\n----------                                                          -----------\\n    370413                                                          370418\\n</code></pre>\\n\\n<p>Difference of 5 days.</p>\\n\\n<pre><code>select trunc(sysdate) - date'0500-01-01' diff from dual;            select date - date'0500-01-01' diff;\\n\\n      DIFF                                                          diff\\n----------                                                          -----------\\n    553038                                                          553039\\n</code></pre>\\n\\n<p>Difference of 1 day.</p>\\n\\n<pre><code>select trunc(sysdate) - date'0001-01-01' diff from dual;            select date - date'0001-01-01' diff;\\n\\n      DIFF                                                          diff\\n----------                                                          -----------\\n    735297                                                          735295\\n</code></pre>\\n\\n<p>Difference of 2 days.</p>\\n\\n<p>Does anyone know why this difference?</p>\\n-Difference in Date between Oracle and Teradata-<sql><oracle><date><calendar><teradata>\",\n",
       " \"<p>A long time ago, I was told that I should not collect statistics on on a Teradata UPI, but I never understood the reason.  It may have been related to the version we were running at that time.  It also may have been that a UPI doesn't need stats so collecting them is a waste of time.</p>\\n\\n<p>My question is: should I continue that practice now that we are using TD 13.10 (and soon moving to TD 14)?  If so, does the size of the table make a difference, such as a 1000-row AVT table versus a 100-million row detail table, both having a single-column UPI widely used by join operations?</p>\\n\\n<p>Any specific references to Teradata documentation will be appreciated.</p>\\n-Should statistics be collected on a UNIQUE PRIMARY INDEX in Teradata?-<teradata>\",\n",
       " '<p>So I figured I should start using Ansible Galaxy when possible, instead of writing my own roles. I just installed my first role and it was installed to <code>/etc/local/ansible/roles</code> (I am on OSX).</p>\\n<p>Now I wonder how you install this roles where I actually need it? Do I just copy the role to where I need it or is there an Ansible way of doing it?</p>\\n-Ansible Galaxy roles install in to a specific directory?-<kubernetes><ansible><ansible-galaxy>',\n",
       " '<p>I have tried connecting to teredata DB through JDBC. I am using the jars \\'terajdbc4.jar\\' and \\'tdgssconfig.jar\\', but I am getting a <code>ClassNotFoundException</code> and <code>NoClassDefFoundError</code> for something kind of <code>com.ncr.teradata.jtdgss.TdgssManager</code> is not found.   </p>\\n\\n<p>Please help me to find a solution. I have provided my code snippet followed by the error log.\\nThe user name and passwords are correct as connect to the teradata DB through UNIX using that ID.</p>\\n\\n<pre><code>public class HelloTeradataJDBC {\\n\\n\\n  public static void main(String[] args) throws Exception {\\n\\n    String url=\"jdbc:teradata://10.10.***.**/DBS_PORT= 1025/DATABASE= ******/TMODE=ANSI,CHARSET=UTF8\";\\n\\n    try{\\n    Class.forName(\"com.ncr.teradata.TeraDriver\");\\n    Connection conn=DriverManager.getConnection(url, \"*****\", \"******\");\\n    //Connection conn=DriverManager.getConnection(connurl, \"javauser1\", \"password1\");\\n\\n    String query=\"select * from xi.san_emp\";\\n\\n\\n    PreparedStatement stmt=conn.prepareStatement(query);\\n    ResultSet rs=stmt.executeQuery();\\n    while(rs.next()) {\\n        String col1=rs.getString(1);\\n        System.out.println(\"col1=\"+col1);\\n    }\\n    }catch(ClassNotFoundException e){\\n        e.printStackTrace();\\n    }\\n  }\\n}\\n</code></pre>\\n\\n<p>Error:</p>\\n\\n<pre><code>Exception in thread \"main\" java.lang.NoClassDefFoundError: com/ncr/teradata/jtdgss/TdgssManager\\n    at com.ncr.teradata.TeraEncrypt.getTDgssVersion(TeraEncrypt.java:548)\\n    at com.ncr.teradata.jdbc_4.parcel.ConfigFeatureTdgss.&lt;init&gt;(ConfigFeatureTdgss.java:44)\\n    at com.ncr.teradata.jdbc_4.statemachine.InitDBConfigState.action(InitDBConfigState.java:68)\\n    at com.ncr.teradata.jdbc_4.statemachine.LogonController.run(LogonController.java:50)\\n    at com.ncr.teradata.jdbc_4.TDSession.&lt;init&gt;(TDSession.java:150)\\n    at com.ncr.teradata.jdbc_3.ifjdbc_4.TeraLocalConnection.&lt;init&gt;(TeraLocalConnection.java:89)\\n    at com.ncr.teradata.jdbc.ConnectionFactory.createConnection(ConnectionFactory.java:50)\\n    at com.ncr.teradata.TeraDriver.connect(TeraDriver.java:214)\\n    at java.sql.DriverManager.getConnection(DriverManager.java:582)\\n    at java.sql.DriverManager.getConnection(DriverManager.java:185)\\n    at HelloTeradataJDBC.main(HelloTeradataJDBC.java:15)\\nCaused by: java.lang.ClassNotFoundException: com.ncr.teradata.jtdgss.TdgssManager\\n    at java.net.URLClassLoader$1.run(URLClassLoader.java:202)\\n    at java.security.AccessController.doPrivileged(Native Method)\\n    at java.net.URLClassLoader.findClass(URLClassLoader.java:190)\\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:306)\\n    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:247)\\n    ... 11 more\\n</code></pre>\\n-Error in JDBC-Teradata Connectivity-<jakarta-ee><jdbc><teradata>',\n",
       " '<p>I tried to upload 3MB really small csv file with 30,000 rows and 4 columns . It took me more than an hour</p>\\n\\n<pre><code>cUrl = \"jdbc:odbc:DSN; TYPE=FASTLOAD\" ;\\nConnection conn = DriverManager.getConnection(cUrl, username, password);\\nString sql = \"insert into Transactions(custID, transaction_date, amount, desc) values(?,?,?,?)\";\\nPreparedStatement ps = conn.prepareStatement(sql);\\n\\n    for ( ... ) \\n    {\\n        ps.setString(1, custID);\\n        ps.setString(2, tran_date);\\n        ps.setString(3, amount);\\n        ps.setString(4, desc);\\n        ps.addBatch(); \\n    }\\n\\n\\n    ps.executeBatch(); \\n</code></pre>\\n\\n<p>The <code>addBatch</code> runs very smoothly. Ones I get to the <code>ps.executeBatch();</code> line it takes it forever . It took over an hour to upload a 3 MB csv file with 30,000 rows . Is it the way it is supposed to be </p>\\n-Why teradata fastload is so slow?-<java><teradata>',\n",
       " '<p>I have VBA excel code that runs a SQL query in Teradata and spits the results back out into Excel. Instead of the following (which I have in my code), I would like to have an \"input box\" pop up for the user to input their user ID and password, instead of having it sitting in the code itself, for security reasons. Any ideas/Is this possible? I have worked with message boxes and inputs before, but am stumped on how to do it for this instance. Thanks!</p>\\n\\n<pre><code> connect.Open \"Data Source=****; Database=****; Persist Security Info=True; User ID=****; Password=****; Session Mode=ANSI;\"\\n</code></pre>\\n-Add Input Box to insert username, password for VBA Excel to Teradata query-<sql><excel><teradata><vba>',\n",
       " '<p>I am trying to find out what it the optimal batch size for teradata uploads.</p>\\n\\n<p>I read some article about the optimal batch size . <a href=\"http://developer.teradata.com/connectivity/articles/speed-up-your-jdbcodbc-applications\" rel=\"nofollow\">One</a> of them states that <code>To get top-notch performance, you need to use a batch size of roughly 50,000 to 100,000</code> rows</p>\\n\\n<p>However, i have seen people saying that their batch included up to a million of rows .</p>\\n\\n<hr>\\n\\n<p>The number of columns could differ and it could be a very heavy 100 rows and 1000 columns dataset . </p>\\n\\n<p>So , is there any optimal batch size in terms of Megabytes ? How many Megabytes are optimal</p>\\n\\n<p>So far in my application i am using 200 Megabytes of data set as a maximum batch regardless of how many rows it includes . But i am not sure about whether it is optimal . </p>\\n-What is the optimal batch size for teradata upload-<java><teradata>',\n",
       " \"<p>I've loaded .tbl files in the tables, now how I can see the total space used on the disk by the database?\\nI'm using Fedora</p>\\n-Monetdb - tables size in the database-<monetdb>\",\n",
       " '<p>Given:</p>\\n\\n<p><strong>Simple.g4</strong></p>\\n\\n<pre><code>grammar Simple;\\n\\nimport SimpleLexer;\\n\\nprog : entry+ EOF;\\nentry : a semi tail;\\nsemi : \\':\\';\\ntail : TAIL;\\na : \\'a\\';\\nW:;\\n</code></pre>\\n\\n<p><strong>SimpleLexer.g4</strong></p>\\n\\n<pre><code>lexer grammar SimpleLexer;\\n\\nTAIL : [a-z]+;\\n</code></pre>\\n\\n<p>They are both in a same package under \\'src/main/antlr4\\'.</p>\\n\\n<p>Maven plugins:</p>\\n\\n<pre class=\"lang-xml prettyprint-override\"><code> &lt;plugins&gt;\\n  ...\\n             &lt;plugin&gt;\\n            &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;\\n            &lt;artifactId&gt;build-helper-maven-plugin&lt;/artifactId&gt;\\n            &lt;version&gt;1.8&lt;/version&gt;\\n            &lt;executions&gt;\\n                &lt;execution&gt;\\n                    &lt;phase&gt;generate-sources&lt;/phase&gt;\\n                    &lt;goals&gt;\\n                        &lt;goal&gt;add-source&lt;/goal&gt;\\n                    &lt;/goals&gt;\\n                    &lt;configuration&gt;\\n                        &lt;sources&gt;\\n                            &lt;source&gt;target/generated-sources/antlr4&lt;/source&gt;\\n                        &lt;/sources&gt;\\n                    &lt;/configuration&gt;\\n                &lt;/execution&gt;\\n            &lt;/executions&gt;\\n        &lt;/plugin&gt;\\n\\n     &lt;plugin&gt;\\n            &lt;groupId&gt;org.antlr&lt;/groupId&gt;\\n            &lt;artifactId&gt;antlr4-maven-plugin&lt;/artifactId&gt;\\n            &lt;version&gt;4.2&lt;/version&gt;          \\n            &lt;executions&gt;\\n                &lt;execution&gt;\\n                    &lt;goals&gt;\\n                        &lt;goal&gt;antlr4&lt;/goal&gt;\\n                    &lt;/goals&gt;\\n                &lt;/execution&gt;\\n            &lt;/executions&gt;\\n        &lt;/plugin&gt;\\n\\n    &lt;/plugins&gt;\\n\\n    &lt;pluginManagement&gt;\\n        &lt;plugins&gt;\\n            &lt;!--This plugin\\'s configuration is used to store Eclipse m2e settings \\n                only. It has no influence on the Maven build itself. --&gt;\\n            &lt;plugin&gt;\\n                &lt;groupId&gt;org.eclipse.m2e&lt;/groupId&gt;\\n                &lt;artifactId&gt;lifecycle-mapping&lt;/artifactId&gt;\\n                &lt;version&gt;1.0.0&lt;/version&gt;\\n                &lt;configuration&gt;\\n                    &lt;lifecycleMappingMetadata&gt;\\n                        &lt;pluginExecutions&gt;\\n                            &lt;pluginExecution&gt;\\n                                &lt;pluginExecutionFilter&gt;\\n                                    &lt;groupId&gt;org.antlr&lt;/groupId&gt;\\n                                    &lt;artifactId&gt;antlr4-maven-plugin&lt;/artifactId&gt;\\n                                    &lt;versionRange&gt;[4.2,)&lt;/versionRange&gt;\\n                                    &lt;goals&gt;\\n                                        &lt;goal&gt;antlr4&lt;/goal&gt;                                         \\n                                    &lt;/goals&gt;                                        \\n                                &lt;/pluginExecutionFilter&gt;\\n                                &lt;action&gt;\\n                                    &lt;execute&gt;\\n                                        &lt;runOnIncremental&gt;true&lt;/runOnIncremental&gt;\\n                                    &lt;/execute&gt;\\n                                &lt;/action&gt;\\n                            &lt;/pluginExecution&gt;\\n                        &lt;/pluginExecutions&gt;\\n                    &lt;/lifecycleMappingMetadata&gt;\\n                &lt;/configuration&gt;\\n            &lt;/plugin&gt;\\n        &lt;/plugins&gt;\\n    &lt;/pluginManagement&gt;\\n</code></pre>\\n\\n<p>Version of the antlr eclipse plugin:</p>\\n\\n<pre><code>ANTLR 4 SDK Feature 0.1.1.201401151138  antlr4ide.sdk.feature.group\\n</code></pre>\\n\\n<p>Eclipse version:</p>\\n\\n<pre><code>Version: Kepler Service Release 2\\nBuild id: 20140224-0627\\n</code></pre>\\n\\n<p>The error on import statement in Simple.g4 that eclipse gives is:</p>\\n\\n<pre><code> can\\'t find or load grammar \\'SimpleLexer\\' from \\'Simple.g4\\'\\n</code></pre>\\n\\n<p>Environment works for single file grammars.</p>\\n-Cannot import antlr4 lexer grammar in combined grammar using Eclipse Kepler-<java><maven><antlr4><eclipse-kepler>',\n",
       " '<p>I have a table as follows:</p>\\n\\n<pre><code>Name     Subjects \\nX         math \\nY         science\\nZ         english\\n</code></pre>\\n\\n<p>I need a report in the following format: </p>\\n\\n<pre><code>  Name      math     science     english\\n    X         Y         N          N\\n    Y         N         Y          N\\n    Z         N         N          Y\\n</code></pre>\\n\\n<p>How do I achieve this using a single select query?</p>\\n-How to convert column values to column names in teradata?-<sql><teradata>',\n",
       " '<p>I am doing analysis on reviews for a particular movie using rapid miner. I used \"getpages\" to extract the reviews from IMDB. There are around 94 reviews listed over the site but after extraction i am getting only 21 out of them. The xml code is:</p>\\n\\n<pre><code>    &lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?&gt;\\n&lt;process version=\"5.3.015\"&gt;\\n  &lt;context&gt;\\n    &lt;input/&gt;\\n    &lt;output/&gt;\\n    &lt;macros/&gt;\\n  &lt;/context&gt;\\n  &lt;operator activated=\"true\" class=\"process\" compatibility=\"5.3.015\" expanded=\"true\" name=\"Process\"&gt;\\n    &lt;process expanded=\"true\"&gt;\\n      &lt;operator activated=\"true\" class=\"read_excel\" compatibility=\"5.3.015\" expanded=\"true\" height=\"60\" name=\"Read Excel\" width=\"90\" x=\"45\" y=\"30\"&gt;\\n        &lt;parameter key=\"excel_file\" value=\"C:\\\\Users\\\\Arbind\\\\Desktop\\\\review\\\\rev.xlsx\"/&gt;\\n        &lt;parameter key=\"imported_cell_range\" value=\"A1:A5\"/&gt;\\n        &lt;parameter key=\"first_row_as_names\" value=\"false\"/&gt;\\n        &lt;list key=\"annotations\"&gt;\\n          &lt;parameter key=\"0\" value=\"Name\"/&gt;\\n        &lt;/list&gt;\\n        &lt;list key=\"data_set_meta_data_information\"&gt;\\n          &lt;parameter key=\"0\" value=\"Link.true.text.attribute\"/&gt;\\n        &lt;/list&gt;\\n      &lt;/operator&gt;\\n      &lt;operator activated=\"true\" class=\"web:retrieve_webpages\" compatibility=\"5.3.001\" expanded=\"true\" height=\"60\" name=\"Get Pages\" width=\"90\" x=\"179\" y=\"30\"&gt;\\n        &lt;parameter key=\"link_attribute\" value=\"Link\"/&gt;\\n      &lt;/operator&gt;\\n      &lt;operator activated=\"true\" class=\"select_attributes\" compatibility=\"5.3.015\" expanded=\"true\" height=\"76\" name=\"Select Attributes\" width=\"90\" x=\"313\" y=\"30\"&gt;\\n        &lt;parameter key=\"attribute_filter_type\" value=\"single\"/&gt;\\n        &lt;parameter key=\"attribute\" value=\"gensym1\"/&gt;\\n      &lt;/operator&gt;\\n      &lt;operator activated=\"true\" class=\"text:process_document_from_data\" compatibility=\"5.3.002\" expanded=\"true\" height=\"76\" name=\"Process Documents from Data\" width=\"90\" x=\"447\" y=\"30\"&gt;\\n        &lt;parameter key=\"prune_method\" value=\"percentual\"/&gt;\\n        &lt;parameter key=\"prune_above_percent\" value=\"90.0\"/&gt;\\n        &lt;list key=\"specify_weights\"/&gt;\\n        &lt;process expanded=\"true\"&gt;\\n          &lt;operator activated=\"true\" class=\"text:cut_document\" compatibility=\"5.3.002\" expanded=\"true\" height=\"60\" name=\"Cut Document\" width=\"90\" x=\"112\" y=\"30\"&gt;\\n            &lt;parameter key=\"query_type\" value=\"Regular Region\"/&gt;\\n            &lt;list key=\"string_machting_queries\"/&gt;\\n            &lt;list key=\"regular_expression_queries\"/&gt;\\n            &lt;list key=\"regular_region_queries\"&gt;\\n              &lt;parameter key=\"extract\" value=\"&amp;lt;hr[^&amp;gt;]\\\\.*align=&amp;quot;center&amp;quot;&amp;gt;.&amp;lt;hr[^&amp;gt;]\\\\.*align=&amp;quot;center&amp;quot;&amp;gt;\"/&gt;\\n            &lt;/list&gt;\\n            &lt;list key=\"xpath_queries\"/&gt;\\n            &lt;list key=\"namespaces\"/&gt;\\n            &lt;list key=\"index_queries\"/&gt;\\n            &lt;process expanded=\"true\"&gt;\\n              &lt;operator activated=\"true\" class=\"text:extract_information\" compatibility=\"5.3.002\" expanded=\"true\" height=\"60\" name=\"Extract Information\" width=\"90\" x=\"112\" y=\"30\"&gt;\\n                &lt;parameter key=\"query_type\" value=\"XPath\"/&gt;\\n                &lt;list key=\"string_machting_queries\"/&gt;\\n                &lt;list key=\"regular_expression_queries\"/&gt;\\n                &lt;list key=\"regular_region_queries\"/&gt;\\n                &lt;list key=\"xpath_queries\"&gt;\\n                  &lt;parameter key=\"review\" value=\"//h:p/text()\"/&gt;\\n                  &lt;parameter key=\"rating\" value=\"//h:img/@alt\"/&gt;\\n                &lt;/list&gt;\\n                &lt;list key=\"namespaces\"/&gt;\\n                &lt;list key=\"index_queries\"/&gt;\\n              &lt;/operator&gt;\\n              &lt;connect from_port=\"segment\" to_op=\"Extract Information\" to_port=\"document\"/&gt;\\n              &lt;connect from_op=\"Extract Information\" from_port=\"document\" to_port=\"document 1\"/&gt;\\n              &lt;portSpacing port=\"source_segment\" spacing=\"0\"/&gt;\\n              &lt;portSpacing port=\"sink_document 1\" spacing=\"0\"/&gt;\\n              &lt;portSpacing port=\"sink_document 2\" spacing=\"0\"/&gt;\\n            &lt;/process&gt;\\n          &lt;/operator&gt;\\n          &lt;connect from_port=\"document\" to_op=\"Cut Document\" to_port=\"document\"/&gt;\\n          &lt;connect from_op=\"Cut Document\" from_port=\"documents\" to_port=\"document 1\"/&gt;\\n          &lt;portSpacing port=\"source_document\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"sink_document 1\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"sink_document 2\" spacing=\"0\"/&gt;\\n        &lt;/process&gt;\\n      &lt;/operator&gt;\\n      &lt;operator activated=\"true\" class=\"write_excel\" compatibility=\"5.3.015\" expanded=\"true\" height=\"76\" name=\"Write Excel\" width=\"90\" x=\"514\" y=\"210\"&gt;\\n        &lt;parameter key=\"excel_file\" value=\"C:\\\\Users\\\\Arbind\\\\Desktop\\\\review\\\\imdb rev2.xlsx\"/&gt;\\n      &lt;/operator&gt;\\n      &lt;connect from_op=\"Read Excel\" from_port=\"output\" to_op=\"Get Pages\" to_port=\"Example Set\"/&gt;\\n      &lt;connect from_op=\"Get Pages\" from_port=\"Example Set\" to_op=\"Select Attributes\" to_port=\"example set input\"/&gt;\\n      &lt;connect from_op=\"Select Attributes\" from_port=\"example set output\" to_op=\"Process Documents from Data\" to_port=\"example set\"/&gt;\\n      &lt;connect from_op=\"Process Documents from Data\" from_port=\"example set\" to_op=\"Write Excel\" to_port=\"input\"/&gt;\\n      &lt;connect from_op=\"Process Documents from Data\" from_port=\"word list\" to_port=\"result 1\"/&gt;\\n      &lt;connect from_op=\"Write Excel\" from_port=\"through\" to_port=\"result 2\"/&gt;\\n      &lt;portSpacing port=\"source_input 1\" spacing=\"0\"/&gt;\\n      &lt;portSpacing port=\"sink_result 1\" spacing=\"0\"/&gt;\\n      &lt;portSpacing port=\"sink_result 2\" spacing=\"0\"/&gt;\\n      &lt;portSpacing port=\"sink_result 3\" spacing=\"0\"/&gt;\\n    &lt;/process&gt;\\n  &lt;/operator&gt;\\n&lt;/process&gt;\\n</code></pre>\\n\\n<p>In the excel file i took the links of reviews one after the other.\\nAlso i need the \"ratings\" given by the users.</p>\\n-text analysis for imdb movie review using rapidminer-<web-scraping><rapidminer><imdb><text-analysis>',\n",
       " '<p>I am working on eclipse kepler rcp. I am using osgi bundles. when i startup the application I am always getting  the following message</p>\\n\\n<blockquote>\\n  <p>Enabling components of bundle com.p2pframework.main did not complete in 30000 ms </p>\\n  \\n  <p>!ENTRY org.eclipse.equinox.ds 2 0 2014-03-10 00:03:09.774 !MESSAGE\\n  [SCR - WorkThread] Timeout occurred! Thread was blocked on processing\\n  [QueuedJob] WorkPerformer:\\n  org.eclipse.equinox.internal.ds.SCRManager@104e583; actionType 1</p>\\n</blockquote>\\n-Warning when starting up the RCP Application-<osgi><eclipse-rcp><eclipse-kepler>',\n",
       " \"<p>I've the following error when running <code>make</code> while installing monetdb5</p>\\n\\n<pre><code>make[9]: Entering directory `/home/lfopa/opt/monetdb/monetdb5/extras/jaql/parser'\\n/bin/bash ../../../../libtool --tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -    I../../../..  -I. -I../ -I./../ -I../../../mal -I./../../../mal -I../../../optimizer -I./../../../optimizer -I../../../../common/options -I./../../../../common/options -I../../../../common/stream -I./../../../../common/stream -I../../../../gdk -I./../../../../gdk  -DLIBJAQLP  -g -O2   -c -o libjaqlp_la-jaql.tab.lo `test -f 'jaql.tab.c' || echo './'`jaql.tab.c\\nlibtool: compile:  gcc -DHAVE_CONFIG_H -I. -I../../../.. -I. -I../ -I./../ -I../../../mal -I./../../../mal -I../../../optimizer -I./../../../optimizer -I../../../../common/options -I./../../../../common/options -I../../../../common/stream -I./../../../../common/stream -I../../../../gdk -I./../../../../gdk -DLIBJAQLP -g -O2 -c jaql.tab.c  -fPIC -DPIC -o .libs/libjaqlp_la-jaql.tab.o\\ny.tab.c: In function 'jaqlparse':\\ny.tab.c:1684:32: error: 'scanner' undeclared (first use in this function)\\ny.tab.c:1684:32: note: each undeclared identifier is reported only once for each function it appears in\\nmake[9]: *** [libjaqlp_la-jaql.tab.lo] Error 1\\nmake[9]: Leaving directory `/home/lfopa/opt/monetdb/monetdb5/extras/jaql/parser'\\nmake[8]: *** [all] Error 2\\nmake[8]: Leaving directory `/home/lfopa/opt/monetdb/monetdb5/extras/jaql/parser'\\nmake[7]: *** [all-recursive] Error 1\\nmake[7]: Leaving directory `/home/lfopa/opt/monetdb/monetdb5/extras/jaql'\\nmake[6]: *** [all] Error 2\\nmake[6]: Leaving directory `/home/lfopa/opt/monetdb/monetdb5/extras/jaql'\\nmake[5]: *** [all-recursive] Error 1\\nmake[5]: Leaving directory `/home/lfopa/opt/monetdb/monetdb5/extras'\\nmake[4]: *** [all] Error 2\\nmake[4]: Leaving directory `/home/lfopa/opt/monetdb/monetdb5/extras'\\nmake[3]: *** [all-recursive] Error 1\\nmake[3]: Leaving directory `/home/lfopa/opt/monetdb/monetdb5'\\nmake[2]: *** [all] Error 2\\nmake[2]: Leaving directory `/home/lfopa/opt/monetdb/monetdb5'\\nmake[1]: *** [all-recursive] Error 1\\nmake[1]: Leaving directory `/home/lfopa/opt/monetdb'\\nmake: *** [all] Error 2\\n</code></pre>\\n\\n<p>The configuration was good according to the readme file here are last line of the configuration</p>\\n\\n<pre><code>* Enabled/disabled build options:\\n    strict     is disabled (by default)\\n    assert     is disabled (by default)\\n    debug      is disabled (by default)\\n    optimize   is disabled (by default)\\n    developer  is disabled (by default)\\n    instrument is disabled (by default)\\n    profile    is disabled (by default)\\n\\n* Enabled/disabled components:\\n    gdk       is enabled\\n    monetdb5  is enabled\\n    sql       is enabled\\n    jaql      is enabled\\n    geom      is disabled (geos library required for geom module)\\n    gsl       is enabled\\n    fits      is disabled (cfitsio library not found)\\n    rdf       is disabled (by default)\\n    datacell  is disabled (by default)\\n    odbc      is enabled\\n    jdbc      is enabled\\n    control   is enabled\\n    testing   is enabled\\n</code></pre>\\n-trouble while making install monetdb5-<monetdb>\",\n",
       " '<p>I\\'m using the implementation of the parallel reduction on CUDA using new kepler\\'s shuffle instructions, similar to this:\\n<a href=\"http://devblogs.nvidia.com/parallelforall/faster-parallel-reductions-kepler/\" rel=\"nofollow\">http://devblogs.nvidia.com/parallelforall/faster-parallel-reductions-kepler/</a></p>\\n\\n<p>I was searching for the minima of rows in a given matrix, and in the end of the kernel I had the following code:</p>\\n\\n<pre><code>my_register = min(my_register, __shfl_down(my_register,8,16));\\nmy_register = min(my_register, __shfl_down(my_register,4,16));\\nmy_register = min(my_register, __shfl_down(my_register,2,16));\\nmy_register = min(my_register, __shfl_down(my_register,1,16));\\n</code></pre>\\n\\n<p>My blocks are 16*16, so everything worked fine, with that code I was getting minima in two sub-rows in the very same kernel.</p>\\n\\n<p>Now I also need to return the indices of the smallest elements in every row of my matrix, so I was going to replace \"min\" with the \"if\" statement and handle these indices in a similar fashion, I got stuck at this code:</p>\\n\\n<pre><code>if (my_reg &gt; __shfl_down(my_reg,8,16)){my_reg = __shfl_down(my_reg,8,16);};\\nif (my_reg &gt; __shfl_down(my_reg,4,16)){my_reg = __shfl_down(my_reg,4,16);};\\nif (my_reg &gt; __shfl_down(my_reg,2,16)){my_reg = __shfl_down(my_reg,2,16);};\\nif (my_reg &gt; __shfl_down(my_reg,1,16)){my_reg = __shfl_down(my_reg,1,16);};\\n</code></pre>\\n\\n<p>No cudaErrors whatsoever, but kernel returns trash now. Nevertheless I have fix for that:</p>\\n\\n<pre><code>myreg_tmp = __shfl_down(myreg,8,16);\\nif (myreg &gt; myreg_tmp){myreg = myreg_tmp;};\\nmyreg_tmp = __shfl_down(myreg,4,16);\\nif (myreg &gt; myreg_tmp){myreg = myreg_tmp;};\\nmyreg_tmp = __shfl_down(myreg,2,16);\\nif (myreg &gt; myreg_tmp){myreg = myreg_tmp;};\\nmyreg_tmp = __shfl_down(myreg,1,16);\\nif (myreg &gt; myreg_tmp){myreg = myreg_tmp;};\\n</code></pre>\\n\\n<p>So, allocating new tmp variable to sneak into neighboring registers saves everything for me.\\nNow the question: Are the kepler shuffle instructions destructive ? in a sense that invoking same instruction twice doesn\\'t issue the same result. I haven\\'t assigned anything to those registers saying \"my_reg > __shfl_down(my_reg,8,16)\" - this adds up to my confusion. Can anyone explain me what is the problem with invoking shuffle twice? I\\'m pretty much a newbie in CUDA, so detailed explanation for dummies is welcomed</p>\\n-Is the nvidia kepler shuffle \"destructive\"?-<cuda><nvidia><kepler>',\n",
       " '<p>I\\'m working on RCP application based on compatibility layer.\\nIn my application I have one main Application.e4xmi and multiple fragment.e4xmi.</p>\\n\\n<p><strong>Case 1:</strong> If I run my application and close it and during close I\\'m saving the workspace then next time when I\\'ll open it without cleaning the workspace then its work properly.</p>\\n\\n<p><strong>Case 2:</strong> If Application is running and you kill it through task manager or simply by eclipse terminate( if you are running it through eclipse). then next time when you open it you will not see then menus which are contributed by Application.e4xmi</p>\\n\\n<p><strong>Observation 1:</strong> By using live edition I\\'m seeing that all the menus and trimbars which are contributed by Application.e4xmi are not available in the application.</p>\\n\\n<p>But other menus which are contributed by fragment.e4xmi are there.</p>\\n\\n<p><strong>Observation 2:</strong> By comparing the \".metadata/plugins/org.eclipse.e4.workbench/workspace.xmi\" before and after UI messed Up. I found that In messed up workspace.xmi there is no entry for menuContributions which are contributed by Application.e4xmi.</p>\\n\\n<p>I can use <code>-persistState false</code> or <code>clearPersistedState</code> but I want my application to open as it was close so this option will not help.</p>\\n\\n<p>Is any thing I\\'m missing and what is went wrong if application is killed.?</p>\\n-workbench.xmi have no menu and trimbar Contributions from Application.e4xmi if RCP kills-<java><eclipse><eclipse-plugin><eclipse-rcp><eclipse-kepler>',\n",
       " '<p>Say I have a query that returns values like this:</p>\\n\\n<pre><code>id    type     value\\naaa   1a        10\\naaa   1b        20\\naaa   1c        7\\nbbb   2a        10\\nbbb   1a        5\\n</code></pre>\\n\\n<p>There are > 50 million rows and 240 possible \"types\".  I want to make a pivot where there is one row for each <code>id</code> and each <code>type</code> is it\\'s own column:</p>\\n\\n<pre><code>id   1a   1b   1c   2a\\naaa  10   20   7    \\nbbb  5              10\\n</code></pre>\\n\\n<p>I can do this in SQL server but I don\\'t know how to do it in Teradata.  There are too many columns for me to make CASE statements.  However, each distinct <code>type</code> is in a table field, if that\\'s any help.</p>\\n-Accomplish pivot in teradata sql-<teradata>',\n",
       " '<p>Previously ran SVN in Indigo without issues. I decided to update to Kepler and have been trying to get SVN setup. Based on all of the documentation I can find I have it setup (no errors) the problem is that it is not recognizing the SVN properties of my projects and I don\\'t see any way to get it to link.(Clicking \"Team\" only shows the menu item \"Apply Patch\"). The Preferences for SVN is totally different from Indigo so I can\\'t match that either.. Clearly I\\'m missing something..</p>\\n\\n<p>I installed the below Subclipse version\\n<a href=\"http://subclipse.tigris.org/update_1.6.x\" rel=\"nofollow\">http://subclipse.tigris.org/update_1.6.x</a></p>\\n-SVN file changes undetectable in Eclipse Kepler Project Explorer-<java><eclipse><svn><subclipse><eclipse-kepler>',\n",
       " '<p>I\\'ve started programming in C so weeks ago, and I choose Eclipse Kepler as my IDE for C, since I had already used it for programming in some other languages and really liked.</p>\\n\\n<p>However, after I had installed <em>Cygwin</em> and the C programming tools in Eclipse, I tried to run the old \"Hello World\" and it didn\\'t work, It didn\\'t appeared anything in the Console, only the message \"Terminated\".</p>\\n\\n<pre><code>#include &lt;stdio.h&gt;\\n\\nint main(){\\n    printf(\"Hello World!\");\\n    return 0;\\n}\\n</code></pre>\\n\\n<p>Anyone has any idea what the problem could be?</p>\\n\\n<hr>\\n\\n<p>Thanks to all of you. I tried everything you said but I wasn\\'t able to get my problem solved. I gave up! Right now, i\\'m using as workspace in eclipse the Cygwin home. I write the program in the Eclipse and run it in the Cygwin command line. \\nAgain, thanks to all for trying to help, it won\\'t be forgotten. You\\'re a really awesome crowd!</p>\\n-Error running *.c file in Eclipse Kepler-<c><eclipse><cygwin><eclipse-kepler>',\n",
       " \"<p>After downloading updates it shows a error that can't install updates .</p>\\n\\n<blockquote>\\n  <p>An error occurred while uninstalling\\n  session context was:(profile=epp.package.cpp, phase=org.eclipse.equinox.internal.p2.engine.phases.Uninstall, operand=[R]org.eclipse.rcp.configuration_root.win32.win32.x86_64 1.0.0.v20130521-1847 --> null, action=org.eclipse.equinox.internal.p2.touchpoint.natives.actions.CleanupzipAction).\\n  Backup of file D:\\\\eclipse\\\\eclipse.exe failed.\\n  Can not remove : D:\\\\eclipse\\\\eclipse.exe</p>\\n</blockquote>\\n\\n<p>I'm running it under win7 and jdk1.7.0_21 64bit .</p>\\n\\n<p>How fix it ?</p>\\n-Eclipse kepler can't install updates-<eclipse-cdt><eclipse-kepler>\",\n",
       " '<p>I created a table with 10GB (60M records), added unique index (hidden_id) manually after data insertion.\\nI had the simplest query but it took one minutes to complete. </p>\\n\\n<pre><code>select hidden_id from netflow where hidden_id = 350000;\\n</code></pre>\\n\\n<p>And also the query took tens of minutes \"<code>select * from netflow order by hidden_id limit 12500 offset 212500;</code>\".\\nIt really confuses me. </p>\\n\\n<p>I post the analysis of the first query below. Any clue why it\\'s so slow?</p>\\n\\n<pre><code>trace select hidden_id from netflow where hidden_id = 350000;\\n\\n+----------+------------------------------------------------------------------+\\n| ticks    | stmt                                                             |\\n+==========+==================================================================+\\n|        3 | X_3 := sql.mvc();                                                |\\n|       15 | X_7=&lt;tmp_2510&gt;[69396995] := sql.bind(X_3=0,\"sys\",\"netflow\",\"hidd |\\n:          : en_id\",0);                                                       :\\n|      227 | X_4:bat[:oid,:oid] =&lt;tmp_13332&gt;[69396995] := sql.tid(X_3=0,\"sys\" |\\n:          : ,\"netflow\");                                                     :\\n| 72978741 | X_36=&lt;tmp_4053&gt;[1] := algebra.subselect(X_7=&lt;tmp_2510&gt;[69396995] |\\n:          : ,X_4=&lt;tmp_13332&gt;:bat[:oid,:oid][69396995],A0=350000:lng,A0=35000 :\\n:          : 0:lng,true,true,false);                                          :\\n|       17 | (X_10=&lt;tmp_2175&gt;[0],r1_10=&lt;tmp_3416&gt;[0]) := sql.bind(X_3=0,\"sys\" |\\n:          : ,\"netflow\",\"hidden_id\",2);                                       :\\n|       14 | X_37=&lt;tmp_13337&gt;[0] := algebra.subselect(r1_10=&lt;tmp_3416&gt;[0],A0= |\\n:          : 350000:lng,A0=350000:lng,true,true,false);                       :\\n|        6 | X_13=&lt;tmp_3416&gt;[0] := sql.bind(X_3=0,\"sys\",\"netflow\",\"hidden_id\" |\\n:          : ,1);                                                             :\\n|       15 | X_38=&lt;tmp_11053&gt;[0] := algebra.subselect(X_13=&lt;tmp_3416&gt;[0],X_4= |\\n:          : &lt;tmp_13332&gt;:bat[:oid,:oid][69396995],A0=350000:lng,A0=350000:lng :\\n:          : ,true,true,false);                                               :\\n|        4 | X_15=&lt;tmp_4053&gt;[1] := sql.subdelta(X_36=&lt;tmp_4053&gt;[1],X_4=&lt;tmp_1 |\\n:          : 3332&gt;:bat[:oid,:oid][69396995],X_10=&lt;tmp_2175&gt;[0],X_37=&lt;tmp_1333 :\\n:          : 7&gt;[0],X_38=&lt;tmp_11053&gt;[0]);                                      :\\n|       20 | X_17=&lt;tmp_11053&gt;[1] := sql.projectdelta(X_15=&lt;tmp_4053&gt;[1],X_7=&lt; |\\n:          : tmp_2510&gt;[69396995],X_10=&lt;tmp_2175&gt;[0],r1_10=&lt;tmp_3416&gt;[0],X_13= :\\n:          : &lt;tmp_3416&gt;[0]);                                                  :\\n|        6 | X_18 := sql.resultSet(1,1,X_17=&lt;tmp_11053&gt;[1]);                  |\\n|        7 | sql.rsColumn(X_18=1,\"sys.netflow\",\"hidden_id\",\"bigint\",64,0,X_17 |\\n:          : =&lt;tmp_11053&gt;[1]);                                                :\\n|        2 | X_23 := io.stdout();                                             |\\n|       25 | sql.exportResult(X_23==\"104d2\":streams,X_18=1);                  |\\n|        1 | end s1_3;                                                        |\\n| 73011629 | X_5:void  := user.s1_3(350000:lng);                              |\\n+----------+------------------------------------------------------------------+\\n</code></pre>\\n\\n<p>This is the table being created.</p>\\n\\n<pre><code>CREATE TABLE \"netflow\" (\\n    \"time_seconds\" double DEFAULT NULL,\\n    \"parsed_date\" timestamp DEFAULT NULL,\\n    \"date_time_str\" varchar(45) DEFAULT NULL,\\n    \"ip_layer_protocol\" bigint DEFAULT NULL,\\n    \"ip_layer_protocol_code\" varchar(45) DEFAULT NULL,\\n    \"first_seen_src_ip\" varchar(45) DEFAULT NULL,\\n    \"first_seen_dest_ip\" varchar(45) DEFAULT NULL,\\n    \"first_seen_src_port\" bigint DEFAULT NULL,\\n    \"first_seen_dest_port\" bigint DEFAULT NULL,\\n    \"more_fragments\" varchar(45) DEFAULT NULL,\\n    \"cont_fragments\" varchar(45) DEFAULT NULL,\\n    \"duration_seconds\" bigint DEFAULT NULL,\\n    \"first_seen_src_payload_bytes\" bigint DEFAULT NULL,\\n    \"first_seen_dest_payload_bytes\" bigint DEFAULT NULL,\\n    \"first_seen_src_total_bytes\" bigint DEFAULT NULL,\\n    \"first_seen_dest_total_bytes\" bigint DEFAULT NULL,\\n    \"first_seen_src_packet_count\" bigint DEFAULT NULL,\\n    \"first_seen_dest_packet_count\" bigint DEFAULT NULL,\\n    \"record_force_out\" varchar(45) DEFAULT NULL\\n);\\n</code></pre>\\n\\n<p><strong>Update:</strong></p>\\n\\n<p>the platform: Windows 7 without parallel</p>\\n\\n<p>MonetDB version: MonetDB 5 server v11.15.19 \"Feb2013-SP6\"</p>\\n\\n<p>description of the table in storage:</p>\\n\\n<pre><code>select * from storage() where \"table\"=\\'netflow\\';\\n+--------+---------+-------------------------------+-----------+----------+-----\\n-----+-----------+------------+------------+---------+--------+\\n| schema | table   | column                        | type      | location | count    | typewidth | columnsize | heapsize   | indices | sorted |\\n+========+=========+===============================+===========+==========+==========+===========+============+============+=========+========+\\n| sys    | netflow | time_seconds                  | double    | 17\\\\1711  | 69396995 |         8 |  555175960 |          0 |       0 | false  |\\n| sys    | netflow | parsed_date                   | timestamp | 20\\\\2054  | 69396995 |         8 |  555175960 |          0 |       0 | false  |\\n| sys    | netflow | date_time_str                 | varchar   | 07\\\\734   | 69396995 |        21 |  277587980 | 2684354560 |       0 | false  |\\n| sys    | netflow | ip_layer_protocol             | bigint    | 62\\\\6261  | 69396995 |         8 |  555175960 |          0 |       0 | false  |\\n| sys    | netflow | ip_layer_protocol_code        | varchar   | 62\\\\6213  | 69396995 |         3 |   69396995 |     524288 |       0 | false  |\\n| sys    | netflow | first_seen_src_ip             | varchar   | 63\\\\6342  | 69396995 |        11 |  138793990 |     524288 |       0 | false  |\\n| sys    | netflow | first_seen_dest_ip            | varchar   | 23\\\\2324  | 69396995 |         8 |  138793990 |     524288 |       0 | false  |\\n| sys    | netflow | first_seen_src_port           | bigint    | 15\\\\1574  | 69396995 |         8 |  555175960 |          0 |       0 | false  |\\n| sys    | netflow | first_seen_dest_port          | bigint    | 23\\\\2370  | 69396995 |         8 |  555175960 |          0 |       0 | false  |\\n| sys    | netflow | more_fragments                | varchar   | 65\\\\6521  | 69396995 |         1 |   69396995 |     524288 |       0 | false  |\\n| sys    | netflow | cont_fragments                | varchar   | 65\\\\6524  | 69396995 |         1 |   69396995 |     524288 |       0 | false  |\\n| sys    | netflow | duration_seconds              | bigint    | 65\\\\6560  | 69396995 |         8 |  555175960 |          0 |       0 | false  |\\n| sys    | netflow | first_seen_src_payload_bytes  | bigint    | 65\\\\6561  | 69396995 |         8 |  555175960 |          0 |       0 | false  |\\n| sys    | netflow | first_seen_dest_payload_bytes | bigint    | 65\\\\6562  | 69396995 |         8 |  555175960 |          0 |       0 | false  |\\n| sys    | netflow | first_seen_src_total_bytes    | bigint    | 65\\\\6563  | 69396995 |         8 |  555175960 |          0 |       0 | false  |\\n| sys    | netflow | first_seen_dest_total_bytes   | bigint    | 65\\\\6564  | 69396995 |         8 |  555175960 |          0 |       0 | false  |\\n| sys    | netflow | first_seen_src_packet_count   | bigint    | 65\\\\6565  | 69396995 |         8 |  555175960 |          0 |       0 | false  |\\n| sys    | netflow | first_seen_dest_packet_count  | bigint    | 65\\\\6566  | 69396995 |         8 |  555175960 |          0 |       0 | false  |\\n| sys    | netflow | record_force_out              | varchar   | 65\\\\6567  | 69396995 |         1 |   69396995 |     524288 |       0 | false  |\\n| sys    | netflow | hidden_id                     | bigint    | 25\\\\2510  | 69396995 |         8 |  555175960 |          0 |       0 | false  |\\n| sys    | netflow | index_id                      | oid       | 73\\\\7375  | 69396995 |         8 |  555175960 |          0 |       0 | true   |\\n+--------+---------+-------------------------------+-----------+----------+----------+-----------+------------+------------+---------+--------+\\n</code></pre>\\n-MonetDB Slow Query with Index-<indexing><monetdb>',\n",
       " '<p>I have created my own custom toolchain plugin for CDT, and it works pretty well. However there is one piece that I cannt figure out.</p>\\n\\n<p><strong>Goal</strong></p>\\n\\n<p>Create a Tool which calls a JAVA class, instead of sending a command line argument.</p>\\n\\n<p>At first I mulled over the \\'CustomBuildStep\\' option for the Tool plugin element, but that is not the solution. How can you do this?</p>\\n\\n<p><strong>Hello World Request</strong></p>\\n\\n<p>Here\\'s a simple one - At the completion of the CDT-Managed Build (e.g. the make call completes), I would like to pop a message box that says \\'Hooray!\\'.</p>\\n\\n<p><strong>Current Workaround</strong></p>\\n\\n<p>Currently I end up just doing </p>\\n\\n<pre><code>java -cp \"some_obtuse_path_into_plugins_dir\" MyClass.doPopup\\n</code></pre>\\n\\n<p>This is a gnarly console output, and a total hack in my opinion.</p>\\n\\n<p>Thoughts?</p>\\n\\n<p>Here is a useful contextual answer (that doesn\\'t answer my question, but may point to \\'sorry, can\\'t do that):</p>\\n\\n<ul>\\n<li><a href=\"https://stackoverflow.com/questions/1098723/eclipse-cdt-whats-the-best-way-to-add-a-custom-build-step\">Stack Overflow: Eclipse-CDT: Whats the best way to add a custom build step?</a></li>\\n</ul>\\n-Eclipse CDT Toolchain - Tool that just calls JAVA code?-<eclipse><eclipse-cdt>',\n",
       " '<p>Request for you help. I am trying to use eclipse for developing a dojo based web app.\\nI am unable to see the dojo widgets that I can drag and drop into the Editor as per the documentation on WDT.\\nIs such an option ( drag and drop of dojo widgets) avaiable for ecplise (free version)?\\nIf avaiable what set up needs to be done to make the widgets visible.</p>\\n\\n<h2>Configuration of my setup</h2>\\n\\n<p>Eclipse: Java EE IDE for Web Developers.\\nVersion: Kepler Service Release 1\\nBuild id: 20130919-0819</p>\\n\\n<p>IBM WDT v.8.5.5.1</p>\\n\\n<p>WDT file: wdt-update-site_8.5.5.1.v20131031_0202.zip</p>\\n\\n<p>Installed on Windows 7 OS.</p>\\n\\n<p>I have installed eclipse (kepler version from ecplise.org) and IBM WAS developer tool(<a href=\"https://www.ibm.com/developerworks/mydeveloperworks/blogs/wasdev/entry/download\" rel=\"nofollow\">https://www.ibm.com/developerworks/mydeveloperworks/blogs/wasdev/entry/download</a>). \\nCreated a web project using the below workflow in Eclipse.\\nFile->New->Web Project\\nProject Template: Dojo toolkit.\\nProgramming Model: Client side only.\\nDojo Project Summary:\\nThe following dojo toolkit:Dojo Toolkit SDK 1.9.0 for WebSphere\\nwill be copied into this project location: dojo</p>\\n\\n<p>The above work creates a project with a folder \\\\webcontent\\\\dojo and subfolder container dojo specific files in the explorer window.</p>\\n\\n<p>help in this regard will be highly appreciated.</p>\\n\\n<p>regards\\nFrank</p>\\n-Eclipse (kepler) Dojo Widgets (drag and drop) not visibile with IBM WDT v.8.5.5.1-<web-applications><dojo><eclipse-kepler>',\n",
       " '<p>I am using shift+alt+r to rename a javascript variable.</p>\\n\\n<p>\"Enter new name, press enter to refactor\" comes up in a tooltip.</p>\\n\\n<p>When I press enter, the refactoring should occur, but instead the variable name is reverted to what it was before.</p>\\n\\n<p>Why?</p>\\n\\n<p>I got a question about what the error log shows.\\nHere it comes:</p>\\n\\n<pre><code>java.lang.reflect.InvocationTargetException\\n    at org.eclipse.jface.operation.ModalContext.run(ModalContext.java:421)\\n    at org.eclipse.ui.internal.WorkbenchWindow$13.run(WorkbenchWindow.java:1812)\\n    at org.eclipse.swt.custom.BusyIndicator.showWhile(BusyIndicator.java:70)\\n    at org.eclipse.ui.internal.WorkbenchWindow.run(WorkbenchWindow.java:1809)\\n    at org.eclipse.wst.jsdt.internal.ui.refactoring.RefactoringExecutionHelper.perform(RefactoringExecutionHelper.java:172)\\n    at org.eclipse.wst.jsdt.ui.refactoring.RenameSupport.perform(RenameSupport.java:191)\\n    at org.eclipse.wst.jsdt.internal.ui.refactoring.reorg.RenameLinkedMode.doRename(RenameLinkedMode.java:329)\\n    at org.eclipse.wst.jsdt.internal.ui.refactoring.reorg.RenameLinkedMode$EditorSynchronizer.left(RenameLinkedMode.java:103)\\n    at org.eclipse.jface.text.link.LinkedModeModel.exit(LinkedModeModel.java:341)\\n    at org.eclipse.jface.text.link.LinkedModeUI$4.run(LinkedModeUI.java:1193)\\n    at org.eclipse.swt.widgets.RunnableLock.run(RunnableLock.java:35)\\n    at org.eclipse.swt.widgets.Synchronizer.runAsyncMessages(Synchronizer.java:135)\\n    at org.eclipse.swt.widgets.Display.runAsyncMessages(Display.java:4145)\\n    at org.eclipse.swt.widgets.Display.readAndDispatch(Display.java:3762)\\n    at org.eclipse.e4.ui.internal.workbench.swt.PartRenderingEngine$9.run(PartRenderingEngine.java:1113)\\n    at org.eclipse.core.databinding.observable.Realm.runWithDefault(Realm.java:332)\\n    at org.eclipse.e4.ui.internal.workbench.swt.PartRenderingEngine.run(PartRenderingEngine.java:997)\\n    at org.eclipse.e4.ui.internal.workbench.E4Workbench.createAndRunUI(E4Workbench.java:138)\\n    at org.eclipse.ui.internal.Workbench$5.run(Workbench.java:610)\\n    at org.eclipse.core.databinding.observable.Realm.runWithDefault(Realm.java:332)\\n    at org.eclipse.ui.internal.Workbench.createAndRunWorkbench(Workbench.java:567)\\n    at org.eclipse.ui.PlatformUI.createAndRunWorkbench(PlatformUI.java:150)\\n    at org.eclipse.ui.internal.ide.application.IDEApplication.start(IDEApplication.java:124)\\n    at org.eclipse.equinox.internal.app.EclipseAppHandle.run(EclipseAppHandle.java:196)\\n    at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.runApplication(EclipseAppLauncher.java:110)\\n    at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.start(EclipseAppLauncher.java:79)\\n    at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:354)\\n    at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:181)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\\n    at java.lang.reflect.Method.invoke(Unknown Source)\\n    at org.eclipse.equinox.launcher.Main.invokeFramework(Main.java:636)\\n    at org.eclipse.equinox.launcher.Main.basicRun(Main.java:591)\\n    at org.eclipse.equinox.launcher.Main.run(Main.java:1450)\\n    at org.eclipse.equinox.launcher.Main.main(Main.java:1426)\\nCaused by: java.lang.NullPointerException\\n    at org.eclipse.wst.jsdt.internal.core.search.matching.MatchLocator.locateMatches(MatchLocator.java:1125)\\n    at org.eclipse.wst.jsdt.internal.core.search.matching.MatchLocator.locateMatches(MatchLocator.java:1186)\\n    at org.eclipse.wst.jsdt.internal.core.search.matching.MatchLocator.locateMatches(MatchLocator.java:1309)\\n    at org.eclipse.wst.jsdt.internal.core.search.JavaSearchParticipant.locateMatches(JavaSearchParticipant.java:104)\\n    at org.eclipse.wst.jsdt.internal.core.search.BasicSearchEngine.findMatches(BasicSearchEngine.java:264)\\n    at org.eclipse.wst.jsdt.internal.core.search.BasicSearchEngine.search(BasicSearchEngine.java:513)\\n    at org.eclipse.wst.jsdt.core.search.SearchEngine.search(SearchEngine.java:243)\\n    at org.eclipse.wst.jsdt.internal.corext.refactoring.RefactoringSearchEngine.internalSearch(RefactoringSearchEngine.java:138)\\n    at org.eclipse.wst.jsdt.internal.corext.refactoring.RefactoringSearchEngine.search(RefactoringSearchEngine.java:125)\\n    at org.eclipse.wst.jsdt.internal.corext.refactoring.rename.RenameNonVirtualMethodProcessor.getOccurrences(RenameNonVirtualMethodProcessor.java:129)\\n    at org.eclipse.wst.jsdt.internal.corext.refactoring.rename.RenameMethodProcessor.doCheckFinalConditions(RenameMethodProcessor.java:353)\\n    at org.eclipse.wst.jsdt.internal.corext.refactoring.rename.RenameNonVirtualMethodProcessor.doCheckFinalConditions(RenameNonVirtualMethodProcessor.java:82)\\n    at org.eclipse.wst.jsdt.internal.corext.refactoring.rename.JavaRenameProcessor.checkFinalConditions(JavaRenameProcessor.java:43)\\n    at org.eclipse.ltk.core.refactoring.participants.ProcessorBasedRefactoring.checkFinalConditions(ProcessorBasedRefactoring.java:224)\\n    at org.eclipse.ltk.core.refactoring.Refactoring.checkAllConditions(Refactoring.java:162)\\n    at org.eclipse.wst.jsdt.internal.ui.refactoring.RefactoringExecutionHelper$Operation.run(RefactoringExecutionHelper.java:69)\\n    at org.eclipse.wst.jsdt.internal.core.BatchOperation.executeOperation(BatchOperation.java:39)\\n    at org.eclipse.wst.jsdt.internal.core.JavaModelOperation.run(JavaModelOperation.java:742)\\n    at org.eclipse.core.internal.resources.Workspace.run(Workspace.java:2345)\\n    at org.eclipse.wst.jsdt.core.JavaScriptCore.run(JavaScriptCore.java:3844)\\n    at org.eclipse.wst.jsdt.internal.ui.actions.WorkbenchRunnableAdapter.run(WorkbenchRunnableAdapter.java:83)\\n    at org.eclipse.jface.operation.ModalContext$ModalContextThread.run(ModalContext.java:121)\\nRoot exception:\\njava.lang.NullPointerException\\n    at org.eclipse.wst.jsdt.internal.core.search.matching.MatchLocator.locateMatches(MatchLocator.java:1125)\\n    at org.eclipse.wst.jsdt.internal.core.search.matching.MatchLocator.locateMatches(MatchLocator.java:1186)\\n    at org.eclipse.wst.jsdt.internal.core.search.matching.MatchLocator.locateMatches(MatchLocator.java:1309)\\n    at org.eclipse.wst.jsdt.internal.core.search.JavaSearchParticipant.locateMatches(JavaSearchParticipant.java:104)\\n    at org.eclipse.wst.jsdt.internal.core.search.BasicSearchEngine.findMatches(BasicSearchEngine.java:264)\\n    at org.eclipse.wst.jsdt.internal.core.search.BasicSearchEngine.search(BasicSearchEngine.java:513)\\n    at org.eclipse.wst.jsdt.core.search.SearchEngine.search(SearchEngine.java:243)\\n    at org.eclipse.wst.jsdt.internal.corext.refactoring.RefactoringSearchEngine.internalSearch(RefactoringSearchEngine.java:138)\\n    at org.eclipse.wst.jsdt.internal.corext.refactoring.RefactoringSearchEngine.search(RefactoringSearchEngine.java:125)\\n    at org.eclipse.wst.jsdt.internal.corext.refactoring.rename.RenameNonVirtualMethodProcessor.getOccurrences(RenameNonVirtualMethodProcessor.java:129)\\n    at org.eclipse.wst.jsdt.internal.corext.refactoring.rename.RenameMethodProcessor.doCheckFinalConditions(RenameMethodProcessor.java:353)\\n    at org.eclipse.wst.jsdt.internal.corext.refactoring.rename.RenameNonVirtualMethodProcessor.doCheckFinalConditions(RenameNonVirtualMethodProcessor.java:82)\\n    at org.eclipse.wst.jsdt.internal.corext.refactoring.rename.JavaRenameProcessor.checkFinalConditions(JavaRenameProcessor.java:43)\\n    at org.eclipse.ltk.core.refactoring.participants.ProcessorBasedRefactoring.checkFinalConditions(ProcessorBasedRefactoring.java:224)\\n    at org.eclipse.ltk.core.refactoring.Refactoring.checkAllConditions(Refactoring.java:162)\\n    at org.eclipse.wst.jsdt.internal.ui.refactoring.RefactoringExecutionHelper$Operation.run(RefactoringExecutionHelper.java:69)\\n    at org.eclipse.wst.jsdt.internal.core.BatchOperation.executeOperation(BatchOperation.java:39)\\n    at org.eclipse.wst.jsdt.internal.core.JavaModelOperation.run(JavaModelOperation.java:742)\\n    at org.eclipse.core.internal.resources.Workspace.run(Workspace.java:2345)\\n    at org.eclipse.wst.jsdt.core.JavaScriptCore.run(JavaScriptCore.java:3844)\\n    at org.eclipse.wst.jsdt.internal.ui.actions.WorkbenchRunnableAdapter.run(WorkbenchRunnableAdapter.java:83)\\n    at org.eclipse.jface.operation.ModalContext$ModalContextThread.run(ModalContext.java:121)\\n</code></pre>\\n-Refactor rename reverts changes when pressing enter-<eclipse><eclipse-kepler><eclipse-pdt>',\n",
       " '<p>I\\'m working on a web crawling project to analyse various crowdfunding sites\\' projects via text mining in Rapidminer 5/6. I have already built a working text analyser, but I\\'m stuck at the web crawling part. The problem is that the web crawler does crawl through the requested sites, but doesn\\'t store them. I have tried experimenting with page size, depth and the like, but still the program just skips those sites. It is probable that the problem is with my storing rules. They look like the following, when trying to crawl through Kickstarter\\'s sites:</p>\\n\\n<p>Follow with matching URL:</p>\\n\\n<p>.+kickstarter.+</p>\\n\\n<p>Store with matching URL:</p>\\n\\n<pre><code>https://www\\\\.kickstarter\\\\.com\\\\/projects.+\\n\\nhttp://www\\\\.kickstarter\\\\.com\\\\/projects.+\\n\\n(?i)http.*://www\\\\.kickstarter\\\\.com\\\\/projects.+\\n</code></pre>\\n\\n<p>An example URL that would need to be stored is:</p>\\n\\n<pre><code>http://www.kickstarter.com/projects/corvuse/bhaloidam-an-indie-tabletop-storytelling-game?ref=spotlight\\n</code></pre>\\n\\n<p>(no advertising intended)</p>\\n\\n<p>And the log looks like the following:</p>\\n\\n<pre><code>Mar 12, 2014 11:50:37 AM INFO: Following link http://www.kickstarter.com/projects/corvuse/bhaloidam-an-indie-tabletop-storytelling-game?ref=spotlight\\nMar 12, 2014 11:50:37 AM INFO: Following link http://kickstarter.tumblr.com/post/12036057734/todays-project-of-the-day-is-bhaloidam-an-indie\\nMar 12, 2014 11:50:37 AM INFO: Following link http://kickstarter.tumblr.com/tagged/bhaloidam\\nMar 12, 2014 11:50:38 AM INFO: Discarded page \"http://kickstarter.tumblr.com/post/79165806431/do-you-like-coloring-and-also-have-questions\" because url does not match filter rules.\\n</code></pre>\\n\\n<p>As you can see, it follows through with the process and just skips these links, and it doesn\\'t even say that it doesn\\'t match the filter rules so it\\'s been discarded, so I\\'m not even sure that in these cases the program compares the links to the rules. I see a lot of links in the log preceded with (\"Following link..\") but very few preceded with (\"Discarded page...\"). Does this mean that it just checks a few pages, or just that it won\\'t notify me for every discarded page? I am able to save these projects\\' sites manually, but understandibly, I wouldn\\'t like to do this with hundreds of sites.</p>\\n\\n<p>I\\'ve also built a model for Indiegogo, and the same model works if I use only the third storing rule (and remove the first two). So I thought this is also the problem with my Kickstarter model, but no luck this way either.</p>\\n\\n<p>Thanks in advance!</p>\\n\\n<p>Cheers</p>\\n-Rapidminer Web Crawling doesn\\'t store sites (Kickstarter)-<web-crawler><rapidminer>',\n",
       " '<p>I\\'m reading the <a href=\"https://www.monetdb.org/Documentation/MonetDB/Introduction\" rel=\"nofollow\">MonetDB Internal Description</a> because I\\'m interested to implement a special procedure direct in the MonetDB Algebra Language (MAL) instead of SQL-frontend (I hope to gain more performance by doing so). Here is a sample MAL code provide in the MonetDB documentation </p>\\n\\n<pre><code> function sample(nme:str, val:any_1):bit;\\n    c := 2 * 3;\\n    b := bbp.bind(nme);  #find a BAT\\n    h := algebra.select(b,val,val);\\n    t := aggr.count(h);\\n    x := io.print(t);\\n    y := io.print(val);\\n end sample;\\n</code></pre>\\n\\n<p>My question is how to execute such a MAL code upon one of my existing database ?</p>\\n\\n<p>Thans for any replay</p>\\n-How to execute a MAL function in monetdb?-<monetdb><monetdb-assembly-language>',\n",
       " \"<p>I want to start learning about hibernate, but I don't know how to configure Hibernate in Eclipse.  </p>\\n\\n<p>Please could you provide instructions for configuring Hibernate in Eclipse.  </p>\\n-How to Install Hibernate In Eclipse kepler?-<java><eclipse><hibernate><eclipse-kepler>\",\n",
       " '<p>While trying to figure out a good workflow with git subtree, I would like to track the upstream branch (generally different than <code>master</code>) that has been used when adding a git sub-tree. </p>\\n\\n<p>More specifically, assuming your defined the following remotes and used to fetch them: </p>\\n\\n<pre><code>  $&gt; git remote add -f easybuild-easyblocks https://github.com/ULHPC/easybuild-easyblocks.git\\n  $&gt; git remote add -f easybuild-easyconfigs https://github.com/ULHPC/easybuild-easyconfigs.git\\n  $&gt; git fetch easybuild-easyblocks\\n  $&gt; git fetch easybuild-easyconfigs \\n</code></pre>\\n\\n<p>You can see the different remote branch: </p>\\n\\n<pre><code>  $&gt; git show-ref | grep -v tags | grep easybuild\\n  f4b4752bcadd5dd44aa74ee03f4bd19b75810bdb refs/remotes/easybuild-easyblocks/develop\\n  49a1e893160c6c1d2ad50109265e55586c377c1e refs/remotes/easybuild-easyblocks/master\\n  173aa5cabddf998e2ad672135752a33875095f8b refs/remotes/easybuild-easyblocks/v1.8.x\\n  49a1e893160c6c1d2ad50109265e55586c377c1e refs/remotes/easybuild-easyblocks/v1.9.x\\n  770c5246667d7741c02d4e1f4d4a50fb8cd3fcbe refs/remotes/easybuild-easyconfigs/develop\\n  d8422c6f7aace405f1089f178edabb9316629d4b refs/remotes/easybuild-easyconfigs/master\\n  5f3be9e2d5cbb2844ebf74ede15e2c30a8b12705 refs/remotes/easybuild-easyconfigs/uni.lu\\n  0382ff527360f1baa192bb92597552cc379bba68 refs/remotes/easybuild-easyconfigs/v1.8.0.x\\n  d8422c6f7aace405f1089f178edabb9316629d4b refs/remotes/easybuild-easyconfigs/v1.9.x\\n</code></pre>\\n\\n<p>Now assumes that you define two new git subtree to track the upstream branch <code>develop</code> for the remote <code>easybuild-easyblocks</code> (respectively <code>v1.8.0.x</code> for the remote <code>easybuild-easyconfigs</code>): </p>\\n\\n<pre><code>  $&gt; git subtree add --prefix easybuild/easyblocks  --squash easybuild-easyblocks/develop\\n  $&gt; git subtree add --prefix easybuild/easyconfigs --squash easybuild-easyconfigs/v1.8.0.x\\n</code></pre>\\n\\n<p>You end with a clean directory layout where <code>easybuild/*</code> hold in separate directories the latest version of the corresponding repository. </p>\\n\\n<p>I can now check the head commit hash of all branch by running:</p>\\n\\n<pre><code> $&gt; git branch -v -r --abbrev=40                                                                                                                                                                                                                                                 \\n easybuild-easyblocks/develop   f4b4752bcadd5dd44aa74ee03f4bd19b75810bdb Merge pull request #314 from boegel/version_bump\\n easybuild-easyblocks/master    49a1e893160c6c1d2ad50109265e55586c377c1e Merge pull request #312 from hpcugent/develop\\n easybuild-easyblocks/v1.8.x    173aa5cabddf998e2ad672135752a33875095f8b Merge pull request #281 from boegel/1.8.2_release_notes\\n easybuild-easyblocks/v1.9.x    49a1e893160c6c1d2ad50109265e55586c377c1e Merge pull request #312 from hpcugent/develop\\n easybuild-easyconfigs/develop  770c5246667d7741c02d4e1f4d4a50fb8cd3fcbe Merge pull request #500 from fgeorgatos/contrib_qtop\\n easybuild-easyconfigs/master   d8422c6f7aace405f1089f178edabb9316629d4b Merge pull request #544 from hpcugent/develop\\n easybuild-easyconfigs/uni.lu   5f3be9e2d5cbb2844ebf74ede15e2c30a8b12705 add Allinea-4.2-34164-Ubuntu-10.04-x86_64.eb\\n easybuild-easyconfigs/v1.8.0.x 0382ff527360f1baa192bb92597552cc379bba68 Merge pull request #473 from boegel/v1.8.0.x\\n easybuild-easyconfigs/v1.9.x   d8422c6f7aace405f1089f178edabb9316629d4b Merge pull request #544 from hpcugent/develop\\n</code></pre>\\n\\n<p>Now I wish to pull the latest changes from the subtrees. Thus I run: </p>\\n\\n<pre><code> $&gt;git fetch easybuild-easyblocks    # fetch latest changes of the remote before merging\\n $&gt; git subtree pull --prefix easybuild/easyblocks easybuild-easyblocks develop --squash\\n\\n $&gt; git fetch easybuild-easyconfigs\\n $&gt; git subtree pull --prefix easybuild/easyconfigs easybuild-easyconfigs v1.8.0.x--squash\\n</code></pre>\\n\\n<p>Is there a simple way to guess/check the upstream branch that has been used to setup the subtree such that I can make a generic command following the format: </p>\\n\\n<pre><code>   git subtree pull --prefix &lt;path/to/subtree&gt; &lt;subtree-remote&gt; &lt;subtree-branch&gt; --squash\\n</code></pre>\\n\\n<p>???</p>\\n-Get the upstream branch from a git subtree-<git><git-subtree>',\n",
       " '<p>Suppose I have 2 tables with same structure and I want to compare data in the tables.\\nIf for a particular field the data is same then i need a report that will say 100% (match) else i need the percentage by which the data is differing in the two columns.</p>\\n\\n<p>NOTE: Need to take a join on unique key. (Assumption the joining key unique and not null)</p>\\n-Compare teradata table fields and return the percentage by which the fields are matching or differing-<sql><compare><field><teradata>',\n",
       " \"<p>I am trying to find difference between 2 timestamps in teradata. I am using the following code:</p>\\n\\n<pre><code>(date1-date2)day(4) to second  as time_diff\\n</code></pre>\\n\\n<p>This is giving the error: <strong>Interval Field Overflow</strong>. What could be reason for that? Is there some other way to calculate the difference between 2 timestamps?</p>\\n\\n<p>And when I am using this:</p>\\n\\n<pre><code>case when(((date2+ INTERVAL '72' hour )-date1) day(4) to second)&gt;0 then '&lt;72 hrs'\\n</code></pre>\\n\\n<p>then the error i am getting is <strong>Invalid operation for DateTime or Interval</strong>.\\nPlease help</p>\\n-Subtraction of Timestamps teradata-<timestamp><teradata>\",\n",
       " '<pre><code># Swiftmailer Configuration\\nswiftmailer:\\n    transport:  smtp\\n    host:       10.8.100.1\\n    port:       25\\n    username:   user\\n    password:   pass\\n    auth_mode:  ~\\n    encryption: ~\\n    spool:      { type: memory }\\n</code></pre>\\n\\n<p>When I try to send a message via <code>$this-&gt;get(\\'mailer\\')-&gt;send($message);</code> I am getting the following error:</p>\\n\\n<pre><code>Fatal error: Uncaught exception \\'Swift_TransportException\\' with message \\'Failed\\nto authenticate on SMTP server with username \"user\" using 0 possible  authenticators\\'\\nin /..path../vendor/swiftmailer/swiftmailer/lib/classes/Swift/Transport/Esmtp/AuthHandler.php:184 Stack trace: #0 \\n</code></pre>\\n\\n<p>I\\'ve tried changeing the <code>auth_mode</code> setting to all possible values <code>plain, login, cram-md5, or null</code> - still the same error message.</p>\\n\\n<hr>\\n\\n<p>Then I wanted to telnet to SMTP server to manually check if i can auth (though, I am 100% sure the credentials are correct).</p>\\n\\n<p><strong>telnet 10.8.100.1 25</strong></p>\\n\\n<pre><code>Trying 10.8.100.1...\\nConnected to 10.8.100.1.\\nEscape character is \\'^]\\'.\\n220 EXC.acme.local Microsoft ESMTP MAIL Service ready at Wed, 19 Mar 2014 10:34:00 +0100\\n</code></pre>\\n\\n<p><strong>EHLO EXC.acme.local</strong></p>\\n\\n<pre><code>250-EXC.acme.local Hello [10.8.100.1]\\n250-SIZE\\n250-PIPELINING\\n250-DSN\\n250-ENHANCEDSTATUSCODES\\n250-STARTTLS\\n250-X-ANONYMOUSTLS\\n250-AUTH NTLM\\n250-X-EXPS GSSAPI NTLM\\n250-8BITMIME\\n250-BINARYMIME\\n250-CHUNKING\\n250-XEXCH50\\n250 XRDST\\n</code></pre>\\n\\n<p>I\\'m no mail server expert, but I was expecting <strong>AUTH LOGIN</strong> there... seems like the server (over which I have no control) has a diffrent authentication method (which is not suppoerted by Swiftmailer bundle?) and that might be the cause of the problem...</p>\\n\\n<p>Are my suspictions correct? Or is there a way to configure Swiftmailer bundle to correctly auth with AUTH NTLM?</p>\\n-How to correctly configure symfony2 swiftmailer bundle to work with SMTP server with NTLM AUTH?-<php><email><symfony><ntlm><swiftmailer>',\n",
       " '<p>I created eclipse plugin from Extenal JAR file, Like <a href=\"http://www.vogella.com/tutorials/EclipseJarToPlugin/article.html\" rel=\"nofollow\">EclipseJarToPlugin</a>.<br>\\nThen in MANIFEST.MF I\\'m adding <code>Activator</code> for this plugin.</p>\\n\\n<p>But the problem is this Activator never called. Even while creating plug-in  there is no option for Activator.</p>\\n\\n<p>Is there is any way so that we can know when first time any Exported class of this plugin(of jar) is called.?</p>\\n-Activator not called if plugin created by JAR-<java><eclipse><eclipse-plugin><eclipse-rcp><eclipse-kepler>',\n",
       " '<p>I am using com.teradata.jdbc.TeraDriver</p>\\n\\n<p>I need to get a list of all databases to display to the user. To do that I create java.sql.Connection and call the following:</p>\\n\\n<pre><code>connection.getMetaData().getSchemas()\\n</code></pre>\\n\\n<p>This returns the list of all teradata databases. However I would like to exclude users from this list. </p>\\n\\n<p>To make myself more clear. If we look at teradata database in Teradata Studio Express we can see a list of all databases. In the Details for a Database there is an attribute called Database Type. The value is either \"user\" or \"database\". I would like to get via jdbc all those databases with the \"database\" type.</p>\\n\\n<p>Any help will be appreciated. Thanks!</p>\\n-Get a list of teradata databases via jdbc?-<jdbc><teradata>',\n",
       " \"<p>I am new to Teradata and created some stored procedure using cursor and getting this error. Please help</p>\\n\\n<p>An owner referenced by user does not have SELECT access to(some column in table)\\nSyntax error, expected something like an 'END' keyword between ';' and the 'DECLARE' keyword.'.\\n Referring to undefined cursor 'abc'.\\nAn owner referenced by user does not have SELECT access to (some column in table)\\nAn owner referenced by user does not have SELECT access to (some column in table)\\n Referring to undefined cursor 'abc'.\\n Referring to undefined cursor 'abc'.</p>\\n-teradata error while creating sp with cursor-<select><cursor><procedure><teradata>\",\n",
       " \"<p>please help me to calculate beginning of the day,end of the day,beginning of month,end of month(along with time upto 3 places) in teradata. I want something like this '2014-03-01 00:00:00.000'; thanks</p>\\n-date/time calculations in Teradata-<date><time><teradata>\",\n",
       " \"<p>I am not able to view any other options apart from 'Apply patch' and 'Share project' when I right click a project and select 'Team'. I am using eclipse Kepler and Subclipse 1.10.4.</p>\\n\\n<p>But I will be able to see the all the options if I import a project from SVN directly through eclipse workspace. I mean, right click --> Import --> SVN --> Checkout from SVN.</p>\\n\\n<p>But the issue is, I am not sure why the options are not showing when I checkout a project in my local drive and then create a project in eclipse workspace referring to this path. </p>\\n-SVN options not visible in Project Explorer-<eclipse><svn><subclipse><eclipse-kepler>\",\n",
       " '<p>I have a teradata database name that contains a dash character -.</p>\\n\\n<p>Searched the web but in vain. Does somebody know how can you escape the special characters in jdbc connection string? The string looks as follows:</p>\\n\\n<pre><code>jdbc:teradata://HostName/DATABASE=Database-Name\\n</code></pre>\\n\\n<p>When I create a connection with this url I get syntax error. Also tried to put database parameter in single or double quotes, and to surround the special charachers with { }.</p>\\n\\n<p>Thanks for help! </p>\\n-Escape characters in teradata jdbc connection string-<jdbc><teradata>',\n",
       " '<p>I want to select only the first row from start that match my MonetDB query and to give it as output and ignoring the rest. How can I do so?</p>\\n-How to select only first rows that satisfies conditions in MonetDB-<monetdb>',\n",
       " '<p>When running Eclipse Kepler and importing an existing Maven project, Eclipse hangs during \"Building Workspace (XX%)\". Switching to the Progress tab reveals a \"Validating nnn\"; it looks like it’s validating a directory containing JavaScript files.</p>\\n-Eclipse building workspace hangs after importing existing maven project because of JavaScript validation-<javascript><eclipse><validation><eclipse-kepler>',\n",
       " \"<p>I have the problem that eclipse hangs every time i open it as it wants to do the javascript validation. Is there any possibilty to disable the javascript validation for every project in the workspace (i don't need and don't want it) without opening eclipse (as it immediately hangs.</p>\\n-Disable JavaScript validation in eclipse kepler for workspace-<javascript><eclipse><validation><eclipse-kepler>\",\n",
       " '<p>I am new at Knime and I have a doubt about the GroupBy node.</p>\\n\\n<p>I have a data set representing a Shopping Cart, with the following columns</p>\\n\\n<ul>\\n<li>Session Number (integer)</li>\\n<li>CustomerID (String) </li>\\n<li>Start Hour</li>\\n<li>Duration</li>\\n<li>ClickedProducts</li>\\n<li>AgeAddress</li>\\n<li>LastOrder</li>\\n<li>Payments</li>\\n<li>CustomerScore</li>\\n<li>Order</li>\\n</ul>\\n\\n<p>where Order (Char meaning Y=purchase or N = nonpurchase)</p>\\n\\n<p>I saw in my data set that Session Number can have more than one row, so I used the GroupBy node and grouped by SessionID, but when I see the resulting table, I only see the column I have chosen.</p>\\n\\n<p>I would like some advice about if I have to aggregate new columns with another node.</p>\\n\\n<p>Thank you</p>\\n-Knime Shopping Cart Prediction-<knime>',\n",
       " '<p>I am new to repidminer and for an academic project I am trying to extract text information from forum posts such as <a href=\"http://www.tripadvisor.com/ShowTopic-g29220-i86-k1487815-Alamo-Maui_Hawaii.html\" rel=\"nofollow\">http://www.tripadvisor.com/ShowTopic-g29220-i86-k1487815-Alamo-Maui_Hawaii.html</a></p>\\n\\n<p>I have tried a lot already and came up with the XPath query: //div[@class=\\'postBody\\']/p[not(*)][text()]</p>\\n\\n<p>Which works fine in google docs but not in rapidminer.</p>\\n\\n<p>edit:\\nSorry, but your sugessted queries don\\'t wprk in rapidminer. see my rapidminer process: </p>\\n\\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?&gt;\\n&lt;process version=\"5.3.015\"&gt;\\n  &lt;context&gt;\\n    &lt;input/&gt;\\n    &lt;output/&gt;\\n    &lt;macros/&gt;\\n  &lt;/context&gt;\\n  &lt;operator activated=\"true\" class=\"process\" compatibility=\"5.3.015\" expanded=\"true\" name=\"Process\"&gt;\\n    &lt;process expanded=\"true\"&gt;\\n      &lt;operator activated=\"true\" class=\"web:get_webpage\" compatibility=\"5.3.001\" expanded=\"true\" height=\"60\" name=\"Get Page\" width=\"90\" x=\"45\" y=\"75\"&gt;\\n        &lt;parameter key=\"url\" value=\"http://www.tripadvisor.com/ShowTopic-g29220-i86-k1487815-Alamo-Maui_Hawaii.html\"/&gt;\\n        &lt;parameter key=\"random_user_agent\" value=\"true\"/&gt;\\n        &lt;list key=\"query_parameters\"/&gt;\\n        &lt;list key=\"request_properties\"/&gt;\\n      &lt;/operator&gt;\\n      &lt;operator activated=\"true\" class=\"text:process_documents\" compatibility=\"5.3.002\" expanded=\"true\" height=\"94\" name=\"Process Documents\" width=\"90\" x=\"380\" y=\"30\"&gt;\\n        &lt;process expanded=\"true\"&gt;\\n          &lt;operator activated=\"true\" class=\"text:extract_information\" compatibility=\"5.3.002\" expanded=\"true\" height=\"60\" name=\"Extract Information\" width=\"90\" x=\"45\" y=\"30\"&gt;\\n            &lt;parameter key=\"query_type\" value=\"XPath\"/&gt;\\n            &lt;list key=\"string_machting_queries\"/&gt;\\n            &lt;list key=\"regular_expression_queries\"/&gt;\\n            &lt;list key=\"regular_region_queries\"/&gt;\\n            &lt;list key=\"xpath_queries\"&gt;\\n              &lt;parameter key=\"xpath1\" value=\"//div[@class=\\'postBody\\']\"/&gt;\\n              &lt;parameter key=\"xpath2\" value=\"//div[@class=\\'postBody\\']/text()\"/&gt;\\n              &lt;parameter key=\"xpath3\" value=\"//div[@class=\\'postBody\\']/p[not(*)][text()]\"/&gt;\\n            &lt;/list&gt;\\n            &lt;list key=\"namespaces\"/&gt;\\n            &lt;list key=\"index_queries\"/&gt;\\n          &lt;/operator&gt;\\n          &lt;connect from_port=\"document\" to_op=\"Extract Information\" to_port=\"document\"/&gt;\\n          &lt;connect from_op=\"Extract Information\" from_port=\"document\" to_port=\"document 1\"/&gt;\\n          &lt;portSpacing port=\"source_document\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"sink_document 1\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"sink_document 2\" spacing=\"0\"/&gt;\\n        &lt;/process&gt;\\n      &lt;/operator&gt;\\n      &lt;connect from_op=\"Get Page\" from_port=\"output\" to_op=\"Process Documents\" to_port=\"documents 1\"/&gt;\\n      &lt;connect from_op=\"Process Documents\" from_port=\"example set\" to_port=\"result 1\"/&gt;\\n      &lt;portSpacing port=\"source_input 1\" spacing=\"0\"/&gt;\\n      &lt;portSpacing port=\"sink_result 1\" spacing=\"0\"/&gt;\\n      &lt;portSpacing port=\"sink_result 2\" spacing=\"0\"/&gt;\\n    &lt;/process&gt;\\n  &lt;/operator&gt;\\n&lt;/process&gt;\\n</code></pre>\\n\\n<p>Any other ideas?             </p>\\n-Rapidminer XPath extract forum post text-<xpath><extract><rapidminer><mining>',\n",
       " '<p>I have the following two tables </p>\\n\\n<p><img src=\"https://i.stack.imgur.com/Pgugq.png\" alt=\"enter image description here\"></p>\\n\\n<p>I need to create a table that would summarize points for each date</p>\\n\\n<p><img src=\"https://i.stack.imgur.com/mg6ed.png\" alt=\"enter image description here\"></p>\\n\\n<p>How can I do this . I have updraded to Teradata 14 . And I am not quite familiar with all the new functions</p>\\n-How to summarize two tables in teradata-<sql><teradata>',\n",
       " '<p>Quite a few people in our dev team (size 20ish java devs) are experiencing the seemingly well-known \"Unhandled event loop exception\" when we click between editors and other panes within the Eclipse window. Some people have had some success by installing the WindowBuilder plugin and setting the WB editor as their default editor for Java files, but most of us still get the error regularly. It doesn\\'t seem memory related because you can get it straight away after a cold boot; i.e. it doesn\\'t \"work for a while\". I think it\\'s likely video-related, because we also get similar video glitch symptoms that have been reported by others, e.g. tab names disappearing until you hover over them etc. </p>\\n\\n<p>Our video card (most of us I think) is AMD Radeon HD 6450 and we\\'re running 64-bit Windows 7. I\\'ve tried shutting down various processes that sound related to the video card, but to no avail.</p>\\n\\n<p>The error is reproducible on a brand new standard Eclipse J2EE install.</p>\\n\\n<p>I\\'ve searched a lot on this because it affects our day-to-day work so much, but I can\\'t find a solution. Is there something I\\'m not doing that could workaround this?</p>\\n\\n<p>Here\\'s my <a href=\"https://drive.google.com/file/d/0B-QHmo7MUgGaQ0V3aDQ0THpQM28/edit?usp=sharing\" rel=\"nofollow\">eclipse config</a> exported.</p>\\n\\n<p>And as requested, here\\'s the <a href=\"https://docs.google.com/document/d/1Ew_V8BB_96WBoLcmbq4QFbzAQznxJFDWpYnOYflx5f4/edit?usp=sharing\" rel=\"nofollow\">stack trace</a> from the Error Log. I\\'ve also pasted the Session Data at the top of that one.  BTW, the system is running a Core I7 3.4ghz CPU, and I have 16gb ram.</p>\\n\\n<p>eclipse.buildId=4.3.0.M20130911-1000\\njava.version=1.7.0_51\\njava.vendor=Oracle Corporation\\nBootLoader constants: OS=win32, ARCH=x86_64, WS=win32, NL=en_AU\\nCommand-line arguments:  -os win32 -ws win32 -arch x86_64</p>\\n\\n<p>org.eclipse.swt.SWTError: No more handles\\n    at org.eclipse.swt.SWT.error(SWT.java:4423)\\n    at org.eclipse.swt.SWT.error(SWT.java:4312)\\n    at org.eclipse.swt.SWT.error(SWT.java:4283)\\n    at org.eclipse.swt.widgets.Widget.error(Widget.java:472)\\n    at org.eclipse.swt.widgets.Control.createHandle(Control.java:704)\\n    at org.eclipse.swt.widgets.Label.createHandle(Label.java:199)\\n    at org.eclipse.swt.widgets.Control.createWidget(Control.java:744)\\n    at org.eclipse.swt.widgets.Control.(Control.java:112)\\n    at org.eclipse.swt.widgets.Label.(Label.java:101)</p>\\n-Unhandled event loop exception on a clean Java EE Kepler install on Windows-<java><eclipse><jakarta-ee><eclipse-kepler>',\n",
       " '<p>In Eclipse Kepler, each time you start a new workspace, the theme reverts to the default one.</p>\\n\\n<p>So you need to go to <code>Windows &gt; Preferences &gt; Appearance &gt; Theme</code> and select the theme you want to use. After that you need to restart your workspace for the theme to be applied completely.</p>\\n\\n<p>This is a repetitive and task and time consuming.</p>\\n\\n<p>Is it possible to specify the theme in the command line or in the <code>eclipse.ini</code> file ?</p>\\n-Eclipse Kepler specify appearance theme through command line-<eclipse><eclipse-kepler>',\n",
       " '<p>I switched from Juno Eclipse to Kepler and imported my Maven project. However \"Maven Dependencies\" doesn\\'t show up under Project-Properties-Java Build Path-Libraries and thus i can\\'t compile my project. In Maven Repositories view i can see all my dependencies jar files under Local Repositories. I can manually add External Jar Files but there must be a right way to load them dynamically. I can run \"maven install\" successfully from Eclipse. I\\'ve tried all suggestions from this forum such as - run Maven-Update Project, unchecked Maven-Resolve dependencies from Workspace projects, Window-Preferences-check \"Download repository index updates on startup\" and \"Update Maven projects on startup\". Something is weird about Kepler Eclipse. What am i missing? </p>\\n\\n<p>Here is my pom.xml</p>\\n\\n<p>\\nhttp://maven.apache.org/xsd/maven-4.0.0.xsd\"></p>\\n\\n<pre><code>&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\\n&lt;groupId&gt;com.xxx.xxx&lt;/groupId&gt;\\n&lt;artifactId&gt;actives&lt;/artifactId&gt;\\n&lt;version&gt;3.1.0&lt;/version&gt;\\n&lt;packaging&gt;pom&lt;/packaging&gt;\\n\\n&lt;build&gt;\\n    &lt;plugins&gt;\\n        &lt;plugin&gt;\\n            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\\n            &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;\\n            &lt;version&gt;3.1&lt;/version&gt;\\n            &lt;configuration&gt;\\n                &lt;source&gt;1.6&lt;/source&gt;\\n                &lt;target&gt;1.6&lt;/target&gt;\\n            &lt;/configuration&gt;\\n        &lt;/plugin&gt;\\n\\n        &lt;plugin&gt;\\n            &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt;\\n            &lt;artifactId&gt;sonar-maven-plugin&lt;/artifactId&gt;\\n            &lt;version&gt;2.0&lt;/version&gt;\\n        &lt;/plugin&gt;\\n\\n        &lt;plugin&gt;\\n            &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;\\n            &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;\\n            &lt;version&gt;2.17&lt;/version&gt;\\n            &lt;configuration&gt;\\n                &lt;skipTests&gt;true&lt;/skipTests&gt;\\n            &lt;/configuration&gt;\\n        &lt;/plugin&gt;\\n\\n\\n        &lt;plugin&gt;\\n            &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;\\n            &lt;version&gt;2.4&lt;/version&gt;\\n            &lt;executions&gt;\\n                &lt;execution&gt;\\n                    &lt;phase&gt;package&lt;/phase&gt;\\n                    &lt;goals&gt;\\n                        &lt;goal&gt;single&lt;/goal&gt;\\n                    &lt;/goals&gt;\\n                    &lt;id&gt;jar-with-dependencies&lt;/id&gt;\\n                &lt;/execution&gt;\\n            &lt;/executions&gt;\\n\\n            &lt;configuration&gt;\\n                &lt;archive&gt;\\n                    &lt;manifest&gt;\\n                        &lt;mainClass&gt;com.xxx.xxx.xxx.xxx.XXX&lt;/mainClass&gt;\\n                    &lt;/manifest&gt;\\n                &lt;/archive&gt;\\n\\n                &lt;descriptors&gt;\\n                    &lt;descriptor&gt;src/main/assembly/bin.xml&lt;/descriptor&gt;  \\n                &lt;/descriptors&gt;\\n\\n                &lt;descriptorRefs&gt;\\n                    &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;!--  Include all depedencies in build --&gt;\\n                &lt;/descriptorRefs&gt;\\n                &lt;!--&lt;appendAssemblyId&gt;false&lt;/appendAssemblyId&gt; --&gt;\\n            &lt;/configuration&gt;\\n        &lt;/plugin&gt;   \\n    &lt;/plugins&gt;\\n\\n    &lt;extensions&gt;\\n        &lt;extension&gt;\\n            &lt;groupId&gt;org.apache.maven.wagon&lt;/groupId&gt;\\n            &lt;artifactId&gt;wagon-ssh&lt;/artifactId&gt;\\n            &lt;version&gt;2.3&lt;/version&gt;\\n        &lt;/extension&gt;\\n    &lt;/extensions&gt;\\n\\n\\n&lt;/build&gt;\\n\\n&lt;properties&gt;\\n    &lt;root&gt;.&lt;/root&gt;\\n    &lt;xxx.releases&gt;http://xxx.xxx.local/mave&lt;/xxx.releases&gt;\\n    &lt;xxxx.releases&gt;http://xxx.xxx.local/xxx/releases&lt;/xxx.releases&gt;\\n    &lt;xxx.snapshots&gt;http://xxx.associatesys.local/xxx/snapshots&lt;/xxx.snapshots&gt;\\n    &lt;xxx.releases.distribution&gt;scp://xxx.associatesys.local/../../../srv/www/release/xxxx/releases&lt;/xxx.releases.distribution&gt;\\n    &lt;xxx.snapshots.distribution&gt;scp://xxx.xxx.local/../../../srv/www/release/xxx/snapshots&lt;/xxx.snapshots.distribution&gt;\\n    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;\\n&lt;/properties&gt;\\n\\n&lt;repositories&gt;\\n    &lt;repository&gt;\\n        &lt;id&gt;central&lt;/id&gt;\\n        &lt;name&gt;central&lt;/name&gt;\\n        &lt;url&gt;http://repo1.maven.org/maven2&lt;/url&gt;\\n        &lt;snapshots&gt;\\n            &lt;enabled&gt;false&lt;/enabled&gt;\\n        &lt;/snapshots&gt;\\n    &lt;/repository&gt;       \\n\\n\\n    &lt;repository&gt;\\n        &lt;id&gt;xxx-central&lt;/id&gt;\\n        &lt;name&gt;XXXCentral&lt;/name&gt;\\n        &lt;url&gt;http://xxx.xxx.local/maven2/central&lt;/url&gt;\\n        &lt;snapshots&gt;\\n            &lt;enabled&gt;false&lt;/enabled&gt;\\n        &lt;/snapshots&gt;\\n    &lt;/repository&gt;\\n\\n    &lt;repository&gt;\\n        &lt;id&gt;xxx-releases&lt;/id&gt;\\n        &lt;name&gt;Internal ReleasesRepository&lt;/name&gt;\\n        &lt;url&gt;${xxx.releases}&lt;/url&gt;\\n        &lt;snapshots&gt;\\n            &lt;enabled&gt;false&lt;/enabled&gt;\\n        &lt;/snapshots&gt;\\n    &lt;/repository&gt;\\n\\n    &lt;repository&gt;\\n        &lt;id&gt;xxx-releases&lt;/id&gt;\\n        &lt;name&gt;XXXReleases Repository&lt;/name&gt;\\n        &lt;url&gt;${xxx.releases}&lt;/url&gt;\\n        &lt;snapshots&gt;\\n            &lt;enabled&gt;false&lt;/enabled&gt;\\n        &lt;/snapshots&gt;\\n    &lt;/repository&gt;\\n\\n    &lt;repository&gt;\\n        &lt;id&gt;xxx-snapshots&lt;/id&gt;\\n        &lt;name&gt;XXXX Snapshots Repository&lt;/name&gt;\\n        &lt;url&gt;${xxx.snapshots}&lt;/url&gt;\\n        &lt;snapshots&gt;\\n            &lt;enabled&gt;true&lt;/enabled&gt;\\n            &lt;updatePolicy&gt;interval:30&lt;/updatePolicy&gt;\\n        &lt;/snapshots&gt;\\n    &lt;/repository&gt;\\n&lt;/repositories&gt;\\n\\n&lt;dependencies&gt;\\n\\n    &lt;dependency&gt;\\n        &lt;groupId&gt;log4j&lt;/groupId&gt;\\n        &lt;artifactId&gt;log4j&lt;/artifactId&gt;\\n        &lt;version&gt;1.2.16&lt;/version&gt;\\n    &lt;/dependency&gt;\\n\\n    &lt;dependency&gt;\\n        &lt;groupId&gt;sqljdbc&lt;/groupId&gt;\\n        &lt;artifactId&gt;sqljdbc&lt;/artifactId&gt;\\n        &lt;version&gt;4&lt;/version&gt;\\n    &lt;/dependency&gt;\\n\\n    &lt;dependency&gt;\\n        &lt;groupId&gt;xxxx&lt;/groupId&gt;\\n        &lt;artifactId&gt;xxx-xxx&lt;/artifactId&gt;\\n        &lt;version&gt;1.0.0&lt;/version&gt;\\n    &lt;/dependency&gt;\\n\\n    &lt;dependency&gt;\\n        &lt;groupId&gt;com.xxx.xxx.client&lt;/groupId&gt;\\n        &lt;artifactId&gt;xxx-client&lt;/artifactId&gt;\\n        &lt;version&gt;1.0.0&lt;/version&gt;\\n    &lt;/dependency&gt;\\n\\n    &lt;dependency&gt;\\n        &lt;groupId&gt;com.xxx.xxx.xxx&lt;/groupId&gt;\\n        &lt;artifactId&gt;xxx-xxx&lt;/artifactId&gt;\\n        &lt;version&gt;1.0.0&lt;/version&gt;\\n    &lt;/dependency&gt;\\n\\n    &lt;dependency&gt;\\n        &lt;groupId&gt;com.xxx.xxx.common&lt;/groupId&gt;\\n        &lt;artifactId&gt;xxx-common&lt;/artifactId&gt;\\n        &lt;version&gt;1.0.0&lt;/version&gt;\\n    &lt;/dependency&gt;\\n\\n&lt;/dependencies&gt;\\n\\n&lt;distributionManagement&gt;\\n    &lt;repository&gt;\\n        &lt;id&gt;xxx-releases&lt;/id&gt;\\n        &lt;name&gt;XXXXReleases Repository&lt;/name&gt;\\n        &lt;url&gt;${xxx.releases.distribution}&lt;/url&gt;\\n    &lt;/repository&gt;\\n\\n    &lt;snapshotRepository&gt;\\n        &lt;id&gt;xxx-snapshots&lt;/id&gt;\\n        &lt;name&gt;XXXSnapshots Repository&lt;/name&gt;\\n        &lt;url&gt;${xxx.snapshots.distribution}&lt;/url&gt;\\n    &lt;/snapshotRepository&gt;\\n&lt;/distributionManagement&gt;\\n</code></pre>\\n\\n<p></p>\\n\\n<p>UPDATE: i solved this problem by copy and paste .classpath and .project files from another Maven project in Eclipse (Kepler) that was created with no issues. I made minor adjustments to these 2 files. I compared pom.xml files and found no discrepancies that would prevent Eclipse from creating my Maven project. It\\'s not an ideal solution but it works for now, no time to solving Eclipse \"mysteries\". Thanks for the below comments, they pointed me into a right direction and helped to understand a problem better. </p>\\n-Can\\'t get Libraries Maven Dependencies to show up-<java><eclipse><maven><m2eclipse><eclipse-kepler>',\n",
       " \"<p>Output of the following SQL statement is '279A'</p>\\n\\n<pre><code>    SELECT\\n    SUBSTR('H0279A',3)\\n</code></pre>\\n\\n<p>I am confused why this query returns 1:</p>\\n\\n<pre><code>        SELECT \\n        CASE \\n            WHEN SUBSTR('H0279A',3) BETWEEN '0000' AND '9999'   \\n            THEN 1\\n        ELSE 0\\n        END\\n</code></pre>\\n\\n<p>How can '279A' be between '0000' AND '9999' ?\\nI am using Teradata.</p>\\n-SUBSTR BETWEEN in Teradata-<sql><teradata>\",\n",
       " '<p>I need to run a RapidMiner process in Java. Like explained in this link :   <a href=\"https://stackoverflow.com/questions/15834182/integration-of-rapidminer-in-java-application\">Integration of RapidMiner in Java application</a>,</p>\\n\\n<pre><code>import com.rapidminer.Process;\\nimport com.rapidminer.RapidMiner;\\nimport com.rapidminer.operator.Operator;\\nimport com.rapidminer.operator.OperatorException;\\nimport com.rapidminer.operator.io.ExcelExampleSource;\\nimport com.rapidminer.tools.XMLException;\\nimport java.io.File;\\nimport java.io.IOException;\\nimport java.lang.Object;\\n\\npublic class ReadRapidminerProcess {\\n  public static void main(String[] args) {\\n    try {\\n\\n      RapidMiner.setExecutionMode(RapidMiner.ExecutionMode.COMMAND_LINE);\\n      RapidMiner.init();\\n\\n      Process process = new Process(new File(\"C:\\\\\\\\Users\\\\\\\\Keshav\\\\\\\\.RapidMiner5\\\\\\\\repositories\\\\\\\\Local Repository\\\\\\\\lsvmtest.rmp\"));\\n      process.run();\\n\\n    } catch (IOException | XMLException | OperatorException ex) {\\n      ex.printStackTrace();\\n    }\\n  }\\n}\\n</code></pre>\\n\\n<p>UPDATE:</p>\\n\\n<p>After several modifications,I am stuck with the following error:</p>\\n\\n<pre><code>INFO: Process C:\\\\Users\\\\Keshav\\\\.RapidMiner5\\\\repositories\\\\Local Repository\\\\linsvmtest.rmp starts\\ncom.rapidminer.operator.UserError: Cannot resolve relative repository location \\'lsvmword\\'. Process is not associated with a repository.\\n    at com.rapidminer.Process.resolveRepositoryLocation(Process.java:1248)\\n    at com.rapidminer.operator.Operator.getParameterAsRepositoryLocation(Operator.java:1456)\\n    at com.rapidminer.operator.io.RepositorySource.getRepositoryEntry(RepositorySource.java:91)\\n    at com.rapidminer.operator.io.RepositorySource.read(RepositorySource.java:105)\\n    at com.rapidminer.operator.io.AbstractReader.doWork(AbstractReader.java:126)\\n    at com.rapidminer.operator.Operator.execute(Operator.java:867)\\n    at com.rapidminer.operator.execution.SimpleUnitExecutor.execute(SimpleUnitExecutor.java:51)\\n    at com.rapidminer.operator.ExecutionUnit.execute(ExecutionUnit.java:711)\\n    at com.rapidminer.operator.OperatorChain.doWork(OperatorChain.java:375)\\n    at com.rapidminer.operator.Operator.execute(Operator.java:867)\\n    at com.rapidminer.Process.run(Process.java:949)\\n    at com.rapidminer.Process.run(Process.java:873)\\n    at com.rapidminer.Process.run(Process.java:832)\\n    at com.rapidminer.Process.run(Process.java:827)\\n    at com.rapidminer.Process.run(Process.java:817)\\n    at ReadRapidminerProcess.main(ReadRapidminerProcess.java:21)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\\n    at java.lang.reflect.Method.invoke(Unknown Source)\\n    at edu.rice.cs.drjava.model.compiler.JavacCompiler.runCommand(JavacCompiler.java:272)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\\n    at java.lang.reflect.Method.invoke(Unknown Source)\\n    at edu.rice.cs.dynamicjava.symbol.JavaClass$JavaMethod.evaluate(JavaClass.java:362)\\n    at edu.rice.cs.dynamicjava.interpreter.ExpressionEvaluator.handleMethodCall(ExpressionEvaluator.java:92)\\n    at edu.rice.cs.dynamicjava.interpreter.ExpressionEvaluator.visit(ExpressionEvaluator.java:84)\\n    at koala.dynamicjava.tree.StaticMethodCall.acceptVisitor(StaticMethodCall.java:121)\\n    at edu.rice.cs.dynamicjava.interpreter.ExpressionEvaluator.value(ExpressionEvaluator.java:38)\\n    at edu.rice.cs.dynamicjava.interpreter.ExpressionEvaluator.value(ExpressionEvaluator.java:37)\\n    at edu.rice.cs.dynamicjava.interpreter.StatementEvaluator.visit(StatementEvaluator.java:106)\\n    at edu.rice.cs.dynamicjava.interpreter.StatementEvaluator.visit(StatementEvaluator.java:29)\\n    at koala.dynamicjava.tree.ExpressionStatement.acceptVisitor(ExpressionStatement.java:101)\\n    at edu.rice.cs.dynamicjava.interpreter.StatementEvaluator.evaluateSequence(StatementEvaluator.java:66)\\n    at edu.rice.cs.dynamicjava.interpreter.Interpreter.evaluate(Interpreter.java:77)\\n    at edu.rice.cs.dynamicjava.interpreter.Interpreter.interpret(Interpreter.java:47)\\n    at edu.rice.cs.drjava.model.repl.newjvm.InterpreterJVM.interpret(InterpreterJVM.java:246)\\n    at edu.rice.cs.drjava.model.repl.newjvm.InterpreterJVM.interpret(InterpreterJVM.java:220)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\\n    at java.lang.reflect.Method.invoke(Unknown Source)\\n    at sun.rmi.server.UnicastServerRef.dispatch(Unknown Source)\\n    at sun.rmi.transport.Transport$1.run(Unknown Source)\\n    at sun.rmi.transport.Transport$1.run(Unknown Source)\\n    at java.security.AccessController.doPrivileged(Native Method)\\n    at sun.rmi.transport.Transport.serviceCall(Unknown Source)\\n    at sun.rmi.transport.tcp.TCPTransport.handleMessages(Unknown Source)\\n    at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(Unknown Source)\\n    at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(Unknown Source)\\n    at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\n    at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\n    at java.lang.Thread.run(Unknown Source)\\n&gt; \\n</code></pre>\\n\\n<p>Can someone please help out with the code?</p>\\n\\n<p>Regards,</p>\\n\\n<p>Keshav</p>\\n-Integrating RapidMiner process with Java-<java><rapidminer>',\n",
       " '<p>I install Eclipse Standard Kepler Service Release 2. And now I decide to update it to support Java EE: using Install Updates->Kepler - <a href=\"http://download.eclipse.org/releases/kepler-\" rel=\"nofollow\">http://download.eclipse.org/releases/kepler-</a>>Web, XML, Java EE....</p>\\n\\n<p>Now I get a lot of features but I can\\'t create Dynamic Web Project.</p>\\n-Eclipse Standard Kepler Service Release 2 isn\\'t updated to Eclipse with JavaEE support correct-<eclipse><jakarta-ee><eclipse-kepler>',\n",
       " '<p>I am using ten fold cross validation operator. I am using rapidminer first time so having some confusion that will I get 10 decision trees as a result. I have read that accuracy is average of all results so what is final output. Average of all?</p>\\n-Rapidminer decision tree using cross validation-<rapidminer>',\n",
       " \"<p>I am trying to port data from a flat file to TD via BTEQ.\\nThe table definition is :<br></p>\\n\\n<pre><code>CREATE MULTISET TABLE _module_execution_log \\n (\\n  system_id INTEGER,\\n  process_id INTEGER,\\n  module_id INTEGER,\\n  julian_dt INTEGER,\\n  referral_dt DATE FORMAT 'YYYY-MM-DD',\\n  start_dt_tm TIMESTAMP(6),\\n  end_dt_tm TIMESTAMP(6),\\n  ref_s_cnt INTEGER,\\n  ref_d_cnt INTEGER)\\n  PRIMARY INDEX ( module_id );\\n</code></pre>\\n\\n<p><br>Following are 2 sample records that i am trying to load in the table :<br><code>1|1|30|2007073|Mar 14 2007 12:00:00:000AM|Mar 15 2007  1:27:00:000PM|Mar 15 2007  1:41:08:686PM|0|0\\n1|1|26|2007073|Mar 14 2007 12:00:00:000AM|Mar 15 2007  1:27:00:000PM|Mar 15 2007  1:59:40:620PM|0|0\\n</code><br>Snippet for my BTEQ script <br></p>\\n\\n<pre><code>USING \\n(   system_id INTEGER\\n    ,process_id INTEGER\\n    ,module_id INTEGER\\n    ,julian_dt INTEGER\\n    ,referral_dt DATE FORMAT 'YYYY-MM-DD'\\n    ,start_dt_tm TIMESTAMP\\n    ,end_dt_tm TIMESTAMP\\n    ,ref_s_cnt INTEGER\\n    ,ref_d_cnt INTEGER\\n)        \\n\\nINSERT INTO _module_execution_log\\n(   system_id\\n    ,process_id\\n    ,module_id\\n    ,julian_dt\\n    ,referral_dt\\n    ,start_dt_tm\\n    ,end_dt_tm\\n    ,ref_s_cnt\\n    ,ref_d_cnt\\n)\\nVALUES ( \\n    :system_id\\n    ,:process_id\\n    ,:module_id\\n    ,:julian_dt\\n    ,:referral_dt\\n    ,:start_dt_tm\\n    ,:end_dt_tm\\n    ,:ref_s_cnt\\n    ,:ref_d_cnt);\\n</code></pre>\\n\\n<p><br>I get the following error during import :<br></p>\\n\\n<pre><code>*** Failure 2665 Invalid date.\\n           Statement# 1, Info =5\\n\\n\\n*** Failure 2665 Invalid date.\\n           Statement# 1, Info =5\\n</code></pre>\\n\\n<p><br> The issue is surely with the exported date in 5th column. I cannot modify the export query.<br> I tried the following in the bteq but still failed : <br> <code>cast(cast(substr(:referral_dt,1,11) as date format 'MMMBDDBYYYY') as date format 'YYYY-MM-DD')\\n</code></p>\\n-Teradata : BTEQ Import Invalid Date Issue-<date><import><teradata>\",\n",
       " '<p>I need a java code that can run rapidminer with R extension. I have read the example of running rapidminer from java from this page (<a href=\"http://rapid-i.com/wiki/index.php?title=Integrating_RapidMiner_into_your_application\" rel=\"nofollow\">http://rapid-i.com/wiki/index.php?title=Integrating_RapidMiner_into_your_application</a>) But there is no example of running extensions through java. Does anyone know of a way to run Rapidminer with R extensions from java?</p>\\n-Running Rapidminer with R extension from Java-<java><r><rapidminer>',\n",
       " '<p>I am trying to reach a remote host that is running Teradata services via ODBC.\\nThe host that I am trying to connect from is 64-bit RHEL 6.x with the following Teradata software installed:</p>\\n\\n<ol>\\n<li>bteq</li>\\n<li>fastexp</li>\\n<li>fastld </li>\\n<li>jmsaxsmod</li>\\n<li>mload    </li>\\n<li>mqaxsmod </li>\\n<li>npaxsmod </li>\\n<li>sqlpp </li>\\n<li>tdodbc</li>\\n<li>tdwallet</li>\\n<li>tptbase </li>\\n<li>tptstream</li>\\n<li>tpump    </li>\\n</ol>\\n\\n<p>When I try to connect to the remote host via Python (interactive session), I receive a \\'Unable to get catalog string\\' error:</p>\\n\\n<pre><code>[@myhost:/path/to/scripts] -&gt;python\\n\\nPython 2.6.6 (r266:84292, Nov 21 2013, 10:50:32)\\n\\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-4)] on linux2\\n\\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n\\n&gt;&gt;&gt; import pyodbc\\n\\n&gt;&gt;&gt; pyodbc.pooling = False\\n\\n&gt;&gt;&gt; cn = pyodbc.connect(\"DRIVER={Teradata}; SERVER=12.245.67.255:1025;UID=usr;PWD=pwd\", ANSI = True)\\n\\nTraceback (most recent call last):\\n\\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\\n\\npyodbc.Error: (\\'28000\\', \\'[28000] [Teradata][ODBC Teradata Driver] Unable to get catalog string. (0) (SQLDriverConnect)\\')\\n</code></pre>\\n\\n<p>Furthermore, when I try to use isql (from the unixODBC yum package), I receive the same error</p>\\n\\n<pre><code>[@my_host:/path/to/scripts] -&gt;isql -v proddsn\\n\\n[28000][Teradata][ODBC Teradata Driver] Unable to get catalog string.\\n\\n[ISQL]ERROR: Could not SQLConnect\\n</code></pre>\\n-Teradata \\'Unable to get catalog string\\' error when using ODBC to connect-<odbc><teradata><rhel><pyodbc><unixodbc>',\n",
       " '<p>I\\'m trying to develop my own workflow in Alfresco 4.2, using the examples of the predefined workflows that brings Alfresco and reusing its forms.</p>\\n\\n<p>Using Eclipse Version: Kepler Service Release 2, I create my own workflow  diagram from the Alfresco-Activiti design templates flows.</p>\\n\\n<p>in all cases, since the start event, it\\'s performed any of the predefines workflows using Form Key: wf:submitGroupReviewTask or wf:submitAdhocTask or other forms of Alfresco (see XML example) and once I copy the Eclipse bpmn file, rename it to bpmn20.xml and upload to Alfresco Enterprise from Admin console (<a href=\"http://xxx.0.0.1:8080/alfresco/activiti-admin#deployment\" rel=\"nofollow\">http://xxx.0.0.1:8080/alfresco/activiti-admin#deployment</a>) when I run the workflow that I created (I perform it on Alfresco Share) the Alfresco normal form appears, but with an additional textbox \"Language:\" which is also blocked and which you can not type (displayed on the screen just above the textbox \"Comment: \" and \" Description: \").</p>\\n\\n<p><strong>Does anyone know how to do for that field \"Language:\" does not appear ? .</strong></p>\\n\\n<pre><code>    &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n\\n-&lt;definitions targetNamespace=\"http://activiti.org/bpmn20\" expressionLanguage=\"http://www.w3.org/1999/XPath\" typeLanguage=\"http://www.w3.org/2001/XMLSchema\" xmlns:omgdi=\"http://www.omg.org/spec/DD/20100524/DI\" xmlns:omgdc=\"http://www.omg.org/spec/DD/20100524/DC\" xmlns:bpmndi=\"http://www.omg.org/spec/BPMN/20100524/DI\" xmlns:activiti=\"http://activiti.org/bpmn\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"http://www.omg.org/spec/BPMN/20100524/MODEL\"&gt;\\n\\n\\n-&lt;process isExecutable=\"true\" name=\"UCAM basado en Ejemplo\" id=\"UCAMbasadoenEjemplo\"&gt;\\n\\n\\n-&lt;extensionElements&gt;\\n\\n\\n-&lt;activiti:executionListener class=\"org.alfresco.repo.workflow.activiti.listener.ScriptExecutionListener\" event=\"start\"&gt;\\n\\n\\n-&lt;activiti:field name=\"script\"&gt;\\n\\n\\n-&lt;activiti:string&gt;\\n\\n&lt;![CDATA[execution.setVariable(\\'wf_approveCount\\', 0); execution.setVariable(\\'wf_actualPercent\\', 0); execution.setVariable(\\'wf_requiredPercent\\', wf_requiredApprovePercent); ]]&gt;\\n\\n&lt;/activiti:string&gt;\\n\\n&lt;/activiti:field&gt;\\n\\n&lt;/activiti:executionListener&gt;\\n\\n&lt;/extensionElements&gt;\\n\\n\\n-&lt;startEvent name=\"Ucam Basado en Ejemplo -1\" id=\"UcamBasadoenEjemplo-1\" activiti:formKey=\"wf:submitGroupReviewTask\"&gt;\\n\\n&lt;documentation&gt;Documentacion START UCAM-1&lt;/documentation&gt;\\n\\n&lt;/startEvent&gt;   \\n</code></pre>\\n-Textbox \"Language \" unwanted using Alfresco forms in Activiti Workflow-<eclipse><workflow><alfresco><alfresco-share><activiti>',\n",
       " '<p>I\\'ve written an Eclipse plugin using Kepler.  Now that it\\'s code complete I attempted to install it using Juno to test backwards compatibility and came up with this error on installation:</p>\\n\\n<pre><code>Only one of the following can be installed at once: \\nExpression Language 3.4.300.v20110228 (org.eclipse.core.expressions \\n3.4.300.v20110228) \\nExpression Language 3.4.401.v20120912-155018 \\n(org.eclipse.core.expressions 3.4.401.v20120912-155018) \\nExpression Language 3.4.401.v20120627-124442 \\n(org.eclipse.core.expressions 3.4.401.v20120627-124442) \\nExpression Language 3.4.400.v20120523-2004 \\n(org.eclipse.core.expressions 3.4.400.v20120523-2004) \\nExpression Language 3.4.501.v20131118-1915 \\n(org.eclipse.core.expressions 3.4.501.v20131118-1915) \\n</code></pre>\\n\\n<p>As seen above, Kepler wants Expression 3.4.500 while Juno uses 3.4.401.  I\\'ve tried removing the minimum version requirement from the org.eclipse.core.expressions plug in (it had been previously set to 3.4.500), but to no avail.</p>\\n\\n<p>What is the correct method to write a plugin that supports both Juno and Kepler?  At the very least what is required to resolve this dependency issue?</p>\\n\\n<p>As requested, here\\'s the manifest:</p>\\n\\n<pre><code>Manifest-Version: 1.0\\nBundle-ManifestVersion: 2\\nBundle-Name: My Plugin\\nBundle-SymbolicName: com.mycompany.eclipse;singleton:=true\\nBundle-Version: 0.2.2\\nBundle-Activator: com.mycompany.eclipse.Activator\\nBundle-Vendor: MyCompany Inc\\nRequire-Bundle: org.eclipse.ui,\\n org.eclipse.core.runtime,\\n org.eclipse.core.resources;bundle-version=\"3.8.1\",\\n org.eclipse.jface.text;bundle-version=\"3.8.2\",\\n org.eclipse.ui.ide;bundle-version=\"3.8.2\",\\n org.eclipse.ui.editors;bundle-version=\"3.8.0\",\\n org.eclipse.jdt.core;bundle-version=\"3.8.3\",\\n com.mycompany.eclipse.dependencies;bundle-version=\"0.1.3\",\\n org.eclipse.ui.forms;bundle-version=\"3.6.0\",\\n org.eclipse.core.expressions;bundle-version=\"3.4.500\"\\nBundle-RequiredExecutionEnvironment: JavaSE-1.7,\\n JavaSE-1.6\\nBundle-ActivationPolicy: lazy\\nsherpaManifest-Version: 1.0\\nBundle-ClassPath: .,\\n resources/,\\n icons/,\\n static/\\n</code></pre>\\n\\n<p>According to a <a href=\"https://stackoverflow.com/questions/22654602/how-to-make-my-plugin-backward-compatible-to-other-versions-of-eclipse?rq=1\">comment on this question</a> plugins are never backwards compatible.  Does anyone have any experience with this?</p>\\n-How to target both Juno and Kepler-<java><eclipse-juno><eclipse-plugin><eclipse-kepler><eclipse-pde>',\n",
       " '<p>i have searched for hours with no real solution.</p>\\n\\n<p>I want to setup an ongoing task (everynight). I have a table in a Teradata database on server 1. Everynight i need to copy an entire table from this teradata instance to my development server (server 2) that has MySQL 5.6.</p>\\n\\n<p>How do i copy an entire table form server 1 to server 2?</p>\\n\\n<p>Things i have tried:\\n1)Select all data from table x into ResultSet from teradata server 1. Insert into mysql via preparedStatement. But this is crazy slow. Also i am not sure how to Drop the table and recreate it each night with the schema from the teradata server.</p>\\n\\n<p>Any help please?</p>\\n-copy table from teradata to mysql-<java><mysql><sql><teradata>',\n",
       " '<p>Here is what I did:</p>\\n\\n<ul>\\n<li>Installed Jadclipse plugin for Eclipse 4.*</li>\\n<li>Ran Eclipse with -clean</li>\\n<li>Set the editor for \"Class without source\" to \"Class decompiler viewer\" (there were no explicit jad decompiler or the like)</li>\\n<li>Opened a .class file with \"Class decompiler viewer\"</li>\\n</ul>\\n\\n<p>It still says that \"Source not found\" and I get the same old...</p>\\n\\n<p>What could have gone wrong?</p>\\n\\n<p>If you can point me to a wiki page or something that would be fine since the plugin\\'s wiki is <strong>empty</strong> and googling for it did not help either.</p>\\n-Jadclipse is not working in Eclipse Kepler SR2-<java><eclipse-kepler><jad>',\n",
       " '<p>I am using <strong>Eclipse Kepler</strong> with <strong>Oracle Enterprise Pack for Eclipse 12.1.2.3</strong> and a Weblogic 12.1.1 server. When setting up the server I get </p>\\n\\n<pre><code>An older version domain is detected.\\nClick here to upgrade it with Upgrade Wizard.\\n</code></pre>\\n\\n<p>I am quite sure that this message is misleading. I have a Weblogic 12 domain but I edited the scripts a bit to use environment variables for domain home and wlserver home instead of having the paths hard coded in the files. So I am able to create a copy of the domain and just change some environment variables to use it.</p>\\n\\n<p>I am able to start the domain from command line using the <code>startWeblogic.sh</code> file, so it is working in general. Only Eclipse does not accept it.</p>\\n\\n<p>Does anybody know what Eclipse is checking for when setting up the server domain?</p>\\n-How is Oracle Pack for Eclipse checking the domain?-<eclipse><weblogic><eclipse-kepler><weblogic12c>',\n",
       " '<p>this tag doesn\\'t work in eclipse kepler and wildfly 8.0, want to work with jpa 2.1 the tag is auto generate by eclipse.</p>\\n\\n<pre><code>&lt;persistence version=\"2.1\"  \\nxmlns=\"http://xmlns.jcp.org/xml/ns/persistence\"\\nxmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" \\nxsi:schemaLocation=\"http://xmlns.jcp.org/xml/ns/persistence     \\nhttp://xmlns.jcp.org/xml/ns/persistence/persistence_2_1.xsd\"&gt;\\n</code></pre>\\n\\n<p>if I try with this tag </p>\\n\\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;persistence xmlns=\"http://java.sun.com/xml/ns/persistence\"\\n             xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\\n             xsi:schemaLocation=\"http://java.sun.com/xml/ns/persistence http://java.sun.com/xml/ns/persistence/persistence_2_0.xsd\"\\n             version=\"2.0\"&gt;\\n</code></pre>\\n\\n<p>everything works fine.</p>\\n\\n<p>please helpme with this issues.</p>\\n\\n<p><img src=\"https://i.stack.imgur.com/NNkSs.png\" alt=\"this is the error\"></p>\\n-JPA 2.1 no work in eclipse kepler-<persistence><eclipse-kepler><jpa-2.1>',\n",
       " '<p>i am using</p>\\n\\n<pre><code>create table xxx fallback,\\n                 before journal,\\n                 after journal,\\n                 checksum=default\\n(\\n    field1 integer,\\n    field2 integer \\n)\\nprimary index field1 ; \\n</code></pre>\\n\\n<hr>\\n\\n<p>But i do not know if this is completely okay . i have some  questions : </p>\\n\\n<p>1) can i use <code>before journal</code> and <code>after journal</code> at the same time ?</p>\\n\\n<p>2) both <code>before journal</code> and <code>after journal</code> are used for backing up the table ? what is the difference ?</p>\\n\\n<p>3) does <code>fallback</code> fully cover for both journals ?</p>\\n\\n<p>4) is it important to have <code>checksum</code> ?</p>\\n-How to create a protected table in teradata?-<teradata>',\n",
       " '<p>I just installed Eclipse KEPLER and am trying to bring all my projects back in from github.  The projects are all gradle projects so my procedure is pretty much...</p>\\n\\n<ol>\\n<li>Clone the git repo</li>\\n<li>gradle eclipse</li>\\n<li>Import Project into Eclipse</li>\\n</ol>\\n\\n<p>Everything seems to be fine, except I can\\'t \"Run As, Groovy Script\" the scripts within the project.  When I try, the console spews...</p>\\n\\n<pre><code>java.lang.NoClassDefFoundError: org/apache/commons/cli/CommandLineParser\\n    at java.lang.Class.getDeclaredMethods0(Native Method)\\n    at java.lang.Class.privateGetDeclaredMethods(Class.java:2521)\\n    at java.lang.Class.getMethod0(Class.java:2764)\\n    at java.lang.Class.getMethod(Class.java:1653)\\n    at org.codehaus.groovy.tools.GroovyStarter.rootLoader(GroovyStarter.java:99)\\n    at org.codehaus.groovy.tools.GroovyStarter.main(GroovyStarter.java:130)\\nCaused by: java.lang.ClassNotFoundException: org.apache.commons.cli.CommandLineParser\\n    at org.codehaus.groovy.tools.RootLoader.findClass(RootLoader.java:156)\\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\\n    at org.codehaus.groovy.tools.RootLoader.loadClass(RootLoader.java:128)\\n    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\\n    ... 6 more\\n</code></pre>\\n\\n<p>.  Interestingly enough, I can \"Run As, Java Application\" without issue.  Its definitely a Groovy project as it has the G in the project icon.  What does the stacktrace mean and how do I overcome it?</p>\\n\\n<p>The repository that is causing my issue is at <a href=\"https://github.com/robertkuhar/StackOverflow\" rel=\"nofollow\">https://github.com/robertkuhar/StackOverflow</a></p>\\n\\n<p>My gradle is 1.11 on groovy 1.8.6 in eclipse kepler with the Groovy-Eclipse plug-in Version: 2.8.0.xx-20130703-1600-e43-RELEASE</p>\\n-NoClassDefFoundError...org/apache/commons/cli/CommandLineParser when Run As, Groovy Script-<eclipse><groovy><gradle><eclipse-kepler>',\n",
       " '<p>Does MonetDB make the current DB name (i.e. not the DBMS hostname, but the DB, or set of tables, that you connect to) available via SQL queries to some of the system table?</p>\\n-In MonetDB, can I obtain the database name via SQL commands?-<sql><reflection><database><monetdb>',\n",
       " '<p>I have a recurring problem using <em>Eclipse</em>. Consider the following example:</p>\\n\\n<p><img src=\"https://i.stack.imgur.com/04dYE.png\" alt=\"Organize imports with deprecated class\"></p>\\n\\n<p>As you can see I\\'ve pressed <kbd>Ctrl</kbd>+<kbd>Shift</kbd>+<kbd>O</kbd>. I can choose from a <em>deprecated</em> and a <em>non-deprecated</em> annotation. My problem is that I am often supplied with dozens of classes and half of them are deprecated (a perfect example is the <strong>JUnit</strong> <code>Assert</code> classes).</p>\\n\\n<p>My question is how can I make <em>Eclipse</em> ignore <strong>all</strong> deprecated classes when organizing imports?</p>\\n-How can I prevent Eclipse from supplying deprecated classes when importing?-<java><eclipse><import><java-7><eclipse-kepler>',\n",
       " \"<p>I have to create a structured User Defined Type and use it as routine's parameters datatype in Teradata.</p>\\n\\n<p>So I've created the following UDT</p>\\n\\n<pre><code>create type sysudtlib.person as(ssn character(11),first_name varchar(20)) not final;\\n</code></pre>\\n\\n<p>However, wherever I try to use the created UDT, either in table DDL or in stored procedure</p>\\n\\n<pre><code>create table twm_source.test_udt(purchase_id int, customer sysudtlib.person);\\n\\ncreate procedure twm_source.sp_test_udt(in p1 sysudtlib.person)\\nbegin\\ninsert into tbl_udt values(p1.ssn, p1.first_name);\\nend;\\n</code></pre>\\n\\n<p>I get the same error:</p>\\n\\n<pre><code>User-defined  Transform is not defined for UDT 'person' .\\n</code></pre>\\n\\n<p>It looks like I have to add a definition of the UDT-GroupName to the DBC.UDTTRANSFORM dictionary table.\\nCould anyone please provide some brief but vivid example of code that illustrates how to do so. Just something I need to do in order to get my structured UDT ready for use.</p>\\n\\n<p>Any help is greatly appreciated.</p>\\n-How to use Teradata structured UDT-<sql><transform><teradata><user-defined-types>\",\n",
       " '<p>in teradata i have two tables . Both of them display data correctly and fine </p>\\n\\n<pre><code>select * from table1;\\nselect * from table2;\\n</code></pre>\\n\\n<hr>\\n\\n<p>however the following query throws an error \\n<code>string contains untranslatable character</code></p>\\n\\n<pre><code>insert into table1\\n(varone)\\nselect varone from table2\\n;\\n</code></pre>\\n\\n<p>why could that happen ? both tables are in teradata which means they do not have any bad characters otherwise they would not be in teradata in the first place   </p>\\n-Teradata : String contains untranslatable character-<teradata>',\n",
       " '<p>I am using Kepler and have been trying to modify the default template for \"for - iterate over array\" at Window > Preferences > Java > Editor > Templates.  The default is:</p>\\n\\n<pre><code>for (int ${index} = 0; ${index} &lt; ${array}.length; ${index}++) {\\n    ${line_selection}${cursor}\\n}\\n</code></pre>\\n\\n<p>and I simply want to indent the closing brace so that we have:</p>\\n\\n<pre><code>for (int ${index} = 0; ${index} &lt; ${array}.length; ${index}++) {\\n    ${line_selection}${cursor}\\n    }\\n</code></pre>\\n\\n<p>The change is being saved (I can return and see the indented brace) but when I try to use this in a .java file by typing \"for\" the popup list shows \"for - iterate over array\" with the original non-indented brace and when I select that I get the default/non-indented brace as:</p>\\n\\n<pre><code>for (int ${index} = 0; ${index} &lt; ${array}.length; ${index}++) {\\n    ${line_selection}${cursor}\\n}\\n</code></pre>\\n\\n<p>If I make some other change to the template such as:</p>\\n\\n<pre><code>for (int ${index} = 0; ${index} &lt; ${array}.length; ${index}++) {\\n    ${line_selection}${cursor}\\n    THIS IS JUST A TEST\\n    }\\n</code></pre>\\n\\n<p>I get:</p>\\n\\n<pre><code>for (int i = 0; i &lt; bBPMN2.length; i++) {\\n\\n    THIS IS JUST A TEST\\n}\\n</code></pre>\\n\\n<p>The new text is used (although I don\\'t know why I get the blank line) so that suggests to me that the template is being activated.  So it seems that I just can\\'t get an indented closing brace.  I am wondering if there is some other \"format\" setting that is overriding the indentation on my closing brace but I can\\'t find anything.  Not that I can see why it would make a difference but I have tried this with \"Use code formatter\" turned off and it makes no difference.</p>\\n\\n<p>How can I get an indented closing brace?</p>\\n\\n<p>Thank you.\\nAl</p>\\n-Eclipse Kepler Java Template Doesn\\'t Work With an Indented Brace-<java><eclipse><templates><eclipse-kepler>',\n",
       " '<p>I want to install Subversive plugin on a laptop that is not connected to Internet .<br>\\nWhat are the steps for installing Subversive on Eclipse Kepler without going through the normal procedure(Help-install New software-...) ? Which jar files I will need in this installation ?  </p>\\n-installing subversive in Eclipse Kepler manually-<svn><eclipse-plugin><eclipse-kepler><subversive>',\n",
       " '<p>I have table in Teradata and have trillion of record.\\n Temp- with cat_nbr as PI</p>\\n\\n<pre><code>Cat_nbr | brand_Nbr |card_nbr\\n1       |  10       | 100\\n1       |   10      |101\\n1       |20         | 100\\n1       | 20        | 102\\n2       |10         | 100\\n2       | 10        |103\\n2       |30         |100\\n2       |30         |105\\n3       |40         |106\\n3       | 30        |107\\n</code></pre>\\n\\n<p>I need to find out categories total no of customer for a particular brand.\\nJust an ex. for brand no:10\\nFirst we need to check which cat have brand no 10, in this cat 1,2 have it.\\nThen for all cutomer in cat 1,2 ; we need count(distinct card_no).</p>\\n\\n<p>result shoul be like </p>\\n\\n<pre><code>brand_nbr|total_cust\\n10       | 5\\n</code></pre>\\n\\n<p>I have written the below query to achive this:-</p>\\n\\n<pre><code>select k.brand_nbr,count(distinct l.card_nbr) \\nfrom temp k join temp l on k.cat_nbr=l.cat_nbr\\ngroup by 1;\\n</code></pre>\\n\\n<p>It give me proper result but the thing , we have trillion of records in table and when I do run the query it goes on processing like more than 2 hrs.</p>\\n\\n<p>I need a solution to improve the performance so that it can max in 30 min.\\nI have checked the amps , there are 16 amps for my database.</p>\\n\\n<p>Please masters help me out if you have any solution for this.</p>\\n\\n<p>Thanks in advance.</p>\\n-Teradata-replacing self join-<sql><teradata>',\n",
       " '<p>Is Teradata is suggested for use in OLTP operation with java web application. Is hibernate is the good choice to use an ORM layer with Teradata ? Please suggest.</p>\\n-Use of Teradata for OLTP operation with java web application-<java><performance><hibernate><teradata>',\n",
       " '<p>We have an requirement wherein we need to have multiple datasources for different teradata database users. Now for these different datasources how we can configure multiple transaction manager.</p>\\n\\n<p>We have different database users as per their privileges and priorities given at DB level. Suggest how we effectively we can configure transaction management with spring.</p>\\n-How to configure mutliple transaction managers with Spring + Teradata-<java><spring><teradata><spring-transactions>',\n",
       " \"<p>I'm using Subversive plugin with Eclipse Kepler.<br>\\nWhen I want to work on a file in Eclipse, Is there an icon that can be enabled, showing that the version of the file in the server (revision number) is greater than the one of the local same file ? (without going through 'synchronise with repository')</p>\\n-Subversive icon showing revision number of a file in the repository is greater than local one-<eclipse><svn><eclipse-kepler><subversive>\",\n",
       " '<p>I have an existing java web app that is currently connected to an Oracle db, I need to also connect this application to a teradata db where it will run scripts based on which db needs to be used. I have never worked with a teradata db before, and just need a little help with setting up my application to use the teradata db along with the existing db. Any tutorials and/or tips would be greatly appreciated.</p>\\n-connect a java web application to both oracle and teradata db-<java><oracle><teradata>',\n",
       " \"<p>In postgresql, one can have all tables names by running the following query</p>\\n\\n<pre><code>SELECT table_name FROM information_schema.tables WHERE table_schema='public';\\n</code></pre>\\n\\n<p>Is there something similar in monetdb to have the list of tables ?</p>\\n-How to have the list of table in a monetdb database?-<monetdb>\",\n",
       " \"<p>I am trying to do a join between a date column and the current date in a local timezone using:</p>\\n\\n<pre><code>SELECT a.* FROM \\na\\njoin b on (a.country_id  = b.country_id)\\nand date_column =  (Current_date at b.capital_timezone)\\n</code></pre>\\n\\n<p>b.capital_timezone has values like Europe Central, Europe Eastern,GMT+4, GMT+5:30 etc.,</p>\\n\\n<p>I get an error saying - 'Invalid time zone specified'</p>\\n-Teradata - Invalid time zone specified while converting time zones-<teradata>\",\n",
       " '<p>I have the following table in Teradata 14 , however I am not allowed to write procedures and functions myself</p>\\n\\n<pre><code>id  property\\n1   1234X (Yel), 2345Y (Red), 1234X (Gre),\\n2   2222Y (Pin), 2222Y (Red), 2222Y (Gre),\\n3   3454E (Yel), 4565Y (Whi), 3454E (Red),\\n</code></pre>\\n\\n<p>All rows are in the same fashion. E.g. row 1 : <code>1234X</code> is an object that has attributes <code>(Yel)</code> and <code>(Gre)</code> . Whereas <code>2345Y</code> has <code>Red</code></p>\\n\\n<hr>\\n\\n<p>How can I group the above table so that each object would have all attributes listed in one brackets</p>\\n\\n<pre><code>id  property\\n1   1234X (Yel Gre), 2345Y (Red), \\n2   2222Y (Pin Red Gre ),\\n3   3454E (Yel Red), 4565Y (Whi), \\n</code></pre>\\n-How to group sub strings in Teradata?-<teradata>',\n",
       " '<p>is it possible to <strong>run</strong> the <strong>e(fx)clipse sample application</strong> on <strong>Kepler &amp; Java7</strong> at all ?</p>\\n\\n<p>I took the steps, descripted in tutorial 4 to integrate the target definition(s)\\n<a href=\"https://wiki.eclipse.org/Efxclipse/Tutorials/Tutorial4\" rel=\"nofollow\">https://wiki.eclipse.org/Efxclipse/Tutorials/Tutorial4</a></p>\\n\\n<p>I tried the all-in-one-eclipse as well as upgrading a common kepler-eclipse with update-site <a href=\"http://download.eclipse.org/efxclipse/updates-released/0.9.0/site\" rel=\"nofollow\">http://download.eclipse.org/efxclipse/updates-released/0.9.0/site</a></p>\\n\\n<p>However, the Sample Project JavaFX/Samples/e4 Media Application can not be compiled.</p>\\n\\n<ul>\\n<li><p>With Target Definition \"RCP 3x Platform\", all! application dependencies can not be resolved</p></li>\\n<li><p>With Target Definition \"Target Platform Feature\" a warning comes up, that the target version is newer than my current Eclipse installation. There are no comile errors anymore, but running the app I will get a lot of runtime errors like:\\nUnresolved requirement: Require-Capability: osgi.ee; filter:=\"(&amp;(osgi.ee=JavaSE)(version=1.8))\" </p></li>\\n</ul>\\n\\n<p>Did anybody get the sample \"e4 Media Application\" working ?</p>\\n\\n<p>My Eclipse: Kepler Service Release 2</p>\\n\\n<p>My Java: JDK 7u40</p>\\n\\n<p>============== EDIT ==============</p>\\n\\n<p>Got it working, thanks to Tom!</p>\\n\\n<ol>\\n<li><p>Install fresh new Eclipse: http:// www.eclipse.org/downloads/packages/eclipse-rcp-and-rap-developers/keplersr2</p></li>\\n<li><p>Install Xtext 2.5.0, Update-Site: http:// download.eclipse.org/modeling/tmf/xtext/updates/composite/releases</p></li>\\n<li><p>Install e(fx)clipse, Update-Site: http:// download.eclipse.org/efxclipse/updates-released/0.9.0/site</p></li>\\n<li><p>Add new Target Platform, Update-Site: http:// download.eclipse.org/efxclipse/runtime-released/0.9.0/site\\nSelect FX Target -> Target Platform Feature</p></li>\\n</ol>\\n-e(fx)clipse 0.9.0 on Kepler with Java 7-<eclipse><javafx><eclipse-kepler><efxclipse>',\n",
       " '<p>I have the following table in <em>Teradata 14</em> ,  I am not allowed to write procedures and functions myself, but i can use <code>strtok</code>, <code>strtok_split_to_table</code> etc</p>\\n\\n<pre><code>id  property\\n1   1234X (Yel), 2225Y (Red), 1234X (Gre),\\n2\\n3   1222Y (Pin), \\n4   1134E (Yel), 4565Y (Whi), 1134E (Red), 2222Y (Red), \\n</code></pre>\\n\\n<p>How can I group the above table so that each object would have all attributes listed in one brackets</p>\\n\\n<pre><code>id  property\\n1   1234X (Yel Gre), 2225Y (Red), \\n2   \\n3   1222Y (Pin ),\\n4   1134E (Yel Red), 4565Y (Whi), 2222Y (Red), \\n</code></pre>\\n\\n<p><em>The property code is always a 5 character string, e.g. <code>1222Y</code> . The color code is always 3 character , e.g. <code>Pin</code></em> </p>\\n\\n<hr>\\n\\n<p>I tried using <a href=\"https://stackoverflow.com/questions/20155600/is-it-possible-to-group-string-within-a-string-in-teradata\">this solution</a> but got an error <code>A column or character expression is larger than max size</code> </p>\\n\\n<p>In addition I tried <code>strtok_split_to_table</code> and was able to create a modified table, but do not how to proceed from that</p>\\n-How to group substrings in Teradata 14?-<teradata><strtok>',\n",
       " '<p>I use <code>Teradata 14</code> with all <code>strtok</code> and other new functions , but i am not allowed to write my own functions .</p>\\n\\n<p>In the following table each person has many skills . How can I concatenate those skills ? </p>\\n\\n<pre><code>team    person\\n1       Mike (swi)\\n1       Nick (dri)\\n1       Mike (coo)\\n2       \\n3       Kate (swi)\\n3       Kate (coo)\\n3       Kate (dri)\\n3       Wend (fly)\\n4       Pete (jum)\\n</code></pre>\\n\\n<hr>\\n\\n<p>Desired table is </p>\\n\\n<pre><code>team    person\\n1       Mike (swi coo), Nick (dri),\\n2       \\n3       Kate (swi coo dri), Wend(fly),\\n4       Pete (jum),\\n</code></pre>\\n\\n<p>How can I concatenate strings ?</p>\\n-How to concatenate strings in Teradata-<sql><teradata>',\n",
       " \"<p>I need to query my database and bring this back:</p>\\n\\n<pre><code>| Policy Num | Coverage Code |\\n</code></pre>\\n\\n<p>My current query returns everything for the year 2014 ( i.e. A row for each claim number with the attached coverage code)</p>\\n\\n<blockquote>\\n  <p>SELECT\\n    SCHEMA.TABLE.POLICY_NUMBER,\\n    P_FAR_BI_VW.V_CLAIM_SERVICE_TYP_DIM.COVERAGE_TYP_CDE</p>\\n  \\n  <p>FROM</p>\\n  \\n  <p>SCHEMA.TABLE</p>\\n  \\n  <p>INNER JOIN SCHEMA.TABLE ON (SCHEMA.TABLE.POLICY_ID=SCHEMA.TABLE.POLICY_ID)</p>\\n  \\n  <p>INNER JOIN SCHEMA.TABLE ON (SCHEMA.TABLE.SERVICE_TYPE_ID=SCHEMA.TABLE.SERVICE_TYPE_ID)</p>\\n  \\n  <p>INNER JOIN SCHEMA.TABLE ON (SCHEMA.TABLE.FISCAL_PERIOD_ID=SCHEMA.TABLE.FISCAL_PERIOD_ID  AND  SCHEMA.TABLE.YEAR_NUM  =  2014)</p>\\n</blockquote>\\n\\n<p>I have tried a few different ways of querying but I can't seem to get it to work. I have tried:</p>\\n\\n<blockquote>\\n  <p>SELECT\\n    distinct(SCHEMA.TABLE.POLICY_NUMBER),\\n    SCHEMA.TABLE.COVERAGE_TYP_CDE</p>\\n  \\n  <p>Inner Joins</p>\\n  \\n  <p>group by\\n    SCHEMA.TABLE.COVERAGE_TYP_CDE</p>\\n</blockquote>\\n\\n<p>and other various ideas.  The Error I am getting in return is: <code>Selected non-aggregate values must be part of the associated group</code></p>\\n\\n<p>I know there must be away to </p>\\n-SQL / Teradata Query - Filter by fields while using multiple Inner Joins-<sql><select><teradata>\",\n",
       " '<p>Suppose I have a two-column table (t1) with the following rows:</p>\\n\\n<pre><code>id   animal\\n---- ------\\n1    dog\\n1    pig\\n1    donkey\\n2    cow\\n2    horse\\n2    dog\\n2    donkey\\n</code></pre>\\n\\n<p>Now, I want to retain only one row for a given id.  I could do a minimum or maximum along with a group-by:</p>\\n\\n<pre><code>create table t2 as (\\n    select id, min(animal)\\n    from t1\\n    group by id\\n) with data unique primary index(id);\\n</code></pre>\\n\\n<p>Is there a way to get a random row for each id?  Something that is less predictable than a min or max.</p>\\n-Teradata: Keep a random single row per column value-<sql><teradata>',\n",
       " '<p>Our monetdbd instance throws the error \"!FATAL: BBPextend: trying to extend BAT pool beyond the limit (16384000)\" after restarting from a normal shutdown (monetdbd start farm works, monetdb start database fails with the given error).</p>\\n\\n<p>The database contains less than 10 tables and each table has min. 3 fields and max. 22 fields.  The overall database size is about 16 GB and a table with 5 fields (3 ints, 1 bigint, 1 date) has 450mil. rows.</p>\\n\\n<p>Has anyone an idea how to solve that problem without loosing the data?</p>\\n\\n<h1>monetdbd --version</h1>\\n\\n<p>MonetDB Database Server v1.7 (Jan2014-SP1)</p>\\n\\n<h1>Server details:</h1>\\n\\n<ul>\\n<li>Ubuntu 13.10 (GNU/Linux 3.11.0-19-generic x86_64)</li>\\n<li>12 Core CPU (hexacore + ht): Intel(R) Core(TM) i7 CPU X 980  @ 3.33GHz</li>\\n<li>24 GB Ram</li>\\n<li>2x 120 GB SSD, Software-Raid 1, LVM</li>\\n</ul>\\n\\n<h1>Further details:</h1>\\n\\n<p># wc BBP.dir: \"240 10153 37679 BBP.dir\"</p>\\n-MonetDB; !FATAL: BBPextend: trying to extend BAT pool beyond the limit (16384000)-<monetdb>',\n",
       " '<p>In case of a nested query in teradata, if I have to collect statistics. Whats the ideal way to do it :</p>\\n\\n<p>(1) Will I collect statistics on the entire nested query.</p>\\n\\n<p>(2) Or will I collect statistics on only that part of a query on which I need statistics on?</p>\\n\\n<p>I know its a kind off ambiguous question but I wanted to know the ideal way to approach this scenario.</p>\\n-Collecting statistics in Teradata-<statistics><teradata>',\n",
       " '<p>I am using the equations and data for Mars here\\n<a href=\"http://ssd.jpl.nasa.gov/txt/aprx_pos_planets.pdf\" rel=\"nofollow\">http://ssd.jpl.nasa.gov/txt/aprx_pos_planets.pdf</a></p>\\n\\n<p>and the solution for the eccentric anomaly kepler equation given here at the top of page four\\n<a href=\"http://murison.alpheratz.net/dynamics/twobody/KeplerIterations_summary.pdf\" rel=\"nofollow\">http://murison.alpheratz.net/dynamics/twobody/KeplerIterations_summary.pdf</a></p>\\n\\n<p>And checking the output by modifying the date in the get_centuries_past to the following dates and looking at page E-7 for the actual x,y,z coordinates of Mars (sample data below, but link for the curious:\\n<a href=\"http://books.google.com/books/about/Astronomical_Almanac_for_the_Year_2013_a.html?id=7fl_-DLwJ8YC\" rel=\"nofollow\">http://books.google.com/books/about/Astronomical_Almanac_for_the_Year_2013_a.html?id=7fl_-DLwJ8YC</a>)</p>\\n\\n<p>date 2456320.5 is 2013, 1, 28 and should output</p>\\n\\n<pre><code>x = 1.283762\\ny = -0.450111\\nz = -0.241123\\n</code></pre>\\n\\n<p>date 2456357.5 is 2013, 3, 6 and should output</p>\\n\\n<pre><code>x = 1.300366\\ny = 0.533593\\nz = 0.209626\\n</code></pre>\\n\\n<p>date 2456539.500000 is 2013, 9, 4 and should output</p>\\n\\n<pre><code>x = - 0.325604\\ny = 1.418110\\nz = 0.659236\\n</code></pre>\\n\\n<p>I tested mean anomaly equation and it was fine.  However, I cannot get a good set of x,y,z coordinates.  I have been tweaking my kepler and coordinate function but cannot get them to match the tables in the astronomical almanac.  </p>\\n\\n<p>Any suggestions or advice on solving the positions of the stars is greatly appreciated. The code below can be put in a .rb file and running it on the command line will output the x,y,z values.  </p>\\n\\n<pre><code>def get_centuries_past_j2000()\\n        #second number is from DateTime.new(2000,1,1,12).amjd.to_f - 1 the modified julian date for the J2000 Epoch\\n        #Date.today.jd.to_f - 51544.5\\n        (DateTime.new(2013,1,28).amjd.to_f - 51544.5)/36525\\nend\\n\\nclass Planet\\n    attr_accessor :semi_major_axis, :semi_major_axis_delta, :eccentricity, :eccentricity_delta,\\n    :inclination, :inclination_delta, :mean_longitude, :mean_longitude_delta, :longitude_of_perihelion, \\n    :longitude_of_perihelion_delta, :longitude_of_ascending_node, :longitude_of_ascending_node_delta, :time_delta\\ndef initialize(semi_major_axis, semi_major_axis_delta, eccentricity, eccentricity_delta,\\ninclination, inclination_delta, mean_longitude, mean_longitude_delta, longitude_of_perihelion, \\nlongitude_of_perihelion_delta, longitude_of_ascending_node, longitude_of_ascending_node_delta, time_delta)\\n    @semi_major_axis = semi_major_axis + (semi_major_axis_delta * time_delta)\\n    @eccentricity = eccentricity + (eccentricity_delta * time_delta)\\n    @inclination = inclination + (inclination_delta * time_delta)\\n    @mean_longitude = mean_longitude + (mean_longitude_delta * time_delta)\\n    @longitude_of_perihelion = longitude_of_perihelion + (longitude_of_perihelion_delta * time_delta)\\n    @longitude_of_ascending_node = longitude_of_ascending_node + (longitude_of_ascending_node_delta * time_delta)\\n    @argument_of_perhelion = @longitude_of_perihelion - @longitude_of_ascending_node\\nend\\n\\ndef mean_anomaly\\n    ((@mean_longitude - @longitude_of_perihelion)%360).round(8)\\nend\\n\\ndef eccentric_anomaly\\n    mod_mean_anomaly = mean_anomaly\\n    if mod_mean_anomaly &gt; 180\\n        mod_mean_anomaly = mod_mean_anomaly - 360\\n    elsif mod_mean_anomaly &lt; -180\\n        mod_mean_anomaly = mod_mean_anomaly + 360\\n    end\\n    e34 = @eccentricity**2\\n    e35 = @eccentricity*e34\\n    e33 = Math.cos(mod_mean_anomaly*Math::PI/180)\\n    mod_mean_anomaly + (-0.5 * e35 + @eccentricity + (e34 + 1.5 * e33 * e35) * e33) * Math.sin(mod_mean_anomaly*Math::PI/180)\\nend\\n\\ndef J2000_ecliptic_plane\\n    x_prime = @semi_major_axis * (Math.cos(eccentric_anomaly*Math::PI/180) - @eccentricity)\\n    y_prime = @semi_major_axis * Math.sqrt(1-@eccentricity**2) * Math.sin(eccentric_anomaly*Math::PI/180)\\n    z_prime = 0\\n    x = x_prime * (Math.cos(@argument_of_perhelion*Math::PI/180) * Math.cos(@longitude_of_ascending_node*Math::PI/180) - Math.sin(@argument_of_perhelion * Math::PI/180) * Math.sin(@longitude_of_ascending_node * Math::PI/180) * Math.cos(@inclination * Math::PI/180)) + y_prime * (-Math.sin(@argument_of_perhelion* Math::PI/180) * Math.cos(@longitude_of_ascending_node * Math::PI/180) - Math.cos(@argument_of_perhelion * Math::PI/180) * Math.sin(@longitude_of_ascending_node * Math::PI/180) * Math.cos(@inclination * Math::PI/180))\\n    y = x_prime * (Math.cos(@argument_of_perhelion*Math::PI/180) * Math.sin(@longitude_of_ascending_node*Math::PI/180) + Math.sin(@argument_of_perhelion * Math::PI/180) * Math.cos(@longitude_of_ascending_node * Math::PI/180) * Math.cos(@inclination * Math::PI/180)) + y_prime * (-Math.sin(@argument_of_perhelion* Math::PI/180) * Math.sin(@longitude_of_ascending_node * Math::PI/180) + Math.cos(@argument_of_perhelion * Math::PI/180) * Math.cos(@longitude_of_ascending_node * Math::PI/180) * Math.cos(@inclination * Math::PI/180))\\n    z = x_prime * Math.sin(@argument_of_perhelion*Math::PI/180) * Math.sin(@inclination*Math::PI/180) + y_prime * Math.cos(@argument_of_perhelion*Math::PI/180) * Math.sin(@inclination*Math::PI/180)\\n    return x, y, z\\nend\\nend\\n\\ntime = get_centuries_past_j2000\\nmars = Planet.new(1.52371034, 0.00001847, 0.09339410, 0.00007882, 1.84969142, -0.00813131, -4.553443205, 19140.30268499, -23.94362959, 0.44441088, 49.55952891, -0.29257343, time)\\nputs time\\nputs mars.mean_anomaly\\nputs mars.eccentric_anomaly\\nputs mars.J2000_ecliptic_plane\\n</code></pre>\\n-Calculating the x,y,z position of solar bodies using ruby-<ruby><position><system><kepler>',\n",
       " \"<p>I'm developing RCP application based on e4 compatibility layer.</p>\\n\\n<p><strong>Question</strong> is there any way so that i can subscribe any particular action (subscribe mean get notification).<br>\\nFor Example <code>CUT</code> or <code>COPY</code> action: when user select these option i want to get notify. So is there is any extension or some thing else to achieve this.</p>\\n-How to subscribe action in RCP application-<java><eclipse><eclipse-plugin><eclipse-rcp><eclipse-kepler>\",\n",
       " \"<p>Here is my Table1</p>\\n\\n<pre><code>personid\\n1\\n?\\n2\\n3\\n4\\n?\\n6\\n</code></pre>\\n\\n<hr>\\n\\n<p>Here is my query</p>\\n\\n<pre><code>select * \\nfrom table2\\nwhere personid not in \\n(\\nselect personid\\nfrom table1\\n)\\n</code></pre>\\n\\n<p>The result is <code>nothing</code></p>\\n\\n<hr>\\n\\n<p>Here is my second query</p>\\n\\n<pre><code>select * \\nfrom table2\\nwhere personid not in \\n(\\nselect personid\\nfrom table1         \\nwhere personid is not null\\n)\\n</code></pre>\\n\\n<p>The result is <code>ok</code></p>\\n\\n<hr>\\n\\n<p>Question : why the first query did not work ? I can't see any logical problem . Do <code>null</code>s skrew up teradata ?</p>\\n-Null values in IN clause - Teradata-<sql><teradata>\",\n",
       " '<p>what are the advantages and disadvantages of each version of Eclipse (Juno and Kepler) ? which one is the more stable version ? how to choose the right version of eclipse?</p>\\n-Eclipse Juno vs Eclipse Kepler-<eclipse><eclipse-juno><eclipse-kepler>',\n",
       " '<p>How can i remove using teradata query all other characters except 15 from string {\"Here we start\":\"15\",\"My Id\":1}\\nIt should return only 15 </p>\\n\\n<p>thanks</p>\\n-Remove characters from a string teradata-<string><teradata>',\n",
       " '<p>I am running this exact query (hard coded values and all) in both Teradata SQL Assistant and via pyodbc in Python 2.7. I receive different results from the two methods, and it appears the pyodbc one is incorrect.</p>\\n\\n<pre><code>SELECT serial_num, event_desc,event_level, count(*) as \"Count\", MAX(event_ts) as \"Latest Event Timestamp\"\\n    FROM mytable\\n    WHERE\\n    serial_num in(\\'serial1\\',\\'serial2\\')\\n    AND event_ts &gt;= \\'2014-01-01 00:00:00.000\\'\\n    AND event_ts &lt;= \\'2014-03-31 23:59:59.999\\'\\n    and event_desc = \\'My Test Event\\'\\n    group by serial_num, event_desc,event_level\\n    order by serial_num, \"Latest Event Timestamp\"\\n</code></pre>\\n\\n<p>Teradata Assistant results look like this</p>\\n\\n<pre><code>serial_num  event_desc      event_level     Count   Latest Event Timestamp\\nserial1     My Test Event   1               5       3/4/2014 10:03:28.000000\\nserial2     My Test Event   1               12      3/27/2014 13:01:25.000000\\nserial2     My Test Event   2               4       3/27/2014 13:32:59.000000\\n</code></pre>\\n\\n<p>However, when run in Python using pyodbc, I get different results:</p>\\n\\n<pre><code>serial_num  event_desc      event_level     Count   Latest Event Timestamp\\nserial1     \\'My Test Event\\' \\'1\\'             2       datetime.datetime(2014, 1, 3, 10, 5, 15\\nserial2     \\'My Test Event\\' \\'2\\'             1       datetime.datetime(2014, 3, 14, 2, 22, 47\\nserial2     \\'My Test Event\\' \\'1\\'             4       datetime.datetime(2014, 3, 22, 6, 36, 40\\n</code></pre>\\n\\n<p>The differences are in the count and timestamp columns. I am not sure what, exactly, the query is doing in python. It returns valid time stamps, but they aren\\'t the max time same for the <code>group by</code> and the count is incorrect. Why?</p>\\n\\n<p>This is the ungroupped data:</p>\\n\\n<pre><code>serial_num  event_desc      event_level     Latest Event Timestamp\\nserial1     My Test Event   1               1/2/2014 07:11:22.000000\\nserial1     My Test Event   1               1/3/2014 10:05:15.000000\\nserial1     My Test Event   1               1/19/2014 13:32:17.000000\\nserial1     My Test Event   1               3/4/2014 09:16:15.000000\\nserial1     My Test Event   1               3/4/2014 10:03:28.000000\\nserial2     My Test Event   1               1/19/2014 14:04:47.000000\\nserial2     My Test Event   1               1/28/2014 13:27:00.000000\\nserial2     My Test Event   2               1/28/2014 13:57:12.000000\\nserial2     My Test Event   1               2/5/2014 01:36:47.000000\\nserial2     My Test Event   2               2/5/2014 02:56:53.000000\\nserial2     My Test Event   1               2/23/2014 01:57:19.000000\\nserial2     My Test Event   1               2/27/2014 13:50:08.000000\\nserial2     My Test Event   1               2/28/2014 13:55:51.000000\\nserial2     My Test Event   1               3/9/2014 15:31:00.000000\\nserial2     My Test Event   1               3/14/2014 01:31:36.000000\\nserial2     My Test Event   2               3/14/2014 02:22:47.000000\\nserial2     My Test Event   1               3/16/2014 03:29:04.000000\\nserial2     My Test Event   1               3/22/2014 02:07:04.000000\\nserial2     My Test Event   1               3/22/2014 06:36:40.000000\\nserial2     My Test Event   1               3/27/2014 13:01:25.000000\\nserial2     My Test Event   2               3/27/2014 13:32:59.000000\\n</code></pre>\\n\\n<p>The python code that is running is this</p>\\n\\n<pre><code>qry = &lt;&lt;Above SQL&gt;&gt;\\nconn=pyodbc.connect(\\'DRIVER={Teradata};DBCNAME=server;UID=user;PWD=password;QUIETMODE=YES;\\', ANSI=True, autocommit=True)\\nteracurs=self.conn.cursor()\\nres = teracurs.execute(qry).fetchall()        \\n</code></pre>\\n\\n<p>Why do the <code>MAX()</code> and <code>count</code> statements work as expected if I run in Teradata SQL Assistant, but not if I run the query via pyodbc? </p>\\n-Query run in Teradata Assistant vs Pyodbc returns different results-<python><python-2.7><teradata><pyodbc>',\n",
       " '<p>Friends of Code. I have a basic sample of odbc db lines running in node.js. Unfortunately for all db queries i make varchar attributes are reported as \"null\".</p>\\n\\n<pre><code>var db = require(\\'odbc\\')()\\n  , cn = process.env.ODBC_CONNECTION_STRING\\n ;\\n\\ndb.open(cn, function (err) {\\n  if (err) return console.log(err);\\n\\n  db.query(\\'select * from users\\',  function (err, data) {\\n    if (err) console.log(err);\\n\\n    console.log(data);\\n\\n    db.close(function () {\\n      console.log(\\'done\\');\\n    });\\n });\\n});\\n</code></pre>\\n\\n<p>Running this results as:</p>\\n\\n<pre><code>[ { name: null, fullname: null, default_schema: 2000 },\\n  { name: null, fullname: null, default_schema: 5899 } ]\\ndone\\n</code></pre>\\n\\n<p>While in monetdb it looks as follows:</p>\\n\\n<pre><code>+---------+---------------+----------------+\\n| name    | fullname      | default_schema |\\n+=========+===============+================+\\n| monetdb | MonetDB Admin |           2000 |\\n| wyh     | wyh analytics |           5899 |\\n+---------+---------------+----------------+\\n</code></pre>\\n\\n<p>Any ideas??</p>\\n-node.js, odbc, monetdb - varchar attributes all null-<node.js><null><odbc><varchar><monetdb>',\n",
       " '<p>I have added the <code>terajdbc4.jar</code> and <code>tdgssconfig.jar</code> into my <code>WebContent/WEB-INF/lib</code> folder but on running the application, i get the following Error</p>\\n\\n<pre><code>java.lang.ClassNotFoundException: com.teradata.jdbc.TeraDriver\\n    at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1720)\\n    at org.apache.catalina.loader.WebappClassLoader.loadClass(WebappClassLoader.java:1571)\\n\\nmy .classpath file in the project application folder also contains reference to the added jars as shown below\\n &lt;classpathentry kind=\"lib\" path=\"WebContent/WEB-INF/lib/tdgssconfig.jar\"/&gt;\\n    &lt;classpathentry kind=\"lib\" path=\"WebContent/WEB-INF/lib/terajdbc4.jar\"/&gt;\\n    &lt;classpathentry kind=\"output\" path=\"build/classes\"/&gt;\\n</code></pre>\\n-Included jar files but get java.lang.ClassNotFoundException: com.teradata.jdbc.TeraDriver Error-<jdbc><teradata>',\n",
       " '<p>I would like to set up a schema which will store the current date as a string fitting in a varchar column of size 10. However CURDATE() returns as a date type, is there any way I can convert this when creating a table to automatically convert this to a string?</p>\\n\\n<p>For reference I am using MonetDB and declaring the column like below, can I cast CURDATE somehow when creating a table?</p>\\n\\n<pre><code> tdate     varchar(10)   default  CURDATE()  ,\\n</code></pre>\\n-SQL: Auto insert current date as string/varchar-<sql><datetime><monetdb>',\n",
       " \"<p>I want to update a column's value in this way</p>\\n\\n<pre><code>new value  = old value + row_number() * 1000\\n</code></pre>\\n\\n<p>also for row_number I want to use order by old value</p>\\n\\n<p>but I didn't find any solution.</p>\\n\\n<p>sample data</p>\\n\\n<pre><code>column    \\n   1\\n   3\\n   5\\n</code></pre>\\n\\n<p>after update query it should be</p>\\n\\n<pre><code>column\\n  1001\\n  2003\\n  3005\\n</code></pre>\\n-How to update a column's value using row number in Teradata-<sql><teradata>\",\n",
       " '<p>An empty error still present in Eclipse <strong>Problems</strong> view.</p>\\n\\n<p><img src=\"https://i.stack.imgur.com/syYtt.png\" alt=\"Eclipse empty error\"></p>\\n\\n<p>I use Eclipse KEPLER SR2.\\nThe project is a Maven War project and is working fine.</p>\\n\\n<p>If I select error, I can delete it, but after a new <strong>Project > Clean...</strong> she returns...</p>\\n\\n<p>Any idea ?</p>\\n-One empty error Eclipse/Kepler in Problems view-<java><eclipse><maven><m2e><eclipse-kepler>',\n",
       " '<p>I\\'m new to using DB within eclipse and I\\'ve ran into a bit of an issue. I\\'m able to create a db connection (and hence create a table), but when I try to query said table, I get the following: Exception in thread \"main\" java.sql.SQLException: No suitable driver found for jdbc:derby:</p>\\n\\n<p>I\\'m using the static call to the JDBC_URL that I used in the first place, so im confused why this is happening. Below shows two main classes. The first one works fine (the creation of the table) but the second one throws the above error. Note I have the derby.jar in the lib folder of my project.</p>\\n\\n<p>Class A) Works fine</p>\\n\\n<pre><code>public class CreateDB {\\n\\n        public static final String DRIVER = \"org.apache.derby.jdbc.EmbeddedDriver\";\\n        public static final String JDBC_URL = \"jdbc:derby:zadb;create=true\";\\n\\n    public static void main(String[] args) throws ClassNotFoundException,\\n            SQLException {\\n        Class.forName(DRIVER);\\n        Connection connection = DriverManager.getConnection(JDBC_URL);\\n        connection\\n                .createStatement()\\n                .execute(\\n                        \"create table channels(channel varchar(20), topic varchar(20), videoclip varchar(20))\");\\n        connection.createStatement().execute(\\n                \"insert into channels values \"\\n                        + \"(\\'oodp\\', \\'creational\\', \\'singleton\\'), \"\\n                        + \"(\\'oodp\\', \\'creational\\', \\'factory method\\'), \"\\n                        + \"(\\'oodp\\', \\'creational\\', \\'abstract factory\\')\");\\n        System.out\\n                .println(\"Table was created and the records were successfully inserted...\");\\n    }\\n}\\n</code></pre>\\n\\n<p>Class B) (SQL exception is thrown)</p>\\n\\n<pre><code>public class QueryDB {\\n\\n    public static final String SQL_STATEMENT = \"Select * from Channels\";\\n    public static void main(String[] args) throws SQLException {\\n\\n        Connection connection = DriverManager.getConnection(CreateDB.JDBC_URL);\\n\\n        Statement statement = connection.createStatement();\\n        ResultSet resultSet = statement.executeQuery(SQL_STATEMENT);\\n        ResultSetMetaData resultSetMetaData = resultSet.getMetaData();\\n        int columnCount = resultSetMetaData.getColumnCount();\\n\\n        for(int x = 1; x &lt;= columnCount; x++){\\n            System.out.print(resultSetMetaData.getColumnName(x) + \"|\");\\n        }\\n        while(resultSet.next()){\\n            System.out.println();\\n            for(int x = 1; x &lt;= columnCount; x++){ System.out.print(resultSet.getString(x) + \"|\");}\\n        }\\n\\n        if(statement!=null) statement.close();\\n        if(connection!=null) connection.close();\\n    }\\n\\n}\\n</code></pre>\\n-Cannot Query DB Eclipse e.g. Exception: java.sql.SQLException: No suitable driver found for jdbc:derby:-<sql><jdbc><derby><sqlexception><eclipse-kepler>',\n",
       " '<p>I am getting the following error when ever i created a maven project in kepler Mac OS X.</p>\\n\\n<p>Description Resource    Path    Location    Type Cannot read lifecycle mapping metadata for artifact  org.apache.maven.plugins:maven-surefire-plugin:maven-plugin:2.10:runtime Cause: error in opening zip file pom.xml</p>\\n\\n<p>I tried on JUNO also same error i am facing. </p>\\n\\n<p>I tried to install m2e connector for build-helper-maven-plugin also from </p>\\n\\n<p><a href=\"https://repository.sonatype.org/content/repositories/forge-sites/m2e-extras/0.15.0/N/0.15.0.201206251206/\" rel=\"nofollow\">https://repository.sonatype.org/content/repositories/forge-sites/m2e-extras/0.15.0/N/0.15.0.201206251206/</a></p>\\n\\n<p>I am trying to fix this from past one week but not able too it..</p>\\n\\n<p>Thanks,</p>\\n-Description\\tResource\\tPath\\tLocation\\tType Cannot read lifecycle mapping metadata for artifact-<macos><maven><jakarta-ee><eclipse-plugin><eclipse-kepler>',\n",
       " \"<p>I get the following error for the following command:</p>\\n\\n<pre><code>tdload -f ./file.csv -t t1 -u u1 -p -d ',' --TargetWorkingDatabase db1 -h host1\\n</code></pre>\\n\\n<p>file.csv is quite small (around 50k lines).</p>\\n\\n<p>Error:</p>\\n\\n<pre><code>Teradata Load Utility Version 14.00.00.08\\n\\nPassword: **********\\nTeradata Parallel Transporter Version 14.00.00.08\\nTPT_INFRA: TPT04039: Error: TPT Job script source 'Generated SQL INSERT Stmt for DBS Table 'D_TBL001'' is empty.\\n\\nJob script preprocessing failed.\\nJob terminated with status 8.\\n</code></pre>\\n\\n<p>How do I go about debugging/fixing this?</p>\\n-Teradata: tdload error-<teradata>\",\n",
       " '<p>I am new in Symfony2. Yesterday I download Eclipse Kepler 4.3 and installed the pdt plugins from <a href=\"http://p2-dev.pdt-extensions.org\" rel=\"nofollow\">http://p2-dev.pdt-extensions.org</a>.</p>\\n\\n<p>After creating a Symfony Project in Eclipse I read some tutorials. \\nIn my Controller LoginController I want to use <code>$this-&gt;createFormBuilder</code> method.</p>\\n\\n<p>But I have no code completion.</p>\\n\\n<p>I can enter the Controller class by holding ctrl and click with mouse on class name on the line with <code>extends Controller</code>, in addition I get code completion if I type <code>self::</code>.</p>\\n\\n<p>But I get nothing if I type <code>$this-&gt;</code>.</p>\\n\\n<p>Anyone know what can be the problem?\\nThe project nature is as follows in .project file:</p>\\n\\n<pre><code>&lt;natures&gt;\\n    &lt;nature&gt;org.eclipse.php.core.PHPNature&lt;/nature&gt;\\n    &lt;nature&gt;org.eclipse.wst.common.project.facet.core.nature&lt;/nature&gt;\\n    &lt;nature&gt;com.dubture.composer.core.composerNature&lt;/nature&gt;\\n    &lt;nature&gt;com.dubture.symfony.core.symfonyNature&lt;/nature&gt;\\n    &lt;nature&gt;com.dubture.doctrine.core.doctrineNature&lt;/nature&gt;\\n&lt;/natures&gt;\\n</code></pre>\\n-Eclipse Kepler Symfony Service Locator on $this Autocompletion-<php><symfony><code-completion><eclipse-kepler>',\n",
       " '<p>I did some research that says JavaFX is included in JDK 7. I\\'m using the latest version of the JDK. As far as I know, I\\'m using the latest version of Eclipse. However, whenever i start typing \"import javax.\", nothing shows up, even after I push the button to get suggestions.\\nHow do i get javafx to work?</p>\\n-How to use JavaFX in Eclipse-<eclipse><javafx><eclipse-kepler>',\n",
       " '<p>I am trying to assign binary values to strings in a column (1 for \"Y\" and 0 for \"N\"), but when I run the following query, it states \"SELECT Failed. 5628:  Column Y not found...\"</p>\\n\\n<pre><code>SELECT CUSTOMER_ID,\\nCASE \\n  WHEN c.Audience=\"Y\" THEN 1 \\n  WHEN c.Audience=\"N\" THEN 0\\n  ELSE \"NA\"\\nEND\\nFROM CUSTOMER_INFO a\\nLEFT JOIN CUST_MAIL b...\\n</code></pre>\\n\\n<p>It seems to be assuming \"Y\" is referring to a column rather than the field value, which is what I would like.  </p>\\n\\n<p>Any ideas on how I can modify this query to accommodate my needs?</p>\\n\\n<p>Thank you.  </p>\\n-How to assign binary value using CASE statement in Teradata SQL-<sql><teradata>',\n",
       " '<p>When I perform the following SELECT statement:</p>\\n<pre><code>SELECT startdate\\n      ,enddate\\n      ,invoicenumber\\n      ,vendornumber\\n      ,upc\\n      ,store\\n      ,cost\\n      ,allowance\\n      ,reason\\n      ,row_number() over(partition by upc order by startdate desc) as rownum \\nFROM db.table\\nWHERE  StartDate=DATE\\'2014-01-01\\' \\n   AND EndDate=DATE\\'2014-01-01\\' \\n   AND InvoiceNumber IS NULL \\n   AND VendorNumber=\\'2505\\' \\n   AND UPC=\\'1234568\\' \\n   AND Store IS NULL \\n   AND Cost=1.01 \\n   AND Allowance IS NULL\\n</code></pre>\\n<p>I get 2 results:\\n<img src=\"https://i.stack.imgur.com/p0KSn.png\" alt=\"results\" /></p>\\n<p>How do I update the UPC on the first row only?</p>\\n-In Teradata, how do I update only one record when duplicate records exist?-<sql><teradata>',\n",
       " '<p>I am working with approximately 300 articles (txt-files), and my cluster-analysis has so far been successful. </p>\\n\\n<p>Now I wish to save/export these 5 clusters separately, so that I can do further analysis on each cluster.</p>\\n\\n<p>How is that possible?</p>\\n\\n<p>I would appreciate any answer!</p>\\n\\n<p>/LK</p>\\n-textmining, Export clusters in Rapidminer-<export><cluster-analysis><rapidminer>',\n",
       " '<p>I have the following <code>Jquery</code> Code and it is an error </p>\\n\\n<blockquote>\\n  <p>$container.montage is not a function</p>\\n</blockquote>\\n\\n<p>I am following this <a href=\"http://tympanus.net/Development/AutomaticImageMontage/\" rel=\"nofollow noreferrer\">library</a> , and my browser is not showing any other errors. I have already checked that I am loading the required library from <code>FireBug</code> and still can\\'t figure out what is wrong here.</p>\\n\\n<pre><code>$(function() {\\n    var $container = $(\\'#am-container\\'),\\n           $imgs = $container.find(\\'img\\').hide(),\\n           totalImgs = $imgs.length,\\n           cnt = 0;\\n\\n        $imgs.each(function(i) {\\n            var $img = $(this);\\n            $(\\'&lt;img/&gt;\\').load(function() {\\n                ++cnt;\\n                if (cnt === totalImgs) {\\n                    $imgs.show();\\n                    $container.montage({\\n                        fillLastRow: true,\\n                        alternateHeight: true,\\n                        alternateHeightRange: {\\n                            min: 75,\\n                            max: 300\\n                        }\\n                    });\\n\\n                    /* \\n                     * just for this demo:\\n                     */\\n                    $(\\'#overlay\\').fadeIn(500);\\n                }\\n            }).attr(\\'src\\', $img.attr(\\'src\\'));\\n        });\\n\\n    });\\n</code></pre>\\n\\n<p>And this is my HTML</p>\\n\\n<pre><code>&lt;div class=\"main-gallary\"&gt;\\n   &lt;div class=\"container\"&gt;\\n      &lt;div id=\"am-container\" class=\"am-container\"&gt;\\n          &lt;a href=\"#\"&gt;\\n             &lt;img class=\"ui-draggable\" src=\"images/gallary/img-1.jpg\" style=\"display: inline; position: relative;\"&gt;\\n          &lt;/a&gt;\\n          &lt;a href=\"#\"&gt;\\n          &lt;a href=\"#\"&gt;\\n     &lt;/div&gt;\\n   &lt;/div&gt;\\n&lt;/div&gt;\\n</code></pre>\\n-TypeError: container.montage is not a function max: 300-<javascript><jquery>',\n",
       " '<p>I have a column which is varchar data type.Some sample values are like </p>\\n\\n<pre><code>abc 56 def\\n34 ghi\\njkl mno 78\\n</code></pre>\\n\\n<p>I wanted to get the numeric values only, like</p>\\n\\n<pre><code>56\\n34\\n78\\n</code></pre>\\n\\n<p>Thanks in advance.</p>\\n-Extracting numeric value in teradata-<sql><teradata>',\n",
       " '<p>I have a set table in teradata , when I load duplicate records throough informatica , session fails because it tries to push duplicate records in SET table.</p>\\n\\n<p>I want that whenever duplicate records being loaded informatica rejects them using TPT or Relation connection </p>\\n\\n<p>can anyone help me with properties I need to set</p>\\n-Teradata set table-<teradata><informatica>',\n",
       " '<p>I am working on a C++ project using <code>Eclipse Kepler V4.3.1</code> with the <code>CDT</code> plugins on Fedora. Up until yesterday, everything seemed to be working fine, but now when I make a change <code>Intellisense</code>/<code>Content Assist</code> doesn\\'t seem to see it. (<em>Note that this is not the same problem with getting autocomplete to make suggestions that I have seen <a href=\"https://stackoverflow.com/questions/11399719/eclipse-cdt-autocomplete-not-working\">others</a> post about.</em>)</p>\\n\\n<p>For example, if Eclipse picks up on an error like passing an invalid argument to a function, when I then go and fix the error Eclipse continues to complain about it. It continues to show me the old line before I made the change. If I open that same file with Emacs or gedit, I see that in fact the change was made and saved.</p>\\n\\n<p>Here is what I have tried without success:</p>\\n\\n<ul>\\n<li>Right click on the project  > Index > Rebuild</li>\\n<li>Right click on the project  > Index > Freshen All Files</li>\\n<li>Right click on the project  > Index > Update with Modifies Files</li>\\n<li>Restarting Eclipse</li>\\n<li>Restarting the entire machine</li>\\n<li>Checking for software updates and installing them (plus restarting).</li>\\n</ul>\\n\\n<p>Any ideas?</p>\\n-Eclipse CDT Intellisense Not Updating C++-<eclipse><eclipse-cdt><eclipse-kepler><content-assist>',\n",
       " '<p>How to add (or create) a foreign dictionary for spell checking in Eclipse Kepler ?</p>\\n-Adding a dictionary to Eclipse for spell checking other than the english one-<java><eclipse><spell-checking><eclipse-kepler>',\n",
       " \"<p>I have SSN column varbyte(100) in TableA.</p>\\n\\n<p>It has data in hexadecimal ,\\nex : 9C-B2-EE\\n     .\\n     .</p>\\n\\n<p>when i wrote</p>\\n\\n<pre><code> sel * from TableA where ssn='9C-B2-EE'\\n</code></pre>\\n\\n<p>SELECT Failed: 3640: Comparing BYTE data in column SSN with other types is illegal</p>\\n\\n<p>When I wrote </p>\\n\\n<pre><code> sel * from TableA where ssn='9C-B2-EE'xb\\n</code></pre>\\n\\n<p>SELECT Failed: 3775: Invalid hexadecimal constant.</p>\\n\\n<p>When I wrote </p>\\n\\n<pre><code> sel * from TableA where ssn='9C-B2-EE'xc\\n</code></pre>\\n\\n<p>SELECT Failed: 3775: Invalid hexadecimal constant.</p>\\n\\n<p>Can any one help how to use ssn in where ?\\nThanks.</p>\\n-Select Failed in Teradata-<sql><teradata>\",\n",
       " '<p>Obviously <a href=\"http://www.datanucleus.org/products/accessplatform_4_0/jdo/maven.html\" rel=\"nofollow\">the documentations</a> are outdated and even having mistakes.</p>\\n\\n<p>This is my POM file:</p>\\n\\n<pre><code>&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\\n    xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt;\\n    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\\n\\n    &lt;groupId&gt;mg.labs.jdo&lt;/groupId&gt;\\n    &lt;artifactId&gt;jdo-enhancement&lt;/artifactId&gt;\\n    &lt;version&gt;1.0&lt;/version&gt;\\n    &lt;packaging&gt;jar&lt;/packaging&gt;\\n\\n    &lt;name&gt;jdo-enhancement&lt;/name&gt;\\n    &lt;url&gt;http://maven.apache.org&lt;/url&gt;\\n\\n    &lt;properties&gt;\\n        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;\\n    &lt;/properties&gt;\\n\\n    &lt;dependencies&gt;\\n        &lt;dependency&gt;\\n            &lt;groupId&gt;org.datanucleus&lt;/groupId&gt;\\n            &lt;artifactId&gt;datanucleus-accessplatform-jdo-rdbms&lt;/artifactId&gt;\\n            &lt;version&gt;3.3.0-release&lt;/version&gt;\\n            &lt;type&gt;pom&lt;/type&gt;\\n        &lt;/dependency&gt;\\n        &lt;dependency&gt;\\n            &lt;groupId&gt;junit&lt;/groupId&gt;\\n            &lt;artifactId&gt;junit&lt;/artifactId&gt;\\n            &lt;version&gt;3.8.1&lt;/version&gt;\\n            &lt;scope&gt;test&lt;/scope&gt;\\n        &lt;/dependency&gt;\\n    &lt;/dependencies&gt;\\n    &lt;build&gt;\\n        &lt;plugins&gt;\\n            &lt;plugin&gt;\\n                &lt;groupId&gt;org.datanucleus&lt;/groupId&gt;\\n                &lt;artifactId&gt;datanucleus-maven-plugin&lt;/artifactId&gt;\\n                &lt;version&gt;3.3.0-release&lt;/version&gt;\\n                &lt;configuration&gt;\\n                    &lt;api&gt;JDO&lt;/api&gt;\\n                    &lt;props&gt;${basedir}/datanucleus.properties&lt;/props&gt;\\n                    &lt;log4jConfiguration&gt;${basedir}/log4j.properties&lt;/log4jConfiguration&gt;\\n                    &lt;verbose&gt;true&lt;/verbose&gt;\\n                &lt;/configuration&gt;\\n                &lt;executions&gt;\\n                    &lt;execution&gt;\\n                        &lt;phase&gt;process-classes&lt;/phase&gt;\\n                        &lt;goals&gt;\\n                            &lt;goal&gt;enhance&lt;/goal&gt;\\n                        &lt;/goals&gt;\\n                    &lt;/execution&gt;\\n                &lt;/executions&gt;\\n            &lt;/plugin&gt;\\n            &lt;plugin&gt;\\n                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;\\n                &lt;configuration&gt;\\n                    &lt;source&gt;1.7&lt;/source&gt;\\n                    &lt;target&gt;1.7&lt;/target&gt;\\n                &lt;/configuration&gt;\\n            &lt;/plugin&gt;\\n        &lt;/plugins&gt;\\n    &lt;/build&gt;\\n&lt;/project&gt;\\n</code></pre>\\n\\n<ol>\\n<li>The api and enhancer versions stated in the docs for for an old version. How can I use <code>3.3.8</code> or <code>4.0.0.M2</code> instead ? Simply changing the numbers didn\\'t work !</li>\\n<li>Eclipse is showing the following error <code>Plugin execution not covered by lifecycle configuration: org.datanucleus:datanucleus-maven-plugin:3.3.0-release:enhance (execution: default, phase: process-classes)</code></li>\\n<li><p>Running the goals <code>install</code> shows the following error:</p>\\n\\n<p>Plugin (Bundle) \"org.datanucleus\" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath. The URL \"file:/home/mgelbana/.m2/repository/org/datanucleus/datanucleus-core/3.2.5/datanucleus-core-3.2.5.jar\" is already registered, and you are trying to register an identical plugin located at URL \"file:/home/mgelbana/.m2/repository/org/datanucleus/datanucleus-core/4.0.0-m2/datanucleus-core-4.0.0-m2.jar.\"</p></li>\\n</ol>\\n\\n<p>Although I haven\\'t included versions <code>3.2.5</code> nor <code>4.0.0-m2</code>. I wish I could be using the latest version but I don\\'t know how, as I said, changing the version numbers doesn\\'t work as expected.</p>\\n\\n<p>My main goal is to run package and enhance the classes before packaging, which is just what the documentation tries to do. <strong>How to do that with the latest datanculeus release version ?</strong></p>\\n\\n<p><strong>EDIT</strong>\\nI debugged as Neil adviced, there is what I think is significant:\\n(At the beginning, these lines indicate the dependencies for the included maven dependency)</p>\\n\\n<pre><code>[DEBUG] mg.labs.jdo:jdo-enhancement:jar:1.0\\n[DEBUG]    org.datanucleus:datanucleus-accessplatform-jdo-rdbms:pom:3.3.0-release:compile\\n[DEBUG]       javax.jdo:jdo-api:jar:3.0.1:compile\\n[DEBUG]          javax.transaction:jta:jar:1.1:compile\\n[DEBUG]       org.datanucleus:datanucleus-core:jar:3.2.5:compile\\n[DEBUG]       org.datanucleus:datanucleus-api-jdo:jar:3.2.4:compile\\n[DEBUG]       org.datanucleus:datanucleus-jdo-query:jar:3.0.2:compile\\n[DEBUG]       org.datanucleus:datanucleus-rdbms:jar:3.2.4:compile\\n[DEBUG]    junit:junit:jar:3.8.1:test\\n</code></pre>\\n\\n<p>(These lines at the starts are the ones indicating some unexpected events for datanucleus plugin, such as not being able to download meta-data xml files and including <code>datanucleus-core 4.0.0-m3</code> files, the successive lines are just a result of that faulty inclusion)</p>\\n\\n<pre><code>[INFO] --- datanucleus-maven-plugin:3.3.0-release:enhance (default) @ jdo-enhancement ---\\n[DEBUG] Could not find metadata org.datanucleus:datanucleus-core/maven-metadata.xml in local (/home/mgelbana/.m2/repository)\\n[DEBUG] Skipped remote update check for org.datanucleus:datanucleus-core/maven-metadata.xml, locally cached metadata up-to-date.\\n[DEBUG] Using connector WagonRepositoryConnector with priority 0.0 for http://www.datanucleus.org/downloads/maven2/\\n[DEBUG] Using connector WagonRepositoryConnector with priority 0.0 for https://oss.sonatype.org/content/repositories/snapshots\\nDownloading: https://oss.sonatype.org/content/repositories/snapshots/org/datanucleus/datanucleus-core/maven-metadata.xml\\nDownloading: http://www.datanucleus.org/downloads/maven2/org/datanucleus/datanucleus-core/maven-metadata.xml\\n[DEBUG] Writing tracking file /home/mgelbana/.m2/repository/org/datanucleus/datanucleus-core/resolver-status.properties\\n[DEBUG] Writing tracking file /home/mgelbana/.m2/repository/org/datanucleus/datanucleus-core/resolver-status.properties\\n[DEBUG] Could not find metadata org.datanucleus:datanucleus-core/maven-metadata.xml in DN_M2_Repo (http://www.datanucleus.org/downloads/maven2/)\\n[DEBUG] Could not find metadata org.datanucleus:datanucleus-core/maven-metadata.xml in sonatype-nexus-snapshots (https://oss.sonatype.org/content/repositories/snapshots)\\n[DEBUG] Dependency collection stats: {ConflictMarker.analyzeTime=0, ConflictMarker.markTime=0, ConflictMarker.nodeCount=41, ConflictIdSorter.graphTime=1, ConflictIdSorter.topsortTime=0, ConflictIdSorter.conflictIdCount=18, ConflictIdSorter.conflictIdCycleCount=0, ConflictResolver.totalTime=1, ConflictResolver.conflictItemCount=40, DefaultDependencyCollector.collectTime=6063, DefaultDependencyCollector.transformTime=2}\\n[DEBUG] org.datanucleus:datanucleus-maven-plugin:jar:3.3.0-release:\\n[DEBUG]    org.datanucleus:datanucleus-core:jar:4.0.0-m3:runtime\\n[DEBUG]    org.codehaus.plexus:plexus-utils:jar:3.0.8:compile\\n[DEBUG]    org.apache.maven:maven-artifact:jar:3.0.4:compile\\n[DEBUG]    org.apache.maven:maven-plugin-api:jar:3.0.4:compile\\n[DEBUG]       org.apache.maven:maven-model:jar:3.0.4:compile\\n[DEBUG]       org.sonatype.sisu:sisu-inject-plexus:jar:2.3.0:compile\\n[DEBUG]          org.codehaus.plexus:plexus-component-annotations:jar:1.5.5:compile\\n[DEBUG]          org.sonatype.sisu:sisu-inject-bean:jar:2.3.0:compile\\n[DEBUG]             org.sonatype.sisu:sisu-guice:jar:no_aop:3.1.0:compile\\n[DEBUG]                org.sonatype.sisu:sisu-guava:jar:0.9.9:compile\\n[DEBUG]    org.codehaus.plexus:plexus-container-default:jar:1.5.5:compile\\n[DEBUG]       org.codehaus.plexus:plexus-classworlds:jar:2.2.2:compile\\n[DEBUG]       org.apache.xbean:xbean-reflect:jar:3.4:compile\\n[DEBUG]          log4j:log4j:jar:1.2.12:compile\\n[DEBUG]          commons-logging:commons-logging-api:jar:1.1:compile\\n[DEBUG]       com.google.collections:google-collections:jar:1.0:compile\\n[DEBUG]       junit:junit:jar:3.8.2:compile\\n[DEBUG] Created new class realm plugin&gt;org.datanucleus:datanucleus-maven-plugin:3.3.0-release\\n[DEBUG] Importing foreign packages into class realm plugin&gt;org.datanucleus:datanucleus-maven-plugin:3.3.0-release\\n[DEBUG]   Imported:  &lt; maven.api\\n[DEBUG] Populating class realm plugin&gt;org.datanucleus:datanucleus-maven-plugin:3.3.0-release\\n[DEBUG]   Included: org.datanucleus:datanucleus-maven-plugin:jar:3.3.0-release\\n[DEBUG]   Included: org.datanucleus:datanucleus-core:jar:4.0.0-m3\\n[DEBUG]   Included: org.codehaus.plexus:plexus-utils:jar:3.0.8\\n[DEBUG]   Included: org.codehaus.plexus:plexus-component-annotations:jar:1.5.5\\n[DEBUG]   Included: org.sonatype.sisu:sisu-inject-bean:jar:2.3.0\\n[DEBUG]   Included: org.sonatype.sisu:sisu-guice:jar:no_aop:3.1.0\\n[DEBUG]   Included: org.sonatype.sisu:sisu-guava:jar:0.9.9\\n[DEBUG]   Included: org.apache.xbean:xbean-reflect:jar:3.4\\n[DEBUG]   Included: log4j:log4j:jar:1.2.12\\n[DEBUG]   Included: commons-logging:commons-logging-api:jar:1.1\\n[DEBUG]   Included: com.google.collections:google-collections:jar:1.0\\n[DEBUG]   Included: junit:junit:jar:3.8.2\\n[DEBUG]   Excluded: org.apache.maven:maven-artifact:jar:3.0.4\\n[DEBUG]   Excluded: org.apache.maven:maven-plugin-api:jar:3.0.4\\n[DEBUG]   Excluded: org.apache.maven:maven-model:jar:3.0.4\\n[DEBUG]   Excluded: org.sonatype.sisu:sisu-inject-plexus:jar:2.3.0\\n[DEBUG]   Excluded: org.codehaus.plexus:plexus-container-default:jar:1.5.5\\n[DEBUG]   Excluded: org.codehaus.plexus:plexus-classworlds:jar:2.2.2\\n[DEBUG] Configuring mojo org.datanucleus:datanucleus-maven-plugin:3.3.0-release:enhance from plugin realm ClassRealm[plugin&gt;org.datanucleus:datanucleus-maven-plugin:3.3.0-release, parent: sun.misc.Launcher$AppClassLoader@591ce4fe]\\n[DEBUG] Configuring mojo \\'org.datanucleus:datanucleus-maven-plugin:3.3.0-release:enhance\\' with basic configurator --&gt;\\n[DEBUG]   (f) alwaysDetachable = false\\n[DEBUG]   (f) api = JDO\\n[DEBUG]   (f) classpathElements = [/home/mgelbana/workspace/kepler_maven_by_example/jdo-enhancement/target/classes, /home/mgelbana/.m2/repository/javax/jdo/jdo-api/3.0.1/jdo-api-3.0.1.jar, /home/mgelbana/.m2/repository/javax/transaction/jta/1.1/jta-1.1.jar, /home/mgelbana/.m2/repository/org/datanucleus/datanucleus-core/3.2.5/datanucleus-core-3.2.5.jar, /home/mgelbana/.m2/repository/org/datanucleus/datanucleus-api-jdo/3.2.4/datanucleus-api-jdo-3.2.4.jar, /home/mgelbana/.m2/repository/org/datanucleus/datanucleus-jdo-query/3.0.2/datanucleus-jdo-query-3.0.2.jar, /home/mgelbana/.m2/repository/org/datanucleus/datanucleus-rdbms/3.2.4/datanucleus-rdbms-3.2.4.jar]\\n[DEBUG]   (f) detachListener = false\\n[DEBUG]   (f) fork = true\\n[DEBUG]   (f) generateConstructor = true\\n[DEBUG]   (f) generatePK = true\\n[DEBUG]   (f) log4jConfiguration = /home/mgelbana/workspace/kepler_maven_by_example/jdo-enhancement/log4j.properties\\n[DEBUG]   (f) metadataDirectory = /home/mgelbana/workspace/kepler_maven_by_example/jdo-enhancement/target/classes\\n[DEBUG]   (f) metadataIncludes = **/*.jdo, **/*.class\\n[DEBUG]   (f) pluginArtifacts = [org.datanucleus:datanucleus-maven-plugin:maven-plugin:3.3.0-release:, org.datanucleus:datanucleus-core:jar:4.0.0-m3:runtime, org.codehaus.plexus:plexus-utils:jar:3.0.8:compile, org.codehaus.plexus:plexus-component-annotations:jar:1.5.5:compile, org.sonatype.sisu:sisu-inject-bean:jar:2.3.0:compile, org.sonatype.sisu:sisu-guice:jar:no_aop:3.1.0:compile, org.sonatype.sisu:sisu-guava:jar:0.9.9:compile, org.apache.xbean:xbean-reflect:jar:3.4:compile, log4j:log4j:jar:1.2.12:compile, commons-logging:commons-logging-api:jar:1.1:compile, com.google.collections:google-collections:jar:1.0:compile, junit:junit:jar:3.8.2:compile]\\n[DEBUG]   (f) quiet = false\\n[DEBUG]   (f) verbose = true\\n[DEBUG] -- end configuration --\\n[DEBUG] Metadata Directory is : /home/mgelbana/workspace/kepler_maven_by_example/jdo-enhancement/target/classes\\n[DEBUG] Executing command line:\\n[DEBUG] /bin/sh -c java -cp /home/mgelbana/.m2/repository/org/datanucleus/datanucleus-maven-plugin/3.3.0-release/datanucleus-maven-plugin-3.3.0-release.jar:/home/mgelbana/.m2/repository/org/datanucleus/datanucleus-core/4.0.0-m3/datanucleus-core-4.0.0-m3.jar:/home/mgelbana/.m2/repository/org/codehaus/plexus/plexus-utils/3.0.8/plexus-utils-3.0.8.jar:/home/mgelbana/.m2/repository/org/codehaus/plexus/plexus-component-annotations/1.5.5/plexus-component-annotations-1.5.5.jar:/home/mgelbana/.m2/repository/org/sonatype/sisu/sisu-inject-bean/2.3.0/sisu-inject-bean-2.3.0.jar:/home/mgelbana/.m2/repository/org/sonatype/sisu/sisu-guice/3.1.0/sisu-guice-3.1.0-no_aop.jar:/home/mgelbana/.m2/repository/org/sonatype/sisu/sisu-guava/0.9.9/sisu-guava-0.9.9.jar:/home/mgelbana/.m2/repository/org/apache/xbean/xbean-reflect/3.4/xbean-reflect-3.4.jar:/home/mgelbana/.m2/repository/log4j/log4j/1.2.12/log4j-1.2.12.jar:/home/mgelbana/.m2/repository/commons-logging/commons-logging-api/1.1/commons-logging-api-1.1.jar:/home/mgelbana/.m2/repository/com/google/collections/google-collections/1.0/google-collections-1.0.jar:/home/mgelbana/.m2/repository/junit/junit/3.8.2/junit-3.8.2.jar:/home/mgelbana/workspace/kepler_maven_by_example/jdo-enhancement/target/classes:/home/mgelbana/.m2/repository/javax/jdo/jdo-api/3.0.1/jdo-api-3.0.1.jar:/home/mgelbana/.m2/repository/javax/transaction/jta/1.1/jta-1.1.jar:/home/mgelbana/.m2/repository/org/datanucleus/datanucleus-core/3.2.5/datanucleus-core-3.2.5.jar:/home/mgelbana/.m2/repository/org/datanucleus/datanucleus-api-jdo/3.2.4/datanucleus-api-jdo-3.2.4.jar:/home/mgelbana/.m2/repository/org/datanucleus/datanucleus-jdo-query/3.0.2/datanucleus-jdo-query-3.0.2.jar:/home/mgelbana/.m2/repository/org/datanucleus/datanucleus-rdbms/3.2.4/datanucleus-rdbms-3.2.4.jar -Dlog4j.configuration=file:/home/mgelbana/workspace/kepler_maven_by_example/jdo-enhancement/log4j.properties org.datanucleus.enhancer.DataNucleusEnhancer -v -api JDO /home/mgelbana/workspace/kepler_maven_by_example/jdo-enhancement/target/classes/mg/labs/jdo/enhancement/App.class /home/mgelbana/workspace/kepler_maven_by_example/jdo-enhancement/target/classes/mg/labs/jdo/enhancement/QApp.class\\n[DEBUG] Exit code: 1\\n[DEBUG] --------------------\\n[DEBUG]  Standard output from the DataNucleus tool org.datanucleus.enhancer.DataNucleusEnhancer :\\n[DEBUG] --------------------\\n[INFO] \\n[DEBUG] --------------------\\n[ERROR] --------------------\\n[ERROR]  Standard error from the DataNucleus tool + org.datanucleus.enhancer.DataNucleusEnhancer :\\n[ERROR] --------------------\\n[ERROR] Exception in thread \"main\" Plugin (Bundle) \"org.datanucleus\" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath. The URL \"file:/home/mgelbana/.m2/repository/org/datanucleus/datanucleus-core/3.2.5/datanucleus-core-3.2.5.jar\" is already registered, and you are trying to register an identical plugin located at URL \"file:/home/mgelbana/.m2/repository/org/datanucleus/datanucleus-core/4.0.0-m3/datanucleus-core-4.0.0-m3.jar.\"\\norg.datanucleus.exceptions.NucleusException: Plugin (Bundle) \"org.datanucleus\" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath. The URL \"file:/home/mgelbana/.m2/repository/org/datanucleus/datanucleus-core/3.2.5/datanucleus-core-3.2.5.jar\" is already registered, and you are trying to register an identical plugin located at URL \"file:/home/mgelbana/.m2/repository/org/datanucleus/datanucleus-core/4.0.0-m3/datanucleus-core-4.0.0-m3.jar.\"\\n</code></pre>\\n-How to use datanucleus\\'s enhances within maven in eclipse kepler?-<maven-3><jdo><datanucleus><eclipse-kepler>',\n",
       " '<p>I tried to install cdt plugin to eclipse kepler through install new software. There are many sites to do so. But that is not accessible. Please help me!</p>\\n-Installing CDT plugin to eclipse kepler in windows-<eclipse><eclipse-cdt><eclipse-kepler>',\n",
       " '<p>Is there a way to make Teradata Sql Assistant default to vertical columns?  I can manually set my results in a vertical tab, but when I execute the query again it puts them right back at the bottom in a horizontal tab.</p>\\n\\n<p>I tried untabbed, and placed the windows where I wanted them, and when I execute a query it sticks the results behind the query window and shrinks the results window!</p>\\n\\n<p>This is driving me absolutely insane... Is there any way to just make the results return in a default vertical window so I can see them and my query side by side?</p>\\n-Make Teradata Sql Assistant default to vertical tabs-<teradata>',\n",
       " \"<p>I've been playing around with Rapidminer and can't quite seem to figure this out. I have a huge list of URLs listed in an excel file and would like to extract a single XPath element from each URL. Is there anyway I could do this with Rapidminer? </p>\\n\\n<p>I've seen the tutorials Neil Mcguigan, but they seem to crawl the web/site in general rather than from a specific set of URLs.</p>\\n-Can Rapidminer extract an XPath value from a Specific list of URLs?-<xpath><web-scraping><web-crawler><rapidminer>\",\n",
       " '<p>I have a c++ eclipse linux project that I want to port to windows as well. I opened the project from my git repo and opened a new build configuration for windows. When I didn\\'t find the makefile generated by eclipse I opened the project properties->c/c++ build and look at the build setting tab, and saw that build type is \"internal build\" and I can\\'t change that (grayed out). so how do I open import a project in windows and enable external builder in order to generate a makefile for windows?\\nusing win7. MinGW. eclipse kepler.</p>\\n-can\\'t generate makefile in eclipse windows-<c++><eclipse><windows><makefile><eclipse-kepler>',\n",
       " '<p>I am very new to rapidminer and data mining in general but I have attempted to make a cursory search for what all of the parameters mean in rapidminers decision tree parameters and came up lacking. I know what a leaf is and a node and am at the point of getting my head around a few of the parameters but any knowledge shared would be appreciated.\\nI.E. What does they all really do?\\ncriterion\\nminimal size for split\\nminimal leaf size\\nminimal gain\\nmaximal depth\\nconfidence</p>\\n\\n<p>Also without using optimization, is trail an error the best way to get the best prediction?\\nThanks,\\nS</p>\\n-Rapidminer: Explaining decision tree parameters-<parameters><decision-tree><rapidminer>',\n",
       " '<p>Looking some like <a href=\"https://stackoverflow.com/questions/849308/pull-push-from-multiple-remote-locations\">pull/push from multiple remote locations</a>  in egit.</p>\\n\\n<p>When I right click on git repository and try to push(using <strong>Push branch...</strong>) I can only push to a single remote repository at a time.</p>\\n\\n<p>Is there way to push all the changes to all the available remote repositories at once?</p>\\n\\n<p>Egit version: 3.2.0.201312181205-r\\nEclipse: Kepler Service Release 2</p>\\n-git push to multiple remote repositories at once in Egit-<eclipse><git><egit><git-push><eclipse-kepler>',\n",
       " '<p>So we are currently analysing if it is feasible to move out from MyEclipse->Eclipse and one of the areas where we are currently stuck is EJB Deploy. </p>\\n\\n<p>In MyEclipse there is an option named \\'Run EJB Deploy\\' which generates .ejbDeploy folder with the stubs and tie\\'s for the EJB project, however in Eclipse Kepler there is \"Prepare For Deployment\" option(Right Click on Project->Java EE Tools->Prepare for Deployment) which I am presuming does the same thing. </p>\\n\\n<p>However, when I click that option ( Prepare for Deployment ) it conveniently ignores it and does not do anything.</p>\\n\\n<p>I am not sure what needs to be done to ensure this works the same way as MyEclipse IDE and it generates a resultant <strong>.ejbDeploy</strong> folder like it does in Eclipse with stubs.</p>\\n\\n<p>Also, if this is not possible, can we achieve similar bahevior using ANT or MAVEN?</p>\\n\\n<p>Any insights will be helpful.</p>\\n\\n<p><strong>P.S:</strong> We are currently using EJB 2.1 and dont have any plans to move to EJB 3.0 soon so steps related to EJB 2.1 will help. Also, for the purpose of this evaluation, we are using Websphere 7.0 which is the server that we want to use</p>\\n\\n<p>Thanks,\\nYogendra</p>\\n-Eclipse Kepler \\'Prepare for Deployment\\' option to create EJB stubs-<eclipse><eclipse-plugin><ejb><eclipse-kepler><ejb-2.x>',\n",
       " \"<p>I am looking to use Teradata to group volume hourly over a date range when 'time' is a timestamp(6)</p>\\n\\n<pre><code>SELECT VOLUME, HOUR(time)\\nFROM table\\nGROUP BY HOUR(time)\\n</code></pre>\\n-Teradata SQL to group by hour interval out of a TIMESTAMP(6)-<sql><datetime><intervals><teradata>\",\n",
       " '<p>Is there a way to raise errors inside of Teradata stored procedures?</p>\\n\\n<p>For example, I want to check if a table is empty.  If the table is empty I wish to cause the stored procedure to error out with the error message \"Table Empty\".  </p>\\n\\n<p>That will allow me to bubble up the error to the calling application.</p>\\n-Teradata: How can I raise an error in a stored procedures-<sql><stored-procedures><error-handling><teradata>',\n",
       " \"<p>i am trying to use Eclipse's nodeclipse plugin to edit coffeescript files. the problem is, when i try to comment a line using ctrl+/, it comments the like with // (java style comment) instead on # (coffeesctipt style comments). anyone knows a solution to that? can i change the comment marker for a specific file type?</p>\\n-eclipse kepler, nodeclipse, coffeesctipt and ctrl+/ comments-<coffeescript><comments><eclipse-kepler><nodeclipse>\",\n",
       " '<p>I tried to install a plugin called EclipseCoder to run Topcoder arena through Eclipse (for c/c++), but it does not work and throws errors. I did following.</p>\\n\\n<ol>\\n<li>I installed Eclipse Kepler Service Release 2 for C/C++, It was\\nworking fine for local codes.</li>\\n<li>I added a plugin by going to help -> Install new software, and used\\n<a href=\"http://fornwall.net/eclipsecoder/\" rel=\"nofollow noreferrer\">http://fornwall.net/eclipsecoder/</a></li>\\n<li>After Installing I opened arena from eclipse and it fetched the\\nproblem statement and code template correctly but it shows following errors.</li>\\n</ol>\\n\\n<p><img src=\"https://i.stack.imgur.com/91MAF.jpg\" alt=\"It Throws Following Exception\"></p>\\n\\n<p>Also eclipse is unable to resolve any header files generated, it says <strong>Unresolved inclusion: header file name</strong>, It creates a .h file but I think it should create .cpp file (if working for c++).</p>\\n\\n<p>I followed steps given at <a href=\"http://fornwall.net/eclipsecoder/\" rel=\"nofollow noreferrer\">http://fornwall.net/eclipsecoder/</a></p>\\n\\n<p>Exception Stack Trace is as Follows :</p>\\n\\n<pre><code>java.lang.RuntimeException: java.lang.RuntimeException: Cannot find binary\\n    at net.fornwall.eclipsecoder.ccsupport.CBinaryLauncher.&lt;init&gt;(CBinaryLauncher.java:71)\\n    at net.fornwall.eclipsecoder.ccsupport.CCLanguageSupport$1.run(CCLanguageSupport.java:120)\\n    at net.fornwall.eclipsecoder.util.Utilities$5.run(Utilities.java:284)\\nCaused by: java.lang.RuntimeException: Cannot find binary\\n    at net.fornwall.eclipsecoder.ccsupport.CBinaryLauncher.&lt;init&gt;(CBinaryLauncher.java:68)\\n</code></pre>\\n\\n<p>Another Error Message in Error Log : \\n<strong>Problems occurred when invoking code from plug-in: \"org.eclipse.core.resources\".</strong></p>\\n\\n<p>And Its Stack Trace is : </p>\\n\\n<pre><code>java.lang.NullPointerException\\n    at org.eclipse.cdt.managedbuilder.internal.core.ManagedBuildInfo.findExistingDefaultConfiguration(ManagedBuildInfo.java:272)\\n    at org.eclipse.cdt.managedbuilder.internal.core.ManagedBuildInfo.getDefaultConfiguration(ManagedBuildInfo.java:254)\\n    at org.eclipse.cdt.managedbuilder.internal.core.BuilderFactory.createBuilders(BuilderFactory.java:369)\\n    at org.eclipse.cdt.managedbuilder.core.ManagedBuilderCorePlugin.createBuilders(ManagedBuilderCorePlugin.java:263)\\n    at org.eclipse.cdt.managedbuilder.internal.core.CommonBuilder.getRule(CommonBuilder.java:1350)\\n    at org.eclipse.core.internal.events.BuildManager.basicBuild(BuildManager.java:183)\\n    at org.eclipse.core.internal.events.BuildManager.basicBuild(BuildManager.java:246)\\n    at org.eclipse.core.internal.events.BuildManager$1.run(BuildManager.java:299)\\n    at org.eclipse.core.runtime.SafeRunner.run(SafeRunner.java:42)\\n    at org.eclipse.core.internal.events.BuildManager.basicBuild(BuildManager.java:302)\\n    at org.eclipse.core.internal.events.BuildManager.basicBuildLoop(BuildManager.java:358)\\n    at org.eclipse.core.internal.events.BuildManager.build(BuildManager.java:381)\\n    at org.eclipse.core.internal.events.AutoBuildJob.doBuild(AutoBuildJob.java:143)\\n    at org.eclipse.core.internal.events.AutoBuildJob.run(AutoBuildJob.java:241)\\n    at org.eclipse.core.internal.jobs.Worker.run(Worker.java:53)\\n</code></pre>\\n\\n<p>How to resolve this issue?</p>\\n-Error after installing EclipseCoder plugin to run Topcoder Arena-<c++><eclipse><plugins><eclipse-kepler>',\n",
       " '<p>How do I erase teradata history (date/time, source, elapsed. rows, result, notes,length, sql statement,stmts,etc.. Is it advisable to delete history at all?\\nMy log/history tab shows all the queries I ran since beginning and I do not want that.. </p>\\n-Teradata :How do I erase teradata history (date/time, source, elapsed. rows, result, notes,length, sql statement,stmts,etc-<sql><teradata><assistant>',\n",
       " \"<p>I have a very simple problem that I could come up with a crude solution to, but it seems to me that there is probably some off the shelf answer.</p>\\n\\n<p><strong>Problem:</strong> I have a list of discrete values (these are mass units) that I want to find within a database of discrete values (known mass units) and their identities, allowing for some inexact match. Example: If I am looking for 500.23 in the database then anything +/- 0.025 would be considered a match (50 ppm or 0.005%). This tolerance should be adjustable. So in this example, 500.23 may return the database text value, 500.25 which is Compound A.</p>\\n\\n<p>I could also make this tool myself if someone would like to suggest the most straightforward approach. I am competent in Matlab, somewhat in R, good in excel, poor in access, and don't know anything about SQL. Best case would be for this tool to be used by non-coders.</p>\\n\\n<p><strong>Background:</strong> The real background of this problem is that I have MALDI TOF data where I have identified peaks of interest from an experiment (masses; m/z). These masses correspond to molecules that were released after enzymatic digestion. This class of molecule has reported masses with known identities, but unlike peptide mass fingerprinting, or metabolomic databases, these known masses are mostly unpublished and/or uncollated, so I would like to cross-reference them with a database of my own making. Each mass corresponds to one identity. The masses will not match exactly, and being able to search with a specified mass tolerance is key.</p>\\n-Simple database setup for MALDI peaks-<database>\",\n",
       " \"<p>I am trying to extract the date and time from a field in Teradata.</p>\\n\\n<p>The field in question is:</p>\\n\\n<pre><code>VwNIMEventFct.EVENT_GMT_TIMESTAMP\\n</code></pre>\\n\\n<p>Here is what the data look like:</p>\\n\\n<pre><code>01/02/2012 12:18:59.306000\\n</code></pre>\\n\\n<p>I'd like the date and time only.</p>\\n\\n<p>I have tried using <code>EXTRACT(Date</code>, <code>EXTRACT(DAY_HOUR</code> and a few others with no success.</p>\\n\\n<p><code>DATE_FORMAT()</code> does not appear to work since I'm on Teradata.</p>\\n\\n<p>How would I select the date and time from <code>VwNIMEventFct.EVENT_GMT_TIMESTAMP</code>?</p>\\n-EXTRACT the date and time - (Teradata)-<sql><datetime><teradata>\",\n",
       " '<p>After spending time trying to find/download Aptana Studio version 3.5.0 and learning on Stackoverflow that the developers of Aptana Studio pulled this version (with no message or news from Aptana) I have repeatedly tried to install the currently available version of Aptana Studio 3 Eclipse Plugin (3.4.2 according to Aptana download site) on Eclipse Kepler IDE for Java Developers (eclipse-java-kepler-SR2-win32-x86_64.zip) on a Windows 7 Pro 64-bit machine by following the Aptana Studio install instructions from their web site exactly (<a href=\"http://www.aptana.com/downloads/start\" rel=\"nofollow\">http://www.aptana.com/downloads/start</a>):</p>\\n\\n<p><em>Installing via Eclipse\\nPlease copy the following Update Site URL to your clipboard and then follow the steps listed below to add this URL to your Available Software Sites list. Attempting to access this URL using your web browser will return an Access Denied error.\\n<a href=\"http://download.aptana.com/studio3/plugin/install\" rel=\"nofollow\">http://download.aptana.com/studio3/plugin/install</a><br>\\nFrom the Help menu, select Install New Software... to open the Install New Software dialog.\\nPaste the URL for the update site into the Work With text box, and hit the Enter (or Return) key.\\nIn the populated table below, check the box next to the name of the plug-in, and then click the Next button.\\nClick the Next button to go to the license page.\\nChoose the option to accept the terms of the license agreement, and click the Finish button.\\nYou may need to restart Eclipse to continue.</em></p>\\n\\n<p>The install completes but when I try and launch Aptana Studio I continually get the following errors and thus can\\'t launch the app:</p>\\n\\n<p>\"Problem Occurred - Launching the studio portal has encountered a problem. An Internal Error has occurred\"</p>\\n\\n<p>\"An internal error occurred during: \"Start Ruble bundle manager\".\\njava.lang.NullPointerException\"</p>\\n\\n<p>I have uninstalled and reinstalled the app several times and have the same problem. I have scoured the web, Stackoverflow, Aptana\\'s support sites including bugs/issues, etc. but have not found any solution.</p>\\n\\n<p>It is very frustrating that the most basic function of trying to use a new app/tool <strong><em>i.e. installing it</em></strong>, doesn\\'t work and I have already wasted several days on this and am ready to ditch Aptana Studio for another web IDE. I hope the Aptana devs can fix whatever is causing this problem ASAP but in the meantime I would appreciate it if anyone can help me with any solutions to this problem so I could at least get it to successfully install so I can launch the app and try it out.</p>\\n\\n<p>EDIT: .log file contents (from .metadata directory) after installing Aptana   Studio Eclipse Plugin:</p>\\n\\n<pre><code>    !SESSION 2014-05-08 01:59:13.298 -----------------------------------------------\\neclipse.buildId=4.3.2.M20140221-1700\\njava.version=1.7.0_51\\njava.vendor=Oracle Corporation\\nBootLoader constants: OS=win32, ARCH=x86_64, WS=win32, NL=en_US\\nFramework arguments:  -product org.eclipse.epp.package.java.product\\nCommand-line arguments:  -os win32 -ws win32 -arch x86_64 -product org.eclipse.epp.package.java.product\\n\\n!ENTRY org.eclipse.jface 2 0 2014-05-08 01:59:37.932\\n!MESSAGE Keybinding conflicts occurred.  They may interfere with normal accelerator operation.\\n!SUBENTRY 1 org.eclipse.jface 2 0 2014-05-08 01:59:37.932\\n!MESSAGE A conflict occurred for CTRL+SHIFT+D:\\nBinding(CTRL+SHIFT+D,\\n    ParameterizedCommand(Command(org.eclipse.jdt.debug.ui.commands.Display,Display,\\n        Display result of evaluating selected text,\\n        Category(org.eclipse.debug.ui.category.run,Run/Debug,Run/Debug command category,true),\\n        org.eclipse.ui.internal.WorkbenchHandlerServiceHandler@90afea4,\\n        ,,true),null),\\n    org.eclipse.ui.defaultAcceleratorConfiguration,\\n    org.eclipse.ui.contexts.dialogAndWindow,,,system)\\nBinding(CTRL+SHIFT+D,\\n    ParameterizedCommand(Command(com.aptana.ide.syncing.ui.commands.download,Download...,\\n        ,\\n        Category(com.aptana.ide.syncing.ui.commands.category,File Transfer,File Transfer Commands,true),\\n        org.eclipse.ui.internal.WorkbenchHandlerServiceHandler@30d3c59f,\\n        ,,true),null),\\n    org.eclipse.ui.defaultAcceleratorConfiguration,\\n    org.eclipse.ui.contexts.window,,,system)\\nBinding(CTRL+SHIFT+D,\\n    ParameterizedCommand(Command(org.radrails.rails.ui.command.debug.server,Debug Server,\\n        ,\\n        Category(org.radrails.rails.ui.category,Rails,null,true),\\n        org.eclipse.ui.internal.WorkbenchHandlerServiceHandler@1f5362c4,\\n        ,,true),null),\\n    org.eclipse.ui.defaultAcceleratorConfiguration,\\n    org.eclipse.ui.contexts.window,,,system)\\n!SUBENTRY 1 org.eclipse.jface 2 0 2014-05-08 01:59:37.932\\n!MESSAGE A conflict occurred for CTRL+SHIFT+U:\\nBinding(CTRL+SHIFT+U,\\n    ParameterizedCommand(Command(org.eclipse.jdt.ui.edit.text.java.search.occurrences.in.file.quickMenu,Show Occurrences in File Quick Menu,\\n        Shows the Occurrences in File quick menu,\\n        Category(org.eclipse.search.ui.category.search,Search,Search command category,true),\\n        org.eclipse.ui.internal.WorkbenchHandlerServiceHandler@1b7941ed,\\n        ,,true),null),\\n    org.eclipse.ui.defaultAcceleratorConfiguration,\\n    org.eclipse.ui.contexts.window,,,system)\\nBinding(CTRL+SHIFT+U,\\n    ParameterizedCommand(Command(com.aptana.ide.syncing.ui.commands.upload,Upload...,\\n        ,\\n        Category(com.aptana.ide.syncing.ui.commands.category,File Transfer,File Transfer Commands,true),\\n        org.eclipse.ui.internal.WorkbenchHandlerServiceHandler@35d73cb2,\\n        ,,true),null),\\n    org.eclipse.ui.defaultAcceleratorConfiguration,\\n    org.eclipse.ui.contexts.window,,,system)\\n\\n!ENTRY org.eclipse.ui 4 4 2014-05-08 01:59:40.512\\n!MESSAGE An internal error has occurred.\\n!STACK 0\\njava.lang.IllegalArgumentException: \\n    at org.eclipse.core.runtime.Assert.isLegal(Assert.java:63)\\n    at org.eclipse.core.runtime.Assert.isLegal(Assert.java:47)\\n    at org.eclipse.e4.ui.internal.workbench.ModelServiceImpl.findElementsRecursive(ModelServiceImpl.java:286)\\n    at org.eclipse.e4.ui.internal.workbench.ModelServiceImpl.findElements(ModelServiceImpl.java:371)\\n    at org.eclipse.e4.ui.internal.workbench.PartServiceImpl.addPart(PartServiceImpl.java:812)\\n    at org.eclipse.e4.ui.internal.workbench.PartServiceImpl.addPart(PartServiceImpl.java:1007)\\n    at org.eclipse.e4.ui.internal.workbench.PartServiceImpl.showPart(PartServiceImpl.java:1029)\\n    at org.eclipse.ui.internal.WorkbenchPage.busyOpenEditor(WorkbenchPage.java:3112)\\n    at org.eclipse.ui.internal.WorkbenchPage.access$21(WorkbenchPage.java:3034)\\n    at org.eclipse.ui.internal.WorkbenchPage$8.run(WorkbenchPage.java:3016)\\n    at org.eclipse.swt.custom.BusyIndicator.showWhile(BusyIndicator.java:70)\\n    at org.eclipse.ui.internal.WorkbenchPage.openEditor(WorkbenchPage.java:3012)\\n    at org.eclipse.ui.internal.WorkbenchPage.openEditor(WorkbenchPage.java:2976)\\n    at org.eclipse.ui.internal.WorkbenchPage.openEditor(WorkbenchPage.java:2959)\\n    at com.aptana.portal.ui.internal.Portal$1.runInUIThread(Portal.java:227)\\n    at org.eclipse.ui.progress.UIJob$1.run(UIJob.java:95)\\n    at org.eclipse.swt.widgets.RunnableLock.run(RunnableLock.java:35)\\n    at org.eclipse.swt.widgets.Synchronizer.runAsyncMessages(Synchronizer.java:135)\\n    at org.eclipse.swt.widgets.Display.runAsyncMessages(Display.java:4145)\\n    at org.eclipse.swt.widgets.Display.readAndDispatch(Display.java:3762)\\n    at org.eclipse.e4.ui.internal.workbench.swt.PartRenderingEngine$9.run(PartRenderingEngine.java:1113)\\n    at org.eclipse.core.databinding.observable.Realm.runWithDefault(Realm.java:332)\\n    at org.eclipse.e4.ui.internal.workbench.swt.PartRenderingEngine.run(PartRenderingEngine.java:997)\\n    at org.eclipse.e4.ui.internal.workbench.E4Workbench.createAndRunUI(E4Workbench.java:140)\\n    at org.eclipse.ui.internal.Workbench$5.run(Workbench.java:611)\\n    at org.eclipse.core.databinding.observable.Realm.runWithDefault(Realm.java:332)\\n    at org.eclipse.ui.internal.Workbench.createAndRunWorkbench(Workbench.java:567)\\n    at org.eclipse.ui.PlatformUI.createAndRunWorkbench(PlatformUI.java:150)\\n    at org.eclipse.ui.internal.ide.application.IDEApplication.start(IDEApplication.java:124)\\n    at org.eclipse.equinox.internal.app.EclipseAppHandle.run(EclipseAppHandle.java:196)\\n    at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.runApplication(EclipseAppLauncher.java:110)\\n    at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.start(EclipseAppLauncher.java:79)\\n    at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:354)\\n    at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:181)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\\n    at java.lang.reflect.Method.invoke(Unknown Source)\\n    at org.eclipse.equinox.launcher.Main.invokeFramework(Main.java:636)\\n    at org.eclipse.equinox.launcher.Main.basicRun(Main.java:591)\\n    at org.eclipse.equinox.launcher.Main.run(Main.java:1450)\\n    at org.eclipse.equinox.launcher.Main.main(Main.java:1426)\\n\\n!ENTRY org.eclipse.ui 4 4 2014-05-08 02:00:13.427\\n!MESSAGE An internal error has occurred.\\n!STACK 0\\njava.lang.IllegalArgumentException: \\n    at org.eclipse.core.runtime.Assert.isLegal(Assert.java:63)\\n    at org.eclipse.core.runtime.Assert.isLegal(Assert.java:47)\\n    at org.eclipse.e4.ui.internal.workbench.ModelServiceImpl.findElementsRecursive(ModelServiceImpl.java:286)\\n    at org.eclipse.e4.ui.internal.workbench.ModelServiceImpl.findElements(ModelServiceImpl.java:371)\\n    at org.eclipse.e4.ui.internal.workbench.PartServiceImpl.addPart(PartServiceImpl.java:812)\\n    at org.eclipse.e4.ui.internal.workbench.PartServiceImpl.addPart(PartServiceImpl.java:1007)\\n    at org.eclipse.e4.ui.internal.workbench.PartServiceImpl.showPart(PartServiceImpl.java:1029)\\n    at org.eclipse.ui.internal.WorkbenchPage.busyOpenEditor(WorkbenchPage.java:3112)\\n    at org.eclipse.ui.internal.WorkbenchPage.access$21(WorkbenchPage.java:3034)\\n    at org.eclipse.ui.internal.WorkbenchPage$8.run(WorkbenchPage.java:3016)\\n    at org.eclipse.swt.custom.BusyIndicator.showWhile(BusyIndicator.java:70)\\n    at org.eclipse.ui.internal.WorkbenchPage.openEditor(WorkbenchPage.java:3012)\\n    at org.eclipse.ui.internal.WorkbenchPage.openEditor(WorkbenchPage.java:2976)\\n    at org.eclipse.ui.internal.WorkbenchPage.openEditor(WorkbenchPage.java:2959)\\n    at com.aptana.portal.ui.internal.Portal$1.runInUIThread(Portal.java:227)\\n    at org.eclipse.ui.progress.UIJob$1.run(UIJob.java:95)\\n    at org.eclipse.swt.widgets.RunnableLock.run(RunnableLock.java:35)\\n    at org.eclipse.swt.widgets.Synchronizer.runAsyncMessages(Synchronizer.java:135)\\n    at org.eclipse.swt.widgets.Display.runAsyncMessages(Display.java:4145)\\n    at org.eclipse.swt.widgets.Display.readAndDispatch(Display.java:3762)\\n    at org.eclipse.e4.ui.internal.workbench.swt.PartRenderingEngine$9.run(PartRenderingEngine.java:1113)\\n    at org.eclipse.core.databinding.observable.Realm.runWithDefault(Realm.java:332)\\n    at org.eclipse.e4.ui.internal.workbench.swt.PartRenderingEngine.run(PartRenderingEngine.java:997)\\n    at org.eclipse.e4.ui.internal.workbench.E4Workbench.createAndRunUI(E4Workbench.java:140)\\n    at org.eclipse.ui.internal.Workbench$5.run(Workbench.java:611)\\n    at org.eclipse.core.databinding.observable.Realm.runWithDefault(Realm.java:332)\\n    at org.eclipse.ui.internal.Workbench.createAndRunWorkbench(Workbench.java:567)\\n    at org.eclipse.ui.PlatformUI.createAndRunWorkbench(PlatformUI.java:150)\\n    at org.eclipse.ui.internal.ide.application.IDEApplication.start(IDEApplication.java:124)\\n    at org.eclipse.equinox.internal.app.EclipseAppHandle.run(EclipseAppHandle.java:196)\\n    at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.runApplication(EclipseAppLauncher.java:110)\\n    at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.start(EclipseAppLauncher.java:79)\\n    at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:354)\\n    at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:181)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\\n    at java.lang.reflect.Method.invoke(Unknown Source)\\n    at org.eclipse.equinox.launcher.Main.invokeFramework(Main.java:636)\\n    at org.eclipse.equinox.launcher.Main.basicRun(Main.java:591)\\n    at org.eclipse.equinox.launcher.Main.run(Main.java:1450)\\n    at org.eclipse.equinox.launcher.Main.main(Main.java:1426)\\n\\n!ENTRY org.eclipse.ui 4 4 2014-05-08 02:00:24.084\\n!MESSAGE An internal error has occurred.\\n!STACK 0\\njava.lang.IllegalArgumentException: \\n    at org.eclipse.core.runtime.Assert.isLegal(Assert.java:63)\\n    at org.eclipse.core.runtime.Assert.isLegal(Assert.java:47)\\n    at org.eclipse.e4.ui.internal.workbench.ModelServiceImpl.findElementsRecursive(ModelServiceImpl.java:286)\\n    at org.eclipse.e4.ui.internal.workbench.ModelServiceImpl.findElements(ModelServiceImpl.java:371)\\n    at org.eclipse.e4.ui.internal.workbench.PartServiceImpl.addPart(PartServiceImpl.java:812)\\n    at org.eclipse.e4.ui.internal.workbench.PartServiceImpl.addPart(PartServiceImpl.java:1007)\\n    at org.eclipse.e4.ui.internal.workbench.PartServiceImpl.showPart(PartServiceImpl.java:1029)\\n    at org.eclipse.ui.internal.WorkbenchPage.busyOpenEditor(WorkbenchPage.java:3112)\\n    at org.eclipse.ui.internal.WorkbenchPage.access$21(WorkbenchPage.java:3034)\\n    at org.eclipse.ui.internal.WorkbenchPage$8.run(WorkbenchPage.java:3016)\\n    at org.eclipse.swt.custom.BusyIndicator.showWhile(BusyIndicator.java:70)\\n    at org.eclipse.ui.internal.WorkbenchPage.openEditor(WorkbenchPage.java:3012)\\n    at org.eclipse.ui.internal.WorkbenchPage.openEditor(WorkbenchPage.java:2976)\\n    at org.eclipse.ui.internal.WorkbenchPage.openEditor(WorkbenchPage.java:2959)\\n    at com.aptana.portal.ui.internal.Portal$1.runInUIThread(Portal.java:227)\\n    at org.eclipse.ui.progress.UIJob$1.run(UIJob.java:95)\\n    at org.eclipse.swt.widgets.RunnableLock.run(RunnableLock.java:35)\\n    at org.eclipse.swt.widgets.Synchronizer.runAsyncMessages(Synchronizer.java:135)\\n    at org.eclipse.swt.widgets.Display.runAsyncMessages(Display.java:4145)\\n    at org.eclipse.swt.widgets.Display.readAndDispatch(Display.java:3762)\\n    at org.eclipse.e4.ui.internal.workbench.swt.PartRenderingEngine$9.run(PartRenderingEngine.java:1113)\\n    at org.eclipse.core.databinding.observable.Realm.runWithDefault(Realm.java:332)\\n    at org.eclipse.e4.ui.internal.workbench.swt.PartRenderingEngine.run(PartRenderingEngine.java:997)\\n    at org.eclipse.e4.ui.internal.workbench.E4Workbench.createAndRunUI(E4Workbench.java:140)\\n    at org.eclipse.ui.internal.Workbench$5.run(Workbench.java:611)\\n    at org.eclipse.core.databinding.observable.Realm.runWithDefault(Realm.java:332)\\n    at org.eclipse.ui.internal.Workbench.createAndRunWorkbench(Workbench.java:567)\\n    at org.eclipse.ui.PlatformUI.createAndRunWorkbench(PlatformUI.java:150)\\n    at org.eclipse.ui.internal.ide.application.IDEApplication.start(IDEApplication.java:124)\\n    at org.eclipse.equinox.internal.app.EclipseAppHandle.run(EclipseAppHandle.java:196)\\n    at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.runApplication(EclipseAppLauncher.java:110)\\n    at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.start(EclipseAppLauncher.java:79)\\n    at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:354)\\n    at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:181)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\\n    at java.lang.reflect.Method.invoke(Unknown Source)\\n    at org.eclipse.equinox.launcher.Main.invokeFramework(Main.java:636)\\n    at org.eclipse.equinox.launcher.Main.basicRun(Main.java:591)\\n    at org.eclipse.equinox.launcher.Main.run(Main.java:1450)\\n    at org.eclipse.equinox.launcher.Main.main(Main.java:1426)\\n</code></pre>\\n-Aptana Studio 3.4.2 Eclipse Plugin Fails with Internal Errors After Installing on Win 7 Pro 64-bit PC running Eclipse Kepler-<eclipse-plugin><windows-7-x64><aptana3><eclipse-kepler>',\n",
       " '<p>I would like to create a WHERE condition to return results where only 1 day has passed between two timestamps. I tried this:</p>\\n\\n<pre><code>SELECT * FROM RDMAVWSANDBOX.VwNIMEventFct\\nINNER JOIN VwNIMUserDim ON VwNIMUserDim.NIM_USER_ID = VwNIMEventFct.NIM_USER_ID\\nINNER JOIN rdmatblsandbox.TmpNIMSalesForceDB ON TmpNIMSalesForceDB.EMAIL = VwNIMUserDim.USER_EMAIL_ADDRESS\\nWHERE (CONTRACT_EFFECTIVE_DATE - EVENT_TIMESTAMP) =1\\n</code></pre>\\n\\n<p>But the result was an error message \"Invalid Operation On An ANSI DATETIME value\".</p>\\n\\n<p>I guess that, looking at the code now, Teradata has no way of knowing whether the \"1\" in \"= 1\" is a day, hour or year.</p>\\n\\n<p>How would I select data where only 1 day has passed between CONTRACT_EFFECTIVE_DATE and EVENT_TIMESTAMP?</p>\\n\\n<p>Same again for 2 days, and 3 days etc?</p>\\n-Invalid Operation On An ANSI DATETIME (Subtracting one timestamp from another in Teradata)-<sql><datetime><teradata>',\n",
       " \"<p>I would like to create a SELECT query that results in a view that is 30 fields long and 1 record thick. 2 records if we're counting the title.</p>\\n\\n<p>The specific part of the larger query in question is:</p>\\n\\n<pre><code>WHERE CAST(EVENT_TIMESTAMP AS DATE) - CONTRACT_EFFECTIVE_DATE = 1\\n</code></pre>\\n\\n<p>This produces the results desired - records where the days between EVENT_TIMESTAMP and CONTRACT_EFFECTIVE_DATE is 1.</p>\\n\\n<p>But I'd like this for 30 days. Something like:</p>\\n\\n<pre><code>WHERE CAST(EVENT_TIMESTAMP AS DATE) - CONTRACT_EFFECTIVE_DATE = 1:30\\n</code></pre>\\n\\n<p>Is that possible? I could just cut n paste the query 30 times and change the number 1 to the corresponding value. But surely there is a better way?</p>\\n\\n<p>The results would look something like:</p>\\n\\n<pre><code>1 day   2 day   3 day   4 day\\n10       11       8      14\\n</code></pre>\\n-Sequencing in Teradata-<sql><sequence><teradata>\",\n",
       " '<p>With help I recently received on SO, I managed to get data using the following snippet within my SELECT query:</p>\\n\\n<pre><code>COUNT(CASE WHEN CAST(EVENT_TIMESTAMP AS DATE) - CONTRACT_EFFECTIVE_DATE = 2 THEN 1 END) AS \"2 day\",\\n</code></pre>\\n\\n<p>This produced the count of instances of an event of a particular condition being met.</p>\\n\\n<p>But I would now like to calculate the number of people (Unique email addresses) associated with these events.</p>\\n\\n<p>I tried this:</p>\\n\\n<pre><code>COUNT(CASE WHEN CAST(EVENT_TIMESTAMP AS DATE) - CONTRACT_EFFECTIVE_DATE = 2 THEN COUNT(DISTINCT TmpNIMSalesForceDB.EMAIL) END) AS \"2 day\"\\n</code></pre>\\n\\n<p>The logic seems right in my mind - if the result of the case is 2 then count the distinct email addresses for that case.</p>\\n\\n<p>But it seems SQL (Or Teradata) does not like nesting in this way. The resulting error message was \"Cannot next aggregate function\".</p>\\n\\n<p>Can I use <code>COUNT(DISTINCT</code> in this way?</p>\\n-Cannot nest aggregate operations - Teradata Case When-<sql><count><case><teradata>',\n",
       " '<p>I have a simple e4 RPC application, based on e4 model. I have two parts in PartSachContainer - left one and right one.\\nAnd I need a certain toolbar item to be visible when one part is active, and another item to be visible when another part is active.</p>\\n\\n<p>I tried to use CoreExpression for this, but to no avail. I can\\'t understand what do I need to write in the expression itself.</p>\\n\\n<p>My config is as follows:</p>\\n\\n<p>First part is defined as</p>\\n\\n<pre>\\nID: simple1.part.leftpart\\nLabel: LeftPart\\nClassURI: bundleclass://Simple1/simple1.parts.LeftPart\\n</pre>\\n\\n<p>The second part is defined as</p>\\n\\n<pre>\\nID: simple1.part.rightpart\\nLabel: RightPart\\nClassURI: bundleclass://Simple1/simple1.parts.RightPart\\n</pre>\\n\\n<p>The ToolBar Contribution is defined as</p>\\n\\n<pre><code>&lt;toolBarContributions xmi:id=\"_gqB4gNaWEeO3iqh7a9kwnw\" elementId=\"simple1.toolbarcontribution.0\" parentId=\"simple1.toolbar.0\"&gt;\\n    &lt;children xsi:type=\"menu:HandledToolItem\" xmi:id=\"_PtGaINaSEeO3iqh7a9kwnw\" elementId=\"simple1.handledtoolitem.leftpitem\" iconURI=\"platform:/plugin/Simple1/icons/sample.png\" command=\"_vFvtcNaREeO3iqh7a9kwnw\"&gt;\\n      &lt;visibleWhen xsi:type=\"ui:CoreExpression\" xmi:id=\"_z_87oNanEeO3iqh7a9kwnw\" coreExpressionId=\"Simple1.RightPartActive\"/&gt;\\n    &lt;/children&gt;\\n  &lt;/toolBarContributions&gt;\\n</code></pre>\\n\\n<p>When I set Visible-When Expression to  I can see the toolbar item.\\nBut when I try to use CoreExpression (and I tried various combinations) I never see the toolbar item.</p>\\n\\n<p>This is what I start with:</p>\\n\\n<pre><code>  &lt;definition id=\"Simple1.RightPartActive\"&gt;\\n     &lt;with variable=\"activePartId\"&gt;\\n        &lt;equals \\n              value=\"simple1.part.rightpart\"&gt;\\n        &lt;/equals&gt;\\n     &lt;/with&gt;\\n  &lt;/definition&gt;\\n</code></pre>\\n\\n<p>No luck.</p>\\n\\n<p>How should I define the CoreExpression to see the toolbar item?</p>\\n\\n<p>And here\\'s to the real problem:\\nThis right part should really be an editor (in e3 terms) - the same implementaion class for multiple various instances of editable data. So, I will use the runtime-generated unique IDs for this parts (the parts themleves would be created from PartDescription).\\nSo how should I address these parts with generated IDs in CoreExpression?</p>\\n\\n<p><strong>UPDATE</strong></p>\\n\\n<p>Ok, I did some debugging and found out that there is a method </p>\\n\\n<pre><code>public EvaluationResult evaluate(IEvaluationContext context)\\n</code></pre>\\n\\n<p>of the class \\n<em>org.eclipse.core.internal.expressions.WithExpression</em>\\nthat is being called.</p>\\n\\n<p>The very first line :</p>\\n\\n<pre><code>Object variable= context.getVariable(fVariable);\\n</code></pre>\\n\\n<p>calls method of the class <em>EclipseContext</em></p>\\n\\n<pre><code>public Object getVariable(String name) {\\n    if (IEclipseContext.class.getName().equals(name)) {\\n    return eclipseContext;\\n    }\\n    Object obj = eclipseContext.getActive(name);\\n    return obj == null ? IEvaluationContext.UNDEFINED_VARIABLE : obj;\\n}\\n</code></pre>\\n\\n<p>where </p>\\n\\n<pre><code>public Object getActive(final String name) {\\n    return getActiveLeaf().get(name);\\n}\\n</code></pre>\\n\\n<p>and </p>\\n\\n<pre><code>public IEclipseContext getActiveLeaf() {\\n    IEclipseContext activeContext = this;\\n    IEclipseContext child = getActiveChild();\\n    while (child != null) {\\n        activeContext = child;\\n    child = child.getActiveChild();\\n    }\\n    return activeContext;\\n}\\n</code></pre>\\n\\n<p>and [b]activeChild[/b] is always null.</p>\\n\\n<p>And I don\\'t see anything in EclipseContext which would be something like \"activePartId\" or anything close to it.</p>\\n\\n<p>So why is it so??\\nHow can we even use the CoreExpressions??</p>\\n-Visibility of toolbar items in Eclipse 4 model-<eclipse><toolbar><eclipse-kepler><e4>',\n",
       " '<p>The \"Dynamic Web Project\" option for a new project in Eclipse does not showing up in the list. <br>Although the \"web\" folder showing up  with option \"static web project\" but there is no option for \"Dynamic Web project\".<br>\\nI have already installed Web, XML, Java EE plugin. <br>\\nI am using Eclipse Kepler realease-2.</p>\\n-Dynamic Web Project option missing in Eclipse Kepler-<eclipse><jakarta-ee><eclipse-kepler><web-project>',\n",
       " \"<p>I'm using Teradata 14, with the Teradata 13 client.</p>\\n\\n<p>I want to pull all records from a table (it contains a time stamp), for the previous month.\\nSomething like:</p>\\n\\n<pre><code>SELECT COL1, COL2, DATECOL\\nFROM TABLE\\nWHERE DATECOL &gt;= FIRST_OF_LAST_MONTH\\n    AND DATECOL IS &lt; FIRST_OF_THIS_MONTH\\n</code></pre>\\n\\n<p>Any help would be appreciated.  I'm new to Teradata.</p>\\n-Teradata - Return records added since first of last month, but before-<sql><teradata>\",\n",
       " '<p>Beginner of Spring</p>\\n\\n<p><strong>Background :</strong> So far, i have been working on core JAVA and now i need to switch to MVC<br />\\nTrying to make my first Spring MVC <em>Hello World Example</em> <a href=\"http://examples.javacodegeeks.com/enterprise-java/spring/mvc/spring-mvc-hello-world-example/\" rel=\"nofollow noreferrer\"> <strong>from this tutorial</strong> </a>, i am getting below error on <code>pom.xml</code>:</p>\\n\\n<pre><code>Multiple annotations found at this line:\\n    - Plugin execution not covered by lifecycle configuration: org.apache.maven.plugins:maven-compiler-plugin:\\n     2.3.2:compile (execution: default-compile, phase: compile)\\n    - Plugin execution not covered by lifecycle configuration: org.apache.maven.plugins:maven-resources-plugin:\\n     2.5:testResources (execution: default-testResources, phase: process-test-resources)\\n    - &lt;packaging&gt;war&lt;/packaging&gt;\\n    - Plugin execution not covered by lifecycle configuration: org.apache.maven.plugins:maven-resources-plugin:\\n     2.5:resources (execution: default-resources, phase: process-resources)\\n    - Plugin execution not covered by lifecycle configuration: org.apache.maven.plugins:maven-compiler-plugin:\\n     2.3.2:testCompile (execution: default-testCompile, phase: test-compile)\\n</code></pre>\\n\\n<p>Error is generated on this line of pom.xml :</p>\\n\\n<pre><code>  &lt;packaging&gt;war&lt;/packaging&gt;\\n</code></pre>\\n\\n<p><strong>pom.xml</strong></p>\\n\\n<pre><code>&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\\n      xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\"&gt;\\n      &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\\n      &lt;groupId&gt;com.javacodegeeks.snippets.enterprise&lt;/groupId&gt;\\n      &lt;artifactId&gt;springexample&lt;/artifactId&gt;\\n      &lt;packaging&gt;war&lt;/packaging&gt;\\n      &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;\\n      &lt;name&gt;springexample Maven Webapp&lt;/name&gt;\\n      &lt;url&gt;http://maven.apache.org&lt;/url&gt;\\n      &lt;dependencies&gt;\\n        &lt;dependency&gt;\\n          &lt;groupId&gt;junit&lt;/groupId&gt;\\n          &lt;artifactId&gt;junit&lt;/artifactId&gt;\\n          &lt;version&gt;3.8.1&lt;/version&gt;\\n          &lt;scope&gt;test&lt;/scope&gt;\\n        &lt;/dependency&gt;\\n\\n        &lt;dependency&gt;\\n                &lt;groupId&gt;org.springframework&lt;/groupId&gt;\\n                &lt;artifactId&gt;spring-core&lt;/artifactId&gt;\\n                &lt;version&gt;${spring.version}&lt;/version&gt;\\n            &lt;/dependency&gt;\\n            &lt;dependency&gt;\\n                &lt;groupId&gt;org.springframework&lt;/groupId&gt;\\n                &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt;\\n                &lt;version&gt;${spring.version}&lt;/version&gt;\\n            &lt;/dependency&gt;\\n      &lt;/dependencies&gt;\\n      &lt;build&gt;\\n        &lt;finalName&gt;springexample&lt;/finalName&gt;\\n      &lt;/build&gt;\\n\\n        &lt;properties&gt;\\n            &lt;spring.version&gt;4.0.2.RELEASE&lt;/spring.version&gt;\\n        &lt;/properties&gt;\\n&lt;/project&gt;\\n</code></pre>\\n\\n<p>I even installed <code>m2e</code> after searching in <a href=\"http://marketplace.eclipse.org/marketplace-client-intro?mpc_install=252\" rel=\"nofollow noreferrer\"> marketplace</a></p>\\n\\n<p>Please go easy, this question comes in as an attempt to learn something totally new to me.\\n<br />\\nDo i need some plugin or something to resolve this? If so, is there any <em>universal</em> kind of plugin to avoid any such future error?\\n<br />\\n<strong>IDE :</strong> Eclipse Kepler</p>\\n\\n<p>P.S :</p>\\n\\n<p>I have already browsed old threads but no luck :</p>\\n\\n<ul>\\n<li><p><a href=\"https://stackoverflow.com/questions/6352208/how-to-solve-plugin-execution-not-covered-by-lifecycle-configuration-for-sprin\">How to solve &quot;Plugin execution not covered by lifecycle configuration&quot; for Spring Data Maven Builds</a></p></li>\\n<li><p><a href=\"https://stackoverflow.com/questions/19432073/plugin-execution-not-covered-by-lifecycle-configuration-error-in-eclipse-with-pl\">Plugin execution not covered by lifecycle configuration error in eclipse with pluginManagement in parent pom</a></p></li>\\n</ul>\\n-Plugin execution not covered by lifecycle configuration-<spring><maven><spring-mvc><pom.xml><eclipse-kepler>',\n",
       " '<p>The data I\\'m trying to import is here: <a href=\"http://archive.ics.uci.edu/ml/machine-learning-databases/car/\" rel=\"nofollow\">http://archive.ics.uci.edu/ml/machine-learning-databases/car/</a></p>\\n\\n<pre><code>car.data 51 K\\n</code></pre>\\n\\n<p>There are no missing values in this data, yet there are lots of <code>\"?\"s</code> in the rapidminer once I imported the data. I looked at to the source and those data, which shown as <code>\"?\"</code>, do exist properly in the source. What may be the problem?</p>\\n\\n<p>by the way, if I download that file it\\'s extension is <code>.data</code>. How should I import that kind of files? I import it as if it is a <code>.csv</code> file and it looks ok at first but there are those <code>\"?\"s</code>. </p>\\n-Can\\'t import data to Rapidminer properly-<csv><import><data-mining><rapidminer>',\n",
       " '<p>After installing maven (m2e) plugin for eclipse kepler I am not able to view maven in the menu (run as).\\nHow can I get Maven visible in the \"run as\" menu?</p>\\n-Maven tool for eclipse-<maven-plugin><eclipse-kepler>',\n",
       " \"<p>My company's firewall doesn't allow the built-in updater to communicate with the Internet to fetch the updates and extensions, and USB drives are disabled by default.</p>\\n\\n<p>Is there a way to <strong>manually</strong> download extensions such as the Weka integration as a <code>.zip</code> file?</p>\\n-Download KNIME extensions manually-<knime>\",\n",
       " '<p>I NEED to turn off auto-indent on Eclipse for C/C++ coding. It DRIVES ME NUTS to be fighting with the editor constantly for the position of the text cursor.</p>\\n\\n<p>This is a common question, but the standard answer does not appear to work on Kepler with C/C++ extentions, so perhaps it got \"unfixed\"???</p>\\n\\n<p>The standard answer I saw was to go to preferences->C/C++->editor->typing and uncheck \"automatically indent\" new lines and braces.</p>\\n\\n<p>The behavior is that entering new line causes the cursor to be aligned on the new line with the block above. I want newline to end up flush left.</p>\\n\\n<p>Thanks,</p>\\n\\n<p>Scott Franco</p>\\n-Turn off indent eclipse C/C++-<c++><c><eclipse><eclipse-kepler>',\n",
       " \"<p>I am trying to submit a two-statement SQL file as a batch job to the isql utility on a Linux machine. </p>\\n\\n<p>The first statement defines a <code>VOLATILE TABLE</code> and the second statement is a <code>SELECT</code> statement against this table.</p>\\n\\n<p>The Teradata server is hosted on a remote machine and I have successfully run <code>SELECT</code> statements to return result sets to my host.</p>\\n\\n<p>Below is an example of the SQL file.</p>\\n\\n<pre><code>CREATE MULTISET VOLATILE TABLE my_temp_table AS (\\n  SELECT \\n    A.ID\\n    , MIN(A.DTE) AS FIRST_DATE\\n  FROM (\\n    SELECT\\n      X.ID\\n      , MIN(X.STRT_DTE) AS DTE\\n    FROM DATABASE0.TABLE_ABC AS X\\n    WHERE X.STRT_DTE BETWEEN (CURRENT_DATE - 7) AND CURRENT_DATE\\n    GROUP BY X.ID\\n    UNION ALL\\n    SELECT\\n      Y.ID\\n      , MIN(Y.STRT_DTE) AS DTE\\n    FROM DATABASE0.TABLE_XYZ AS Y\\n    WHERE Y.STRT_DTE BETWEEN (CURRENT_DATE - 7) AND CURRENT_DATE\\n    GROUP BY Y.ID\\n  ) AS A\\nGROUP BY A.ID\\n)\\nWITH DATA\\nON COMMIT PRESERVE ROWS\\n;\\nSELECT TOP 10 * FROM my_temp_table;\\n</code></pre>\\n\\n<p>When I run the following command</p>\\n\\n<pre><code>cat my_two_statement_query.sql | isql -v -b -d',' mydsn myuid mypwd\\n</code></pre>\\n\\n<p>I receive the following error</p>\\n\\n<p><code>[25000][Teradata][ODBC Teradata Driver][Teradata Database] Data definition not valid unless solitary.</code></p>\\n\\n<p>I am able to run these statements in Teradata client applications - Studio and SQL Assistant - with no issues.  </p>\\n\\n<p>UPDATE: I've included the contents of the odbc.ini file</p>\\n\\n<pre><code>[ODBC]\\nInstallDir=/path/to/installation/dir\\nTrace=0\\nTraceDll=/path/to/dll/odbctrac.so\\nTraceFile=/path/to/tracefile/odbc_trace.log\\nTraceAutoStop=0\\n\\n[ODBC Data Sources]\\nproddsn=tdata.so\\n\\n[proddsn]\\nDriver=/path/to/driver/tdata/so\\nDescription=Teradata database\\nDBCName=TDPROD\\nLastUser=\\nUsername=\\nPassword=\\nDatabase=DATABASE0\\nDefaultDatabase=DATABASE0\\nNoScan=Yes\\n</code></pre>\\n-Create Teradata volatile table and select from table using isql-<teradata><isql><ddl>\",\n",
       " '<p>I am working with R and <a href=\"http://labs.genetics.ucla.edu/horvath/CoexpressionNetwork/Rpackages/WGCNA/\" rel=\"nofollow\">\"WGCNA\" package</a>. I am doing an integrative analysis of transcriptome and metabolome. </p>\\n\\n<p>I have two <code>data.frames</code>, one for the transcriptome data: <code>datExprFemale</code>, and one for the metabomics data: <code>allTraits</code>, but I am having trouble merging the two <code>data.frames</code> together.</p>\\n\\n<pre><code>&gt; datExprFemale[1:5, 1:5]\\nID    gene1         gene2       gene3        gene4\\nF16 -0.450904880  0.90116800 -2.710879397  0.98942336\\nF17 -0.304889916  0.70307639 -0.245912838 -0.01089557\\nF18  0.001696330  0.43059153 -0.177277078 -0.24611398\\nF19 -0.005428231  0.32838938  0.001070509 -0.31351216\\nH1   0.183912553 -0.10357460  0.069589703  0.15791036\\n\\n&gt; allTraits[1:5, 1:5]\\nIND   met1          met2        met3         met4\\nF15   6546          68465       56465        6548\\nF17   89916         7639        2838         9557\\nF20   6330          53          7078         11398\\nF1    231           938         509          351216\\n</code></pre>\\n\\n<p>The individuals in <code>allTraits</code> have measurements in <code>datExprFemale</code>, but some individuals in <code>datExprFemale</code> do not occur in <code>allTraits</code>.</p>\\n\\n<p>Here is what I have tried to merge the two <code>data.frames</code> together:</p>\\n\\n<pre><code># First get a vector containing the row names (individual\\'s ID) in datExprFemale\\nIND=rownames(datExprFemale)\\n# Get the rows in which two variables have the same individuals\\ntraitRows = match(allTraits$IND, IND)\\ndatTraits = allTraits[traitRows, -1]\\n</code></pre>\\n\\n<p>This gives me the following:</p>\\n\\n<pre><code>         met1                       met2    met3                      met4\\n11       0.0009                     0.0559   7.1224                    3.3894\\n12       0.0006                     0.0370  10.5776                   14.4437\\n15       0.0011                     0.0295   5.7941                   19.0225\\n16       0.0010                     0.0531   6.1010                    4.7698\\n17       0.0016                     0.0462   7.7819                    7.8796\\n19       0.0011                     0.0192  12.7126                    9.2564\\n20       0.0007                     0.0502   9.4147                   15.3579\\n21       0.0025                     0.0455   8.4129                   17.7273\\nNA           NA                         NA       NA                        NA\\nNA.1         NA                         NA       NA                        NA\\nNA.2         NA                         NA       NA                        NA\\nNA.3         NA                         NA       NA                        NA\\nNA.4         NA                         NA       NA                        NA\\n3        0.0017                     0.0375   8.8503                    8.7581\\n7        0.0006                     0.0156   7.9272                    4.9887\\n8        0.0011                     0.0154   8.4716                    8.6515\\n9        0.0010                     0.0306   9.1220                    3.5843\\n</code></pre>\\n\\n<p>As you see there are some <code>NA</code> values, but I\\'m not sure why?</p>\\n\\n<p>Now when I want to assign the ID of each individual to the corresponding row using the following code :</p>\\n\\n<pre><code>rownames(datTraits) = allTraits[traitRows, 1]\\n</code></pre>\\n\\n<p>R gives this error:</p>\\n\\n<pre><code>Error in `row.names&lt;-.data.frame`(`*tmp*`, value = value) : \\n  duplicate \\'row.names\\' are not allowed\\nIn addition: Warning message:\\nnon-unique values when setting \\'row.names\\': \\n</code></pre>\\n\\n<p>I\\'m not sure what I\\'m doing wrong,</p>\\n-Identifying individuals with observations across two datasets-<r><dataframe>',\n",
       " '<p>I\\'m looking to split a string in Teradata.</p>\\n\\n<p>The table might look something like this.</p>\\n\\n<pre><code>column1\\nhello:goodbye:afternoon\\n</code></pre>\\n\\n<p>I\\'m trying to use SUBSTRING and INSTR to extract specific words.  So, say I want to select \"goodbye\". I\\'m trying the following query. </p>\\n\\n<pre><code>SELECT SUBSTRING(a.column1 from index(a.column1,\\':\\')+1 for INSTR(a.column1,\\':\\',0,2))\\nFROM db.table as a\\n</code></pre>\\n\\n<p>I get the following error.</p>\\n\\n<pre><code>SELECT Failed. [3707] Syntax error, expected something like \\')\\' between the word \\'INSTR\\' and \\'(\\'\\n</code></pre>\\n\\n<p>I\\'m not sure why I\\'m getting that error.  It lets me use INDEX to deduce a number in place of INSTR, so I\\'m not sure why it is acting this way when I use INSTR.</p>\\n-Split String Teradata SQL-<sql><database><substring><teradata>',\n",
       " '<p>I\\'m developing a QT based application on ARM, iMX6 device.\\nUsing toolchain from Freescale and Eclipse CDT Kepler, I would like to use pretty printer with GDB.\\nI followed the tutorial on <a href=\"http://anadoxin.org/blog/node/98\" rel=\"nofollow\">http://anadoxin.org/blog/node/98</a> and the setup seems fine (no error while starting GDB).\\nHowever when  looking at debug view panel, I\\'m not able to look at any datatype (not only QT but also int, double etc, the whole panel is blank. Furthermore also writing print variable to gdb Eclipse console doesn\\'t generate a valid output).</p>\\n\\n<p>I think my setup is fine:</p>\\n\\n<ol>\\n<li>GNU gdb (GDB) 7.6 for ARM device</li>\\n<li>Python: Python 2.7.3</li>\\n<li>Eclipse: Kepler</li>\\n<li>KUbuntu 12.04</li>\\n</ol>\\n\\n<p>Is there anyone that can help me understand what is wrong? \\nCan I send any command to gdb console in order to understand why pretty printer aren\\'t working fine, and even worse, they corrupt the whole gdb debug view variables?\\nAny help will be really appreciated</p>\\n-Pretty print in Eclipse CDT: unable to look at any variable-<python><qt><eclipse-cdt><pretty-print><kepler>',\n",
       " '<p>I am a masters student, and as I am preparing my final project study which requires to have a graphical editor for a DSL named <code>\"VERISENSOR\"</code>, I need to work with <code>\"SPRAY\"</code>. \\nwhile following the tutorial that I found on eclipse, I found a problem in the file ( .spray )\\nonce I add a class in this file, the following errors are displayed: </p>\\n\\n<pre><code>org.eclipse.emf.ecore.impl.DynamicEObjectImpl cannot be cast \\nto org.eclipselabs.spray.mm.spray.ShapeInSpray \\norg.eclipse.emf.ecore.impl.DynamicEObjectImpl \\n</code></pre>\\n-error in a spray project-<eclipse-kepler><spray>',\n",
       " \"<p>Table A<br>\\nId varchar(30)</p>\\n\\n<p>I'm trying to re-create a logic where I have to use 9 digit Ids irrespective of the actual length of the Value of the Id field.<br>\\nSo for instance, if the Id is of length 6, I'll need to left pad with 3 leading zeros. The actual length can be anything ranging from 1 to 9.</p>\\n\\n<p>Any ideas how to implement this in Teradata SQL?</p>\\n-zero padding in teradata sql-<sql><teradata>\",\n",
       " '<p>Im am using <em>Maven 3.2.1</em> with <em>Eclipse Kepler</em>. I have a dynamic web project which has a dependency to another component which includes some JSPs. I now want the JSPs from the dependency to become part of the web root of the dynamic web project. I chose to accomplish this by using <em>unpack</em> goal of the <em>maven-dependency-plugin</em>.</p>\\n\\n<p>I added a plugin definition to unpack the JARs into <code>/target/m2e-wtp/web-resources</code>. Unfortunatley Eclipse from time to time cleans this folder and teh JSPs are gone. In order to unpack them again I have to delete the <code>target/dependency-maven-plugin-markers</code>folder. Otherwise the plugin will not unpack the files again.</p>\\n\\n<p>Is it possible to force the unpacking and ignore the plugin markers?</p>\\n\\n<p>Is there a better way to get web resources from a dependency into my Dynamic Web Project? </p>\\n-Force unpacking with maven-dependency-plugin-<eclipse><maven><m2eclipse><m2e><eclipse-kepler>',\n",
       " \"<p>I'm running a Naive Bayes process in RapidMiner on Fisher's Iris dataset.</p>\\n\\n<p>My main process is as follows:</p>\\n\\n<blockquote>\\n  <p>Retrieve Iris, Set Role, Validation</p>\\n</blockquote>\\n\\n<p>The Validation subprocess is as follows:</p>\\n\\n<blockquote>\\n  <p>Training Set: Naive Bayes; Test Set: Apply Model, Performance</p>\\n</blockquote>\\n\\n<p>When I run the process, there are no results. I've never had such an issue before with RapidMiner and can't find anything on the issue when I Google it or search Stackoverflow.</p>\\n-Naive Bayes Results not Generating in RapidMiner-<naivebayes><rapidminer>\",\n",
       " '<p>I have a wide table filled with ID numbers (starting with a variable number of zeros) and I want to import it into KNIME but the columns are automatically detected as Integer. I tried to manually modify the <code>settings.xml</code> file corresponding to the import node in order to enforce a String type import without spending my afternoon clicking on each column, every time I get a new file. The entry is now:</p>\\n\\n<pre><code>&lt;entry key=\"cell_class\" type=\"xstring\" value=\"org.knime.core.data.def.StringCell\"/&gt;\\n</code></pre>\\n\\n<p>I get an error when re-opening the workflow. So I also modified the <code>MissValuePattern</code> entry to:</p>\\n\\n<pre><code>&lt;entry key=\"MissValuePattern\" type=\"xstring\" value=\"?\"/&gt;\\n</code></pre>\\n\\n<p>Still getting an error when re-opening the workflow. I don\\'t see any difference between a string and an integer column so I\\'m a bit stuck.</p>\\n-KNIME manually modify node settings-<knime>',\n",
       " '<p>I need to merge the data from annual tables into one large table. I am confused about how dynamic SQL (or else) should be used for this.</p>\\n\\n<p>All this is in MonetDB, which follows the SQL 2008 standard, I think. I am not sure they support partitioning though, and I would rather merge my tables in any case.</p>\\n\\n<p>The discussion on <code>SELECT * FROM sales + @yymm</code> in <a href=\"http://www.sommarskog.se/dynamic_sql.html\" rel=\"nofollow noreferrer\">The Curse and Blessings of Dynamic SQL</a> does not mention a solution in the end.</p>\\n\\n<p>I received some guidance about the relevant pieces from a MonetDB expert in a comment below <a href=\"https://dba.stackexchange.com/a/65174/38399\">his answer</a> over on DBA, but without the loop over years, which I still need.</p>\\n\\n<p>Think of my data having tables like <code>CIVIL_1969</code>, <code>CIVIL_1970</code> … <code>CIVIL_2012</code>. These usually follow the same schema, but have no <code>year</code> column. I would want to have a single <code>CIVIL</code> table, with a <code>year</code> columns as well.</p>\\n\\n<p>By the way, there are tables where the schema do change from year to year (e.g. tax forms collected different records for different years). Is it possible to merge these tables as well? Sure, some of the columns would have sparse records, missing for many years.</p>\\n\\n<p>Some very tentative pseudocode on this:</p>\\n\\n<pre><code>USE dbfarm\\nDECLARE @i INT\\nSET @i = 1990\\nSELECT name FROM tables WHERE name LIKE \\'data_@i\\';\\nWHILE @i &lt; 2013\\nDO\\n    ALTER TABLE data_@i ADD COLUMN \"year\" INTEGER; UPDATE data_@i SET \"year\" = @i;\\n    SET @i = @i +1\\nEND WHILE\\nCREATE TABLE data AS SELECT * FROM data_1990 UNION ALL SELECT * FROM data_1991 UNION ALL [...] WITH DATA;\\n</code></pre>\\n-dynamic sql: merge partitioned tables (annual tables into a single table of all years)-<sql><union><dynamic-sql><database-partitioning><monetdb>',\n",
       " '<p>I am using Eclipse Kepler on Ubuntu 14.04.</p>\\n\\n<p>I was trying to import Openjdk8 project following \\n<a href=\"https://java.net/projects/adoptopenjdk/pages/EclipseProjectsForOpenJDK\" rel=\"nofollow\">https://java.net/projects/adoptopenjdk/pages/EclipseProjectsForOpenJDK</a></p>\\n\\n<p>and in the process was trying to create <b>swing</b> project. But kepler stopped unexpectedly while creating <b>a new java project from existing ant build file.</b></p>\\n\\n<p>When I re-started Kepler, it refused to start. I checked in .log file for some error and found </p>\\n\\n<pre><code>!ENTRY org.eclipse.osgi 4 0 2014-05-18 11:27:24.097\\n!MESSAGE Application error\\n!STACK 1\\norg.eclipse.swt.SWTError: Not implemented [multiple displays]\\nat org.eclipse.swt.SWT.error(SWT.java:4423)\\nat org.eclipse.swt.widgets.Display.checkDisplay(Display.java:767)\\nat org.eclipse.swt.widgets.Display.create(Display.java:908)...\\n</code></pre>\\n\\n<p>Since error occurred while creating <b>swing</b> project, I decided to remove it manually by renaming it at.</p>\\n\\n<ul>\\n<li>current Workspace.</li>\\n<li>.metadata/org.eclipse.core.resources/.projects </li>\\n</ul>\\n\\n<p>When I re-started Kepler. I was getting error related to project <b>swing</b>.</p>\\n\\n<pre><code>!SUBENTRY 1 org.eclipse.core.resources 4 567 2014-05-18 11:27:23.553\\n!MESSAGE Could not read metadata for \\'swing\\'.\\n!STACK 1\\n org.eclipse.core.internal.resources.ResourceException(~/.../.metadata/.plugins/org.eclipse.core.resources/.projects/swing/1.tree)[568]: java.io.FileNotFoundException: ~/.../.metadata/.plugins/org.eclipse.core.resources/.projects/swing/1.tree (No such file or directory)\\nat java.io.FileOutputStream.open(Native Method)\\n\\n\\n!MESSAGE Could not write metadata for \\'/swing\\'.\\n!STACK 0\\njava.io.FileNotFoundException:     \\n~/.../.metadata/.plugins/org.eclipse.core.resources/.projects/swing/1.tree (No such file or directory)\\n...\\n\\nContains: The project description file (.project) for \\'swing\\' is missing.  This file   contains important information about the project.  The project will not function properly   until this file is restored.\\n!SUBENTRY 2 org.eclipse.core.resources 4 271 2014-05-18 11:27:24.608\\n!MESSAGE Errors occurred while refreshing resources with the local file system.\\n</code></pre>\\n\\n<p>I have already created a new Workspace as suggested in other posts and it is working fine.</p>\\n\\n<p>But I am curious to know where else my project references are stored and how can I remove them manually to work on existing Workspace.</p>\\n-how to remove custom eclipse project and all its references externally-<eclipse><eclipse-kepler>',\n",
       " '<p>I have a WAR project which uses \\'org.reflections\\' JAR, and it throws this error:</p>\\n\\n<p>org.reflections.Reflections - could not create Vfs.Dir from url. ignoring the exception and continuing.</p>\\n\\n<p>This only happens with this WAR, I have others that work perfectly.</p>\\n\\n<p>The weird thing is that I downloaded Reflections project from google repository, and if I add it to my WAR build path as a project instead of adding the JAR, it works fine. The JAR is exported from this project so the version is the same.</p>\\n\\n<p>Any ideas? I thought there may be some credential issues with the JAR, but as in other WAR is working, that is no option.</p>\\n\\n<p>I\\'m using eclipse Kepler 64 bits.</p>\\n\\n<p>Thank you!\\nRegards.</p>\\n-\"could not create Vfs.Dir\" in WAR project with JAR-<java><jar><kepler><reflections>',\n",
       " '<p>I have a column of data in [DATE] format. It is a record of the first time an order was purchased. I am attempting to query the average date in this column. Meaning, I want to know what the average \"first purchase\" is.</p>\\n\\n<pre><code>Purchase_dt\\n01-01-2014\\n02-01-2014\\n03-05-2014\\n</code></pre>\\n\\n<p>I need something to show what the average purchase_dt is.</p>\\n\\n<p>Cheers</p>\\n-Average of DATE field in Teradata-<sql><date><teradata>',\n",
       " '<p>I get an error while trying to update Eclipse (Kepler Service Release 1; Build id: 20130919-0819)</p>\\n<p>Eclipse wants to update the following package:</p>\\n<ul>\\n<li>Eclipse Standard/SDK 2.0.2.20140224-0000</li>\\n</ul>\\n<blockquote>\\n<p>An error occurred while uninstalling</p>\\n<p>session context was:(profile=epp.package.standard, phase=org.eclipse.equinox.internal.p2.engine.phases.Uninstall, operand=[R]org.eclipse.rcp.configuration_root.win32.win32.x86 1.0.0.v20130521-1847 --&gt; null, action=org.eclipse.equinox.internal.p2.touchpoint.natives.actions.CleanupzipAction).</p>\\n<p>Backup of file E:\\\\Program Files (x86)\\\\eclipse\\\\eclipse.exe failed.</p>\\n<p>Can not remove : E:\\\\Program Files (x86)\\\\eclipse\\\\eclipse.exe</p>\\n</blockquote>\\n<p>This error also occurs when I try to update as administrator.</p>\\n<p>Does anybody have any idea what would cause this?</p>\\n-Error while updating Eclipse-<eclipse><eclipse-kepler>',\n",
       " '<p>I want to create a new WSDL in Eclipse Kepler.</p>\\n\\n<p>In project, New-->Other-->Webservices and New-->Other-->XML, I could not find them.</p>\\n\\n<p>Do I need to install any new plugin?</p>\\n-Create a WSDL in Eclipse-<xml><eclipse><web-services><wsdl><eclipse-kepler>',\n",
       " '<p>I am trying to execute following bteq command on linux environment but couldn\\'t load data properly into Teradata DB server. Can someone please advise me to resolve the below issue that I am facing while loading.</p>\\n\\n<p>BTEQ Command used :</p>\\n\\n<pre><code>.SET width 64000;\\n.SET session transaction btet;\\n.logmech ldap\\n.logon XXXXXXX/XXXXXXXX,********;\\n\\nDATABASE corecm;\\n\\n.PACK 1000\\n.IMPORT VARTEXT \\'~\\' FILE=/v/global/user/application_event_bus_evt\\n.REPEAT *\\nUSING(APPLICATION_EVENT_ID CHAR(24),BUS_EVT_ID CHAR(24),BUS_EVT_VID BIGINT,BUS_EVT_RESTATE_IN SMALLINT)\\n\\ninsert into corecm.application_event_bus_evt (APPLICATION_EVENT_ID\\n, BUS_EVT_ID\\n, BUS_EVT_VID\\n, BUS_EVT_RESTATE_IN\\n)\\nvalues\\n( COALESCE(:APPLICATION_EVENT_ID,1)\\n, COALESCE(:BUS_EVT_ID,1)\\n, COALESCE(:BUS_EVT_VID,1)\\n, COALESCE(:BUS_EVT_RESTATE_IN,1)\\n) ;\\n.LOGOFF;\\n.EXIT;\\n</code></pre>\\n\\n<p>SAMPLE INPUT FILE DELIMITTER \"~\" [ /v/global/user/application_event_bus_evt ] :</p>\\n\\n<pre><code>Ckn3gMxLEeOgIQBQVgErYA==~g+GDDtlaY3n7BdUrYshDFA==~1~1\\nCL1kEcxLEeOgIQBQVgErYA==~qoKoiuGDbClpcGt/z6RKGw==~1~1\\noYIVcMxKEeOgIQBQVgErYA==~mfmQiwl7yAteevzJfilMvA==~1~1\\n5N7ME5bM4xGhM7exj3ykUw==~yFM2FZbM4xGhM7exj3ykUw==~1~0\\nJLBH4JfM4xGDH9s5+Ds/8w==~doZ/7pfM4xGDH9s5+Ds/8w==~1~0\\nfGvpoMxKEeOgIQBQVgErYA==~mQUQIK2mY6WIPcszfp5BTQ==~1~1\\n</code></pre>\\n\\n<p>Table Definition :</p>\\n\\n<pre><code>CREATE MULTISET TABLE CORECM.APPLICATION_EVENT_BUS_EVT ,NO FALLBACK ,\\n     NO BEFORE JOURNAL,\\n     NO AFTER JOURNAL,\\n     CHECKSUM = DEFAULT,\\n     DEFAULT MERGEBLOCKRATIO\\n     (\\n      APPLICATION_EVENT_ID CHAR(26) CHARACTER SET LATIN NOT CASESPECIFIC NOT NULL,\\n      BUS_EVT_ID CHAR(26) CHARACTER SET LATIN NOT CASESPECIFIC NOT NULL,\\n      BUS_EVT_VID BIGINT NOT NULL,\\n      BUS_EVT_RESTATE_IN SMALLINT)\\nUNIQUE PRIMARY INDEX ( APPLICATION_EVENT_ID ,BUS_EVT_ID ,BUS_EVT_VID )\\nINDEX APPLICATION_EVENT_BUS_EVT_IDX1 ( APPLICATION_EVENT_ID )\\nINDEX APPLICATION_EVENT_BUS_EVT_IDX2 ( BUS_EVT_ID ,BUS_EVT_VID );\\n</code></pre>\\n\\n<p>Results set in DB server as,</p>\\n\\n<pre><code>    APPLICATION_EVENT_ID        BUS_EVT_ID                  BUS_EVT_VID             BUS_EVT_RESTATE_IN \\n\\n1    Ckn3gMxLEeOgIQBQVgErYA     == g+GDDtlaY3n7BdUrYshD     85,849,873,219,141,958  12,544\\n2    CL1kEcxLEeOgIQBQVgErYA     == qoKoiuGDbClpcGt/z6RK     85,849,873,219,155,783  12,544\\n3    oYIVcMxKEeOgIQBQVgErYA     == mfmQiwl7yAteevzJfilM     85,849,873,219,142,006  12,544\\n4    5N7ME5bM4xGhM7exj3ykUw     == JAf0GpbM4xGhM7exj3yk     85,849,873,219,155,797  12,288\\n5    JLBH4JfM4xGDH9s5+Ds/8w     == Du6T7pfM4xGDH9s5+Ds/     85,849,873,219,155,768  12,288\\n6    fGvpoMxKEeOgIQBQVgErYA     == mQUQIK2mY6WIPcszfp5B     85,849,873,219,146,068  12,544\\n</code></pre>\\n\\n<p>If we look at the Data, we can see two issues as,</p>\\n\\n<ol>\\n<li><p>First two column data length is 24 CHARACTERS ( as per input file ), but the issue is that it been shifted two characters in next column.</p></li>\\n<li><p>Column <code>BUS_EVT_VID</code> and <code>BUS_EVT_RESTATE_IN</code> has wrong data 85,849,873,219,141,958 and 12,544 instead of 1 and 1 respectively (this may be because first two column data got shifted)</p></li>\\n</ol>\\n\\n<p>I tried following options to resolve the above issue but couldn\\'t resolve the issue,</p>\\n\\n<ol>\\n<li>Modified the Table Definition, i.e. changed datatype to\\nCHAR(28),CHAR(24),CHAR(26) </li>\\n<li>Modified the Table Definition column\\n    datatypes to VARCHAR(24), VARCHAR(26) </li>\\n<li>Modified BTEQ command, i.e. altered datatype in below line,\\n         USING(APPLICATION_EVENT_ID CHAR(24),BUS_EVT_ID CHAR(24),BUS_EVT_VID BIGINT,BUS_EVT_RESTATE_IN SMALLINT)</li>\\n</ol>\\n\\n<p>Thanks in advance.</p>\\n-How to import data into teradata tables from delimited file using BTEQ import?-<import><teradata>',\n",
       " '<p>I installed JD-Eclipse from the update site but I can\\'t get it to work in Eclipse Kepler. I\\'ve done this with previous versions of Eclipse with no problems.</p>\\n\\n<p>The issue here is that when I go to Preferences > General > Editors > File associations, the default associated editor for the *.class files is \"Class File Viewer (default) (locked by \\'Java Class File\\' content type)\".\\nIf I set the \"Class File Editor\" as default, the changes won\\'t be saved.</p>\\n\\n<p>How can I unlock the Class File Viewer?</p>\\n-Can\\'t decompile classes using JD-Eclipse in Eclipse Kepler-<java><eclipse-kepler><jd-eclipse>',\n",
       " '<p>I tried to upload data in a CSV file to an empty table I created. But got the following error:</p>\\n\\n<pre><code>May 21, 2014 10:18:18 AM &lt;INFO&gt; Execution Progress.Initializing properties: 1%\\nMay 21, 2014 10:18:18 AM &lt;INFO&gt; Execution Progress.Initialized connection id=in_file, CsvConnection, Dialect{CSV 1.0}, properties {}: 3%\\nMay 21, 2014 10:18:19 AM &lt;INFO&gt; Execution Progress.Initialized connection id=db, JdbcConnection{com.teradata.jdbc.jdk6.JDK6_FastLoadManager_Connection}, Dialect{Teradata Teradata Database 13.10.07.24}, properties {statement.batchSize=100000}: 5%\\nMay 21, 2014 10:18:19 AM &lt;INFO&gt; Execution Progress./etl/query[1] prepared: 10%\\nMay 21, 2014 10:18:19 AM &lt;INFO&gt; Registered JMX mbean: scriptella:type=etl,url=\"file:/xxx/fastload.xml\"\\nMay 21, 2014 10:18:44 AM &lt;INFO&gt; Execution Progress./etl/query[1] executed: 95%\\nMay 21, 2014 10:18:44 AM &lt;INFO&gt; Execution Progress.Complete\\nMay 21, 2014 10:18:55 AM &lt;WARNING&gt; Unable to rollback transaction for connection CsvConnection: Transactions are not supported by CsvConnection\\nMay 21, 2014 10:18:55 AM &lt;SEVERE&gt; Script /xxx/fastload.xml execution failed.\\nUnable to commit transaction - cannot flush cache\\nJDBC provider exception: Unable to commit transaction - cannot flush cache\\nError codes: [HY000, 1154]\\nDriver exception: java.sql.BatchUpdateException: [Teradata JDBC Driver] [TeraJDBC 14.10.00.17] [Error 1154] [SQLState HY000] A failure occurred while inserting the batch of rows destined for database table \"mydatabase\".\"myemptytable\". Details of the failure can be found in the exception chain that is accessible with getNextException.\\n</code></pre>\\n\\n<p>This is my fastload.xml file:</p>\\n\\n<pre><code>&lt;!DOCTYPE etl SYSTEM \"http://scriptella.javaforge.com/dtd/etl.dtd\"&gt;\\n&lt;etl&gt;\\n&lt;description&gt;Scriptella  script&lt;/description&gt;\\n&lt;properties&gt;\\n    &lt;include href=\"$config\"/&gt; &lt;!--Load from external properties file--&gt;\\n&lt;/properties&gt;\\n&lt;connection id=\"in_file\" driver=\"csv\" url=\"$inputfile\"&gt;\\n&lt;/connection&gt;\\n&lt;connection id=\"db\" driver=\"$driver\" url=\"$url\" user=\"$user\" password=\"$password\" classpath=\"lib/terajdbc4.jar\"&gt;\\nstatement.batchSize=100000\\n&lt;/connection&gt;\\n\\n&lt;query connection-id=\"in_file\"&gt;\\n&lt;!-- Empty query means select all columns --&gt;\\n&lt;script connection-id=\"db\"&gt;\\nINSERT INTO  mydatabase.myemptytable VALUES (?1,?2,?3);\\n&lt;/script&gt;\\n&lt;/query&gt;\\n\\n&lt;/etl&gt;\\n</code></pre>\\n\\n<p>This is my connection URL:</p>\\n\\n<pre><code>url=jdbc:teradata://mypath/TMODE=ANSI,CHARSET=UTF8,TYPE=FASTLOAD\\n</code></pre>\\n\\n<p>Uploading without FASTLOAD works for the same file. \\nI tried to google the error message but did not find anything. Anyone know what\\'s the problem here? Thanks.</p>\\n-Cannot flush cache error when uploading data with scriptella to Teradata using FASTLOAD-<sql><jdbc><etl><teradata><scriptella>',\n",
       " \"<p>Eclipse <code>.classpath</code> file  adds a line for every Maven dependency that the project needs. If I update my <code>pom.xml</code> file with a new dependency, Eclipse doesn't find this dependency and only the way to make it find it is to run <code>eclipse:eclipse</code>. After running <code>eclipse:eclipse</code> the eclipse <code>.classpath</code> file has new entries for new dependencies.</p>\\n-New Maven dependency not put in Eclipse class path until I run eclipse:eclipse-<maven><classpath><eclipse-kepler>\",\n",
       " '<p>I want to be able to do something like</p>\\n\\n<pre><code>SELECT cast(my_date_col AS int) FROM my_table;\\n</code></pre>\\n\\n<p>I would like to get the integer which MonetDB uses internally, i.e. the value you\\'d find if you looked into the BAT structure and got the appropriate element in code in MonetDB\\'s GDK. Now, AFAICT, this internal value is the number of days since the Epoch, being Jan 1st on \"Year 0\" (so January 3rdt year 2 would be 366+365+2 = 732).</p>\\n\\n<p>The best I could actually manage is</p>\\n\\n<pre><code>SELECT my_date_col AS int - cast(\\'1-1-1\\' AS date) - 366 FROM my_table;\\n</code></pre>\\n\\n<p>As MonetDB won\\'t accept \"Year zero\" dates. This is rather an ugly hack, I\\'d like to do better. Help me?</p>\\n-In MonetDB, how can I get the date as an integer?-<sql><date><integer><representation><monetdb>',\n",
       " '<p>I have this task to bridge an analytics engine(like KNIME/Weka) to  a software. However, I am new to KNIME and APIs and most of CSE.\\nCould someone possibly guide me as to how do I bridge the softwares? A brief explaining KNIME APIs would be very helpful or any other tips for that matter.\\nThanks !!</p>\\n-How to configure KNIME/Weka to interact with my own software?-<api><weka><knime>',\n",
       " \"<p>I'm currently exploring Tera data Aster(Aster express 6). I've gone through documentation and blogs but no where it is explained how querying works.\\nsince it is distributed how they sql and sql-mr fetches data, do they generate map reduce jobs internally ?</p>\\n\\n<p>for example customer table has 10 records which are distributed by hash(customer id) and say they're 3 workers(nothing but nodes in asterdata), data split among them as 3 records each on two nodes and 4 on one node.</p>\\n\\n<p>In SQL \\n the simple select * from customers; will work in this case?</p>\\n\\n<p>but in aster data this query works. </p>\\n\\n<p>How does it fetches records from 3 nodes? if normal sql query can fecth records then why we need MapReduce, we can use mutliple sub queries to accomplish are tasks?</p>\\n\\n<p>If data is distributed among multiple machines then mapreduce is the only way to process data?</p>\\n\\n<p>It'll really help me if someone explains this!</p>\\n\\n<p>Pradi</p>\\n-How does SQL and SQL-MR query works internally in Teradata aster?-<sql><hadoop><mapreduce><teradata>\",\n",
       " \"<p>I am using <strong>Oracle</strong> and <strong>Teradata</strong> both databases in my Java based project. I want to setup global transaction so that I can perform operations on both the Database under one transaction. </p>\\n\\n<p>For global transactions such as JTA or atomikos database must have XA driver support. But as my findings Teradata doesn't have XA driver. </p>\\n\\n<p>So now how could I setup the global transaction and performance operations on both database under 1 transaction?</p>\\n-Global Transaction with Teradata-<java><transactions><teradata><spring-transactions><xa>\",\n",
       " '<p>I am working on a project that uses REST service(Jersey implementation) for some features like login,register etc.</p>\\n\\n<p>At the login time i want to store the current user details in DB and that is successfully done with the below piece of code:-</p>\\n\\n<pre><code>Userlogin userlogin = new Userlogin(user,loginDateTime,null,ipAddress,tokenEnc,salt);\\nuserloginBean.persist(userlogin);\\nuserLoginId = userlogin.getId();\\nlogger.log(Level.INFO, \"userLoginId: \"+userLoginId);\\n</code></pre>\\n\\n<p>Now look at the third line of the code, i am trying to get the recently inserted id from the userlogin object. But it giving me null. In 4th line when i try to print it in log it show null.</p>\\n\\n<p>I put some logs to check the execution and i found that the actual mysql query is executing after the 4th line.</p>\\n\\n<p>So please tell me how can i sure that mysql has completed its query?</p>\\n\\n<p>Also Explain me if possible ,why it is behaving like this as i use the same type code and i never get such situation.</p>\\n\\n<p>I am using eclipse Kepler, Jsf 2.2 <br/>\\nMy project is Dynamic web project not maven.<br/>\\nFor mysql query execution i am using Criteria API in DAO class.\\nI am persisting database using JPA.</p>\\n\\n<p>Here is the persist method of my UserloginBean.java class:</p>\\n\\n<pre><code>public void persist(Userlogin transientInstance) {\\n    log.log(Level.INFO, \"persisting Userlogin instance\");\\n    try {\\n        entityManager.persist(transientInstance);\\n        log.log(Level.INFO, \"persist successful\");\\n    } catch (RuntimeException re) {\\n        log.log(Level.INFO, \"persist failed\", re);\\n        throw re;\\n    }\\n}\\n</code></pre>\\n\\n<p>Here is some piece of my eclipse log:</p>\\n\\n<pre><code>2014-05-27T00:26:17.737+0530|INFO: persisting Userlogin instance\\n2014-05-27T00:26:17.813+0530|INFO: persist successful\\n2014-05-27T00:26:17.814+0530|INFO: userLoginId: null\\n2014-05-27T00:26:17.830+0530|FINE: INSERT INTO userlogin (help, ipaddress, \\nlogin, logout, token, user_id) VALUES (?, ?, ?, ?, ?, ?)\\n    bind =&gt; [6 parameters bound]\\n2014-05-27T00:26:17.842+0530|FINE: SELECT LAST_INSERT_ID()\\n</code></pre>\\n\\n<p>Thanks in advance.\\nSorry for my english mistake if i did any.</p>\\n-Stop java code execution until mysql query completes in Java?-<java><rest><criteria-api><jsf-2.2><eclipse-kepler>',\n",
       " '<p>i am using rapidminer 5.3.I took a small document which contains around three english sentences , tokenized it and filtered it with respect to the length of words.i want to write the output into a different word document.i tried using Write document utility but it is not working,it is simply writing the same original document into the new one.However when i write the output to the console,it gives me the expected answer.Something wrong with the write document utility.\\nHere is my process</p>\\n\\n<blockquote>\\n  <blockquote>\\n    <p>READ DOCUMENT --> TOKENIZE  -->  FILTER TOKENS --> WRITE DOCUMENT</p>\\n  </blockquote>\\n</blockquote>\\n-how to write output from rapidminer to a txt file?-<data-mining><text-mining><rapidminer>',\n",
       " '<p>I scripted in Python SQL calls to my MonetDB server (which I verify is running, of course). When I print the calls instead of calling them, the commands look OK, but if I run the original script, it does not crash, it does use the CPU and memory, but nothing is changed in the database, not even the first line is executed. Why?</p>\\n\\n<p>The Python script looks like this:</p>\\n\\n<pre><code># script to merge tables in MonetDB\\nimport re\\n\\nfrom monetdb import mapi\\nserver = mapi.Server()\\nserver.connect(hostname=\"localhost\", port=50000, username=\"monetdb\", password=\"monetdb\", database=\"dbfarm\", language=\"sql\")\\n\\ndef tablemerge(stub,yearlist):\\n    for year in yearlist:\\n#        server.cmd(\\'ALTER TABLE %s_%d ADD COLUMN \"year\" INTEGER DEFAULT %d;\\' % (stub,year,year))\\n        print \\'ALTER TABLE %s_%d ADD COLUMN \"year\" INTEGER DEFAULT %d;\\' % (stub,year,year)\\n        newstub = re.sub(r\\'sys.ds_chocker_lev_\\', r\\'\\', stub)\\n        if year == yearlist[0]:\\n            unioncall = \\'CREATE TABLE %s AS SELECT * FROM %s_%d \\' % (newstub,stub,year)\\n        else:\\n            unioncall += \\'UNION ALL SELECT * FROM %s_%d \\' % (stub,year)\\n    unioncall += \\';\\'\\n    server.cmd(unioncall)\\n#    print unioncall\\n    for year in yearlist:\\n        server.cmd(\\'DROP TABLE %s_%d;\\' % (stub,year))\\n#        print \\'DROP TABLE %s_%d;\\' % (stub,year)\\n    print \\'%s done.\\' % stub\\nfor stub in [\\'civandr\\']:\\n    tablemerge(\\'sys.ds_chocker_lev_%s\\' % stub,xrange(1998,2013))\\n</code></pre>\\n\\n<p>E.g. the first call would be:</p>\\n\\n<pre><code>ALTER TABLE sys.ds_chocker_lev_civandr_1998 ADD COLUMN \"year\" INTEGER DEFAULT 1998;\\n</code></pre>\\n\\n<p>But not even this happens. There is no <code>year</code> column in the table.</p>\\n\\n<p>Or could I run the script in the console with more output than what I print myself?</p>\\n-sql (MonetDB) commands from Python stalling-<python><sql><monetdb>',\n",
       " '<p>Now I am using Eclipse Kepler Version: 3.9.1.201308190730, but i don\\'t how to configure \"CodePro AnalytiX\" plugin in Eclipse Kepler. Is it possible to configure \"CodePro AnalytiX\"  in my current Eclipse Kepler?\\nAnd is \"CodePro AnalytiX\" plugin the correct tool for testing code perfomance? Otherwise suggest any other tool for code testing.</p>\\n-How to install Codepro plugin in Eclipse Kepler-<eclipse><eclipse-kepler>',\n",
       " \"<p>I have eclipse Kepler as IDE and I have installed svn plugin a long ago it could be either subclipse or subversion. I can't remember which one I installed. Is there a way easy to verify which plugin is installed?</p>\\n-How to identify which svn plugin is installed in eclipse IDE-<eclipse><svn><subclipse><eclipse-kepler>\",\n",
       " '<p>According to the Kepler whitepage, the warp size for a Kepler based GPU is 32 and each multiprocessor contains 4 warp schedulars which select two independant instructions from a chosen warp. This means that each clock cycle, 32*4*2 = 256 calculations are to be performed, but a multiprocessor only contains 192 ALUs. How are these calculations performed then?</p>\\n-CUDA Kepler: not enough ALUs-<cuda><kepler><warp-scheduler>',\n",
       " \"<p>I am trying to get the definition of table in teradata and I am using following command</p>\\n\\n<pre><code>   show table DatabaseName.TableName;\\n</code></pre>\\n\\n<p>This works fine for small tables but for large table it's not showing the full definition.\\n   Is there any other way for this.</p>\\n\\n<p>I am running this query on teradata sql assistant version 13.0</p>\\n\\n<p>please help.</p>\\n-Unable to get full definition of table in teradata-<teradata>\",\n",
       " \"<p>I am using below bteq to fetch data .</p>\\n\\n<pre><code>.Set Separator '|'\\n.SET TITLEDASHES OFF;\\n.SET NULL AS ''\\n\\n.EXPORT REPORT FILE = /app2/test.txt\\nsel\\nemp_id,\\nfloat_perc,\\nCAST( 0  AS DECIMAL(18.2) ) AS var\\nfrom emp\\n</code></pre>\\n\\n<p>I am getting below output:</p>\\n\\n<pre><code>5|.99|.00\\n4|.78|.00\\n</code></pre>\\n\\n<p>But we want output in below format:  </p>\\n\\n<pre><code>5|0.99|0.00\\n4|0.78|0.00\\n</code></pre>\\n\\n<p>Can anyone please help on this.\\nCan we replace |. with |0. in unix (Sun OS) with sed or tee?</p>\\n-Format output in bteq in teradata-<sql><unix><teradata>\",\n",
       " '<p>While trying to install HP ALI dev on Eclipse I am getting below:</p>\\n\\n<pre><code>Unable to read repository at https://hpln.hp.com/node/11065/attachment/content.xml.\\nsun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\\n</code></pre>\\n\\n<p>I tried by setting Window:>Preferences:>Network Connections:>Active Provider as Manual/Direct/Native. But after attempting in all the ways, I am unable to install HP ALI Dev on my Eclipse.</p>\\n-Installing HP ALI Dev on Eclipse Kepler-<eclipse><eclipse-kepler>',\n",
       " '<p>I am submitting queries with <code>vb script</code> (NOT sql assistant). Now I accidentally deleted that vb script file . How can I recover the queries that I submitted ? Where are they stored in Teradata ? </p>\\n-How to recover queries with Teradata?-<teradata>',\n",
       " '<p>I want to execute a Rapidminer process from Java to use the output ExampleSet (Process Result) for subsequent operations (with Java).</p>\\n\\n<p>I managed the process execution with the code below, but I don\\'t have a clue how to obtain the process Result Example Set.</p>\\n\\n<p>Ideally, I want to get any Example Set independent of the variables, but if you need to generate the metadata beforehand, will have to be.</p>\\n\\n<pre><code>package com.companyname.rm;\\n\\nimport com.rapidminer.Process;\\nimport com.rapidminer.RapidMiner;\\nimport com.rapidminer.operator.OperatorException;\\nimport com.rapidminer.tools.XMLException;\\n\\nimport java.io.File;\\nimport java.io.IOException;\\n\\npublic class RunProcess {\\n    public static void main(String[] args) {\\n        try {\\n            RapidMiner.setExecutionMode(RapidMiner.ExecutionMode.COMMAND_LINE);\\n            RapidMiner.init();\\n\\n            Process process = new Process(new File(\"//my_path/..../test_JAVA.rmp\"));\\n            process.run();\\n\\n        } catch (IOException | XMLException | OperatorException ex) {\\n            ex.printStackTrace();\\n        }\\n    }\\n}\\n</code></pre>\\n-Integration of Rapidminer with Java: Obtaining the output Example Set (Process Result)-<java><rapidminer>',\n",
       " '<p>I would like to run the equivalent of PostgreSQL\\'s</p>\\n\\n<pre><code>SELECT * FROM GENERATE_SERIES(1, 10000000)\\n</code></pre>\\n\\n<p>I\\'ve read this:</p>\\n\\n<p><a href=\"http://blog.jooq.org/2013/11/19/how-to-create-a-range-from-1-to-10-in-sql/\" rel=\"nofollow\">http://blog.jooq.org/2013/11/19/how-to-create-a-range-from-1-to-10-in-sql/</a></p>\\n\\n<p>But most suggestions there don\\'t really take an arbitrary length - the query depends on the length otherwise than by just replacing a number. Also, some suggestions do not apply in MonetDB. So, what\\'s my best course of action (if any)?</p>\\n\\n<p><strong>Notes:</strong>\\n- I\\'m using a version from February 2013. Answers about more recent features are also welcome, but are exactly what I\\'m looking for.\\n- Assume the existing tables don\\'t have enough lines; and do not assume that, say, a Cartesian product of the longest table with itself is sufficient (or alternatively, maybe that\\'s too costly to perform).</p>\\n-How do I generate a (dummy) column of arbitrary length with MonetDB?-<sql><idioms><monetdb><data-generation>',\n",
       " \"<p>Is there a way for the open source version of RapidMiner to export a process to an executable Java stand-alone file?\\nWhat I need is to deploy a process to a linux server, where it is executed regulary by a scheduler without the GUI, in batch mode. </p>\\n\\n<p>There is something for the commercial version, RapidMiner Server, but didn't find something comparable for the open source version.</p>\\n\\n<p>Can this be done somehow for with open source version? </p>\\n-RapidMiner 5 - Export job to runnable standalone?-<data-mining><bigdata><rapidminer>\",\n",
       " \"<p>I use Eclipse Kepler on Windows 8. When I open Eclipse Market place, its progressing bar still remains in the same place. I have proper internet connection. What might be the problem? Help me.. I couldn't install any new plugins..</p>\\n-Couldn't load Eclipse Marketplace in Eclipse Kepler-<eclipse><eclipse-kepler>\",\n",
       " '<p>the tutorial from IBM connections playground is not working for me\\n<a href=\"https://greenhouse.lotus.com/sbt/SBTPlayground.nsf/JavaScriptSnippets.xsp#snippet=Social_ActivityStreams_Controls_Simple_Stream_All_Extensions\" rel=\"nofollow\">https://greenhouse.lotus.com/sbt/SBTPlayground.nsf/JavaScriptSnippets.xsp#snippet=Social_ActivityStreams_Controls_Simple_Stream_All_Extensions</a></p>\\n\\n<p>this works</p>\\n\\n<pre><code>require([\"sbt/dom\", \"sbt/config\", \"sbt/connections/controls/astream/ActivityStreamWrapper\"], function(dom, config, ActivityStreamWrapper) {\\nvar activityStreamWrapper = new ActivityStreamWrapper({\\n    feedUrl: \"/basic/rest/activitystreams/@public/@all/@all?rollup=true\"\\n});\\n\\ndom.byId(\"activityStreamDiv\").appendChild(activityStreamWrapper.domNode);\\nactivityStreamWrapper.startup();\\n});\\n</code></pre>\\n\\n<p>efter adding the extension property in configuration, it stops working, nothing displays and no script error can be seen from developer tool, </p>\\n\\n<pre><code>require([\"sbt/dom\", \"sbt/config\", \"sbt/connections/controls/astream/ActivityStreamWrapper\"], function(dom, config, ActivityStreamWrapper) {\\nvar activityStreamWrapper = new ActivityStreamWrapper({\\n    feedUrl: \"/basic/rest/activitystreams/@public/@all/@all?rollup=true\",\\n    extensions: {\\n        refreshButton: true\\n    }\\n});\\n\\ndom.byId(\"activityStreamDiv\").appendChild(activityStreamWrapper.domNode);\\nactivityStreamWrapper.startup();\\n});\\n</code></pre>\\n\\n<p>I am using Xpages to render the activity stream, the component in use to render all the JS and CSS files is named xe:sbtClient, xpages is using dojo 1.8.6, the rendered sbt js file is dojo 1.4.3. don\\'t know if that is the problem. Here is the complete soucre code in my applicatoin. </p>\\n\\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n&lt;xp:view xmlns:xp=\"http://www.ibm.com/xsp/core\"\\nxmlns:xe=\"http://www.ibm.com/xsp/coreex\" dojoParseOnLoad=\"true\"\\nxmlns:xc=\"http://www.ibm.com/xsp/custom\"&gt;\\n\\n&lt;xe:sbtClient id=\"sbtClient1\" loadDojo=\"true\" loadCSS=\"true\"&gt;\\n&lt;/xe:sbtClient&gt;\\n&lt;div id=\"activityStreamDiv\" style=\"height:100%;\"&gt;&lt;/div&gt;\\n\\n&lt;xp:scriptBlock id=\"scriptBlock1\"&gt;\\n    &lt;xp:this.value&gt;&lt;![CDATA[\\nrequire([\"sbt/dom\", \"sbt/config\", \"sbt/connections/controls/astream /ActivityStreamWrapper\"], function(dom, config, ActivityStreamWrapper) {\\nvar activityStreamWrapper = new ActivityStreamWrapper({\\n    feedUrl: \"/basic/rest/activitystreams/@public/@all/@all?rollup=true\",\\n    extensions: {\\n        refreshButton: true\\n    } \\n});\\ndom.byId(\"activityStreamDiv\").appendChild(activityStreamWrapper.domNode);\\nactivityStreamWrapper.startup();\\n});]]&gt;&lt;/xp:this.value&gt;\\n&lt;/xp:scriptBlock&gt;\\n\\n\\n&lt;/xp:view&gt;\\n</code></pre>\\n-SBT javascript library-<xpages><ibm-sbt>',\n",
       " '<p>Although, not the ideal way, text search in all documents using <kbd>Ctrl</kbd> + <kbd>Shift</kbd> + <kbd>L</kbd> was a great convenience in Eclipse and neither is it working in Eclipse Kepler nor can I find a key binding to do the same.</p>\\n\\n<p>The next closest option is <kbd>Ctrl</kbd> + <kbd>Alt</kbd> + <kbd>G</kbd> that searches text that you have already selected or <kbd>Ctrl</kbd> + <kbd>H</kbd> and then full search a resource\\n<img src=\"https://i.stack.imgur.com/9Qej6.png\" alt=\"Eclipse Kepler key binding\"></p>\\n\\n<p><img src=\"https://i.stack.imgur.com/pf2cS.png\" alt=\"Eclipse Juno key binding\"></p>\\n-where has the text search shortcut Ctrl+Shift+L gone in Eclipse Kepler?-<eclipse><search><text><eclipse-kepler>',\n",
       " '<p>I am using Eclipse Modeling Tools  <code>Version: Kepler Service Release 2</code> <code>Build id: 20140224-0627</code> for programming in Java</p>\\n\\n<p>so, I would like to create new XML file as illustrated in the following link:</p>\\n\\n<p><a href=\"http://www.tutorialspoint.com/eclipse/eclipse_create_xml_file.htm\" rel=\"nofollow noreferrer\">http://www.tutorialspoint.com/eclipse/eclipse_create_xml_file.htm</a></p>\\n\\n<p>but this option not exist. This problem also regarded for Eclipse Juno in <a href=\"https://stackoverflow.com/questions/18175400/why-create-new-xml-file-option-not-exist\">this stackoverflow entry</a></p>\\n\\n<p>I tried to resolve the similar issue using the answers in the above link. I am trying to install  \"Eclipse XML Editors and Tools\" from Help ->  Install New Software using the \"The Eclipse Web Tools Platform (WTP) software repository - <a href=\"http://download.eclipse.org/webtools/repository/kepler\" rel=\"nofollow noreferrer\">http://download.eclipse.org/webtools/repository/kepler</a>\" in the \"Work with\" field. However, the installation is ignored as I have the newer version of \"Eclipse XML Editors and Tools\" &amp; \"Eclipse XML Editors and Tools SDK\". </p>\\n\\n<p>Why? I wonder how I could resolve the missing create \\'new xml file\\' issue? </p>\\n-Why create \\'new xml file\\' does not exist in Kepler 4.3?-<xml><eclipse><eclipse-plugin><eclipse-kepler><eclipse-mdt>',\n",
       " \"<p>Monetdb profiler stethoscope shows that MAL instructions are assigned to different threads.  What's the policy inside monetdb governing this? To which thread, a MAL instruction should be assigned? Is there a maximum number threads to be used? </p>\\n\\n<p>And, does monetdb manages the CPU affiliation of its threads? To which socket, a monetdb thread will be assigned?</p>\\n\\n<p>Is there any document available talking about this? I tried but didn't find any. </p>\\n\\n<p>Thanks!</p>\\n-How does Monetdb schedule its MAL instructions and threads?-<database><multithreading><monetdb>\",\n",
       " '<p>Monetdb\\'s Stethoscope is a profiling tool and has a list of trace options. In the following measured result, I get confused by some metrics. </p>\\n\\n<p>What exactly is utime, cutime, stime and cstime? They seem to be CPU time in user and kernel space but why the \\'start\\' state of a MAL instruction has such utime but the \\'end\\' state does not have? And, what\\'s its unit? </p>\\n\\n<p>What\\'s the unit of rss?</p>\\n\\n<p>blk.reads blk.writes should be the speed of I/O in terms of block. Why some instructions have non-zero blk.reads or blk.writes but their rbytes (bytes read) and wbytes (bytes written) are zero? </p>\\n\\n<p><img src=\"https://i.stack.imgur.com/APoDa.png\" alt=\"Screenshot on part of a Stethoscope measured result\"></p>\\n\\n<p>This measurement is enabled by this trace option: +SatefiITcmrwbsy which is explained below the the document online. I am not quite sure about their meanings.  </p>\\n\\n<pre><code>The trace options (default \\'ISTest\\'):\\n  S = monitor start of instruction profiling\\n  a = aggregate clock ticks per instruction\\n  e = event counter\\n  f = module.function name\\n  i = instruction counter\\n  I = interpreter thread number\\n  T = wall clock time\\n  t = ticks in microseconds\\n  c = cpu statistics (utime,ctime,stime,cstime)\\n  m = memory resources as provided by OS\\n  r = block reads\\n  w = block writes\\n  b = bytes read/written\\n  s = MAL statement\\n  y = MAL argument types\\n  p = process statistics, e.g. page faults, context switches\\n  u = user id\\n  D = Generate dot file upon query start\\n  F = Dataflow memory claims\\n</code></pre>\\n-What does the metrics c, m, r, w and b of Monetdb\\'s Stethoscope mean?-<database><cpu><cpu-usage><monetdb><cpu-time>',\n",
       " \"<p>I have a table  with date fields of <code>timestamp(6)</code> fields .  </p>\\n\\n<pre><code>create table test_time \\n(\\n  t1  timestamp(6)  format 'mm/dd/yyyy hh:mm:si' ,\\n);  \\n</code></pre>\\n\\n<p>I want to insert into this table with current date and time rounded.<br>\\ni.e. say for example if the current date time is  <code>08/07/2014 10:34:56</code> then  the value in the table should be  <code>08/07/2014 10:00:00</code> .<br>\\n(or) if current data and time is  <code>08/07/2014 10:54:56</code>   then also the value should be<br>\\n<code>08/07/2014 10:34:56</code>  </p>\\n-How to round current time in teradata and insert into timestamp(6) fields-<date><teradata><rounding>\",\n",
       " '<p>I am using <code>KNIME 2.9.4</code> and I have <code>HBase</code> installed (version - <code>0.94.8</code>) in a remote Linux server. With the host IP, I am trying to connect to <code>HBase</code>.</p>\\n\\n<p>Here is what I did.</p>\\n\\n<p>Added the \"<code>hbase-0.94.8.jar</code>\" in the preferences page and the Database driver <code>org.apache.hadoop.hbase.jdbc.Driver</code> is loaded properly.</p>\\n\\n<p>Now, the question is what to keep in the database URL. I kept <code>10.207.5.21:2181</code> as <code>10.207.5.2</code>1 is my remote ip and <code>2181</code> is the zookeeper port.</p>\\n\\n<p>I am getting the below error:</p>\\n\\n<p>Error during fetching metadata from database, reason: <code>org.knime.core.node.InvalidSettingsException: Driver</code> <code>org.apache.hadoop.hbase.jdbc.Driver</code> does not accept URL: <code>10.207.5.21:2181</code></p>\\n\\n<p>Can someone please let me know, what would have gone wrong here.</p>\\n\\n<p>Thanks in advance</p>\\n-Connecting to HBase in KNIME-<hbase><apache-zookeeper><knime>',\n",
       " '<p>The CDT repositories are different for Kepler/Juno/Indigo.<br/>\\nKepler: <a href=\"http://download.eclipse.org/tools/cdt/releases/kepler\" rel=\"nofollow\">http://download.eclipse.org/tools/cdt/releases/kepler</a> <br/>\\nJuno: <a href=\"http://download.eclipse.org/tools/cdt/releases/juno\" rel=\"nofollow\">http://download.eclipse.org/tools/cdt/releases/juno</a><br/>\\nIndigo: <a href=\"http://download.eclipse.org/tools/cdt/releases/indigo\" rel=\"nofollow\">http://download.eclipse.org/tools/cdt/releases/indigo</a><br/>\\nAlso, I noticed that CDT version for Kepler is the latest, juno and indigo have relatively older CDT versions. <br/>\\nI wanted to have latest CDT version in Juno. Will it cause any harm if I upgrade the CDT in Juno using the kepler repository?</p>\\n-Why does CDT have different repository for Kepler/Juno/Indigo?-<eclipse><eclipse-juno><eclipse-cdt><eclipse-kepler>',\n",
       " \"<p>Since X100 project has been commercialized into the Actian/VectorWise company. I wonder if it's technology remains in MonetDB's code base. </p>\\n\\n<p>In paper 'MonetDB: Two Decades of Research in Column-oriented Database Architectures', it is said that both fundamental and high risk projects are fully materialized within the MonetDB kernel and are disseminated as open source code together with the rest\\nof the MonetDB code family. Does it mean that all those rearch project have there code in MonetDB code base?</p>\\n-Does MonetDB's code contain the X100(VectorWise) 's research?-<database><monetdb>\",\n",
       " '<p>I tried calling a stored procedure using DatabaseReader node but it does not worked for me.</p>\\n\\n<p>Can anyone suggest me how I can call a stored procedure in Knime?</p>\\n\\n<p>Thanks,\\nKrishna</p>\\n-Calling Stored Procedure using Knime-<knime>',\n",
       " \"<p>All</p>\\n\\n<p>I have a main procedure which is used to invoke other sub-procedures. I've added the 'DECLARE EXIT HANDLER FOR SQLEXCEPTION' in the main procedure. But when any exception raised in sub-procedure, the HANDLER in main procedure doesn't work. </p>\\n\\n<p>So how can I catch exceptions generated in all sup-procedures?\\nvertion of my Teradata is 13.1, and the following is the simplified version of my code.</p>\\n\\n<pre><code>REPLACE PROCEDURE proc_main()\\nBEGIN\\n\\n    -- # Handl SQLException\\n    DECLARE EXIT HANDLER FOR SQLEXCEPTION\\n    BEGIN\\n        insert log table.\\n    END;\\n\\n    CALL proc_sub();\\nEND; \\n</code></pre>\\n\\n<p>Thanks!</p>\\n\\n<p>Frank Liu</p>\\n-Teradata - How to handle exceptions 'Call failed' in procedure?-<stored-procedures><exception><teradata>\",\n",
       " \"<p>I have a float-type-field which contains big number (more than 20 digits). Then I want to convert it to varchar, without rounding. I tried several queries, but nothing.</p>\\n\\n<p>Here is for example :</p>\\n\\n<pre><code>SELECT CAST(A AS VARCHAR(25)) AS B\\n    , TRIM(TRAILING '.' FROM CAST(CAST(A AS DECIMAL(25)) AS VARCHAR(25))) AS C\\n    , TRIM(LEADING '0' FROM CAST(CAST(A AS FORMAT '9(25)') AS VARCHAR(25))) AS D\\nFROM ( SELECT CAST(79999999999999999999.000 AS FLOAT) A ) t0\\n</code></pre>\\n\\n<p>And the result</p>\\n\\n<pre><code>B : 8.00000000000000E 019\\nC : 80000000000000000000\\nD : 80000000000000000000\\n</code></pre>\\n\\n<p>I expect to get '79999999999999999999'</p>\\n\\n<p>Any advises are appreciated. </p>\\n-How to convert big-number to varchar in Teradata-<casting><teradata>\",\n",
       " \"<p>When working with KNIME for testing and learning, we generally use a folder structure like</p>\\n\\n<pre><code>MyProject/\\n    MyKNIMEWorkspace/\\n    MyKNIMEDataFolder/\\n</code></pre>\\n\\n<p>on each computer. Everyone is free to write their own workflows, don't have to share them, etc. But we want to be able to send each other workflows where we don't need to change the paths to the data folder manually, if everyone follows the given structure, i.e. always looking one level higher and find the folder <code>MyKNIMEDataFolder</code> and take file <code>xyz.table</code>.</p>\\n\\n<p>I have played with <em>path variables</em> (basically fixed for each installation), with <em>workflow variables</em> and with <em>flow variables</em> (being sent around when sending the workflows), tried to connect both with the syntax from the help documentation on <em>path variables</em> (i.e. <code>${VAR}</code>) but I cannot find the proper way to fully disconnect my workflow from my local path.</p>\\n\\n<ol>\\n<li>Do you see anything wrong with our structure?</li>\\n<li>How would you deal with the problem of sending workflows around?</li>\\n</ol>\\n-How to use linked resources / path variables in KNIME-<knime>\",\n",
       " '<p>My data consists of annual tables for many workers (e.g. a <code>file2006</code>,<code>file2007</code>) incl. an employer ID as well as a firm ID. I also made a view to group firms by some calculations (<code>groups</code> below), which seem to complete fine so they are not shown. However, I thought I had the query to calculate the change in the total wage bill for each firm from 2006 to 2007 by these grouping using the code below. Instead of the blazing fast calculation of the previous views, this stalls the system for long, and finally breaks with some error message about mapping problems. What is the right way to do this then?</p>\\n\\n<pre><code>CREATE VIEW sys.change2007 (firmid,groups,wagebillchange) AS (\\nSELECT file2006.firmid,MAX(groups.groups), (\\n    SELECT SUM(file2007.wage)/SUM(file2006.wage) FROM file2006, file2007\\n    WHERE file2006.firmid = file2007.firmid\\n    )\\nFROM file2006, file2007, groups\\nWHERE file2006.firmid = file2007.firmid AND file2006.peorglopnr = groups.firmid\\nGROUP BY file2006.firmid\\n);\\n</code></pre>\\n-changes in aggregates from annual tables by grouping in sql (monetdb)-<sql><group-by><aggregate-functions><sql-view><monetdb>',\n",
       " \"<p>I have installed rJava in R and the next step is to configure RapidMiner such that it can incorporate the R extension...The variables are set as:</p>\\n\\n<p>R_HOME=/home/.../R-2.12.2\\nJAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64/jre</p>\\n\\n<p>I have exported both of these to PATH as well as $R_HOME/bin in which I also copied the libR.so file which was originally in $R_HOME/lib</p>\\n\\n<p>However when I try to configure RapidMiner with: java -jar rapidminer.jre\\nI am getting the following message in a window:</p>\\n\\n<blockquote>\\n  <p>R extension could not be initialized. Error while loading native R library. Please check PATH, R_HOME and JAVA_HOME environment variable settings.</p>\\n  \\n  <p>Reason:\\n  /home/username/R/x86_64-unknown-linux-gnu-library/2.12/rJava/jri/libjri.so: liR.so: cannot open shared object file: No such file or directory</p>\\n</blockquote>\\n\\n<p>The file /home/username/R/x86_64-unknown-linux-gnu-library/2.12/rJava/jri/libjri.so is definitely there...I have no idea what to do...please help!</p>\\n\\n<p>I am running a 64-bit Ubuntu.</p>\\n\\n<p>PS: I read in some posts that I should change my JAVA_HOME to $RAPIDMINER_HOME/jre, but there is no jre directory in my $RAPIDMINER home</p>\\n\\n<blockquote>\\n  <p>UPDATE: I reinstalled R and rJava again and before that I deleted a .RapidMiner5 file from my home and rebuilt the rapidminer directory again...this time when I start rapidminer.jar it doesn't show any error messages, instead it starts as if rJava has not been installed at all and shows no reaction!</p>\\n</blockquote>\\n-R extension on RapidMiner-<r><rapidminer>\",\n",
       " '<p>I am confused whether at-sign variables could ever work in MonetDB. Is it standard SQL or only mySQL? (See e.g. <a href=\"https://stackoverflow.com/a/7230099/938408\">this</a> answer here on SO.) MonetDB claims to support SQL:2003 (full feature list <a href=\"https://www.monetdb.org/book/export/html/314\" rel=\"nofollow noreferrer\">here</a>, hard for me to parse), but <a href=\"https://www.monetdb.org/Documentation/Manuals/SQLreference/Variables\" rel=\"nofollow noreferrer\">this</a> is what they say on variables.</p>\\n\\n<p>The following line fails in MonetDB complaining about the unexpected symbol <code>:</code>. But is there a way to get this work? I see no way to combine a <code>SET</code> (after <code>DECLARE</code>) with <code>SELECT</code>.</p>\\n\\n<pre><code>SELECT @firstq := QUANTILE(share26_2007,0.25) FROM sys.share26_2007;\\n</code></pre>\\n\\n<p>(Afterwards, the following is the intended use case:)</p>\\n\\n<pre><code>SELECT peorglopnr, CASE WHEN share26_2007 &lt; @firstq THEN 1\\n</code></pre>\\n-variables in SQL for monetdb-<mysql><sql><variables><select><monetdb>',\n",
       " '<p>Basically, is the code below efficient (if I cannot use @ variables in MonetDB), or will this call the subqueries more than once each?</p>\\n\\n<pre><code>CREATE VIEW sys.share26cuts_2007 (peorglopnr,share26cuts_2007) AS (\\nSELECT peorglopnr, CASE WHEN share26_2007 &lt; (SELECT QUANTILE(share26_2007,0.25) FROM sys.share26_2007) THEN 1\\n                        WHEN share26_2007 &lt; (SELECT QUANTILE(share26_2007,0.5) FROM sys.share26_2007) THEN 2\\n                        WHEN share26_2007 &lt; (SELECT QUANTILE(share26_2007,0.75) FROM sys.share26_2007) THEN 3\\n                        ELSE 4 END AS share26cuts_2007\\nFROM sys.share26_2007\\n);\\n</code></pre>\\n\\n<p>I would rather not use a user-defined function either, though this came up in <a href=\"https://stackoverflow.com/q/23296223/938408\">other questions</a>.</p>\\n-are SQL subqueries within CASE WHEN run once for a query, or for each row?-<sql><subquery><case><case-when><monetdb>',\n",
       " '<p>I have annual earnings (<code>loneink</code>) for 2007 for each individial ID (<code>personlopnr</code>), and I want to calculate for each company ID (<code>peorglopnr</code>) how much of the total wage bill was paid to worker born after 1980 (birth year is <code>fodelsear</code>), in aggregate. However, the code below produces a <code>share</code> column which is 0 for a vast majority of the cases and 1 for the rest. (To be clear, the code with <code>WHERE loneink &gt; 0</code> as below produces only 1s — the zero comeback without that condition and having a <code>NULLIF</code> to make sure I never divide by zero.) While there are many firms with no young workers, it is clearly not the case that all the other firms are young-only.</p>\\n\\n<p>What is wrong here? This wasn\\'t the way to generate a \"young-wage\" variable where for older workers earnings are zero, so the sum is only for the young? Or in theory this is OK, but I got the <code>CASE WHEN</code> wrong? Or the <code>SUM</code>/<code>SUM</code> misbehaves with <code>GROUP BY</code>?</p>\\n\\n<p>What is a better way to do this?</p>\\n\\n<pre><code>CREATE VIEW sys.over26_2007 (personlopnr,peorglopnr,loneink,below26_loneink) AS (\\nSELECT personlopnr,peorglopnr,loneink, CASE WHEN fodelsear &lt; 1981 THEN 0 ELSE loneink END AS below26_loneink\\nFROM sys.ds_chocker_lev_lisaindivid_2007 WHERE loneink &gt; 0\\n);\\nSELECT COUNT(*) FROM over26_2007;\\n\\nCREATE VIEW sys.share26_2007 (peorglopnr,share26_2007) AS (\\nSELECT peorglopnr, SUM(below26_loneink)/SUM(loneink)\\nFROM sys.over26_2007\\nWHERE loneink &gt; 0\\nGROUP BY peorglopnr\\n);\\n</code></pre>\\n\\n<p>My actual use case is in MonetDB, so hopefully we can stick to SQL:2003 solutions only, no mySQL or Oracle extensions.</p>\\n-calculating ratio of aggregates for subsample satisfying a condition-<sql><case><aggregate-functions><case-when><monetdb>',\n",
       " '<p>Eclipse hangs.\\nI\\'ve run jstack on the process and got the following:</p>\\n\\n<pre><code> \"main\" prio=10 tid=0x00007f531000b000 nid=0xbd3 waiting for monitor entry [0x00007f5317b69000]\\n   java.lang.Thread.State: BLOCKED (on object monitor)\\n    at org.tmatesoft.svn.core.internal.util.jna.SVNGnomeKeyring$3.callback(SVNGnomeKeyring.java:88)\\n    - waiting to lock &lt;0x0000000606027b40&gt; (a java.lang.Object)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n    at java.lang.reflect.Method.invoke(Method.java:606)\\n    at com.sun.jna.CallbackReference$DefaultCallbackProxy.invokeCallback(CallbackReference.java:455)\\n    at com.sun.jna.CallbackReference$DefaultCallbackProxy.callback(CallbackReference.java:485)\\n    at org.eclipse.swt.internal.gtk.OS._g_main_context_iteration(Native Method)\\n    at org.eclipse.swt.internal.gtk.OS.g_main_context_iteration(OS.java:2302)\\n    at org.eclipse.swt.widgets.Display.readAndDispatch(Display.java:3361)\\n    at org.eclipse.e4.ui.internal.workbench.swt.PartRenderingEngine$9.run(PartRenderingEngine.java:1113)\\n    at org.eclipse.core.databinding.observable.Realm.runWithDefault(Realm.java:332)\\n    at org.eclipse.e4.ui.internal.workbench.swt.PartRenderingEngine.run(PartRenderingEngine.java:997)\\n    at org.eclipse.e4.ui.internal.workbench.E4Workbench.createAndRunUI(E4Workbench.java:140)\\n    at org.eclipse.ui.internal.Workbench$5.run(Workbench.java:611)\\n    at org.eclipse.core.databinding.observable.Realm.runWithDefault(Realm.java:332)\\n    at org.eclipse.ui.internal.Workbench.createAndRunWorkbench(Workbench.java:567)\\n    at org.eclipse.ui.PlatformUI.createAndRunWorkbench(PlatformUI.java:150)\\n    at org.eclipse.ui.internal.ide.application.IDEApplication.start(IDEApplication.java:124)\\n    at org.eclipse.equinox.internal.app.EclipseAppHandle.run(EclipseAppHandle.java:196)\\n    at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.runApplication(EclipseAppLauncher.java:110)\\n    at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.start(EclipseAppLauncher.java:79)\\n    at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:354)\\n    at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:181)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n    at java.lang.reflect.Method.invoke(Method.java:606)\\n    at org.eclipse.equinox.launcher.Main.invokeFramework(Main.java:636)\\n    at org.eclipse.equinox.launcher.Main.basicRun(Main.java:591)\\n    at org.eclipse.equinox.launcher.Main.run(Main.java:1450)\\n    at org.eclipse.equinox.launcher.Main.main(Main.java:1426)\\n</code></pre>\\n\\n<p>Eclipse version is:</p>\\n\\n<pre><code>Eclipse Standard/SDK\\nVersion: Kepler Service Release 2\\nBuild id: 20140224-0627\\n</code></pre>\\n\\n<p>Plugins:</p>\\n\\n<pre><code>Subclipse 1.10.5 with:\\n  Subclipse (Required)  1.10.5  org.tigris.subversion.subclipse.feature.group   tigris.org\\n  Subversion Client Adapter (Required)  1.10.1  org.tigris.subversion.clientadapter.feature.feature.group   tigris.org\\n  SVNKit Client Adapter (Not required)  1.8.9   org.tigris.subversion.clientadapter.svnkit.feature.feature.group    tigris.org\\n  SVNKit Library    1.7.9.r9659_v20130411_2103  org.tmatesoft.svnkit.feature.group  TMate Software\\n</code></pre>\\n\\n<p>OS:</p>\\n\\n<pre><code>lsb_release -a\\nNo LSB modules are available.\\nDistributor ID: Ubuntu\\nDescription:    Ubuntu 14.04 LTS\\nRelease:    14.04\\nCodename:   trusty\\n</code></pre>\\n\\n<p>Any ideas how to resove it? I tried to reinstall plugins, reinstall Eclipse itself. The problem still exists.</p>\\n-Eclipse stops working, hangs and stops responding to any actions-<java><eclipse><freeze><eclipse-kepler>',\n",
       " '<p>How can I write ngrams extracted from Text to a new XLS or CSV file?</p>\\n\\n<p>The process I created is shown below. I would like to know how to connect the <code>Write Document</code> utility and at which level. In the <code>Main Process</code> or in the <code>Vector Creation</code>? Which pipe goes where?</p>\\n\\n<p><strong>Screenshot Main Process:</strong>\\n<img src=\"https://i.stack.imgur.com/Z8VL5.png\" alt=\"enter image description here\"></p>\\n\\n<p><strong>Screenshot Vector Creation process:</strong></p>\\n\\n<p><img src=\"https://i.stack.imgur.com/yvJ3z.png\" alt=\"enter image description here\"></p>\\n\\n<p><strong>Screenshot ngrams produced:</strong></p>\\n\\n<p><img src=\"https://i.stack.imgur.com/hGOkX.png\" alt=\"enter image description here\"></p>\\n\\n<p><strong>Screenshot Write Document operator:</strong></p>\\n\\n<p><img src=\"https://i.stack.imgur.com/0cUfY.png\" alt=\"enter image description here\"></p>\\n\\n<p>I am using RapidMiner Studio 6.0.003 Community Edition</p>\\n\\n<p><strong>EDIT</strong> Solution:</p>\\n\\n<p><img src=\"https://i.stack.imgur.com/NeF7v.png\" alt=\"enter image description here\"></p>\\n-RapidMiner Text Processing: How To write ngrams to file-<rapidminer>',\n",
       " '<p>In the below code snippet I keep receiving the following error in the <code>Provider</code> class. </p>\\n\\n<blockquote>\\n  <p>Type mismatch: cannot convert from DemoParamConverter to\\n  ParamConverter</p>\\n</blockquote>\\n\\n<pre><code>package com.ofss.shop.application.translators;\\n\\nimport java.lang.annotation.Annotation;\\nimport java.lang.reflect.Type;\\nimport javax.ws.rs.ext.ParamConverter;\\nimport javax.ws.rs.ext.Provider;\\n\\n@Provider\\npublic class DemoParamConverterProvider {\\n\\n    private final DemoParamConverter dpc = new DemoParamConverter();\\n\\n    public &lt;T&gt; ParamConverter&lt;T&gt; getConverter(Class&lt;T&gt; rawType,\\n            Type genericType, Annotation[] annotations) {\\n\\n        return dpc;\\n    }\\n\\n\\n}\\n\\npackage com.ofss.shop.application.translators;\\nimport javax.ws.rs.ext.ParamConverter;\\nimport com.restfully.shop.domain.JavaConversionTarget;\\npublic class DemoParamConverter implements ParamConverter&lt;JavaConversionTarget&gt; {\\n\\n    @Override\\n    public JavaConversionTarget fromString(String value) {\\n\\n        JavaConversionTarget jct = new JavaConversionTarget();\\n        jct.setJctName(value);\\n        return jct;\\n    }\\n\\n    @Override\\n    public String toString(JavaConversionTarget value) {\\n        return value.toString();\\n    }\\n\\n}\\n\\npublic class JavaConversionTarget {\\n\\n    private String jctName;\\n\\n    /**\\n     * @return the jctName\\n     */\\n    public String getJctName() {\\n        return jctName;\\n    }\\n\\n    /**\\n     * @param jctName the jctName to set\\n     */\\n    public void setJctName(String jctName) {\\n        this.jctName = jctName;\\n    }\\n\\n    @Override\\n    public String toString() {      \\n        return \"JavaConversionTarget-toString with jctName\"+ jctName;\\n    }\\n\\n}\\n</code></pre>\\n\\n<p><em>I\\'m using Eclipse Kepler, JDK1.7, Jersey2.4.</em> </p>\\n-ParamConverterProvider method return type mismatch-<jax-rs><java-7><eclipse-kepler><jersey-2.0>',\n",
       " '<p>have list of company names in an excel file. is it possible to get the respective company links in Rapidminer? If yes, kindly let me know how.</p>\\n\\n<p>Example:</p>\\n\\n<p>Company Name: Accurate Automation Corporation (input)</p>\\n\\n<p>using the company name mentioned above, i need to extract its company link which i have shown below in rapidminer.</p>\\n\\n<p>Company Link <a href=\"http://www.accurate-automation.com\" rel=\"nofollow\">http://www.accurate-automation.com</a> (output)</p>\\n\\n<p>Thank You in Advance</p>\\n\\n<p>Vijay</p>\\n-Is it possible to get the company\\'s link, Using Company Name in RapidMiner?-<rapidminer>',\n",
       " \"<p>is there a way how to beside of SQLCODE and SQLSTATE return the actual error message text?</p>\\n\\n<p>Of course I can look for the error message in DBC.ERRORMSGS by SQLCODE but clearly I am not able to resolve the error-related object names from there.</p>\\n\\n<p>Fe. all I can get from DBC.ERRORMSGS is 'Object '%VSTR' does not exist.'</p>\\n\\n<p>Is there a way how to resolve the object name so I would get something like 'Object DATABASEXOXO.TABLEXOXO does not exist.'?</p>\\n\\n<p>Thanks</p>\\n-Teradata Stored Procedure - How to get error message text from error handler-<stored-procedures><error-handling><teradata>\",\n",
       " '<p>I was trying to implement the W-Multilayer Perceptron from the Weka Rapidminer plugin. When I run it in my dataset it takes around 1.5 hours to finish training a simple 5 layer perceptron. </p>\\n\\n<p>However although the Perceptron itself seem to be working properly when I put it on the validation operator, it gets stuck in the validation phase consuming more and more memory. I left it running during the night and it has been there for 15 hours. To me it doesn\\'t make sense since after creating the mode, applying it shouldn\\'t take nearly so much time. Does anyone who understood the workings of this can tell me what\\'s happening?</p>\\n\\n<p>The way I am using the operator is the following, in my scheme it\\'s directly connected to a read database operator with only the set role operator between them.</p>\\n\\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?&gt;\\n&lt;process version=\"5.3.008\"&gt;\\n  &lt;context&gt;\\n    &lt;input/&gt;\\n    &lt;output/&gt;\\n    &lt;macros/&gt;\\n  &lt;/context&gt;\\n  &lt;operator activated=\"true\" class=\"process\" compatibility=\"5.3.008\" expanded=\"true\" name=\"Process\"&gt;\\n    &lt;process expanded=\"true\"&gt;\\n      &lt;operator activated=\"true\" class=\"split_validation\" compatibility=\"5.3.008\" expanded=\"true\" height=\"112\" name=\"Validation (6)\" width=\"90\" x=\"45\" y=\"120\"&gt;\\n        &lt;process expanded=\"true\"&gt;\\n          &lt;operator activated=\"true\" breakpoints=\"after\" class=\"weka:W-MultilayerPerceptron\" compatibility=\"5.3.001\" expanded=\"true\" height=\"76\" name=\"W-MultilayerPerceptron\" width=\"90\" x=\"69\" y=\"30\"&gt;\\n            &lt;parameter key=\"N\" value=\"100.0\"/&gt;\\n            &lt;parameter key=\"S\" value=\"30.0\"/&gt;\\n            &lt;parameter key=\"H\" value=\"5\"/&gt;\\n          &lt;/operator&gt;\\n          &lt;connect from_port=\"training\" to_op=\"W-MultilayerPerceptron\" to_port=\"training set\"/&gt;\\n          &lt;connect from_op=\"W-MultilayerPerceptron\" from_port=\"model\" to_port=\"model\"/&gt;\\n          &lt;portSpacing port=\"source_training\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"sink_model\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"sink_through 1\" spacing=\"0\"/&gt;\\n        &lt;/process&gt;\\n        &lt;process expanded=\"true\"&gt;\\n          &lt;operator activated=\"true\" class=\"apply_model\" compatibility=\"5.3.008\" expanded=\"true\" height=\"76\" name=\"Apply Model (6)\" width=\"90\" x=\"45\" y=\"30\"&gt;\\n            &lt;list key=\"application_parameters\"/&gt;\\n          &lt;/operator&gt;\\n          &lt;operator activated=\"true\" class=\"performance\" compatibility=\"5.3.008\" expanded=\"true\" height=\"76\" name=\"Performance (6)\" width=\"90\" x=\"147\" y=\"30\"/&gt;\\n          &lt;connect from_port=\"model\" to_op=\"Apply Model (6)\" to_port=\"model\"/&gt;\\n          &lt;connect from_port=\"test set\" to_op=\"Apply Model (6)\" to_port=\"unlabelled data\"/&gt;\\n          &lt;connect from_op=\"Apply Model (6)\" from_port=\"labelled data\" to_op=\"Performance (6)\" to_port=\"labelled data\"/&gt;\\n          &lt;connect from_op=\"Performance (6)\" from_port=\"performance\" to_port=\"averagable 1\"/&gt;\\n          &lt;portSpacing port=\"source_model\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"source_test set\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"source_through 1\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"sink_averagable 1\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"sink_averagable 2\" spacing=\"0\"/&gt;\\n        &lt;/process&gt;\\n      &lt;/operator&gt;\\n      &lt;portSpacing port=\"source_input 1\" spacing=\"0\"/&gt;\\n      &lt;portSpacing port=\"sink_result 1\" spacing=\"0\"/&gt;\\n    &lt;/process&gt;\\n  &lt;/operator&gt;\\n&lt;/process&gt;\\n</code></pre>\\n-Validating Weka Multilayer Perceptron in Rapidminer-<neural-network><data-mining><weka><rapidminer>',\n",
       " \"<p><strong>I am trying to connect TD with Python in Linux.</strong></p>\\n\\n<p>I got this working on Windows:</p>\\n\\n<pre><code>import pyodbc\\nconn = pyodbc.connect('DRIVER={Teradata};DBCNAME=&lt;HOST IP&gt;;UID=&lt;UID&gt;;PWD=&lt;UID&gt;;QUIETMODE=YES;')\\ncursor = conn.cursor()\\nsql = 'select * from table_name'\\ncursor.execute(sql)\\nrows = cursor.fetchall()\\nconn.close()\\nprint rows\\n</code></pre>\\n\\n<p>Steps I followed to do it on Linux:</p>\\n\\n<ol>\\n<li>Install pyodbc</li>\\n<li>Install TeraGSS, tdicu, cliv2, piom, tdodbc</li>\\n<li>Run the following:</li>\\n</ol>\\n\\n\\n\\n<pre><code>import pyodbc\\nconn = pyodbc.connect('DRIVER={Teradata};DBCNAME=&lt;HOST IP&gt;;UID=&lt;UID&gt;;PWD=&lt;UID&gt;;QUIETMODE=YES;')\\n</code></pre>\\n\\n<blockquote>\\n  <p>pyodbc.Error: ('IM002', '[IM002] [unixODBC][Driver Manager]Data source name not found, and no default driver specified (0) (SQLDriverConnect)')</p>\\n</blockquote>\\n\\n<p>Am I missing any configuration step after installations?</p>\\n\\n<p>Environment:</p>\\n\\n<p>Python 2.6.6 (r266:84292, Oct 12 2012, 14:23:48)\\n[GCC 4.4.6 20120305 (Red Hat 4.4.6-4)] on linux2</p>\\n-How to connect Teradata with Python in Linux?-<python><odbc><teradata><pyodbc><unixodbc>\",\n",
       " '<p>I have a dataset of 1000 examples, 500 positive and 500 negative. I am validating them with 0.7 split ratio, and then put them on rapidminers MP with default parameter except having two layers of 25 nodes. </p>\\n\\n<p>However when I validate it all my prediction are negative I have no idea why? Even with poor optimized MP (like in this very example) I should have getting at least a single positive prediction.</p>\\n\\n<p>Well, it\\'s the first time I am doing this on rapidminer and probably it\\'s a very basic mistake but I can\\'t find it.</p>\\n\\n<p><img src=\"https://i.stack.imgur.com/TvmDi.png\" alt=\"enter image description here\"></p>\\n\\n<p><img src=\"https://i.stack.imgur.com/UnCAt.png\" alt=\"enter image description here\"></p>\\n\\n<p>XML code: </p>\\n\\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?&gt;\\n&lt;process version=\"5.3.008\"&gt;\\n  &lt;context&gt;\\n    &lt;input/&gt;\\n    &lt;output/&gt;\\n    &lt;macros/&gt;\\n  &lt;/context&gt;\\n  &lt;operator activated=\"true\" class=\"process\" compatibility=\"5.3.008\" expanded=\"true\" name=\"Process\"&gt;\\n    &lt;process expanded=\"true\"&gt;\\n      &lt;operator activated=\"true\" class=\"split_validation\" compatibility=\"5.3.008\" expanded=\"true\" height=\"112\" name=\"Validation (6)\" width=\"90\" x=\"112\" y=\"255\"&gt;\\n        &lt;process expanded=\"true\"&gt;\\n          &lt;operator activated=\"true\" class=\"neural_net\" compatibility=\"5.3.008\" expanded=\"true\" height=\"76\" name=\"Neural Net\" width=\"90\" x=\"69\" y=\"30\"&gt;\\n            &lt;list key=\"hidden_layers\"&gt;\\n              &lt;parameter key=\"Layer\" value=\"25\"/&gt;\\n              &lt;parameter key=\"Layer2\" value=\"25\"/&gt;\\n            &lt;/list&gt;\\n            &lt;parameter key=\"training_cycles\" value=\"100\"/&gt;\\n            &lt;parameter key=\"shuffle\" value=\"false\"/&gt;\\n          &lt;/operator&gt;\\n          &lt;connect from_port=\"training\" to_op=\"Neural Net\" to_port=\"training set\"/&gt;\\n          &lt;connect from_op=\"Neural Net\" from_port=\"model\" to_port=\"model\"/&gt;\\n          &lt;portSpacing port=\"source_training\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"sink_model\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"sink_through 1\" spacing=\"0\"/&gt;\\n        &lt;/process&gt;\\n        &lt;process expanded=\"true\"&gt;\\n          &lt;operator activated=\"true\" class=\"apply_model\" compatibility=\"5.3.008\" expanded=\"true\" height=\"76\" name=\"Apply Model (6)\" width=\"90\" x=\"45\" y=\"30\"&gt;\\n            &lt;list key=\"application_parameters\"/&gt;\\n          &lt;/operator&gt;\\n          &lt;operator activated=\"true\" class=\"performance\" compatibility=\"5.3.008\" expanded=\"true\" height=\"76\" name=\"Performance (6)\" width=\"90\" x=\"147\" y=\"30\"/&gt;\\n          &lt;connect from_port=\"model\" to_op=\"Apply Model (6)\" to_port=\"model\"/&gt;\\n          &lt;connect from_port=\"test set\" to_op=\"Apply Model (6)\" to_port=\"unlabelled data\"/&gt;\\n          &lt;connect from_op=\"Apply Model (6)\" from_port=\"labelled data\" to_op=\"Performance (6)\" to_port=\"labelled data\"/&gt;\\n          &lt;connect from_op=\"Performance (6)\" from_port=\"performance\" to_port=\"averagable 1\"/&gt;\\n          &lt;portSpacing port=\"source_model\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"source_test set\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"source_through 1\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"sink_averagable 1\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"sink_averagable 2\" spacing=\"0\"/&gt;\\n        &lt;/process&gt;\\n      &lt;/operator&gt;\\n      &lt;portSpacing port=\"source_input 1\" spacing=\"0\"/&gt;\\n      &lt;portSpacing port=\"sink_result 1\" spacing=\"0\"/&gt;\\n    &lt;/process&gt;\\n  &lt;/operator&gt;\\n&lt;/process&gt;\\n</code></pre>\\n-Rapidminer\\'s Multilayer Perceptron strange results-<algorithm><neural-network><data-mining><rapidminer><perceptron>',\n",
       " '<p>I import a maven project from web and once I open the project in eclipse Kepler I have the following error in POM: </p>\\n\\n<pre><code>Missing artifact org.modeshape:modeshape-common:jar:3.7-SNAPSHOT\\n</code></pre>\\n\\n<p>any help please </p>\\n-Eclipse maven projects error-<eclipse><pom.xml><snapshot><eclipse-kepler>',\n",
       " '<p>What is the best maven archetype to start a JSF 2.2 project in Eclipse EE with?</p>\\n\\n<p>I am currently learning JSF and am looking for a clean maven archetype to start my JSF project with. I tried a few archetypes with jsf in the title, but they don\\'t seem to create a \"clean\" (no problems found) in eclipse. I am looking for an answer from experience.</p>\\n-Best startup JSF 2.2 archetype-<eclipse><maven><jsf><maven-3><eclipse-kepler>',\n",
       " \"<p>I am trying to get a resultset from Teradata stored Procedure to my Mapping.</p>\\n\\n<p>The stored procedure is to hold multiple select statements and the final output is to be sent to informatica. Below is a sample of how the select statement looks like </p>\\n\\n<pre><code>sel 'INH1' AS QC_CODE,count(*) from Table 1 \\nUNION\\nsel 'INH2' AS QC_CODE,count(*) from Table 2   \\nUNION\\nsel 'INH3' AS QC_CODE,count(*) from table 3\\n</code></pre>\\n\\n<p>I need a stored procedure that can send the output of the above query (2 columns, 3 records) to Informatica, where I can call the stored procedure in my source qualifier or through SP transformation</p>\\n\\n<p>Any help??</p>\\n-Informatica - Getting multiple records using Teradata stored procedure-<stored-procedures><teradata><informatica>\",\n",
       " '<p>Is there a way to query the <a href=\"https://www.monetdb.org/Documentation/Manuals/MonetDB/Architecture/Storagemodel\" rel=\"nofollow\" title=\"MonetDB Storage Model\">OIDs</a> in MonetDB using SQL? That is, I\\'d like to do something along the lines of </p>\\n\\n<pre><code>SELECT &lt;oid&gt;, &lt;column 1&gt;, ..., &lt;column N&gt; FROM &lt;table&gt;\\n</code></pre>\\n\\n<p>and </p>\\n\\n<pre><code>SELECT * FROM &lt;table&gt; WHERE &lt;oid&gt; IN (...)\\n</code></pre>\\n\\n<p>I looked through the documentation and source examples, but found no mention to querying OIDs or even if the OIDs are accessible in MonetDB/SQL.</p>\\n-OIDs in MonetDB-<monetdb>',\n",
       " '<p>I cannot install new softwares (plugins) in my eclipse, also I cannot update it. </p>\\n\\n<pre><code>     Eclipse Java EE IDE for Web Developers.\\n\\n     Version: Kepler Service Release 2\\n     Build id: 20140224-0627\\n</code></pre>\\n\\n<ul>\\n<li><p>Check for updates</p>\\n\\n<p>No updates were found<br>\\nSome sites could not be found.  See the error log for more detail.\\nNo repository found at <a href=\"http://update.eclemma.org/\" rel=\"nofollow\">http://update.eclemma.org/</a>.\\nNo repository found at <a href=\"http://download.eclipse.org/releases/kepler\" rel=\"nofollow\">http://download.eclipse.org/releases/kepler</a>.\\nNo repository found at <a href=\"http://veloeclipse.googlecode.com/svn/trunk/update/\" rel=\"nofollow\">http://veloeclipse.googlecode.com/svn/trunk/update/</a>.</p></li>\\n</ul>\\n\\n<p><strong>I can use the eclipse internal web-browser, and I have also set the proxy settings</strong></p>\\n-eclipse cannot update and cannot install new software-<eclipse><eclipse-plugin><eclipse-kepler>',\n",
       " '<p>I\\'m new to both knime and R so sorry if my question is a bit silly.</p>\\n\\n<p>I\\'ve created a simple workflow to:</p>\\n\\n<p>1) open multiple file and join them togheter with only one x axis (PPM) and several y axis (the name is created through the cycle from the file location)</p>\\n\\n<p>2)plot each indivual x and y couple and save them to a specific folder (with a R snippet)</p>\\n\\n<p>3)plot all the curve in a single plot (with a R snippet) with different line style</p>\\n\\n<p>The last point unfortunately is not working and I don\\'t know why.</p>\\n\\n<p>Here is the link to Gdrive with the workflow and some data</p>\\n\\n<p><a href=\"https://drive.google.com/file/d/0B6XSS-i7eUt7SlRVOWZ2RElKa0k/edit?usp=sharing\" rel=\"nofollow\">https://drive.google.com/file/d/0B6XSS-i7eUt7SlRVOWZ2RElKa0k/edit?usp=sharing</a></p>\\n\\n<p>Any help (and suggestion to improve the code) will be appreciated :)</p>\\n\\n<p>Cheers,</p>\\n\\n<p>Michele</p>\\n-R/Knime: plotting multiple curves in a single plot using a R snippet inside a KNIME loop-<r><plot><knime>',\n",
       " \"<p>I am having a rather odd problem with my C project. I need to use a variable of type size_t. </p>\\n\\n<p>For this purpose, I have included stddef.h, but my environment (Kepler) is unable to resolve type size_t. I spent efforts in verifying the include paths and indexer settings - but it wasn't quite necessary as I just found out that the other two types namely ptrdiff_t and wchar_t are being declared without any error. Kepler doesn't make any issue when I declare a variable of one of these two types. Anyone has any idea what could be wrong?</p>\\n\\n<p>Might be relevant: I am having this problem with this project only (which is on git repository). Other projects that are not, don't have any such problem. \\nAlso, I haven't done any editing in stddef.h :)</p>\\n-size_t type not resolved but other types definitions in stddef.h available-<c><eclipse-kepler><size-t>\",\n",
       " \"<p>here is what i have:</p>\\n\\n<p>SELECT * FROM Trans\\nWHERE TRANSID IN (</p>\\n\\n<p>select CAST(TRIM(FIRSTNAME) AS INT)  from Customer\\nwhere trim(firstname) between '0' and '9999999999999999'\\nAND custid not in\\n(select custid from address) )</p>\\n\\n<p>Have numerical values in this firstname column that i have to trace back to the trans table, in which the values in the firstname column are transid's.</p>\\n\\n<p>getting an error:  SELECT Failed. 2620: The format or data contains a bad character</p>\\n-Cast varchar to an integer in Teradata-<teradata>\",\n",
       " '<p>Is it possible, using streams, to have multiple unique kernels on the same streaming multiprocessor in Kepler 3.5 GPUs? I.e. run 30 kernels of size <code>&lt;&lt;&lt;1,1024&gt;&gt;&gt;</code> at the same time on a Kepler GPU with 15 SMs?</p>\\n-Concurrent, unique kernels on the same multiprocessor?-<concurrency><cuda><kepler><cuda-streams>',\n",
       " \"<p>Has anyone tried eclipse luna + adt? How is it? Is it worth upgrading? </p>\\n\\n<p>Just found out about the dark theme and it looks really sweet.</p>\\n\\n<p>And how do i upgrade from kepler to luna? I tried to check for updates doesn't seem to do it. Thanks guys!</p>\\n-Is Eclipse Luna plus ADT good-<eclipse><eclipse-kepler><eclipse-adt><eclipse-luna>\",\n",
       " '<p>I am in the feature selection stage of a class data mining project, the main objective of it is to compare several data mining techniques (Naive Baiyes, SVM,etc...). In this stage I am using a wrapper with X-Validation,like in the example below:</p>\\n\\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?&gt;\\n&lt;process version=\"5.3.008\"&gt;\\n  &lt;context&gt;\\n    &lt;input/&gt;\\n    &lt;output/&gt;\\n    &lt;macros/&gt;\\n  &lt;/context&gt;\\n  &lt;operator activated=\"true\" class=\"process\" compatibility=\"5.3.008\" expanded=\"true\" name=\"Process\"&gt;\\n    &lt;process expanded=\"true\"&gt;\\n      &lt;operator activated=\"true\" class=\"optimize_selection\" compatibility=\"5.3.008\" expanded=\"true\" height=\"94\" name=\"Optimize Selection (3)\" width=\"90\" x=\"179\" y=\"120\"&gt;\\n        &lt;parameter key=\"generations_without_improval\" value=\"100\"/&gt;\\n        &lt;parameter key=\"limit_number_of_generations\" value=\"true\"/&gt;\\n        &lt;parameter key=\"maximum_number_of_generations\" value=\"-1\"/&gt;\\n        &lt;process expanded=\"true\"&gt;\\n          &lt;operator activated=\"true\" class=\"x_validation\" compatibility=\"5.3.008\" expanded=\"true\" height=\"112\" name=\"Validation\" width=\"90\" x=\"179\" y=\"75\"&gt;\\n            &lt;process expanded=\"true\"&gt;\\n              &lt;operator activated=\"true\" class=\"naive_bayes\" compatibility=\"5.3.008\" expanded=\"true\" height=\"76\" name=\"Naive Bayes (4)\" width=\"90\" x=\"119\" y=\"30\"/&gt;\\n              &lt;connect from_port=\"training\" to_op=\"Naive Bayes (4)\" to_port=\"training set\"/&gt;\\n              &lt;connect from_op=\"Naive Bayes (4)\" from_port=\"model\" to_port=\"model\"/&gt;\\n              &lt;portSpacing port=\"source_training\" spacing=\"0\"/&gt;\\n              &lt;portSpacing port=\"sink_model\" spacing=\"0\"/&gt;\\n              &lt;portSpacing port=\"sink_through 1\" spacing=\"0\"/&gt;\\n            &lt;/process&gt;\\n            &lt;process expanded=\"true\"&gt;\\n              &lt;operator activated=\"true\" class=\"apply_model\" compatibility=\"5.3.008\" expanded=\"true\" height=\"76\" name=\"Apply Model (8)\" width=\"90\" x=\"45\" y=\"30\"&gt;\\n                &lt;list key=\"application_parameters\"/&gt;\\n              &lt;/operator&gt;\\n              &lt;operator activated=\"true\" class=\"performance\" compatibility=\"5.3.008\" expanded=\"true\" height=\"76\" name=\"Performance (8)\" width=\"90\" x=\"209\" y=\"30\"/&gt;\\n              &lt;connect from_port=\"model\" to_op=\"Apply Model (8)\" to_port=\"model\"/&gt;\\n              &lt;connect from_port=\"test set\" to_op=\"Apply Model (8)\" to_port=\"unlabelled data\"/&gt;\\n              &lt;connect from_op=\"Apply Model (8)\" from_port=\"labelled data\" to_op=\"Performance (8)\" to_port=\"labelled data\"/&gt;\\n              &lt;connect from_op=\"Performance (8)\" from_port=\"performance\" to_port=\"averagable 1\"/&gt;\\n              &lt;portSpacing port=\"source_model\" spacing=\"0\"/&gt;\\n              &lt;portSpacing port=\"source_test set\" spacing=\"0\"/&gt;\\n              &lt;portSpacing port=\"source_through 1\" spacing=\"0\"/&gt;\\n              &lt;portSpacing port=\"sink_averagable 1\" spacing=\"0\"/&gt;\\n              &lt;portSpacing port=\"sink_averagable 2\" spacing=\"0\"/&gt;\\n            &lt;/process&gt;\\n          &lt;/operator&gt;\\n          &lt;connect from_port=\"example set\" to_op=\"Validation\" to_port=\"training\"/&gt;\\n          &lt;connect from_op=\"Validation\" from_port=\"averagable 1\" to_port=\"performance\"/&gt;\\n          &lt;portSpacing port=\"source_example set\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"source_through 1\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"sink_performance\" spacing=\"0\"/&gt;\\n        &lt;/process&gt;\\n      &lt;/operator&gt;\\n      &lt;portSpacing port=\"source_input 1\" spacing=\"0\"/&gt;\\n      &lt;portSpacing port=\"sink_result 1\" spacing=\"0\"/&gt;\\n    &lt;/process&gt;\\n  &lt;/operator&gt;\\n&lt;/process&gt;\\n</code></pre>\\n\\n<p>The issue is that if I want to compare the several techniques I must guarantee that the sets generated in the Cross Validation phase are identical for all the techniques so that I know the accuracy of the results generated were made under the exact same conditions. However inside the X-Validation operator I can\\'t put more than one model creating operator, so I don\\'t know how to guarantee that. </p>\\n-Guaranteeing the same subset for several techniques in Rapidminer\\'s X-Validation-<validation><data-mining><rapidminer><cross-validation>',\n",
       " '<p>I am using Teradata via Sql Assistant. When I want to look up a relationship between two table I do the following : <code>show table table1</code> and can see the <code>create</code> statement that generated the table with all primary and foreign keys. However, this is not very convenient because I might be missing something. So, is there any way to get the Entity Relationship Diagram ? I am interested in about 20 tables. So, how can I get relationships between them ? </p>\\n-How to get ERD in Teradata?-<teradata>',\n",
       " '<p>I customised the Eclipse tool bar by visiting <code>Window &gt; Customize Perspective &gt; Tool Bar Visibility.</code> </p>\\n\\n<p>My customisation is automatically messed up in due course. When I try to re-customise, the <code>Tool Bar Visibility</code> window is not synchronised with the current status of the tool bar. This makes customisation is difficult and annoying.</p>\\n\\n<p>I am using Kepler. The recently released Luna also have the similar behaviour. The Spring Tool Suite (which is basically Eclipse) also have the similar behaviour.</p>\\n\\n<p>If this is an Eclipse bug, are there any workaround? </p>\\n\\n<p>If this is not a bug, what am I missing?</p>\\n\\n<p>[I am using Eclipse on OS X Mavericks]</p>\\n-Eclipse Tool Bar Bug-<eclipse><eclipse-kepler><spring-tool-suite><eclipse-luna>',\n",
       " '<p>Is there syntax which allows a parameter marker in the middle of a table name? For example, consider the queries</p>\\n\\n<pre><code>    sel * from t?x\\n</code></pre>\\n\\n<p>and</p>\\n\\n<pre><code>    sel * from t?x_blah.\\n</code></pre>\\n\\n<p>Both execute as </p>\\n\\n<pre><code>    sel * from t1\\n</code></pre>\\n\\n<p>if the user inputs 1. In the first query <code>x = 1</code> and in the second query <code>x_blah = 1</code>. I would like to modify the second query to set <code>x = 1</code> and execute as</p>\\n\\n<pre><code>    sel * from t1_blah.\\n</code></pre>\\n\\n<p>Is there a way to do this?</p>\\n\\n<p>Thanks!</p>\\n-Teradata SQL parameter syntax-<sql><parameters><teradata>',\n",
       " \"<p>I am trying to export a query from <code>Teradata Studio 14.10</code> / <code>Aster</code> using the <code>ncluster_export</code> command line function.  I can't find a working example to go off of.  Does anyone have one they could share?  Also how would I actually run the script (ie <code>ncluster_export myScript1.bat</code>)?</p>\\n\\n<p>Assume the following :</p>\\n\\n<pre><code>File location: 'myhomedir'\\nUsername: 'user1'\\nPassword: 'pass1'\\nOutfile name: 'outFile1.csv'\\n</code></pre>\\n-Teradata / Aster : Fast Export / ncluster_export using query-<sql><export><teradata><teradata-aster><fastexport>\",\n",
       " '<p>I have servers like tomcat and glassfish intalled in my eclipse kepler. When I right click and see properties of any server,\\nI see a property called location under the general options in eclipse. There is a switch location button next to it. \\nI can switch the location to - </p>\\n\\n<ol>\\n<li>[Workspace metadata]</li>\\n<li>Installation folder of my web server</li>\\n</ol>\\n\\n<p>What is the difference between these two ? How do I decide which one is better ?</p>\\n-Eclipse web servers - What is the difference between locations?-<java><eclipse><eclipse-kepler>',\n",
       " '<p>I\\'m using the below query in access to build a database. I want to use this query with different date ranges and a different \"ndc\" number. In the below query I used question marks but do not get results. I know I can use ? in Sql server, but teradata is not accepted the ambiguity. Can i do this somehow rather than having to specify the date range and the \"ndc\" number?  </p>\\n\\n<pre><code> select ls.str_nbr, ndc11, wic_nbr, prod_name, pkg_sz, pkg_qty, count(*) as rx_volume, sum(fill_qty_dspn), sum(fill_gross_profit_dlrs), sum(fill_revenue_dlrs), sum(fill_cost_dlrs)     \\n    from (select * from prdedwvwh.prescription_fill_sold where fill_sold_dt Between ? And ? ) as pf           \\n    inner join  \\n                    prdedwvwh.prescription_fill_sales_metric pfsm\\n    on            \\n    pf.str_nbr=pfsm.str_nbr and     \\n    pf.rx_nbr=pfsm.rx_nbr and       \\n    pf.rx_fill_nbr= pfsm.rx_fill_nbr and            \\n    pf.rx_partial_fill_nbr =pfsm.rx_partial_fill_nbr and      \\n    pf.fill_enter_dt=pfsm.fill_enter_dt and       \\n    pf.fill_sold_dt=pfsm.fill_sold_dt                 \\n    inner join  \\n                    prdedwvwh.drug_cur dd\\n    on            \\n                    pf.drug_id=dd.drug_id\\n    and ndc11 in (\\'?\\') \\n    inner join  \\n                    prdedwvwh.location_store ls \\n    on            \\n                    pf.str_nbr=ls.str_nbr \\n    and ls.mail_retail_ind =\\'RETAIL\\'               \\n\\n    group by 1,2,3,4,5,6     \\n</code></pre>\\n-How can I use \"?\" in teradata so that I am prompted to put in new data each time I run the query?-<teradata>',\n",
       " \"<p>I've met the problem when using execute immediate in Teradata.</p>\\n\\n<pre><code>SET str_sql = 'UPDATE TABLE\\n                    SET COLA = 0';\\nEXECUTE IMMEDIATE str_sql;\\n</code></pre>\\n\\n<p>The above code works fine.</p>\\n\\n<pre><code>SET str_sql = 'UPDATE TABLE\\n                    SET COLA = 0,\\n                        COLB = ''test''';\\nEXECUTE IMMEDIATE str_sql;\\n</code></pre>\\n\\n<p>The above code with string returns error.</p>\\n\\n<p>The following is the error message:</p>\\n\\n<pre><code>Executed as Single statement.  Failed [3706 : 42000] Table:Syntax error: expected   something between a string or a Unicode character literal and the word 'test'. \\nElapsed time = 00:00:00.212 \\n\\nSTATEMENT 1: CALL  failed. \\n</code></pre>\\n\\n<p>Anyone know how to invoke the execute immediate with String in the sql?\\nThanks!</p>\\n\\n<p>Frank Liu</p>\\n-execute immediate with string in sql statement in teradata-<sql><teradata><dynamic-sql><execute-immediate>\",\n",
       " \"<p>Using KNIME, I would like to analyze data in a specific subset of columns in my database \\nbut without using limiting SQL queries such as</p>\\n\\n<p>Select *\\nFrom table\\nWhere name like 'PAIN%'</p>\\n\\n<p>Is there a way to do this in KNIME?</p>\\n-passing a variable to SQL statement in KNIME-<mysql><sql><variables><parameter-passing><knime>\",\n",
       " '<p>It\\'s the x-th time that saving my KNIME workflow gets stuck at \"Weka Predictor (3.7) - Internals\". It is always a Weka node blocking the saving process, eventually I have to force KNIME to quit and lose my changes.</p>\\n\\n<p>My memory status is roughly 900MB out of 3300 currently, 6000MB being the maximum allowance for my Java VM. KNIME gets 5% of my CPU power, nothing else is running.</p>\\n\\n<p>Anybody experiencing the same problem? Any advice or solution?</p>\\n-KNIME: Saving workflow stuck at Weka Predictor Internals-<knime>',\n",
       " \"<p>I've Eclipse Kepler SR2 on SUSE Linux server which has the below components:</p>\\n\\n<p>C/C++ Development Tools (CDT) - 8.3\\nRational Clearcase plug-ins - 7.6.2.v201309301552\\nJRE 1.6.0_45</p>\\n\\n<p>I'm using the vmargs as below:\\n-vmargs -XX:MaxPermSize=512m -Xms4096m -Xmx4096m </p>\\n\\n<p>I'm trying to launch Kepler SR2 with the Juno workspace. But the first time launch took nearly 15mins.Why it took that much time even though my -vm settings are pretty good? From the second launch onwards, it took below 20 secs.</p>\\n\\n<p>Even after the launch, when I try to edit the C/C++ source files, Kepler is responding very slow. Even a single click on the variables, taking 20 to 30 sec time to highlight the variable. This is happening only to the source files which has huge file size(nearly 1MB) and when the content is still not saved. No performance issues with the smaller files.The used heap is fluctuating between 300MB and 1100MB. </p>\\n\\n<p>Found another scenario: Kepler is sluggish when we switch between editors and placing the cursor on the variables. Hover doesn't display the popup at all, single click doesn't highlight the variables for nearly 50 to 60 sec, also double click doesn't select the variable for nearly 50 to 60 sec.</p>\\n\\n<p>How to solve the sluggish behavior?</p>\\n-Eclipse Kepler SR2 is sluggish on Linux while editing large source files-<eclipse><eclipse-plugin><eclipse-cdt><eclipse-kepler>\",\n",
       " '<p>I added the primefaces.jar in to the classpath of the javaserver faces project. But in the web page editor palette the primefaces tab is empty!.<br>\\n<img src=\"https://i.stack.imgur.com/spRru.png\" alt=\"empty primefaces tab\">  </p>\\n\\n<p>The JSF HTML tabs are full with standard JSF components. As shown below!<br>\\n<img src=\"https://i.stack.imgur.com/9O6xZ.png\" alt=\"JSF HTML tab is full\">  </p>\\n\\n<p>I checked this question as well: <a href=\"https://stackoverflow.com/questions/13841769/cannot-see-primefaces-components-in-visual-tab-of-eclipse\">Cannot see primefaces components in visual tab of eclipse</a>.<br>\\nI really can get why the web editor does not preview the PrimeFaces components, since they are \"hot deployed on the fly\" as mentioned in the linked question. But shouldn\\'t the components be visible in the palette so we could add them by drag &amp; drop.</p>\\n-PrimeFaces components does not show up in the eclipse kepler palette-<jsf><jsf-2><primefaces><eclipse-kepler>',\n",
       " '<p>I would like tokenize and apply stop word filter on Twitter comments contained in a database, but Process Document does nothing. What am I doing wrong?</p>\\n\\n<p>My goal is to apply these filters but keep the comments in rows instead of a single word vector.</p>\\n\\n<pre class=\"lang-xml prettyprint-override\"><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?&gt;\\n&lt;process version=\"5.3.015\"&gt;\\n  &lt;context&gt;\\n    &lt;input/&gt;\\n    &lt;output/&gt;\\n    &lt;macros/&gt;\\n  &lt;/context&gt;\\n  &lt;operator activated=\"true\" class=\"process\" compatibility=\"5.3.015\" expanded=\"true\" name=\"Process\"&gt;\\n    &lt;process expanded=\"true\"&gt;\\n      &lt;operator activated=\"true\" class=\"read_database\" compatibility=\"5.3.015\" expanded=\"true\" height=\"60\" name=\"Server Connection (2)\" width=\"90\" x=\"45\" y=\"30\"&gt;\\n        &lt;parameter key=\"connection\" value=\"sqlserver2014\"/&gt;\\n        &lt;parameter key=\"query\" value=\"select top 60 tweetid,content from [Tweets General]\"/&gt;\\n        &lt;enumeration key=\"parameters\"/&gt;\\n      &lt;/operator&gt;\\n      &lt;operator activated=\"true\" class=\"text:data_to_documents\" compatibility=\"5.3.002\" expanded=\"true\" height=\"60\" name=\"Data to Documents\" width=\"90\" x=\"246\" y=\"30\"&gt;\\n        &lt;parameter key=\"select_attributes_and_weights\" value=\"true\"/&gt;\\n        &lt;list key=\"specify_weights\"/&gt;\\n      &lt;/operator&gt;\\n      &lt;operator activated=\"true\" class=\"text:process_documents\" compatibility=\"5.3.002\" expanded=\"true\" height=\"94\" name=\"Process Documents\" width=\"90\" x=\"447\" y=\"30\"&gt;\\n        &lt;process expanded=\"true\"&gt;\\n          &lt;operator activated=\"true\" class=\"text:tokenize\" compatibility=\"5.3.002\" expanded=\"true\" height=\"60\" name=\"Tokenize (3)\" width=\"90\" x=\"246\" y=\"75\"/&gt;\\n          &lt;connect from_port=\"document\" to_op=\"Tokenize (3)\" to_port=\"document\"/&gt;\\n          &lt;connect from_op=\"Tokenize (3)\" from_port=\"document\" to_port=\"document 1\"/&gt;\\n          &lt;portSpacing port=\"source_document\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"sink_document 1\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"sink_document 2\" spacing=\"0\"/&gt;\\n        &lt;/process&gt;\\n      &lt;/operator&gt;\\n      &lt;connect from_op=\"Server Connection (2)\" from_port=\"output\" to_op=\"Data to Documents\" to_port=\"example set\"/&gt;\\n      &lt;connect from_op=\"Data to Documents\" from_port=\"documents\" to_op=\"Process Documents\" to_port=\"documents 1\"/&gt;\\n      &lt;connect from_op=\"Process Documents\" from_port=\"example set\" to_port=\"result 1\"/&gt;\\n      &lt;portSpacing port=\"source_input 1\" spacing=\"0\"/&gt;\\n      &lt;portSpacing port=\"sink_result 1\" spacing=\"0\"/&gt;\\n      &lt;portSpacing port=\"sink_result 2\" spacing=\"0\"/&gt;\\n    &lt;/process&gt;\\n  &lt;/operator&gt;\\n&lt;/process&gt;\\n</code></pre>\\n-Tokenize and stopword don\\'t work in Tweets DB using RapidMiner-<twitter><tokenize><stop-words><rapidminer><text-classification>',\n",
       " '<p>I have tried installing m2e-wtp plugin but getting below error:-</p>\\n\\n<pre><code>STDERR: Cannot complete the install because of a conflicting dependency.\\n Software being installed: m2e-wtp - Maven Integration for WTP 1.0.1.20130911-1545 (org.eclipse.m2e.wtp.feature.feature.group 1.0.1.20130911-1545)\\n Software currently installed: m2e - Maven Integration for Eclipse 1.3.0.20130129-0926 (org.eclipse.m2e.feature.feature.group 1.3.0.20130129-0926)\\n Only one of the following can be installed at once:\\n  Maven Integration for Eclipse JDT 1.4.0.20130601-0317 (org.eclipse.m2e.jdt 1.4.0.20130601-0317)\\n  Maven Integration for Eclipse JDT 1.2.0.20120903-1050 (org.eclipse.m2e.jdt 1.2.0.20120903-1050)\\n  Maven Integration for Eclipse JDT 1.1.0.20120530-0009 (org.eclipse.m2e.jdt 1.1.0.20120530-0009)\\n  Maven Integration for Eclipse JDT 1.4.1.20140328-1905 (org.eclipse.m2e.jdt 1.4.1.20140328-1905)\\n  Maven Integration for Eclipse JDT 1.3.0.20130129-0926 (org.eclipse.m2e.jdt 1.3.0.20130129-0926)\\n\\nCannot satisfy dependency:\\n  From: m2e - Maven Integration for Eclipse 1.3.0.20130129-0926 (org.eclipse.m2e.feature.feature.group 1.3.0.20130129-0926)\\n  To: org.eclipse.m2e.jdt [1.3.0.20130129-0926]\\n\\nCannot satisfy dependency:\\n  From: Maven Integration for Eclipse WTP 1.0.1.20130911-1545 (org.eclipse.m2e.wtp 1.0.1.20130911-1545)\\n  To: bundle org.eclipse.m2e.jdt [1.4.0,2.0.0)\\n\\nCannot satisfy dependency:\\n  From: m2e-wtp - Maven Integration for WTP 1.0.1.20130911-1545 (org.eclipse.m2e.wtp.feature.feature.group 1.0.1.20130911-1545)\\n  To: org.eclipse.m2e.wtp [1.0.1.20130911-1545]\\n</code></pre>\\n-error while installing m2e-wtp plugin for eclipse kepler-<eclipse-plugin><m2e><eclipse-kepler><m2e-wtp>',\n",
       " '<p>I\\'m currently working with RapidMiner 6 and at the end of a process I have to pipe out the Data Set to a Java Program.</p>\\n\\n<p>In the documentation says that a process like : \"Write CSV -> Execute Program\" will send the data set through the standard output to the program I\\'m executing, but my cuestion is: How can I know, in my Java program, when the data has been sent?</p>\\n\\n<p>The current program just reads from the standard input and saves what was written as a csv file. The code is:</p>\\n\\n<pre><code>public class WakaMain {\\n\\npublic static void main(String[] args) throws IOException {\\n    ArrayList&lt;String&gt; input = new ArrayList&lt;String&gt;();\\n    Scanner in = new Scanner(System.in);\\n    String inLine = in.nextLine();\\n    while(inLine != null){\\n        input.add(inLine);\\n        inLine = in.nextLine();\\n    }\\n    in.close();\\n    writeStream(input);\\n    System.out.println(\"Done: \"+input.size()+\" Inputs\");\\n}\\n\\npublic static void writeStream(ArrayList&lt;String&gt; in) throws IOException{\\n    FileWriter fw = new FileWriter(\"StreamWriteOutput.csv\");\\n    PrintWriter pw = new PrintWriter(fw);\\n    for(int i = 0; i&lt;in.size();i++){\\n        pw.println(in.get(i));\\n    }\\n    pw.close();\\n}}\\n</code></pre>\\n\\n<p>I first thought \" inLine != null \" would work as it does while reading a text file, but it doesn\\'t.</p>\\n\\n<p>Do you know how can I do this?</p>\\n\\n<p>Thanks, guys.</p>\\n-RapidMiner ExecuteProgram Operator Text Stream-<java><input><stream><operator-keyword><rapidminer>',\n",
       " '<p>I want to load data from SQL to Teradata using SSIS. As per my R&amp;D I got to know that we can use third party tool which is provided in the link,   <a href=\"http://www.attunity.com/products/attunity-connect/ssis-connectors-for-oracle-and-teradata\" rel=\"nofollow\">http://www.attunity.com/products/attunity-connect/ssis-connectors-for-oracle-and-teradata</a></p>\\n\\n<p>IS there any other way to load the data into <strong>Teradata</strong> using <strong>SSIS</strong>?? </p>\\n\\n<p>Thanks<br>\\nVenky</p>\\n-Loading data into teradata using SSIS-<sql-server><ssis><teradata>',\n",
       " '<p>I\\'m trying to use SVMs in RapidMiner to classify tweets contained in a database (TASS 2014 corpus), however, regardless of what that says Performance module, the applied model ends always evaluating all tweets in one way (ie all positive, all negative, all neutral, or all none). Not sure if I\\'m taking a misconfiguration. </p>\\n\\n<p><a href=\"http://www.subirimagenes.net/i/140711042146793099.png\" rel=\"nofollow\">After Multipliy operator</a>, roles outputs are (first roles, second name, third type): </p>\\n\\n<ul>\\n<li>text, text, text </li>\\n<li>label, value, nominal </li>\\n<li>id, tweetid, nominal </li>\\n</ul>\\n\\n<p><a href=\"http://www.subirimagenes.net/i/140711042147212040.png\" rel=\"nofollow\">After September Operator Role (2)</a>, roles outputs are (roles first, second name, third type): </p>\\n\\n<ul>\\n<li>label, text, text </li>\\n<li>id, tweetid, nominal</li>\\n</ul>\\n\\n<p>Then, I add the XML code of RapidMiner:</p>\\n\\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?&gt;\\n&lt;process version=\"5.3.015\"&gt;\\n  &lt;context&gt;\\n    &lt;input/&gt;\\n    &lt;output/&gt;\\n    &lt;macros/&gt;\\n  &lt;/context&gt;\\n  &lt;operator activated=\"true\" class=\"process\" compatibility=\"5.3.015\" expanded=\"true\" name=\"Process\"&gt;\\n    &lt;process expanded=\"true\"&gt;\\n      &lt;operator activated=\"false\" class=\"read_database\" compatibility=\"5.3.015\" expanded=\"true\" height=\"60\" name=\"Read Database\" width=\"90\" x=\"45\" y=\"30\"&gt;\\n        &lt;parameter key=\"connection\" value=\"sqlserver2014\"/&gt;\\n        &lt;parameter key=\"query\" value=\"SELECT TOP 4246 &amp;quot;content&amp;quot;, &amp;quot;tweetid&amp;quot;, &amp;quot;value&amp;quot;&amp;#10;FROM &amp;quot;dbo&amp;quot;.&amp;quot;TweetsTrainClean&amp;quot;&amp;#10;WHERE \\'POS\\'=SUBSTRING(value,1,3)&amp;#10;UNION ALL&amp;#10;SELECT TOP 4246 &amp;quot;content&amp;quot;, &amp;quot;tweetid&amp;quot;, &amp;quot;value&amp;quot;&amp;#10;FROM &amp;quot;dbo&amp;quot;.&amp;quot;TweetsTrainClean&amp;quot;&amp;#10;WHERE \\'NEU\\'=SUBSTRING(value,1,3)&amp;#10;UNION ALL&amp;#10;SELECT TOP 4246 &amp;quot;content&amp;quot;, &amp;quot;tweetid&amp;quot;, &amp;quot;value&amp;quot;&amp;#10;FROM &amp;quot;dbo&amp;quot;.&amp;quot;TweetsTrainClean&amp;quot;&amp;#10;WHERE \\'NEG\\'=SUBSTRING(value,1,3)\"/&gt;\\n        &lt;enumeration key=\"parameters\"/&gt;\\n      &lt;/operator&gt;\\n      &lt;operator activated=\"true\" class=\"read_database\" compatibility=\"5.3.015\" expanded=\"true\" height=\"60\" name=\"Read Database (5)\" width=\"90\" x=\"45\" y=\"300\"&gt;\\n        &lt;parameter key=\"connection\" value=\"sqlserver2014\"/&gt;\\n        &lt;parameter key=\"query\" value=\"SELECT top 1000 *&amp;#10;FROM &amp;quot;dbo&amp;quot;.&amp;quot;TweetsGeneralClean&amp;quot;\"/&gt;\\n        &lt;enumeration key=\"parameters\"/&gt;\\n      &lt;/operator&gt;\\n      &lt;operator activated=\"false\" class=\"read_database\" compatibility=\"5.3.015\" expanded=\"true\" height=\"60\" name=\"Read Database (2)\" width=\"90\" x=\"45\" y=\"210\"&gt;\\n        &lt;parameter key=\"connection\" value=\"sqlserver2014\"/&gt;\\n        &lt;parameter key=\"query\" value=\"SELECT *&amp;#10;FROM (&amp;#10;     SELECT *, ROW_NUMBER() OVER (ORDER BY tweetid) AS RowNum&amp;#10;     FROM &amp;quot;dbo&amp;quot;.&amp;quot;TweetsGeneralClean&amp;quot;&amp;#10;     ) AS tabla&amp;#10;WHERE tabla.RowNum BETWEEN 30000 AND 64798\"/&gt;\\n        &lt;enumeration key=\"parameters\"/&gt;\\n      &lt;/operator&gt;\\n      &lt;operator activated=\"true\" class=\"select_attributes\" compatibility=\"5.3.015\" expanded=\"true\" height=\"76\" name=\"Select Attributes (2)\" width=\"90\" x=\"179\" y=\"255\"&gt;\\n        &lt;parameter key=\"attribute_filter_type\" value=\"subset\"/&gt;\\n        &lt;parameter key=\"attributes\" value=\"|tweetid|content\"/&gt;\\n      &lt;/operator&gt;\\n      &lt;operator activated=\"true\" class=\"text:process_document_from_data\" compatibility=\"5.3.002\" expanded=\"true\" height=\"76\" name=\"Process Documents from Data (2)\" width=\"90\" x=\"313\" y=\"255\"&gt;\\n        &lt;parameter key=\"keep_text\" value=\"true\"/&gt;\\n        &lt;list key=\"specify_weights\"/&gt;\\n        &lt;process expanded=\"true\"&gt;\\n          &lt;connect from_port=\"document\" to_port=\"document 1\"/&gt;\\n          &lt;portSpacing port=\"source_document\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"sink_document 1\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"sink_document 2\" spacing=\"0\"/&gt;\\n        &lt;/process&gt;\\n      &lt;/operator&gt;\\n      &lt;operator activated=\"true\" class=\"set_role\" compatibility=\"5.3.015\" expanded=\"true\" height=\"76\" name=\"Set Role (2)\" width=\"90\" x=\"447\" y=\"255\"&gt;\\n        &lt;parameter key=\"attribute_name\" value=\"text\"/&gt;\\n        &lt;parameter key=\"target_role\" value=\"label\"/&gt;\\n        &lt;list key=\"set_additional_roles\"&gt;\\n          &lt;parameter key=\"tweetid\" value=\"id\"/&gt;\\n        &lt;/list&gt;\\n      &lt;/operator&gt;\\n      &lt;operator activated=\"true\" class=\"read_database\" compatibility=\"5.3.015\" expanded=\"true\" height=\"60\" name=\"Read Database (3)\" width=\"90\" x=\"45\" y=\"75\"&gt;\\n        &lt;parameter key=\"connection\" value=\"sqlserver2014\"/&gt;\\n        &lt;parameter key=\"query\" value=\"select &amp;quot;content&amp;quot;,&amp;quot;tweetid&amp;quot;,&amp;quot;value&amp;quot; from &amp;quot;dbo&amp;quot;.&amp;quot;TweetsTrainClean&amp;quot;&amp;#10;where \\'POS\\'=SUBSTRING(&amp;quot;value&amp;quot;,1,3) or \\'NEG\\'=SUBSTRING(&amp;quot;value&amp;quot;,1,3)&amp;#10;&amp;#9;or \\'NEU\\'=SUBSTRING(&amp;quot;value&amp;quot;,1,3) or \\'NON\\'=SUBSTRING(&amp;quot;value&amp;quot;,1,3)&amp;#10;order by rand()\"/&gt;\\n        &lt;enumeration key=\"parameters\"/&gt;\\n      &lt;/operator&gt;\\n      &lt;operator activated=\"true\" class=\"text_to_nominal\" compatibility=\"5.3.015\" expanded=\"true\" height=\"76\" name=\"Text to Nominal\" width=\"90\" x=\"112\" y=\"120\"&gt;\\n        &lt;parameter key=\"attribute_filter_type\" value=\"single\"/&gt;\\n        &lt;parameter key=\"attribute\" value=\"value\"/&gt;\\n        &lt;parameter key=\"attributes\" value=\"|type|value\"/&gt;\\n      &lt;/operator&gt;\\n      &lt;operator activated=\"true\" class=\"set_role\" compatibility=\"5.3.015\" expanded=\"true\" height=\"76\" name=\"Set Role\" width=\"90\" x=\"179\" y=\"30\"&gt;\\n        &lt;parameter key=\"attribute_name\" value=\"tweetid\"/&gt;\\n        &lt;parameter key=\"target_role\" value=\"id\"/&gt;\\n        &lt;list key=\"set_additional_roles\"&gt;\\n          &lt;parameter key=\"value\" value=\"label\"/&gt;\\n        &lt;/list&gt;\\n      &lt;/operator&gt;\\n      &lt;operator activated=\"true\" class=\"nominal_to_text\" compatibility=\"5.3.015\" expanded=\"true\" height=\"76\" name=\"Nominal to Text\" width=\"90\" x=\"246\" y=\"120\"&gt;\\n        &lt;parameter key=\"attribute_filter_type\" value=\"single\"/&gt;\\n        &lt;parameter key=\"attribute\" value=\"content\"/&gt;\\n      &lt;/operator&gt;\\n      &lt;operator activated=\"true\" class=\"text:process_document_from_data\" compatibility=\"5.3.002\" expanded=\"true\" height=\"76\" name=\"Process Documents from Data\" width=\"90\" x=\"313\" y=\"30\"&gt;\\n        &lt;parameter key=\"keep_text\" value=\"true\"/&gt;\\n        &lt;list key=\"specify_weights\"/&gt;\\n        &lt;process expanded=\"true\"&gt;\\n          &lt;connect from_port=\"document\" to_port=\"document 1\"/&gt;\\n          &lt;portSpacing port=\"source_document\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"sink_document 1\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"sink_document 2\" spacing=\"0\"/&gt;\\n        &lt;/process&gt;\\n      &lt;/operator&gt;\\n      &lt;operator activated=\"true\" class=\"multiply\" compatibility=\"5.3.015\" expanded=\"true\" height=\"94\" name=\"Multiply\" width=\"90\" x=\"380\" y=\"120\"/&gt;\\n      &lt;operator activated=\"true\" class=\"x_validation\" compatibility=\"5.3.015\" expanded=\"true\" height=\"112\" name=\"Validation\" width=\"90\" x=\"514\" y=\"30\"&gt;\\n        &lt;parameter key=\"number_of_validations\" value=\"5\"/&gt;\\n        &lt;parameter key=\"sampling_type\" value=\"linear sampling\"/&gt;\\n        &lt;process expanded=\"true\"&gt;\\n          &lt;operator activated=\"true\" class=\"select_attributes\" compatibility=\"5.3.015\" expanded=\"true\" height=\"76\" name=\"Select Attributes\" width=\"90\" x=\"45\" y=\"30\"&gt;\\n            &lt;parameter key=\"attribute_filter_type\" value=\"no_missing_values\"/&gt;\\n            &lt;parameter key=\"attribute\" value=\"content\"/&gt;\\n            &lt;parameter key=\"attributes\" value=\"tweetid|type||content\"/&gt;\\n          &lt;/operator&gt;\\n          &lt;operator activated=\"true\" class=\"nominal_to_binominal\" compatibility=\"5.3.015\" expanded=\"true\" height=\"94\" name=\"Nominal to Binominal\" width=\"90\" x=\"45\" y=\"120\"&gt;\\n            &lt;parameter key=\"attribute_filter_type\" value=\"single\"/&gt;\\n            &lt;parameter key=\"attribute\" value=\"value\"/&gt;\\n          &lt;/operator&gt;\\n          &lt;operator activated=\"true\" class=\"polynomial_by_binomial_classification\" compatibility=\"5.3.015\" expanded=\"true\" height=\"76\" name=\"Polynominal by Binominal Classification\" width=\"90\" x=\"45\" y=\"255\"&gt;\\n            &lt;process expanded=\"true\"&gt;\\n              &lt;operator activated=\"true\" class=\"support_vector_machine_linear\" compatibility=\"5.3.015\" expanded=\"true\" height=\"76\" name=\"SVM (Linear)\" width=\"90\" x=\"45\" y=\"255\"/&gt;\\n              &lt;connect from_port=\"training set\" to_op=\"SVM (Linear)\" to_port=\"training set\"/&gt;\\n              &lt;connect from_op=\"SVM (Linear)\" from_port=\"model\" to_port=\"model\"/&gt;\\n              &lt;portSpacing port=\"source_training set\" spacing=\"0\"/&gt;\\n              &lt;portSpacing port=\"sink_model\" spacing=\"0\"/&gt;\\n            &lt;/process&gt;\\n          &lt;/operator&gt;\\n          &lt;connect from_port=\"training\" to_op=\"Select Attributes\" to_port=\"example set input\"/&gt;\\n          &lt;connect from_op=\"Select Attributes\" from_port=\"example set output\" to_op=\"Nominal to Binominal\" to_port=\"example set input\"/&gt;\\n          &lt;connect from_op=\"Nominal to Binominal\" from_port=\"example set output\" to_op=\"Polynominal by Binominal Classification\" to_port=\"training set\"/&gt;\\n          &lt;connect from_op=\"Polynominal by Binominal Classification\" from_port=\"model\" to_port=\"model\"/&gt;\\n          &lt;portSpacing port=\"source_training\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"sink_model\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"sink_through 1\" spacing=\"0\"/&gt;\\n        &lt;/process&gt;\\n        &lt;process expanded=\"true\"&gt;\\n          &lt;operator activated=\"true\" class=\"apply_model\" compatibility=\"5.3.015\" expanded=\"true\" height=\"76\" name=\"Apply Model\" width=\"90\" x=\"45\" y=\"30\"&gt;\\n            &lt;list key=\"application_parameters\"/&gt;\\n          &lt;/operator&gt;\\n          &lt;operator activated=\"true\" class=\"performance\" compatibility=\"5.3.015\" expanded=\"true\" height=\"76\" name=\"Performance\" width=\"90\" x=\"179\" y=\"30\"/&gt;\\n          &lt;connect from_port=\"model\" to_op=\"Apply Model\" to_port=\"model\"/&gt;\\n          &lt;connect from_port=\"test set\" to_op=\"Apply Model\" to_port=\"unlabelled data\"/&gt;\\n          &lt;connect from_op=\"Apply Model\" from_port=\"labelled data\" to_op=\"Performance\" to_port=\"labelled data\"/&gt;\\n          &lt;connect from_op=\"Performance\" from_port=\"performance\" to_port=\"averagable 1\"/&gt;\\n          &lt;portSpacing port=\"source_model\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"source_test set\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"source_through 1\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"sink_averagable 1\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"sink_averagable 2\" spacing=\"0\"/&gt;\\n        &lt;/process&gt;\\n      &lt;/operator&gt;\\n      &lt;operator activated=\"true\" class=\"apply_model\" compatibility=\"5.3.015\" expanded=\"true\" height=\"76\" name=\"Apply Model (2)\" width=\"90\" x=\"648\" y=\"210\"&gt;\\n        &lt;list key=\"application_parameters\"/&gt;\\n      &lt;/operator&gt;\\n      &lt;connect from_op=\"Read Database (5)\" from_port=\"output\" to_op=\"Select Attributes (2)\" to_port=\"example set input\"/&gt;\\n      &lt;connect from_op=\"Select Attributes (2)\" from_port=\"example set output\" to_op=\"Process Documents from Data (2)\" to_port=\"example set\"/&gt;\\n      &lt;connect from_op=\"Process Documents from Data (2)\" from_port=\"example set\" to_op=\"Set Role (2)\" to_port=\"example set input\"/&gt;\\n      &lt;connect from_op=\"Set Role (2)\" from_port=\"example set output\" to_op=\"Apply Model (2)\" to_port=\"unlabelled data\"/&gt;\\n      &lt;connect from_op=\"Read Database (3)\" from_port=\"output\" to_op=\"Text to Nominal\" to_port=\"example set input\"/&gt;\\n      &lt;connect from_op=\"Text to Nominal\" from_port=\"example set output\" to_op=\"Set Role\" to_port=\"example set input\"/&gt;\\n      &lt;connect from_op=\"Set Role\" from_port=\"example set output\" to_op=\"Nominal to Text\" to_port=\"example set input\"/&gt;\\n      &lt;connect from_op=\"Nominal to Text\" from_port=\"example set output\" to_op=\"Process Documents from Data\" to_port=\"example set\"/&gt;\\n      &lt;connect from_op=\"Process Documents from Data\" from_port=\"example set\" to_op=\"Multiply\" to_port=\"input\"/&gt;\\n      &lt;connect from_op=\"Multiply\" from_port=\"output 1\" to_op=\"Validation\" to_port=\"training\"/&gt;\\n      &lt;connect from_op=\"Multiply\" from_port=\"output 2\" to_port=\"result 1\"/&gt;\\n      &lt;connect from_op=\"Validation\" from_port=\"model\" to_op=\"Apply Model (2)\" to_port=\"model\"/&gt;\\n      &lt;connect from_op=\"Validation\" from_port=\"averagable 1\" to_port=\"result 3\"/&gt;\\n      &lt;connect from_op=\"Apply Model (2)\" from_port=\"labelled data\" to_port=\"result 2\"/&gt;\\n      &lt;portSpacing port=\"source_input 1\" spacing=\"0\"/&gt;\\n      &lt;portSpacing port=\"sink_result 1\" spacing=\"0\"/&gt;\\n      &lt;portSpacing port=\"sink_result 2\" spacing=\"0\"/&gt;\\n      &lt;portSpacing port=\"sink_result 3\" spacing=\"0\"/&gt;\\n      &lt;portSpacing port=\"sink_result 4\" spacing=\"0\"/&gt;\\n    &lt;/process&gt;\\n  &lt;/operator&gt;\\n&lt;/process&gt;\\n</code></pre>\\n-TextMining of Twitter DB in RapidMiner evaluates all comments in a only way-<twitter><svm><text-mining><rapidminer>',\n",
       " \"<p>I am evaluating MonetDB for a multi-tenanted OLAP solution. I understand that there is a concept of schema similar to mysql in MonetDb but is it really scalable in terms of schema's. Can I put a reasonable number of schemas and all performing nearly the same or multiple instances of the monetdb server will be needed ?</p>\\n\\n<p>Also is it scalable in terms of number of indexes? Coz in a multi-tenanted environment our number of indexes will also increase with number of tenants.</p>\\n\\n<p>I have looked all over to find out about this but could not get a satisfactory answer.</p>\\n-MonetDB multi-tenancy?-<multi-tenant><monetdb>\",\n",
       " '<p>I have some C++ functions and which are used to check for valid decimal, date, timestamp etc. \\nWhat I want is to import those functions as UDF into Teradata in order make use them in my SQLs. Is that possible?</p>\\n-Import C++ function into teradata-<c++><import><user-defined-functions><teradata>',\n",
       " '<p>Is there a way to do a bulk insert using MonetDB.R (not via a for loop and dbSendUpdate)?</p>\\n\\n<p>Does dbWriteTable allow for updates (append=TRUE)?</p>\\n\\n<p>About \"INSERT INTO\" the MonetDB documentation states:\\n\"The benefit is clear: this is very straightforward.  However, this is a seriously inefficient way of doing things in MonetDB.\"</p>\\n\\n<p>Thanks.</p>\\n-MonetDB.R bulk insert-<r><monetdb>',\n",
       " '<p>I wrote a plugin five years ago and it was correct (with no compiler error) in all version of Eclipse until <strong>Luna</strong>. When I updated my eclipse to Luna, I got two compiler errors. </p>\\n\\n<p>The error is because of these two import statements. (The import org.eclipse.jface cannot be resolved, The import org.eclipse.swt cannot be resolved)</p>\\n\\n<pre><code>import org.eclipse.jface.resource.ImageDescriptor;\\nimport org.eclipse.swt.graphics.Image;\\n</code></pre>\\n\\n<p>I wanted to add two new dependencies to the plugin file, but I could not find any relevent. For example, I could not find org.eclipse.jface.resource in dependency page.</p>\\n\\n<p>Does anyone have any idea why the plugin works in <strong>Kepler</strong>, but has the above compiler errors in <strong>Luna</strong>?</p>\\n-The import org.eclipse.jface cannot be resolved in Eclipse Luna-<eclipse-plugin><compiler-errors><eclipse-kepler><eclipse-luna>',\n",
       " '<p>I have several jar files (tasks.jar, calendar.jar, user.jar, authentication.jar) which I have developed which contains persistence.xml file inside each and every one of them. All these persistence.xml files have the same persistence unit name, say for example \\'TASK_MANAGER\\'.<br>\\nMy question is can I add these jar files in to the JSF web project\\'s lib folder and use them via a managed session bean, which again uses JPA entity manager, with the persistence unit name \\'TASK_MANAGER\\'. Ex:  </p>\\n\\n<pre><code>//All the imports go here (import com.app.model.task) etc....\\n@ManagedBean     \\n@RequestScoped  \\npublic class TaskController{\\n  //Access the entity manager and work with their methods here\\n}\\n</code></pre>\\n\\n<p>I know that by <a href=\"http://docs.oracle.com/javaee/6/tutorial/doc/bnbqw.html\" rel=\"nofollow noreferrer\">http://docs.oracle.com/javaee/6/tutorial/doc/bnbqw.html</a>  </p>\\n\\n<blockquote>\\n  <p>If you package the persistence unit in a JAR file that will be\\n  included in a WAR or EAR file, the JAR file should be located in\\n  either  </p>\\n  \\n  <ul>\\n  <li>The WEB-INF/lib directory of a WAR  </li>\\n  <li>The EAR file’s library directory</li>\\n  </ul>\\n</blockquote>\\n\\n<p>But my question is can I do that with multiple persistence.xml files.</p>\\n\\n<p>Because in Eclipse when I try to achieve this, only the final jar file\\'s persistence file is loaded, I know this because other entities are not recognised when I try to run the application.  </p>\\n\\n<p>How can I include multiple persistence.xml files into one project.</p>\\n\\n<p>I checked these already <a href=\"https://stackoverflow.com/questions/8370043/configure-persistence-units-to-be-available-in-several-jars-of-an-ear\">Configure persistence units to be available in several jars of an ear</a> and <a href=\"https://stackoverflow.com/questions/23398636/combining-entities-across-jars-in-a-single-persistence-xml-file\">https://stackoverflow.com/questions/23398636/combining-entities-across-jars-in-a-single-persistence-xml-file</a>.</p>\\n-How to combine several jar files with peristence.xml files into one jsf project-<jsf><jpa><eclipse-kepler><persistence.xml>',\n",
       " '<p>I\\'m using STS based on Kepler 4.3.2. I am not able to download support for Glassfish from the Oracle site. Whenever I try to download via Eclipse Marketplace, I get this error message:</p>\\n\\n<p>\"No repository found at <a href=\"http://download.oracle.com/otn_software/oepe/kepler\" rel=\"nofollow\">http://download.oracle.com/otn_software/oepe/kepler</a>.\"</p>\\n\\n<p>Could someone please enlighten me what the problem is with the site?</p>\\n-Unable to download Glassfish tools for Eclipse Kepler?-<eclipse><oracle><download><glassfish><eclipse-kepler>',\n",
       " '<pre><code>-drop table r.stray_cust;\\n\\nCREATE VOLATILE TABLE r.stray_cust\\n(\\nCustomerID INTEGER\\n) \\nPRIMARY INDEX ( CustomerID );\\n\\nINSERT INTO  r.stray_cust\\nSELECT DISTINCT c.customerid AS customerid\\nFROM customer c\\nWHERE c.customerid \\nNOT IN (\\nSELECT a.customerid \\nFROM address a );\\n\\nON COMMIT PRESERVE ROWS;\\n\\nselect * from r.stray_cust;\\n</code></pre>\\n\\n<p>Keep getting an error message - query failed please see history log for details</p>\\n-error with creating volatile table in teradata-<sql><teradata>',\n",
       " '<p>I\\'m using eclipse kepler with java SE 1.8 jdk system library but when I use any javafx classes or methods eclipse gives a warning like <code>Access restriction: The type \\'TableColumn&lt;?,?&gt;\\' is not API (restriction on required library \\'C:\\\\Program Files\\\\Java\\\\jdk1.8.0_05\\\\jre\\\\lib\\\\ext\\\\jfxrt.jar\\')</code>. </p>\\n\\n<p>I have tried adding an access rule to the system library to allow access for all jars in the library using the wildcard <code>*</code> but the warning is still there. Am also using maven for building project.\\nThe code works but the warning is nagging and is affecting readability as I code. Any help is appreciated.</p>\\n\\n<p>Edit: I also tried adding <code>@SuppressWarnings(\"restriction\")</code> at the beginning of my classes but this still doesn\\'t solve the unknown cause and hides all other restriction warnings that I may need to see</p>\\n-Eclipse gives warning on access restriction for jfxrt.jar classes-<eclipse><maven><javafx><java-8><eclipse-kepler>',\n",
       " '<p>I am attempting to build the Perl DBD::Teradata DBI driver on 64 bit linux.  However, I do not have the header files necessary to do so.  According to the documentation (<a href=\"http://www.presicient.com/tdatdbd/\" rel=\"nofollow\">http://www.presicient.com/tdatdbd/</a>), the following files are required:</p>\\n\\n<pre><code>parcel.h\\ndbcarea.h\\ncoperr.h\\ncoptypes.h \\n</code></pre>\\n\\n<p>I\\'ve spent hours scouring Teradata\\'s site and the internet at large with no success.  I saw mention of a CLIv2 developer\\'s kit, but could not locate this either.</p>\\n\\n<p>Could anyone point me to where I can get these files?  I would sincerely appreciate the help.</p>\\n-Where are Teradata header files downloaded from? (DBD::Teradata build)-<teradata><dbi>',\n",
       " '<p>When I query Teradata for a specific table in bteq , I get the following error. \\nHow to query this table in BTEQ or any other TD SQL Client using JDBC or ODBC.</p>\\n\\n<pre><code> *** Failure 9804 Response Row size or Constant Row size overflow.\\n                Statement# 1, Info =0 \\n *** Total elapsed time was 1 second.\\n</code></pre>\\n-Teradata Row size Error-<teradata>',\n",
       " \"<p>Here is the Output what i have tried through the lead function, i just got the result But i need to Update the Table as below result set . \\nCan anyone share the Log for this I have tried but there is no luck Thanks</p>\\n\\n<p>here is my output,  need an Update logic to update the records based in the effout_date \\nbut i want to update the records exist in the table</p>\\n\\n<p>5th record of MFG_DATE - 30-Jun-14 is updated to 4th record of effout_date - 30-Jun-14 and\\n4th record of MFG_DATE - 29-Jun-14 is updated to 3rd record of data effout_date - 29-Jun-14\\n3rd record of MFG_DATE - 5-Jun-14 is updated to 2nd record of data effout_date - 5-Jun-14</p>\\n\\n<pre><code>DROP TABLE t_plant;\\nCreate MULTISET Table  t_plant\\n( \\nplant_no INTEGER NOT NULL\\n,plant_Name CHAR(9) \\n,part1 CHAR(9) \\n,part2 CHAR(9) \\n,part3 CHAR(9) \\n,mfg_date DATE FORMAT 'YY/MM/DD'\\n,effout_date DATE FORMAT 'YY/MM/DD'\\n,currency CHAR(9)  \\n);\\nINSERT INTO t_plant\\nVALUES( 1,'Detroit','A11','B22','C33',TO_DATE(  '2014-02-01', 'YYYY-MM-DD') ,  TO_DATE( '9999-12-31', 'YYYY-MM-DD'),'USD');\\nINSERT INTO t_plant\\nVALUES( 2,'Detroit','A11','B22','C33',TO_DATE(  '2014-03-01', 'YYYY-MM-DD') ,  TO_DATE( '9999-12-31', 'YYYY-MM-DD'),'USD');\\nINSERT INTO t_plant\\nVALUES( 3,'Detroit','A11','B22','C33', TO_DATE(  '2014-06-05', 'YYYY-MM-DD'),   TO_DATE( '9999-12-31', 'YYYY-MM-DD'),'USD');\\nINSERT INTO t_plant\\nVALUES( 3,'Detroit','A11','B22','C33',  TO_DATE( '2014-06-05', 'YYYY-MM-DD') , TO_DATE( '9999-12-31', 'YYYY-MM-DD'),'USD');\\nINSERT INTO t_plant\\nVALUES( 4,'Detroit','A11','B22','C33', TO_DATE(  '2014-06-29', 'YYYY-MM-DD'),   TO_DATE( '9999-12-31', 'YYYY-MM-DD'),'USD');\\nINSERT INTO t_plant\\nVALUES( 5,'Detroit','A11','B22','C33', TO_DATE(  '2014-06-30', 'YYYY-MM-DD'),   TO_DATE( '9999-12-31', 'YYYY-MM-DD'),'USD');\\nINSERT INTO t_plant\\nVALUES( 6,'INDIA','A10','B20','C30', TO_DATE(  '2014-05-15', 'YYYY-MM-DD') ,  TO_DATE( '9999-12-31', 'YYYY-MM-DD'),'USD');\\nINSERT INTO t_plant\\nVALUES( 7,'INDIA','A10','B20','C30', TO_DATE(  '2014-05-15', 'YYYY-MM-DD') ,  TO_DATE( '9999-12-31', 'YYYY-MM-DD'),'USD');\\nINSERT INTO t_plant\\nVALUES( 8,'INDIA','A10','B20','C30', TO_DATE(  '2014-06-30', 'YYYY-MM-DD') ,  TO_DATE( '9999-12-31', 'YYYY-MM-DD'),'USD');\\n\\nSELECT plant_Name,part1, part2, part3, mfg_date\\n      ,COALESCE(  MIN(mfg_date)\\n           OVER(PARTITION BY  plant_Name,PART1, part2, part3  ORDER BY mfg_date\\n             ROWS BETWEEN 1 FOLLOWING AND 1 FOLLOWING )\\n                 ,CAST('9999-12-31'AS DATE) ) AS effout_date_Lead_COALESCE --Tomorrow\\n FROM   t_plant\\n GROUP BY  plant_Name,PART1, PART2, PART3, mfg_date ;\\n\\nSelect * FROM   t_plant Order by part1, part2, part3, mfg_date;\\n</code></pre>\\n\\n<p>Here is the Output what i have tried through the lead function, i just got the result But i need to Update the Table as below result set .\\nCan anyone share the Log for this I have tried but there is no luck Thanks</p>\\n\\n<pre><code>Select t_plant_1.* , t_plant_2.effout_date\\nFrom  t_plant t_plant_1\\n,\\n(\\nSELECT plant_Name,PART1, PART2, PART3, mfg_date  \\n      ,COALESCE(  MIN(mfg_date)\\n            OVER(PARTITION BY  plant_Name,PART1, PART2, PART3  ORDER BY mfg_date\\n                        ROWS BETWEEN 1 FOLLOWING AND 1 FOLLOWING )\\n                        ,CAST('9999-12-31'AS DATE)  ) AS effout_date --Lead_COALESCE\\n FROM   t_plant\\n GROUP BY  plant_Name,PART1, PART2, PART3, mfg_date      \\n ) t_plant_2\\nWhere  t_plant_1.plant_Name = t_plant_2.plant_Name\\nand t_plant_1.PART1 = t_plant_2.PART1\\nand t_plant_1.PART2 = t_plant_2.PART2\\nand t_plant_1.PART3 = t_plant_2.PART3\\nand t_plant_1.mfg_date = t_plant_2.mfg_date\\nOrder by   t_plant_1.plant_Name,t_plant_1.PART1 ,t_plant_1.PART2,t_plant_1.PART3,t_plant_1.mfg_date \\n</code></pre>\\n\\n<p>OUT/PUT</p>\\n\\n<pre><code>   plant_no plant_Name  part1           part2          part3            mfg_date    effout_date_lead currency\\n1   1   Detroit     A11         B22         C33         2/1/2014    3/1/2014    USD      \\n2   2   Detroit     A11         B22         C33         3/1/2014    6/5/2014    USD      \\n3   3   Detroit     A11         B22         C33         6/5/2014    6/29/2014   USD      \\n4   3   Detroit     A11         B22         C33         6/5/2014    6/29/2014   USD      \\n5   4   Detroit     A11         B22         C33         6/29/2014   6/30/2014   USD      \\n6   5   Detroit     A11         B22         C33         6/30/2014   12/31/9999  USD      \\n7   7   INDIA       A10         B20         C30         5/15/2014   6/30/2014   USD      \\n8   6   INDIA       A10         B20         C30         5/15/2014   6/30/2014   USD      \\n9   8   INDIA       A10         B20         C30         6/30/2014   12/31/9999  USD      \\n</code></pre>\\n\\n<p>Trying to Update the records but no luck</p>\\n\\n<pre><code>Update t_plant\\nFROM\\n    (\\n        SELECT t_plant_1.* \\n        FROM  t_plant t_plant_1\\n            ,\\n            (\\n            SELECT plant_Name,PART1, PART2, PART3, mfg_date  \\n                  ,COALESCE(  MIN(mfg_date)\\n                        OVER(PARTITION BY  plant_Name,PART1, PART2, PART3  ORDER BY mfg_date\\n                                    ROWS BETWEEN 1 FOLLOWING AND 1 FOLLOWING )\\n                                    ,CAST('9999-12-31'AS DATE)  ) AS effout_date --Lead_COALESCE\\n             FROM   t_plant\\n             GROUP BY  plant_Name,PART1, PART2, PART3, mfg_date      \\n             ) t_plant_2\\n        WHERE  t_plant_1.plant_Name = t_plant_2.plant_Name\\n        AND t_plant_1.PART1 = t_plant_2.PART1\\n        AND t_plant_1.PART2 = t_plant_2.PART2\\n        AND t_plant_1.PART3 = t_plant_2.PART3\\n        AND t_plant_1.mfg_date = t_plant_2.mfg_date\\n        ORDER BY   t_plant_1.plant_Name,t_plant_1.PART1 ,t_plant_1.PART2,t_plant_1.PART3,t_plant_1.mfg_date \\n    ) B1\\nSET effout_date = B1.effout_date\\nWHERE  plant_Name = B1.plant_Name\\nAND PART1 = B1.PART1\\nAND PART2 = B1.PART2\\nAND PART3 = B1.PART3\\nAND mfg_date = B1.mfg_date;\\n</code></pre>\\n\\n<p>output need is</p>\\n\\n<pre><code>part1       mfg_date    effout_date_old effout_date_New\\nA11         2/1/2014    12/31/9999  3/1/2014\\nA11         3/1/2014    12/31/9999  6/5/2014\\nA11         6/5/2014    12/31/9999  6/29/2014\\nA11         6/5/2014    12/31/9999  6/29/2014\\nA11         6/29/2014   12/31/9999  6/30/2014\\nA11         6/30/2014   12/31/9999  12/31/9999\\nA10         5/15/2014   12/31/9999  6/30/2014\\nA10         5/15/2014   12/31/9999  6/30/2014\\nA10         6/30/2014   12/31/9999  12/31/9999\\n</code></pre>\\n-logic for the Update statement in Teradata-<teradata>\",\n",
       " '<p>How to get the only numeric characters in teradata input table, using abinitio. </p>\\n\\n<p>I have a requirement to pass the product number from alpha numeric input values.</p>\\n\\n<p>For Input data:<br>\\n          N345VM\\n          D5689NM\\n          T309MN\\n          R4567LK</p>\\n\\n<p>I need the output as:</p>\\n\\n<p>345\\n   5689\\n   309\\n   4567</p>\\n\\n<p>Thanks,\\nSharan</p>\\n-usage of filter in Teradata, using abinitio-<teradata>',\n",
       " '<p>Windows\\' (7) ODBC administrator is configured with my username and password. I am having trouble connecting to my Teradata DB using ODBC and php. I have no trouble connecting with Python by simply using the connection string \"DSN=Teradata\"</p>\\n\\n<pre><code>import pypyodbc\\nconn = pypyodbc.connect(\\'DSN=Teradata\\')\\n</code></pre>\\n\\n<p>or R, </p>\\n\\n<pre><code>require(RODBC)\\nodbcConnect(\\'Teradata\\')\\n</code></pre>\\n\\n<p>But the following code </p>\\n\\n<pre><code>&lt;?php\\n  $conn = odbc_connect(\\'DSN=Teradata\\');\\n  $query = odbc_exec($conn, \"select top 10 * from pretendTable\");\\n  while(odbc_fetch_row($query)) {\\n    odbc_result($query, 1);\\n  }\\n  echo \"END\";\\n?&gt;\\n</code></pre>\\n\\n<p>only outputs \"END\". Any ideas?</p>\\n\\n<p><strong>EDIT</strong></p>\\n\\n<p>and also, VBA</p>\\n\\n<pre><code>Dim cn As New ADODB.Connection\\nDim rs As New ADODB.Recordset\\n\\ncn.Open \"Teradata\"\\nrs.Open \"Select top 10 * from pretendTable\", cn\\n</code></pre>\\n\\n<p>php is the only one that is not cooperating</p>\\n\\n<p><strong>EDIT 2</strong></p>\\n\\n<p>Somewhat repaired. </p>\\n\\n<p>I have two installs of php on my computer. One was installed by wamp, the other using the php.net binary executable. It is the latter that is not working. This leads me to believe that the ODBC package/functionality does not come installed by default. However, I\\'ll defer to the hive... </p>\\n\\n<p>The former works with <code>$conn = odbc_connect(\"Teradata\",\"pretendUsername\",\"pretendPassword\");</code></p>\\n-Unable to ODBC connect to Teradata via PHP-<php><odbc><teradata>',\n",
       " '<p>Quoting the \"Kepler Tuning Guide\" provided by NVIDIA:</p>\\n\\n<blockquote>\\n  <p>Also note that Kepler GPUs can utilize ILP in place of\\n  thread/warp-level parallelism (TLP) more readily than Fermi GPUs can.</p>\\n</blockquote>\\n\\n<p>In my opinion, the following code snippet</p>\\n\\n<pre><code>a = .....;\\na2 = f(a); \\na3 = g(a2);  \\n</code></pre>\\n\\n<p>can be improved as follows</p>\\n\\n<pre><code>a = ...;\\nb = ....;\\na2 = f(a);\\nb2 = f(b);\\na3 = g(a2);\\nb3 = g(b2);\\n</code></pre>\\n\\n<p>So in my projects, I have a section of code as follows (example 1)</p>\\n\\n<pre><code>if(x &lt; src.cols &amp;&amp; y &lt; src.rows)\\n{\\n    if(!mask(y,x))\\n    {\\n        src.ptr(y)[x] = make_short4(0,0,0,0);\\n    }\\n}\\n</code></pre>\\n\\n<p>and I rewrite it as follows (example2)</p>\\n\\n<pre><code>if(x &lt; src.cols &amp;&amp; y &lt; src.rows)\\n{\\n    if(!mask(y,x))\\n    {\\n        short4 t;\\n        t.x = 0;\\n        t.y = 0;\\n        t.z = 0;\\n        t.w = 0;\\n        src.ptr(y)[x].x = t.x;\\n        src.ptr(y)[x].y = t.y;\\n        src.ptr(y)[x].z = t.z;\\n        src.ptr(y)[x].w = t.w;  \\n     }\\n}\\n</code></pre>\\n\\n<p>In the Kepler architecture, the example2 will be more efficient and exhibit better performance than example1, is that right?</p>\\n-The efficiency and performance of ILP for the NVIDIA Kepler architecture-<cuda><kepler>',\n",
       " '<p>Can anyone please help me in unstanding below csum function.\\nWhat will be the output in each case. \\n   csum(1,1),\\n    csum(1,1) + emp_no\\n    csum(1,emp_no)+emp_no</p>\\n-Output of CSUM() in teradata-<sql><teradata>',\n",
       " '<p>I have Eclipse standard but even that has unnecessary packages bundled and I would really like to get rid of the Plug-in Development Environment that comes packaged with the standard version. How do I go about this? Or at least disabling this package</p>\\n-Remove Plug-in Development Environment in Eclipse-<eclipse><eclipse-kepler>',\n",
       " \"<p>I am using Teradata bteq utility to run teradata commands form unix server.</p>\\n\\n<p>I am able to connect to teradata, but while fetching data it gives only 7 columns and a dot(.) at end of decimal field.\\nI am using query,\\nselect * from databasename.tablename</p>\\n\\n<p>output\\ncolumn1(decimal)   column2 column3(decimal)\\n   74664.              S        67469.</p>\\n\\n<p>Don't know why it giving dot(.)</p>\\n\\n<p>Can anybody help??</p>\\n-Fetching data from teradata using teradata bteq utility-<unix><automation><teradata>\",\n",
       " \"<p>We have a table with an auto-increment column defined as follows:</p>\\n\\n<pre><code> Col1 BIGINT NOT NULL GENERATED BY DEFAULT AS IDENTITY\\n       (START WITH 1 \\n        INCREMENT BY 1 \\n        MINVALUE -999999999999999999 \\n        MAXVALUE 999999999999999999 \\n        NO CYCLE),\\n</code></pre>\\n\\n<p>Now, when we inserted 49 records for the first time, it took 100001 to 100049 as values for this Col1, instead of starting at 1. For insertion at the next hour, it took values 200001 to 200049. And so on. After the 9th hour it took 1000001 to 1000049 values. </p>\\n\\n<p>How exactly does auto-increment in Teradta work? Is it being shared in memory with other auto-increment columns from other tables? </p>\\n\\n<p>Why didn't it start with 1 and kept incrementing by one for the next insertions? How to fix this?</p>\\n-Teradata auto-increment column taking different ranges as starting point while inserting records at different time periods-<sql><auto-increment><teradata>\",\n",
       " '<p>Is there ANY way to run a .vbs file with a connection with a larger timeout? </p>\\n\\n<p><b>Background:</b>\\n<p>\\nI\\'m using Excel VBA to upload data to Teradata.  It\\'s relatively easy to create an insert statement and then execute it with rs.Execute().  </p>\\n\\n<p>However, for around 80k rows, this takes about an hour to run.  I thought of two possible speedups.  </p>\\n\\n<p>1) Send multiple rows (say, 500) to the rs.Execute() at a time, separated by semicolons, and let Teradata insert them in parallel.  </p>\\n\\n<p>2) Send multiple insert requests at a time without waiting for the 1st one to finish.  This is difficult in VBA, since there\\'s no support for multithreading and rs.Execute is blocking.  So to get around this, I created .vbs files which just contain this:</p>\\n\\n<pre><code>Dim rs\\nSet rs = CreateObject(\"ADODB.Recordset\")\\nconstr = \"Driver={Teradata};commandTimeout=0;DBCNAME=xx;UID=xx;PWD=xx;\"\\nrs.Open \"insert statement1;insert statement2;\", constr, adOpenForwardOnly\\n</code></pre>\\n\\n<p>And I spin off a bunch of those.  however, if i send too much data at once, it times out (Teradata has a 30 second timeout for queries run this way.  \"commandTimeout\" doesn\\'t appear to have any effect whether I set it to 0 or 1200 or whatever.  I can\\'t find an appropriate size packet to insert since the load on Teradata keeps changing with use throughout the day.  Sometimes 500 at a time works fine and sometimes only 15 stays under the 30s timeout.</p>\\n-Run a .vbs script that connects to Teradata with no timeout-<vba><odbc><teradata>',\n",
       " '<p>I recently installed maven plug-in in Eclipse Kepler.  But when I try to add a dependency (open the pom.xml, go to dependencies tab and click add) I can input some string like \"spring\" in the search box but nothing happens. Ideally a list of all dependencies containing \"spring\" should pop up.  But it shows: \"Artifact Id cannot be empty\".  </p>\\n\\n<p>I  have changed Windows- preference- Maven -“Download repository index updates on  startup” also I have changed the central repository to “Enable full Index” and did rebuild index. Still no luck</p>\\n\\n<p>When i tried to add the dependency manually to the pom.xml and save it (in eclipse itself) the changes does not get saved.</p>\\n\\n<p>I do not  have any network issues. </p>\\n\\n<p>Please help me out here. It would be of great help .</p>\\n-Cannot search for artifact in Eclipse Kepler using m2e plugin - \"Gives Artifact id empty\"-<eclipse><maven-plugin><m2eclipse><eclipse-kepler>',\n",
       " '<p>According to the <a href=\"http://www.nvidia.com/content/PDF/kepler/NVIDIA-Kepler-GK110-Architecture-Whitepaper.pdf\" rel=\"nofollow\">GK110 whitepaper</a>, each SMX has a maximum of 64 warps and a maximum thread capacity of 2048 threads.</p>\\n\\n<p>My question is this: Does each SMX always operate at this maximum resident warp number of 64 (assuming no thread divergence and a block size that is a multiple of 64)?</p>\\n\\n<p>I have reason to believe that if your number of threads on an SMX &lt; 1024, you will only get a maximum of 32 warps per multiprocessor.</p>\\n\\n<p>(I believe this because my similarly clocked Fermi card is showing similar speeds to my Kepler card when the number of threads is 1024 on 1 block when running the same code)</p>\\n-Max Warps on a GK110 GPU-<cuda><kepler>',\n",
       " \"<p>I'm running Ubuntu 14.04 and Eclipse Kepler. Since yesterday I've been experiencing a strange issue: when I open up Eclipse, all that I'm seeing is the menu bar on the upper Ubuntu bar. No code editor, window whatsoever.</p>\\n\\n<p>In order to see a window I go to Window -> New window, but this is just a workaround. It starts up a new window with no preferences set, no last edited files etc. Moreover it shows that I have 2 Eclipse windows opened, while I can only see 1.</p>\\n\\n<p>Any ideas on how to fix that?</p>\\n-Eclipse in Ubuntu starting without any window visible (only menu bar)-<eclipse><eclipse-kepler><ubuntu-14.04>\",\n",
       " '<p>I want to redirect the standard input in my Clojure benchmark. I have passed the file as an argument and I am trying to find the equivalant code in java: \\n<strong>System.setIn(new FileInputStream(filename));</strong>\\nbut for <strong>Clojure</strong>.</p>\\n\\n<p>The main issue is that I use the DaCapo suite to calculate the performance of Benchmark and the method that loads the benchmark does not recognize special characters like \"&lt;\" in contrast to the cmd (running the benchmark\\'s jar directly from the cmd..).</p>\\n\\n<p>This is what I am trying to do..but still does not work.. I think that br has the standard input from the in and it is used by the rest of the program. How can I change the in while I have in the args the desired path, so I can run the benchmark correctly? This is my effort with the \"system/setin\"</p>\\n\\n<pre><code>(defn -main [&amp; args]\\n  (let [max-dna-chars-per-line 60\\n    jk  (with-open [is (clojure.java.io/input-stream (first args))]\\n        (System/setIn is))\\n        br (java.io.BufferedReader. *in*)\\n        bw (java.io.BufferedWriter. *out* (* 16 8192)) ; 16 * default size, I think\\n        ;; We could use the map complement-dna-char-map instead of\\n        ;; complement-dna-char-fn, but when I tested that, the program\\n        ;; spent a lot of time running the hashCode method on\\n        ;; characters.  I\\'m hoping this is faster.\\n        complement-dna-char-vec (make-vec-char-mapper complement-dna-char-map)]\\n    (loop [[desc-str dna-seq-str more] (fasta-slurp-br br)]\\n      (println-string-to-buffered-writer bw desc-str)\\n      (print-reverse-complement-of-str-in-lines bw dna-seq-str\\n                                                complement-dna-char-vec\\n                                                max-dna-chars-per-line)\\n      (when more\\n        (recur (fasta-slurp-br br))))\\n    (. bw flush))) \\n</code></pre>\\n-Redirect input in Clojure-<eclipse><clojure><eclipse-kepler><dacapo>',\n",
       " '<p>Would anyone be able to help me write a query where a column is selected in a imported excel file that will parse out a fullname and divided the firstname, middle, lastname and suffix into individual columns?</p>\\n-use substring to parse out a fullname into firstname column, middle, lastname in teradata-<sql><teradata>',\n",
       " \"<p>I have quite impressed with this deployment kit. Instead of buying a new CUDA card, which might require new main board and etc, this card seems provide all in one.</p>\\n\\n<p>At it's specs it says it has CUDA compute capability 3.2. AFAIK dynamic parallelism and more comes with cm_35, cuda compute capability 3.5. Does this card support Dynamic Parallelism and HyperQ features of Kepler architecture?</p>\\n-Nvidia Jetson TK1 Development Board - Cuda Compute Capability-<cuda><embedded><specifications><kepler><dynamic-parallelism>\",\n",
       " '<p>I am trying to find a clustering algorithm to cluster nominal data with python. For that purpose I tried DBSCAN algorithm with RapidMiner and it worked with nominal data. But when I try same dataset with DBSCAN algorithm which is provided by scikit-learn it gave error that says function could not convert string to float. </p>\\n\\n<p>Are DBSCANs in rapidminer and scikit-learn different and how can I solve that problem? \\nAlso if you tell me another clustering algorithm that works with nominal data it would be great?</p>\\n-DBSCAN algorithms in rapidminer and scikit-learn-<cluster-analysis><data-mining><scikit-learn><rapidminer><dbscan>',\n",
       " '<p>Eclipse IDE console view is not displaying in Debug perspective is selected.\\nIf I select Console view from <code>Window -&gt; Show View -&gt; Console</code> then there is no <code>Console View</code> is displayed it is hiding. How to display Console view in Eclipse IDE when Debug perspective is selected.</p>\\n\\n<p>Thanks.</p>\\n-console view not displaying in eclipse when debug perspective is selected-<eclipse><ide><console><window><eclipse-kepler>',\n",
       " '<p>I would like to know how to query multiple databases of the same server of Teradata in SAS (unix). I can do it for one database but there are few different databases involved in my queries. The only related article was <a href=\"https://stackoverflow.com/questions/8237581/sas-connection-to-teradata-database-using-teradata-odbc\">SAS connection to Teradata Database using Teradata ODBC</a> but could not get the right answer. Could you please share syntax/snippet. Any comment is appreciated.\\nThanks!</p>\\n\\n<p>Jas</p>\\n\\n<p>Edits:</p>\\n\\n<p>Please see the below script, I want to do something like this.</p>\\n\\n<pre><code>libname lib \\'mylibraryPath\\\\\\';\\n\\nproc sql;\\nconnect to teradata  (user=\"userid\" password=\"pwaaaowrd\" mode=teradata  database=DB1   database=DB2   database=DB3   tdpid=\"MyServer\");\\nexecute (\\ncreate volatile table lib.tab1 as \\n(\\n\\nSelect statements and several joins of different tables from different databases (server is same)\\n)\\nWITH DATA\\nPRIMARY INDEX (abcd)\\nON COMMIT PRESERVE ROWS;\\n)\\nBy Teradata;\\nexecute (commit work) by Teradata;\\ndisconnect from teradata;\\nquit;\\n</code></pre>\\n-Multiple databases of Teradata and SAS in UNIX-<sql><unix><sas><teradata>',\n",
       " '<p>I use Teradata SQL Assistant to run SQL queries against a DB2 database accessed via an ODBC connection. This is an entirely interactive process whereby I first start the SQL Assistant app, then connect to the correct data source and finally write and execute my query. </p>\\n\\n<p>What I would like to do is to be able to achieve the same result, i.e. get the result set from a query, but via some sort of script, which would connect to the the data source and run my query.</p>\\n\\n<p>Is this possible?</p>\\n-Issuing Teradata SQL Assistant query via script-<sql><scripting><db2><teradata>',\n",
       " '<p>I get the following when appending boolean data to an existing table.</p>\\n\\n<blockquote>\\n  <p>&gt; dbWriteTable(conn, \"myTable\", myData, overwrite = F, append = T, csvdump = T)</p>\\n  \\n  <p>Error in .local(conn, statement, ...) : \\n    Unable to execute statement \\'COPY 1292 RECORDS INTO myTable FROM \\'C:\\\\Path\\\\to\\\\AppData\\\\Local\\\\Temp\\\\Rtmp80mc5L\\\\file3e24401f42a.cs...\\'.</p>\\n  \\n  <p>Server says \\'!value \\'TRUE\\' from line 1 field 20 not inserted, expecting type boolean\\'.</p>\\n</blockquote>\\n\\n<p>I\\'ve taken some code of the connector which deals with this:</p>\\n\\n<blockquote>\\n  <p>&gt; tmp &lt;- tempfile(fileext = \".csv\")</p>\\n  \\n  <p>&gt; write.table(myData, tmp, sep = \",\", quote = TRUE,row.names = FALSE, col.names = FALSE,na=\"\")</p>\\n  \\n  <p>&gt; tmp</p>\\n  \\n  <p>[1] \"C:\\\\Path\\\\to\\\\AppData\\\\Local\\\\Temp\\\\Rtmp80mc5L\\\\file3e24401f42a.csv\"</p>\\n</blockquote>\\n\\n<p>When changing TRUE to 1 and FALSE to 0 and run the following SQL code:</p>\\n\\n<blockquote>\\n  <p>COPY 1292 RECORDS INTO myTable FROM \\'C:\\\\Path\\\\to\\\\AppData\\\\Local\\\\Temp\\\\Rtmp80mc5L\\\\file3e24401f42a.csv\\' USING DELIMITERS \\',\\',\\'\\\\n\\',\\'\"\\' NULL AS \\'\\' LOCKED;</p>\\n</blockquote>\\n\\n<p>then all data is inserted.</p>\\n-MonetDB does not accept BOOLEANs from dbWriteTable (csvdump = T)-<r><monetdb>',\n",
       " '<p>Whenever I start Eclipse, it starts with all of the editors minimized, and every time I select a different program (such as the web browser), and then return to Eclipse every editor window gets minimized again.</p>\\n\\n<p>The picture below shows what I mean by minimized.</p>\\n\\n<p><img src=\"https://i.stack.imgur.com/wYEiP.jpg\" alt=\"eclipse window\"></p>\\n\\n<p>This is very frustrating, and I will appreciate any help. </p>\\n-Eclipse kepler minimized on startup and on every reselection of Eclipse-<eclipse><eclipse-kepler>',\n",
       " '<p>I\\'m just following tutorials exactly to learn how to use rapidminer and I can\\'t figure out what I\\'m doing wrong. One tutorial I tried was this: <a href=\"http://auburnbigdata.blogspot.com/2013/04/web-crawling-with-rapidminer.html\" rel=\"nofollow\">http://auburnbigdata.blogspot.com/2013/04/web-crawling-with-rapidminer.html</a></p>\\n\\n<p>I set up the crawl web processes and connected it to the results port. My parameters are as follows:</p>\\n\\n<p>url: <a href=\"http://auburnbigdata.blogspot.com\" rel=\"nofollow\">http://auburnbigdata.blogspot.com</a></p>\\n\\n<p>crawling rules: store_with_matching_url    .+auburnblogspot.+</p>\\n\\n<p>follow_link_with_matching_url    .+auburnblogspot.+</p>\\n\\n<p>write pages into files: checked</p>\\n\\n<p>add pages as attribute: checked</p>\\n\\n<p>output_dir: C:\\\\Users\\\\Owen Capobianco\\\\Desktop\\\\Crawldata</p>\\n\\n<p>extension: txt</p>\\n\\n<p>max pages: (blank)</p>\\n\\n<p>max depth: 2</p>\\n\\n<p>domain: web</p>\\n\\n<p>delay: 500</p>\\n\\n<p>max threads: 1</p>\\n\\n<p>user agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36</p>\\n\\n<p>obey robot exclusion: checked</p>\\n\\n<p>My Crawldata folder remains empty and there are no results. I\\'m sure I\\'m doing something stupid wrong as this is basically my first time using the software.</p>\\n\\n<p>Thanks.</p>\\n-Rapidminer 6.0.008 not saving web crawl results-<web-crawler><rapidminer>',\n",
       " '<p>I am using the Teradat 14 .NET provider.  I want to execute the following SQL in a single transaction:</p>\\n\\n<pre><code>delete mydb.mytable;\\ninsert into mydb.mytable select * from mydb.myothertable;\\n</code></pre>\\n\\n<p>This issue I have encountered is that, while the delete is instant, the insert takes a few seconds.  If a select occurs after the delete, but before the insert has committed (as opposed to before the insert has been executed), no rows return.  Therefore, I don\\'t want the results of both statements to be visible by any other <code>SELECT</code> statement until the transaction is committed.  <code>IsolationLevel.Snapshot</code> has a description that best matches what I want:</p>\\n\\n<p><code>Reduces blocking by storing a version of data that one application can read while another is modifying the same data. Indicates that from one transaction you cannot see changes made in other transactions, even if you requery.</code></p>\\n\\n<p>The problem is that Teradata 14 appears to not support this type of transaction:</p>\\n\\n<p><code>The isolation level is not supported by this version of Teradata Database.</code></p>\\n\\n<p>What do I have to do to keep the results of the <code>delete</code> and <code>insert</code> from being visible to other <code>select</code> statements until the transaction has been committed?</p>\\n\\n<p><strong>Edit</strong></p>\\n\\n<p>Here is the code I am using after dnoeth\\'s answer.  I am using a Teradata session and am putting all SQL into a single string which is still returning no results if a select is done after the delete, but before the insert as completed.  dnoeth, am I doing it as you suggested?  Notice that there is no <code>TdTransaction</code> object since I am doing <code>bt;et;</code> in the SQL.</p>\\n\\n<pre><code>Using con As TdConnection = GetNewConnection()\\n    Using cmd As TdCommand = con.CreateCommand\\n        cmd.CommandText = \"bt;delete mydb.mytable;insert into mydb.mytable select * from mydb.myview;et;\"\\n        cmd.ExecuteNonQuery()\\n    End Using\\nEnd Using\\n</code></pre>\\n-What transaction isolation level in Teradata 14 closely matches IsolationLevel.Snapshot?-<transactions><teradata><transaction-isolation>',\n",
       " \"<p>Sometimes when I try to Run or Debug program several times in a row without changes Ecipse does nothing but outputs current Java version in console. Sometimes it works OK and I didn't find any correlation between my actions and its behavior. Any insignificant change in text helps, so this error is not serious but rather irritating. But anyway, do you have any idea why it happens?</p>\\n\\n<p>Eclipse Kepler. Problem stays both for Java programs and C++ ones (via CDT).</p>\\n-Eclipse outputs java version in console instead of running program-<java><eclipse><console><version><eclipse-kepler>\",\n",
       " '<p>I am running </p>\\n\\n<pre><code>cursor.execute(\"\"\"{call myStoredProcedure(\\'abc\\')}\"\"\")\\n</code></pre>\\n\\n<p>I get the following error:</p>\\n\\n<pre><code>Error: (\\'HY000\\', \"[HY000] [Teradata][ODBC Teradata Driver][Teradata Database] \\'myStoredProcedure\\' is not a macro.  (-3855) (SQLExecDirectW)\")\\n</code></pre>\\n\\n<p>Why is Teradata thinking that myStoredProcedure is macro and not a stored procedure?</p>\\n\\n<p>I am able to run the usual select commands on the database. I can also run the stored procedure from the Teradata SQL Assisstant.</p>\\n-How to call a stored procedure from PyODBC on a Teradata database?-<sql><stored-procedures><odbc><teradata><pyodbc>',\n",
       " '<p>In the below query, i am trying to create a table. The table is created but there is not observations recorded. I have also tried using \"With Data\" but getting error for that. Could you please tell me what is wrong with the query. I am fairly new to this entire system. Any help would be appreciated.</p>\\n\\n<pre><code>options mlogic mprint symbolgen COMPRESS=YES;\\n\\nproc sql;\\n\\n         *this is changed .... libname lib1  teradata  user=abcdedede password=\"abcdcdcd123@\" database=Mydatabase tdpid=\"Myserver\" ;\\n\\n         connect to teradata  as td (user=\"abcdedede\" password=\"abcdcdcd123@\" mode=teradata fastexport=yes database=Mydatabase  tdpid=\"Myserver\");\\n\\nlibname lib \\'/export/home/userID/\\';\\n\\n\\n         create  table lib.feedbackds AS select * from connection to td\\n         (\\n         SELECT\\n         z.USER_ID,\\n         y.respns,\\n         y.txt,\\n         b.item_price*b.quantity as total\\n\\n         FROM DB1.Table2 x\\n\\n\\n         LEFT  JOIN DB1.Table1 y\\n         ON\\n         x.RSPNS_ID=y.RSPNS_ID\\n         AND\\n         x.CRE_DATE=y.CRE_DATE\\n\\n         JOIN DB2.table3 z\\n         ON\\n         x.S_ID=z.S_ID\\n\\n         LEFT JOIN MyDatabase.table4 b\\n         ON\\n         z.USER_ID=b.G_id OR z.USER_ID=b.D_id\\n\\n\\n         WHERE y.CRE_DATE BETWEEN \\'2014-06-25\\' and \\'2014-06-30\\'\\n         AND y.respns IS NOT NULL\\n         AND  x.CRE_DATE  BETWEEN \\'2014-06-29\\' and \\'2014-06-30\\'\\n        AND x.col_ID IS NOT NULL\\n         AND x.S_ID=5000001479\\n         ) ;\\n         quit;\\n</code></pre>\\n-unix sas teradata sql: no observation recoreded in the created table-<sql><database><unix><sas><teradata>',\n",
       " '<p>I have project in Eclipse that contains a file named \"feature.xml\". If I open the file Eclipse uses the Feature Manifest Editor. In my case this file has nothing to do with Eclipses features and I simply want to open it in the XML Editor. If I try this I get a dialog informing me of \"Unsupported content type in editor.\"</p>\\n\\n<p>I have tried various combinations of Content Type and File Association in the Preferences but am unable to get anywhere.</p>\\n\\n<p>Is it possible to open a file named feature.xml in the XML Editor of Eclipse?</p>\\n\\n<p>I am using Kepler.</p>\\n-Is it possible to open a file named feature.xml in the XML Editor of Eclipse?-<eclipse><eclipse-kepler>',\n",
       " \"<p>How can I get the server name using query in Teradata?\\nThat is, if I am writing a query on the 'dev' server, it should return the dev server name.</p>\\n\\n<p>for example, in Sybase : we will be using select @@servername.</p>\\n-Teradata : How to get the server name using query-<teradata>\",\n",
       " '<p>I\\'m using <em>fabric8-karaf-1.1.0.CR5</em> in a stand-alone installation and it works fine there.</p>\\n\\n<p>Now I wanted to embedd it into <em>Eclipse Kepler SR2</em>.</p>\\n\\n<p>I basically followed <a href=\"https://felix.apache.org/site/integrating-felix-with-eclipse.html\" rel=\"nofollow\">Integrating Felix with Eclipse</a>. But, instead of just adding <code>bin/felix.jar</code> to the project\\'s build path as in <em>Figure 8</em> I added all jars of <code>lib</code> and its sub-directories <code>bin</code> and <code>endorsed</code> (after <em>a lot</em> of errors were thrown at me by just adding <code>lib/karaf.jar</code>).</p>\\n\\n<p>There are still three issues:</p>\\n\\n<ol>\\n<li><p>The following is printed to System.err:</p>\\n\\n<pre><code>Jul 28, 2014 12:12:24 AM org.apache.karaf.main.SimpleFileLock lock\\nInformation: locking\\n</code></pre>\\n\\n<p>What does this error(?) message try to inform me of? That a file named <code>lock</code> is created in the project\\'s root directory? Thank you, but why?</p></li>\\n<li><p>The following is printed to System.err:</p>\\n\\n<pre><code>Error in initialization script: \\\\shell.init.script\\n(The system cannot find the file specified)\\n</code></pre>\\n\\n<p>The parenthesized is the Windows version of a FileNotFoundException\\'s detail message. There exists an <code>etc\\\\shell.init.script</code> and for the sake of the directory not mentioned I copied it to the project\\'s root folder, to no avail.</p></li>\\n<li><p>I had to comment the following bundles in <code>etc\\\\config.properties</code>:</p>\\n\\n<pre><code>#org/apache/sshd/sshd-core/0.9.0/sshd-core-0.9.0.jar=30\\n#org/apache/karaf/shell/org.apache.karaf.shell.ssh/2.3.0.redhat-610379/org.apache.karaf.shell.ssh-2.3.0.redhat-610379.jar=30\\n#io/fabric8/fabric-zookeeper/1.1.0.CR5/fabric-zookeeper-1.1.0.CR5.jar=39\\n#io/fabric8/fabric-api/1.1.0.CR5/fabric-api-1.1.0.CR5.jar=40\\n#io/fabric8/fabric-agent/1.1.0.CR5/fabric-agent-1.1.0.CR5.jar=40\\n#io/fabric8/fabric-core/1.1.0.CR5/fabric-core-1.1.0.CR5.jar=45\\n#io/fabric8/fabric-boot-commands/1.1.0.CR5/fabric-boot-commands-1.1.0.CR5.jar=45\\n#io/fabric8/fabric-commands/1.1.0.CR5/fabric-commands-1.1.0.CR5.jar=50\\n#io/fabric8/fabric-configadmin/1.1.0.CR5/fabric-configadmin-1.1.0.CR5.jar=50\\n</code></pre>\\n\\n<p>... to get rid off errors of the form:</p>\\n\\n<pre><code>ERROR: Bundle &lt;Bundle-SymbolicName&gt; [&lt;ID&gt;]\\nError starting mvn:&lt;bundle-path&gt; (org.osgi.framework.BundleException:\\nUnresolved constraint in bundle &lt;Bundle-SymbolicName&gt;:\\nUnable to resolve &lt;ID&gt;.0: missing requirement [&lt;ID&gt;.0] osgi.wiring.package;\\n(&amp;(osgi.wiring.package=&lt;Other Bundle-SymbolicName&gt;)(&lt;versions&gt;))))\\n</code></pre>\\n\\n<p>... but with these bundles disabled Fabric8 shuts down immediately.</p>\\n\\n<p>It\\'s right, the few missing  bundles I have checked aren\\'t in my local .m2 repository. But Maven works well with all other projects, in and out of Eclipse. Go on Fabric8, download them!</p></li>\\n</ol>\\n\\n<p>What am I missing?</p>\\n-How to integrate Fabric8 with Eclipse?-<java><osgi><eclipse-kepler><fabric8>',\n",
       " '<p>What is the easiest way to strip milliseconds from a timestamp field. I am extracting data from a table and I want the data to be in YYYY-MM-DD hh:mi:ss format. but I am getting ss.ssssss in the data.</p>\\n-Strip millisecond in Teradata-<teradata>',\n",
       " '<p>I have a dataset with 5000 records.  I want to add 3 columns to this dataset: 1st one with values from 1 to 5, 2nd with values from 1 to 4, 3rd with values from 1 to 3 such as follows - </p>\\n\\n<pre><code>record_id   ser_num5    ser_num4    ser_num3\\n1           1           1           1\\n2           2           2           2\\n3           3           3           3\\n4           4           4           1\\n5           5           1           2\\n6           1           2           3\\n7           2           3           1\\n8           3           4           2\\n9           4           1           3\\n10          5           2           1\\n11          1           3           2\\n12          2           4           3\\n13          3           1           1\\n14          4           2           2\\n15          5           3           3\\n16          1           4           1\\n17          2           1           2\\n18          3           2           3\\n19          4           3           1\\n20          5           4           2\\n….          \\n5000            \\n</code></pre>\\n\\n<p>How can I do this in Teradata SQL? I\\'ve tried the following code but it didn\\'t work -</p>\\n\\n<pre><code>CREATE SEQUENCE Test.SerNum5\\n    AS Int \\n    START WITH 1\\n    INCREMENT BY 1\\n    MINVALUE 1\\n    MAXVALUE 5\\n    CYCLE\\n;\\n\\nSELECT NEXT VALUE FOR Test.SerNum5;\\n</code></pre>\\n\\n<p>Also, is there anyway I can somehow merge this piece into an existing \"select columns\" statement if i can\\'t make the create sequence to work. Thanks for any help!</p>\\n-Teradata SQL - Adding columns of repeating values to existing table-<sql><teradata>',\n",
       " \"<p>I want to install node.js module in my eclipse kepler. For that i search out lot and i find out Enide-Studio-2014-011-20140303-linux-gtk3-x86_64 but i don't want to install new eclipse for node.js i just want to integrate node.js plugin in my existing eclipse kepler .Please help me to overcome this problem.</p>\\n-How to install node.js module in my eclipse kepler in ubuntu-<node.js><eclipse-kepler>\",\n",
       " '<pre><code>BEGIN;\\nCREATE TEMP FACT TABLE new_table  DISTRIBUTE BY hash(a.var3) as\\nSELECT (YEAR(a.var1) - b.var2), a.var3\\nFROM d.data1 a, c.data2 b\\nWHERE (YEAR(var1) - var2) &gt; 40\\n;\\n\\nGO\\nSELECT * FROM age_DRG;\\nEND;\\n</code></pre>\\n\\n<p>I am not sure what is wrong with this code.  I keep getting this error </p>\\n\\n<pre><code>\"Executed as Single statement.  Failed [34 : HY000] [AsterData][ASTERJDBCDSII](34) NOTICE: current transaction is aborted, queries ignored until end of transaction block () \\nElapsed time = 00:00:00.016 \\n\\nSTATEMENT 1: BEGIN;\\nCREATE Statement failed. \"\\n</code></pre>\\n-How to create a temp table in teradata-<sql><teradata><temp><teradata-aster>',\n",
       " '<p>I need some help understanding why merging two streams blocks one of the spouts of class <code>FixedBatchSpout</code>.</p>\\n\\n<p><strong>Short Description</strong>: I’m trying to merge two streams s1 and s2, but calling <code>topology.merge(s1, s2)</code> blocks the <code>FixedBatchSpout</code> (a trident spout) from which s1 originates, whereas the <code>BaseRichSpout</code> (a storm spout) from s2 seems to work properly.</p>\\n\\n<p><strong>Details</strong>: In the below main method, just adding the line <code>topology.merge(s1, s2);</code> prevents the <code>FixedBatchSpout</code> to emit past its first batch. This happens with <code>multireduce</code> as well.</p>\\n\\n<pre><code>FixedBatchSpout spout1 = new FixedBatchSpout(new Fields(\"sentence\"), 2,\\n                new Values(\"the cow jumped over the moon\"),\\n                new Values(\"the man went to the store and bought some candy”));\\n\\nFixedLoopSpout spout2 = new FixedLoopSpout(new Fields(\"sentence\"),\\n                new Values(\"THE COW JUMPED OVER THE MOON\"),\\n                new Values(\"THE MAN WENT TO THE STORE AND BOUGHT SOME CANDY\"));\\n\\nStream s1 = topology.newStream(\"hello\", spout1);\\nStream s2 = topology.newStream(\"world\", spout2);\\ntopology.merge(s1, s2);\\n\\npublic class FixedLoopSpout extends BaseRichSpout {\\n\\n    Values[] values;\\n    List&lt;Values&gt; loop = new LinkedList&lt;Values&gt;();\\n    Iterator&lt;Values&gt; head;\\n    private SpoutOutputCollector collector;\\n    private final Fields outputFields;\\n\\n    private long emitted = 0;\\n\\n    public FixedLoopSpout(Fields outputFields, Values... values) {\\n        this.outputFields = outputFields;\\n        this.values = values;\\n    }\\n\\n    public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) {\\n        this.collector = collector;\\n        for (Values value: this.values) {\\n            this.loop.add(value);\\n        }\\n        this.head = this.loop.iterator();\\n    }\\n\\n    public void nextTuple() {\\n        if (!this.head.hasNext()) {\\n            // wrap\\n            this.head = this.loop.iterator();\\n        }\\n        this.collector.emit(this.head.next(), this.emitted++);\\n        try {\\n            Thread.sleep(100);\\n        } catch (InterruptedException e) {\\n            e.printStackTrace();\\n        }\\n    }\\n\\n    public void declareOutputFields(OutputFieldsDeclarer declarer) {\\n        declarer.declare(this.outputFields);\\n    }\\n}\\n</code></pre>\\n\\n<p>Help is appreciated, Thanks!\\nJacques</p>\\n-Merging trident streams blocks the trident spout whereas the storm spout keeps working-<apache-storm><trident>',\n",
       " '<p>I have a Rapidminer process which reads from a web API, uses Read XML to process the response and XPATH to capture one of the elements in the XML. The elements can be of any number and the resulting  attribute is a concatenated string of the element/text(). </p>\\n\\n<p>As a result of the concatenated string, I have to split the string into multiple columns like this:  </p>\\n\\n<pre><code>ID  Col1  Col2 Col3 Col4 Col5 Col6\\nA   1     5    7    8\\nB   2\\nC   4\\nD   3     9    10   11   12   13\\n</code></pre>\\n\\n<p>My final goal is to transpose it into the following format:</p>\\n\\n<pre><code>ID  NewCol\\nA   1\\nA   5\\nA   7\\nA   8\\nB   2\\nC   4\\nD   3\\nD   9\\nD   10\\nD   11\\nD   12\\nD   13\\n</code></pre>\\n\\n<p>Two questions:<br>\\n 1. Can the Read XML operator be configured to read data into multiple rows instead of a long concatenated string?<br>\\n 2. If answer to 1 is negative, is there any operator which can perform the \"transpose\" task as described above(similar to melt function in R)?</p>\\n-Rapidminer data transpose equivalent to melt in R-<xml><xpath><transpose><rapidminer>',\n",
       " '<p>I am performing text mining in RapidMiner. I am crawling a website and doing some pre-processing tasks like tokenizing, lowercasing and filtering English stopwords; but still I am getting some nonsense words like \"xckxzaz\", \"xkaffqoxzomd\" or JavaScript code words like \"wpcf\". My question is, is there any way in RapidMiner to get rid of these words? I have been told that creating a stopwords dictionary is one solution but that means I\\'d have to create an entire English dictionary which doesn\\'t look so optimal. Any hint will be much appreciated!</p>\\n-How to remove non-English words using RapidMiner-<web-crawler><text-mining><stop-words><rapidminer><web-mining>',\n",
       " '<p>I am struggling to set up Glassfish 4 with Eclipse Kepler. No matter what I do, the VM arguments of the Glassfish launch configuration are not passed on to the VM running the server. Not even the argument (-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=9009) that is put into the launch configuration by default by the Glassfish Tools for Eclipse plugin.</p>\\n\\n<p>The running server has all system properties set according to the <code>domain.xml</code> of the started domain. The VM arguments specified in the start configuration are all lost on the way.</p>\\n\\n<p>I have updated the Glassfhish plugin to the latest version. I have downloaded and unzipped Glassfish and then told Eclipse where to find the runtime. I have also let Eclipse download and install the server through the add-new-runtime wizard. It makes no difference.</p>\\n\\n<p>The VM arguments text box contents are used though. If I enter two dashes I get an error, as expected, telling me that two dashes are not a valid VM option.</p>\\n\\n<p>I was not able to find anything along the lines of this problem on the web. Which gives me the feeling that I am going about this the wrong way. But how?</p>\\n\\n<p>The launch configuration:\\n<img src=\"https://i.stack.imgur.com/YFnsV.png\" alt=\"Launch configuration\"></p>\\n\\n<p>VisualVM and <code>ps -e |\\xa0grep glassfish</code> show none of the VM arguments. </p>\\n-Eclipse Glassfish launch configuration - VM arguments are ignored-<java><eclipse><glassfish><glassfish-4><eclipse-kepler>',\n",
       " '<p>I have eclipse kepler installed as well as apatna studio, While eclipse is working fine, whenever i try to edit any page on aptana, i am getting this error :</p>\\n\\n<pre><code> An error has occurred. See error log for more details.\\n org.eclipse.swt.custom.StyledText.getScrollbarsMode()I\\n</code></pre>\\n\\n<p><code>.log</code> from the <code>.metadata</code> file says (<em>showing only 1st 4 lines</em>):</p>\\n\\n<pre><code>!ENTRY org.eclipse.ui.workbench.texteditor 4 2 2014-07-31 22:44:29.901\\n!MESSAGE Problems occurred when invoking code from plug-in: \"org.eclipse.ui.workbench.texteditor\".\\n!STACK 0\\njava.lang.NoSuchMethodError: org.eclipse.swt.custom.StyledText.getScrollbarsMode()I\\n</code></pre>\\n\\n<p>I have searched all possible google links for this error, can any one please help me resolve the issue or point on right direction?</p>\\n\\n<p>** <em>feel free to retag</em></p>\\n-Not able to edit pages in Aptana-Studio-<ubuntu-12.04><aptana><eclipse-kepler>',\n",
       " '<p>I use the following code in Kepler:</p>\\n\\n<pre class=\"lang-java prettyprint-override\"><code>ProjectExplorer projExplorer = (ProjectExplorer) PlatformUI.getWorkbench().getActiveWorkbenchWindow().\\ngetActivePage().findView(IPageLayout.ID_PROJECT_EXPLORER);\\n</code></pre>\\n\\n<p>And I have added org.eclipse.ui.navigator.resources.jar, however, when I compile my project, it shows an error message:</p>\\n\\n<pre><code>The type org.eclipse.ui.navigator.CommonNavigator cannot be resolved. It is indirectly referenced from required .class \\n</code></pre>\\n\\n<p>files</p>\\n-The compiler show error message when using ProjectExplorer in Eclipse Kpler-<eclipse><eclipse-kepler>',\n",
       " '<p>I am using Eclipse Kepler with jdk1.8.0.\\nI do not face any compilation problems in import statements related to java8 or collect construct. However I get the compilation issue in:- Dish::getName...it says.. Syntax errors on tokens, delete the tokens.</p>\\n\\n<p>Just trying to run the following code:</p>\\n\\n<pre><code>List&lt;String&gt; dishNames = menu.stream()\\n                              .map(Dish::getName)\\n                              .collect(toList());\\n\\nSystem.out.println(dishNames);\\n</code></pre>\\n\\n<p>Dish is a java object with String property name.</p>\\n\\n<p>I have installed JDK8, added the latest JRE in eclipse Java Build Path.\\nJava compiler compliance.</p>\\n\\n<p>Any hints what something silly I am missing?</p>\\n-Eclipse-Kepler, Java 8 compilation issue \"syntax errors on tokens\"-<eclipse-kepler>',\n",
       " '<p>I have a checked-out version of the <a href=\"http://sourceforge.net/projects/redmin-mylyncon/\" rel=\"nofollow\">Redmine-Mylyn connector</a> open-source project. There are eight Eclipse plug-in projects plus two Eclipse features projects plus <code>.git/</code> as their sibling in the local file system. It turns out that after importing them as <code>Plug-ins and Fragments</code> (with options <code>Projects with source folders</code>) Eclipse recognizes all plug-in projects but <code>net.sf.redmine_mylyn</code> in the Git repository (i.e. it shows a corresponding suffix next to these project names in its Project Explorer view.) </p>\\n\\n<p>So <code>net.sf.redmine_mylyn</code> is apparently not recognized with these apparent effects: it has no such suffix in the Project Explorer view and its <code>Team</code> menu list only <code>Apply Patch ...</code> (but no other option related to either Git or sharing). However, if I run <code>git status</code> from the command line e.g. on <code>net.sf.redmine_mylyn/build.properties</code> I can see that it very well also part of the same repository. The Project Explorer does not show the two feature projects at all.</p>\\n\\n<p>What could be the reason that Eclipse (Kepler) apparently does not recognize one plug-in project in the Git repository and how could I make it do so?</p>\\n\\n<p><strong>UPDATE</strong></p>\\n\\n<p>Here is how the <code>.project</code> files are arranged in the file system (output from <code>find . -name .project</code>):</p>\\n\\n<pre><code>./net.sf.redmine_mylyn/.project\\n./net.sf.redmine_mylyn.api/.project\\n./net.sf.redmine_mylyn.api-test/.project\\n./net.sf.redmine_mylyn.common/.project\\n./net.sf.redmine_mylyn.core/.project\\n./net.sf.redmine_mylyn.core-test/.project\\n./net.sf.redmine_mylyn.core.extension.timesheet_extensions_plugin/.project\\n./net.sf.redmine_mylyn.extensions.feature/.project\\n./net.sf.redmine_mylyn.feature/.project\\n./net.sf.redmine_mylyn.ui/.project\\n</code></pre>\\n\\n<p>The <code>.project</code> files reference three or four <code>buildCommand</code>s (<code>org.eclipse.jdt.core.javabuilder</code>, <code>org.eclipse.pde.ManifestBuilder</code>, <code>org.eclipse.pde.SchemaBuilder</code>, <code>org.eclipse.pde.ds.core.builder</code>)\\nand two natures (<code>org.eclipse.pde.PluginNature</code>, <code>org.eclipse.jdt.core.javanature</code>). The offending project is one of those which do not reference <code>org.eclipse.pde.ds.core.builder</code>.</p>\\n\\n<p>And <code>find . -name .git</code> gives:</p>\\n\\n<pre><code>./.git\\n</code></pre>\\n-Eclipse apparently picks up all but one projects from Git repository-<eclipse><git><egit><eclipse-kepler>',\n",
       " \"<p>I am stuck in an issue where I want the two latest values of a column(REVISION) based on the max(last_updated_date).</p>\\n\\n<p><strong>Scenario:</strong></p>\\n\\n<p>There is a Revision column corresponding to each part column( hardware part). Whenever there is any change in the part, the revision gets changed and the last_updated date also changes. Revision can be anything numbers, alphabets, - _ etc. </p>\\n\\n<p>Suppose if I have 100 parts and 60 parts do not change and 40 changes. So 40 will have at least two latest revisions. In total there would be <strong>60+40*2=140 parts</strong> in the output.</p>\\n\\n<p>Without using QUALIFY, I am getting more than 5M distinct parts. So at least I should get 5M records(in case when no part has underwent any revision).</p>\\n\\n<pre><code>SELECT DISTINCT FROM_NAME\\nFROM(\\nselect\\nCDR_ODS_R_GE_OBJ_HST .FROM_ID as FROM_ID,\\nCDR_ODS_R_GE_OBJ_HST .FROM_NAME as FROM_NAME  ,\\nCDR_ODS_R_GE_OBJ_HST .FROM_REVISION as FROM_REVISION,\\nmax(CDR_ODS_R_GE_OBJ_HST .LAST_UPDATE_DATE) as LAST_UPDATE_DATE\\n--RANK( )  OVER ( ORDER BY max(CDR_ODS_R_GE_OBJ_HST .LAST_UPDATE_DATE) DESC) AS RANK1\\nfrom GEEDW_PLM_ODS_BULK_V.CDR_ODS_R_GE_OBJ_HST CDR_ODS_R_GE_OBJ_HST\\n--WHERE CDR_ODS_R_GE_OBJ_HST.FROM_name='323A4747UUP15A' \\n--QUALIFY RANK1&lt;=2 \\ngroup by 1,2,3\\n) TM\\n</code></pre>\\n\\n<p>14728.20721.304.13308(from_id)       R-0331128(from_name)                   -(revision)                     8/7/2013 20:30:02(last_updated date)</p>\\n\\n<p>But while using qualify &lt;=2 with rank getting only 186 parts.</p>\\n\\n<pre><code>select\\nCDR_ODS_R_GE_OBJ_HST .FROM_ID as FROM_ID,\\nCDR_ODS_R_GE_OBJ_HST .FROM_NAME as FROM_NAME  ,\\nCDR_ODS_R_GE_OBJ_HST .FROM_REVISION as FROM_REVISION,\\nmax(CDR_ODS_R_GE_OBJ_HST .LAST_UPDATE_DATE) as LAST_UPDATE_DATE\\n,RANK( )  OVER ( ORDER BY max(CDR_ODS_R_GE_OBJ_HST .LAST_UPDATE_DATE) DESC) AS RANK1\\nfrom GEEDW_PLM_ODS_BULK_V.CDR_ODS_R_GE_OBJ_HST CDR_ODS_R_GE_OBJ_HST\\n--WHERE CDR_ODS_R_GE_OBJ_HST.FROM_name='323A4747UUP15A' \\nQUALIFY RANK1&lt;=2\\ngroup by 1,2,3\\n</code></pre>\\n\\n<p>In Nutshell, from the below query, i want the top two values for revision 0024 corresponding to the latest 2 updated date. </p>\\n\\n<pre><code>select FROM_NAME,FROM_REVISION, LAST_UPDATE_DATE\\nfrom GEEDW_PLM_ODS_BULK_V.CDR_ODS_R_GE_OBJ_HST CDR_ODS_R_GE_OBJ_HST\\n\\n\\n0024    301345498360631 1/24/2014 11:22:17\\n0024    431365606243002 12/16/2013 20:16:44\\n0024    491333037555534 6/6/2013 18:08:51\\n</code></pre>\\n-Teradata-- Qualify with Rank not giving exact results-<sql><teradata>\",\n",
       " '<p>I wanted to ask this question in RapidMiner Community but after 2 days, still no answer.\\nIt might be an easy question for you. I want to find a meaningful relation between a couple of column values. \\n the table is like this:</p>\\n\\n<p><em>SiteID</em>  , <em>Number Of 2MB users</em> ,   <em>Numberof 4MB users</em>, <em>Average 2MB speed Usage</em>, <em>Average 4MB speed Usage</em> , <em>Congestion Status</em></p>\\n\\n<p>It is clear that there is a relation between the number of users in each site and Average Usage of them and Congestion Status of the site.</p>\\n\\n<p>but how to find it? a step by step guide would be helpful . \\nSo many thanks</p>\\n-Association Rules in Rapid Miner-<rapidminer>',\n",
       " \"<p>I'm new to Teradata and I'm facing a problem I didn't have with the previous database I used.\\nBasically, I'm trying to reduce the number of rows returned in subqueries inside a where clause. I had no problem doing this previously with the ROWNUM function.</p>\\n\\n<p>My previous query was something like:</p>\\n\\n<pre><code>SELECT * FROM myTable\\nWHERE field1 = 'foo' AND field2 in(\\n    SELECT field2 FROM anotherTable\\n    WHERE field3 = 'bar' AND ROWNUM&lt;100);\\n</code></pre>\\n\\n<p>Since I can't use ROWNUM in TD, I've looked for equivalent functions or at least functions that would get me where I wanted even if they were'nt exactly equivalent. \\nI found and tried : ROW_NUMBER, TOP and SAMPLE.</p>\\n\\n<p>I tried ROW_NUMBER() but Teradata doesn't allow analytic functions in WHERE clauses.\\nI tried TOP N but this option is not supported in a subquery.\\nI tried SAMPLE N but it is not supported in subqueries either.</p>\\n\\n<p>So... I have to admit I'm a bit stuck right now and was wondering if there was any solution that would allow me to limit the number of rows returned in a subquery using Teradata and that would be pretty similar to what I did up to now?\\nAlso, if there aren't any, how would it be possible to build the query differently to use it appropriately with Teradata?</p>\\n\\n<p>Thanks!</p>\\n-Limiting the number of rows in subqueries with Teradata-<sql><subquery><limit><teradata>\",\n",
       " '<p>I have a table with the following columns and data. Data describes certain customer activity periods</p>\\n\\n<pre><code>cust_id    s_date       e_date\\n11111    01.03.2014   31.03.2014\\n11111    10.04.2014   30.04.2014\\n11111    01.05.2014   10.05.2014\\n11111    15.06.2014   31.07.2014\\n22222    01.04.2014   31.05.2014\\n22222    01.06.2014   30.06.2014\\n22222    01.07.2014   15.07.2014\\n</code></pre>\\n\\n<p>And I want to write a query which gives this result:</p>\\n\\n<pre><code>cust_id    s_date       e_date\\n11111    01.03.2014   10.05.2014\\n11111    15.06.2014   31.07.2014\\n22222    01.04.2014   15.07.2014\\n</code></pre>\\n\\n<p>The query result purpose is to \"merge\" rows into one row when customer IN-activity period is less than 15 days. I can handle with \"1 row preceding\" but if needed to merge 3 or more rows then it does not work. I run out of ideas how to write this query.</p>\\n\\n<p>My \"half\" 1 row preceding query:</p>\\n\\n<pre><code>SELECT cust_id\\n     , start_date     as current_period_start_date\\n     , end_date       as current_period_end_date\\n     , end_date+15    as current_period_expired_date\\n     , coalesce(\\n            min(current_period_expire_date)\\n           over(partition by cust_id\\n                    order by start_date\\n                     rows between 1 preceding and 1 preceding)\\n               , cast(\\'1900-01-01\\' as date)) as previous_period_expire_date\\n     , case \\n         when current_period_start_date &lt;= previous_period_expire_date\\n         then min(current_period_start_date)\\n             over(partition by cust_id\\n                      order by start_date\\n                       rows between 1 preceding and current row)\\n         else current_period_start_date\\n       end as new_current_period_start_date\\n\\n  FROM MY_DB.my_table\\n     . . .\\n</code></pre>\\n\\n<p>Also, is it possible to change preceding into dynamical way like this?</p>\\n\\n<pre><code>... over(partition by ... order by ... rows between X preceding and current row)\\n</code></pre>\\n-Teradata partitioned query ... following rows dynamically-<sql><teradata>',\n",
       " '<p>Is there a way to automate dealing with/ignoring duplicate names when creating a new table in Teradata.</p>\\n\\n<p>Assume I\\'m joining a table with itself. The table has columns </p>\\n\\n<pre><code>id, email, address1, address2, city, state, zip.\\n</code></pre>\\n\\n<p>,</p>\\n\\n<pre><code>create table mytable2 as (\\n  select a.*, b.* from mytable a\\n  left join mytable b on a.email = b.email\\n  where a.id &lt;&gt; b.id\\n) with data\\n</code></pre>\\n\\n<p>requires me to write out </p>\\n\\n<pre><code>a.address1 as aaddress1, b.address1 as baddress1, a.address2 as aaddress2...\\n</code></pre>\\n\\n<p>Is there an easier way of dealing with this? When I\\'m joining many tables, this hurts my productivity. Often these tables are for research and the potential ambiguity is not a problem. </p>\\n-Dealing with duplicate names in Teradata \"create table with data\"-<sql><teradata>',\n",
       " \"<p>I am using a mongojdbcdriver-0.0.2-javadoc.jar driver to extend mongodb with rapidminer. It really doesn't works. The driver added to jdbc_properties.xml is not getting listed in the rapidminer studio edition. </p>\\n\\n<p>Could anyone please help on this ?</p>\\n-How to integrate MongoDB and Rapidminer?-<mongodb><rapidminer>\",\n",
       " '<p>I am a very beginner for Teradata.And I am migrating Oracle queries to Teradata. I have a query like below.</p>\\n\\n<pre><code>CREATE BITMAP INDEX &lt;INDEX_NAME&gt; ON &lt;TABLE_NAME&gt;(&lt;COLUMN_NAME&gt;)LOCAL STORE IN(&lt;TABLE_SPACE&gt;);\\n</code></pre>\\n\\n<p>As per understanding, we dont have BITMAP index in Teradata, but how to implement the above in Teradata. Please help.</p>\\n\\n<p>Thanks in advance.</p>\\n-Teradata Equivalent for Oracle BITMAP INDEX Local store IN-<sql><database><teradata>',\n",
       " \"<p>I have a query that is using <code>Table1</code> and <code>Table2</code> with left outer join on <code>'usage'</code>. Now i have to join that query with the <code>Table3</code> with (I guess recursive sql) to generate the <code>'Resulting table'</code>.</p>\\n\\n<p>I saw lot of examples on recursive sql, but didnt find any thing that is using left outer join.\\n Now my existing query is like this</p>\\n\\n<pre><code>     select PIN,  startDt,  StartTm,  usage,  Min \\n     from Table1 t1 left Outer join Table2 t2 on t1.usage= t2.usage;\\n</code></pre>\\n\\n<p>How can i do the <code>Table3</code> with this query, so that ratGrp will be in comma separated way? Please help!!</p>\\n\\n<pre><code>     Table1\\n     PIN  startDt  StartTm  usage  Min\\n     -----------------------------------------\\n     123 08/03/2014 12:12:00 500  4567\\n     234 08/04/2014 12:12:00 200  4568\\n     .....\\n     Table2\\n     1stCol  2ndCol  usage\\n      ------------------------\\n     abc        234      500\\n\\n    Table3\\n    PIN   ratGrp\\n    -----------------\\n    123   3300\\n    123  100\\n    123  103\\n    234  3300\\n    234  550\\n\\n    Resulting table\\n    PIN startDt  StartTm  usage  Min  ratGrp\\n    -----------------------------------------------\\n    123 08/03/2014 12:12:00 500  4567   3300,100,103\\n    234 08/04/2014 12:12:00 200  4568   3300,550\\n</code></pre>\\n-Recursive sql with Left outer join in Teradata-<sql><teradata><recursive-query>\",\n",
       " '<p>I am a relatively new user to KNIME. I have been trying to install the \\'gWidgetsRGtk2\\' package through the R snippet, but it keeps throwing me an error when i try to load it. </p>\\n\\n<pre><code>if(require(\"gWidgetsRGtk2\")){\\n  print(\"gWidgetsRGtk2 is loaded correctly\")\\n}else{\\n  print(\"Trying to install gWidgetsRGtk2\")\\n  install.packages(\"gWidgets\",dependencies=TRUE)\\n\\noptions(\"guiToolkit\"=\"RGtk2\")\\n</code></pre>\\n\\n<p>I have tried changing the R path in the global preferences, and also tried adding this package directly into the KNIME-R folder.</p>\\n\\n<p>None of these methods seem to work. Can anyone suggest a way out?</p>\\n-KNIME R integration - Package installation-<r><user-interface><gwidgets><knime>',\n",
       " '<p>I am completely new in Grails. We are trying to create a web application in which we are fetching data from teradata and displaying it to the user. \\nCan we connect to teradata using Grails? How can this be implemented?\\nIf yes then is this connection going to be asynchronous web service?</p>\\n-Connecting to teradata using Grails-<grails><teradata>',\n",
       " '<p>Let me simplify my problem with sample Iris dataset. Here is the output I see with FPGrowth operator launched over Iris dataset:</p>\\n\\n<p><img src=\"https://i.stack.imgur.com/LFljR.png\" alt=\"enter image description here\"></p>\\n\\n<p>See my notes in red, in this sample I only need a3_range1 and a2_range5 as well as all 3-item sets, because all the other ones are subsets of the larger sets. </p>\\n\\n<p>Is there any way I can do it with the help of FPGrowth itself? Or do I have to workaround with ExecuteScript? In latter case, I can\\'t seem to be able even import and iterate over the input after FPGrowth: if I do this:</p>\\n\\n<pre><code>ExampleSet exampleSet = operator.getInput(ExampleSet.class);\\n</code></pre>\\n\\n<p>I\\'m getting \"The operator needs some input of ExampleSet which is not provided\". If I change it to:</p>\\n\\n<pre><code>FPGrowth exampleSet = operator.getInput(FPGrowth.class);\\n</code></pre>\\n\\n<p>it complaints that it cannot resolve FPGrowth class. </p>\\n\\n<p>How do I import it in ExecuteScript Groovy code?</p>\\n-Rapidminer FPGrowth returning subsets as well. I only need the maximum frequent item sets-<r><groovy><data-mining><text-mining><rapidminer>',\n",
       " '<p>i am developing one project in eclipse Kepler version \\ni want to put a lock for eclipse project source code, so that in other machines my project source code should not work in other eclipse.  </p>\\n\\n<p>for this any solution is there ? guide me on this.\\n    is there any tool for this type of security issues?.</p>\\n\\n<p>thanks in advance.</p>\\n-is there any lock for eclipse project source code-<eclipse-plugin><eclipse-kepler>',\n",
       " \"<p>All my Ansible playbooks/roles are checked in to my git repo.</p>\\n\\n<p>However, for Ansible Galaxy roles I always have to explicitly download them one by one on every machine I want to run Ansible from.</p>\\n\\n<p>It's even tough to know in advance exactly which Ansible Galaxy roles are needed until Ansible complains about a missing role at runtime.</p>\\n\\n<p>How is one supposed to manage the Ansible Galaxy role dependencies? I would like to either have them checked into my git repo along with the rest of my ansible code or have them automatically be identified and downloaded when I run Ansible on a new machine.</p>\\n-How to automatically install Ansible Galaxy roles?-<ansible><ansible-galaxy>\",\n",
       " \"<p>Hi i am relatively new to teradata. I have a row with 2 dates. I need to get the min between these tow dates and show it as third date. The dates are in YYYYMMDD format. Here are all the posiblities</p>\\n\\n<p>example table</p>\\n\\n<blockquote>\\n  <ul>col1--date1--date2 \\n  <li>123--20140802--20140619\\n  <li>124--20140802--0\\n  <li>124--0--20140802    \\n  <li>125--0--0 </ul>\\n</blockquote>\\n\\n<p>I need my result set to be</p>\\n\\n<blockquote>\\n  <ul>col1--date1--date2--min_date\\n  <li>123--20140802--20140619--6/19/2014\\n  <li>124--20140802--0--8/2/2014(non zero will be min_date)\\n  <li>124--0--20140802--8/2/2014(non zero will be min_date)\\n  <li>125--0--0--?</ul>\\n</blockquote>\\n\\n<p>This is what i could come up with </p>\\n\\n<pre><code>select col1, date1, date2, \\ncase \\n     when date1 &lt;&gt; 0 and date2 = 0 then  cast((date1 - 19000000) as date)\\n     when date1  = 0 and date2 &lt;&gt; 0 then  cast((date2 - 19000000) as date)\\n     when date1 = 0 and date2  = 0 then cast(null as date)\\n     when date1  &gt; date2 \\n            then \\n                cast((date2 - 19000000) as date) \\n            else \\n                 cast((date1 - 19000000) as date)\\nend as min_date\\n</code></pre>\\n\\n<p>This does give me result, but query is slow. I wanted to know if there was better and efficient way of doing this, please let me know. I also need to show the min_date in another format as below</p>\\n\\n<blockquote>\\n  <p>col1--date1--date2--min_date--min_month\\n  <li>123--20140802--20140619--6/19/2014--Jun, 2014\\n  <li>124--20140802--0--8/2/2014--Aug, 2014\\n  <li>124--0--20140802--8/2/2014--Aug, 2014\\n  <li>125--0--0--null--null</p>\\n</blockquote>\\n\\n<p>For min_month i know i can use this if i know which of the dates to use</p>\\n\\n<pre><code>cast(cast((date2 or date2 - 19000000) as date format 'MMM') as Char(3))\\n|| ', ' || \\ncast(cast((date2 or date2 - 19000000) as date format 'YYYY') as Char(4))\\n</code></pre>\\n\\n<p>Any help is highly appreciated.\\nThanks in advance..</p>\\n-Teradata min between two dates-<teradata>\",\n",
       " '<p>I am working on a Oracle to TD migration project. In that, I have to write a external file from Teradata SP which is called from a bteq (through ksh in unix). I thought of writing a file from bteq itself, but the problem is, I am having lot variables which i cant use in bteq. Please help me here.</p>\\n\\n<p>Thanks in Advance,</p>\\n-how to write external files from Teradata SP?-<sql><teradata>',\n",
       " '<p>First time into the realm of Luigi (and Python!) and have some questions.  Relevant code is:</p>\\n\\n<pre><code>from Database import Database\\nimport luigi\\n\\nclass bbSanityCheck(luigi.Task):\\n\\n  conn = luigi.Parameter()\\n  date = luigi.Parameter()\\n  def __init__(self, *args, **kwargs):\\n    super(bbSanityCheck, self).__init__(*args, **kwargs)\\n    self.has_run = False\\n\\n  def run(self):\\n    print \"Entering run of bb sanity check\"\\n    # DB STUFF HERE THAT DOESN\"T MATTER\\n   print \"Are we in la-la land?\"\\n\\n  def complete(self):\\n    print \"BB Sanity check being asked for completeness: \" , self.has_run\\n    return self.has_run\\n\\nclass Pipeline(luigi.Task):\\n  date = luigi.DateParameter()\\n\\n  def requires(self):\\n    db = Database(\\'cbs\\')\\n    self.conn = db.connect()\\n    print \"I\\'m about to yield!\"\\n    return bbSanityCheck(conn = self.conn, date = self.date)\\n\\n\\n  def run(self):\\n    print \"Hello World\"\\n    self.conn.query(\"\"\"SELECT * \\n              FROM log_blackbook\"\"\")\\n    result = conn.store_result()\\n\\n    print result.fetch_row()\\n\\n  def complete(self):\\n    return False\\n\\nif __name__==\\'__main__\\':\\n  luigi.run()\\n</code></pre>\\n\\n<p>Output is here (with relevant DB returns removed \\'cause):</p>\\n\\n<pre><code>DEBUG: Checking if Pipeline(date=2013-03-03) is complete\\nI\\'m about to yield!\\nINFO: Scheduled Pipeline(date=2013-03-03)\\nI\\'m about to yield!\\nDEBUG: Checking if bbSanityCheck(conn=&lt;_mysql.connection open to \\'sas1.rad.wc.truecarcorp.com\\' at 223f050&gt;, date=2013-03-03) is complete\\nBB Sanity check being asked for completeness:  False\\nINFO: Scheduled bbSanityCheck(conn=&lt;_mysql.connection open to \\'sas1.rad.wc.truecarcorp.com\\' at 223f050&gt;, date=2013-03-03)\\nINFO: Done scheduling tasks\\nDEBUG: Asking scheduler for work...\\nDEBUG: Pending tasks: 2\\nINFO: [pid 5150] Running   bbSanityCheck(conn=&lt;_mysql.connection open to \\'sas1.rad.wc.truecarcorp.com\\' at 223f050&gt;, date=2013-03-03)\\nEntering run of bb sanity check\\nAre we in la-la land?\\nINFO: [pid 5150] Done      bbSanityCheck(conn=&lt;_mysql.connection open to \\'sas1.rad.wc.truecarcorp.com\\' at 223f050&gt;, date=2013-03-03)\\nDEBUG: Asking scheduler for work...\\nINFO: Done\\nINFO: There are no more tasks to run at this time\\nINFO: There are 1 pending tasks possibly being run by other workers\\nINFO: Worker was stopped. Shutting down Keep-Alive thread\\n</code></pre>\\n\\n<p>So the questions:</p>\\n\\n<p>1.) Why does \"I\\'m about to yield\" get printed twice?</p>\\n\\n<p>2.) Why is \"hello world\" never printed?</p>\\n\\n<p>3.) What is the \"1 pending tasks possibly run by other workers\"?</p>\\n\\n<p>I prefer super-ultra clean output because it is way easier to maintain.  I\\'m hoping I can get these warning equivalents ironed out.</p>\\n\\n<p>I\\'ve also noted that requires either \"yield\" or \"return item, item2, item3\".  I\\'ve read about yield and understand it.  What I don\\'t get is which convention is considered superior here or if their are subtle differences that I being new to the language am not getting.</p>\\n-Where did the Luigi task go?-<python><hadoop><luigi>',\n",
       " \"<p>I am developing an application which uses external datasources. My Application supports multiple databases(viz. MySQl,MsSQl,Teradata, Oracle, DB2 etc.). When i create a datasource, I allow user to assign a primary key(pk) to the datasource. Now, <strong>I am not checking if the user selected column is primary key or not in actual database</strong>. I just want that, while retrieving data from database, the records which have null/blank value in <em>user selected primary key</em> should get dropped. I have created a filter supporting all other databases except for DB2 and Teradata.</p>\\n\\n<p>Sample Query for other databases:</p>\\n\\n<ol>\\n<li><code>Select * from MY_TABLE where PK_COLUMN IS NOT NULL and PK_COLUMN !='';</code></li>\\n<li><code>Select * from MY_TABLE where PK_COLUMN IS NOT NULL AND cast(PK_COLUMN as varchar) !=''</code></li>\\n</ol>\\n\\n<p>DB2 and Teradata:</p>\\n\\n<p>The <code>PK_COLUMN !=''</code> and <code>cast(PK_COLUMN as varchar) !=''</code> conditions gives error for int datatype in DB2 and teradata because: \\n - column with int type cannot be gven the above mentioned conditions and also we cannot cast the int type columns to varchar directly in DB2 and Teradata.</p>\\n\\n<p>I want to create a query to drop null/blank value from the database provided table name and user pk column name as string. (I do not know the pk_column_type while creating the query so the query should be uniform to support all datatypes)</p>\\n\\n<p><strong>NOTE: The <code>pk</code> here is not actual pk, it is just a dummy pk assigned by my application user. So this can be a normal column and thus can have null/blank values.</strong></p>\\n\\n<p>I have created a query as:</p>\\n\\n<p><code>Select * from MY_TABLE where PK_COLUMN IS NOT NULL AND cast(cast(PK_COLUMN as char) as varchar) !=''</code></p>\\n\\n<p>My Question:</p>\\n\\n<ol>\\n<li>Will this solution(double casting) support all datatypes in DB2 and Teradata?</li>\\n<li>If not, can I come up with a better solution?</li>\\n</ol>\\n-SQL query to drop null/blank values from Database (DB2 and Teradata specific)-<mysql><sql><database><db2><teradata>\",\n",
       " '<p>Iam trying to load some data using tpump utility from Unix Console. \\nThe data has various datatypes viz., text, number, decimal, date. </p>\\n\\n<p>Now, iam stuck as what should be the FORMAT type i need to specify in the tpump script. \\nI went through the tpump manual, but could not decipher the FORMAT type to used. \\nThe data/columns are delimited by \"|\" symbol.</p>\\n\\n<p>Any info/hint in using the appropriate FORMAT type would be of great help.</p>\\n\\n<p>If this is a duplicate question, please help me with the actual question link.</p>\\n\\n<p>Thanks a lot in advance.</p>\\n-Teradata tpump utility-<unix><teradata>',\n",
       " \"<p>When I perform the following statement, the date formatting is lost in epc.LAST_LICPLT_REPRINT_DT:</p>\\n\\n<pre><code> ,CASE When epc.LAST_LICPLT_REPRNT_DT is null Then ''\\n ELSE '' + epc.LAST_LICPLT_REPRNT_DT \\n END as LAST_LICPLT_REPRNT_DT\\n</code></pre>\\n\\n<p>I've tried to use </p>\\n\\n<pre><code>  cast(epc.LAST_LICPLT_REPRNT_DT as date)\\n</code></pre>\\n\\n<p>and   </p>\\n\\n<pre><code> cast(epc.LAST_LICPLT_REPRNT_DT as DATE FORMAT 'YYYYMMDD')\\n</code></pre>\\n\\n<p>nether work.  </p>\\n-SQL Teradata Date Case When Then Loses Date Format-<sql><teradata>\",\n",
       " '<p>I have the following table for people running a marathon</p>\\n\\n<pre><code>person  start       end\\nmike    2-Jun-14    2-Jul-14\\nnike    3-Jul-14    9-Aug-14\\nmini    1-Aug-14    3-Sep-14\\nsara    25-Jun-14   27-Jun-14\\nsteve   12-Jun-14   3-Jul-14\\nstan    2-Jun-14    2-Aug-14\\npete    3-Jul-14    9-Aug-14\\ntara    5-Jul-14    5-Sep-14\\n</code></pre>\\n\\n<p>I need to create a table that shows if a person was in the process of running at the beginning of every month  </p>\\n\\n<ul>\\n<li><em>On Jul 1, 2014</em> there were 3 people running the marathon: mike,\\nsteve, stan </li>\\n<li><em>On Aug 1,2014</em> there were 5 people: nike, mini, stan,\\npete,tara </li>\\n<li><em>On Sep 1,2014</em> there were 2 people: mini, tara  </li>\\n<li>etc . . </li>\\n</ul>\\n\\n<p><strong>The desired table should look like this</strong> </p>\\n\\n<pre><code>person  running on\\nmike    1-Jul-14\\nnike    1-Aug-14\\nmini    1-Aug-14\\nmini    1-Sep-14\\nsteve   1-Jul-14\\nstan    1-Jul-14\\nstan    1-Aug-14\\npete    1-Aug-14\\ntara    1-Aug-14\\ntara    1-Sep-14\\n</code></pre>\\n\\n<hr>\\n\\n<p>Right now, to do this I am running a separate query for each month. <strong>This is very tedious</strong>   I suspect that i should be using<code>recursive</code>, but I do not know how to implement it here. </p>\\n\\n<p><em>PS: I have Teradata 14 with all functions, except I cannot write my own <code>udf</code>s and <code>procedures</code>. How can I get the final table using Teradata sql ?</em>  </p>\\n-How to use recursive with dates in Teradata?-<teradata>',\n",
       " '<p>I am able to use standard spout,bolt combination to do streaming aggregation\\nand works very well in happy case, when using tick tuples to persist data at some interval\\nto make use of batching. Right now i am doing some failure management (tracking off tuples not saved etc) myself.(i.e not ootb from storm)</p>\\n\\n<p>But i have read that trident gives you a higher abstraction and better failure management.\\nWhat i dont understand is whether there is tick tuple support in trident. Basically\\nI would like to batch in memory for the current minute or so and persist any aggregated data\\nfor the previous minutes using trident.</p>\\n\\n<p>Any pointers here or design suggestions would be helpful.</p>\\n\\n<p>Thanks</p>\\n-Using tick tuples with trident in storm-<apache-storm><trident>',\n",
       " '<p>I\\'m trying to convert SAS code into Teradata and I\\'m struggling how to convert the code below. The issue is how to convert the retain statement into sql.</p>\\n\\n<p>** Datastep code</p>\\n\\n<pre><code>data test2;\\n  set test1;\\n  by name_id;\\n  format counter 8.0;\\n  retain counter year_counter;\\n  if first.name_id then do;\\n    counter = month_start;\\n    year_counter=year_start;\\n  end;\\n  else counter=counter+1;\\n  if(counter=13) then do;\\n    counter=1;\\n    year_counter=year_counter+1;\\n  end;\\n  mmyy_counter=compress(counter|\"/\"|year_counter);\\n  mmyy=compress(month|\"/\"|year);\\nrun;\\n</code></pre>\\n\\n<p>**Data</p>\\n\\n<pre><code>name_id  month_start  month_year\\n12131as  07           2010\\n12132ab  02           2003\\n</code></pre>\\n\\n<p>**My thoughts</p>\\n\\n<p>I was thinking of putting this in a procedure, although I\\'m not sure how.\\nAnother possibility is to use row_number, but again I am not sure how to start.</p>\\n-How to convert a SAS retain in Teradata sql?-<sql><sas><teradata>',\n",
       " \"<p>For my thesis, I'm trying to perform sentiment analysis on larger (up to 2GB) files of text documents (product reviews) using RapidMiner Community Edition with the Text Mining extension.</p>\\n\\n<p>I'd like to generate bigrams for that. But for a small 70MB csv file with not even 100.000 documents the generation of bigrams already takes 12 hours on an eight-core 32GB RAM machine (with RapidMiner configured to use 28GB and all cores). </p>\\n\\n<p>I've been able to reduce it to 3 hours using filters with the downside of having bigrams, which do not exist in the original documents. I'm not sure, if this is as far as I can get. </p>\\n\\n<p>My current process looks like this:</p>\\n\\n<p><code>Read CSV</code> -> <code>Process documents</code> (<code>Tokenize</code> -> <code>Stem</code> -> <code>Filter stop words</code> -> <code>Filter length &lt; 2</code> -> <code>Generate n-grams</code> ) -> <code>Split Validation</code></p>\\n\\n<p>Removing the n-gram operator drops precision and recall down to ~60%, which is far from being acceptable. Thanks for any ideas, how to speed this up.</p>\\n-RapidMiner: How can I efficiently generate n-grams?-<text-mining><rapidminer>\",\n",
       " '<p>I have a TIMESTAMP column in a teradata table. I want to convert the timestamp to epoch value. Can someone shed some light on how to do this.</p>\\n-Timestamp to epoch conversion in teradata-<teradata>',\n",
       " '<p>I tried my best to resolve this issue by myself but didn\\'t get any solution. I don\\'t know why this error coming while it was working fine earlier.</p>\\n\\n<p>System: 32-bit Windows\\nEclipse Kepler</p>\\n\\n<pre><code>&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\"   xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\\nxsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt;\\n&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;\\n</code></pre>\\n\\n<p>This error showing in pom file in header</p>\\n\\n<p>Failure to transfer org.apache.maven.plugins:maven-plugins:pom:21 from <a href=\"http://repo.maven.apache.org/maven2\" rel=\"nofollow\">http://repo.maven.apache.org/maven2</a> was cached in the local repository, resolution will not be \\n reattempted until the update interval of central has elapsed or updates are forced. Original error: Could not transfer artifact org.apache.maven.plugins:maven-plugins:pom:\\n 21 from/to central (<a href=\"http://repo.maven.apache.org/maven2\" rel=\"nofollow\">http://repo.maven.apache.org/maven2</a>): No response received after 60000</p>\\n-Unable to build and run pom file, Its showing error. Failure to transfer org.apache.maven.plugins:maven-plugins:pom:21-<maven><selenium-webdriver><maven-2><eclipse-kepler>',\n",
       " '<p>I am new to datamining and learning rapidminer. I need to implement an SVM for a project I am doing. However I am stuck since no matter the SVM It just runs for hours and days without knowing if it\\'s close or not to finish.</p>\\n\\n<p>I already removed  as many features possible using the Relieff filter and the Forward Selection wrapper and I am using the Linear Karnel which should be the quickest,the SVM has C of 0. The dataset itself as 3950 objects with 14 dimensions which I don\\'t think it\\'s a lot.</p>\\n\\n<p>The only reason why I can think of for taking so much time is that I am using 10-cross validation, but even so It shouldn\\'t take several days.\\nSo my question are:</p>\\n\\n<p>1-Seeing how I implemented my svm in the exemple bellow is there anything I can change to reduce the runtime?</p>\\n\\n<p>2-In rapidminer is there any sort of way to see what\\'s happening in the SVM to see why is it taking so long? Or at least to check in which iteration of the Cross Validation is?</p>\\n\\n<p>The process itself using already the files post pre-processing( I can\\'t share the dataset) is like this:</p>\\n\\n<pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?&gt;\\n&lt;process version=\"5.3.008\"&gt;\\n  &lt;context&gt;\\n    &lt;input/&gt;\\n    &lt;output/&gt;\\n    &lt;macros/&gt;\\n  &lt;/context&gt;\\n  &lt;operator activated=\"true\" class=\"process\" compatibility=\"5.3.008\" expanded=\"true\" name=\"Process\"&gt;\\n    &lt;parameter key=\"logverbosity\" value=\"all\"/&gt;\\n    &lt;parameter key=\"logfile\" value=\"D:\\\\testexrff.xrff\"/&gt;\\n    &lt;process expanded=\"true\"&gt;\\n      &lt;operator activated=\"true\" class=\"read_xrff\" compatibility=\"5.3.008\" expanded=\"true\" height=\"60\" name=\"Read XRFF (4)\" width=\"90\" x=\"45\" y=\"165\"&gt;\\n        &lt;parameter key=\"data_file\" value=\"C:\\\\Users\\\\glintthssig\\\\Desktop\\\\wrapper\"/&gt;\\n      &lt;/operator&gt;\\n      &lt;operator activated=\"true\" class=\"x_validation\" compatibility=\"5.3.008\" expanded=\"true\" height=\"112\" name=\"Validation (11)\" width=\"90\" x=\"246\" y=\"120\"&gt;\\n        &lt;parameter key=\"use_local_random_seed\" value=\"true\"/&gt;\\n        &lt;process expanded=\"true\"&gt;\\n          &lt;operator activated=\"true\" class=\"remap_binominals\" compatibility=\"5.3.008\" expanded=\"true\" height=\"76\" name=\"Remap Binominals (5)\" width=\"90\" x=\"45\" y=\"30\"&gt;\\n            &lt;parameter key=\"attribute_filter_type\" value=\"single\"/&gt;\\n            &lt;parameter key=\"attribute\" value=\"REINTERNAMENTO\"/&gt;\\n            &lt;parameter key=\"negative_value\" value=\"N\"/&gt;\\n            &lt;parameter key=\"positive_value\" value=\"S\"/&gt;\\n          &lt;/operator&gt;\\n          &lt;operator activated=\"true\" class=\"nominal_to_numerical\" compatibility=\"5.3.008\" expanded=\"true\" height=\"94\" name=\"Nominal to Numerical\" width=\"90\" x=\"45\" y=\"165\"&gt;\\n            &lt;list key=\"comparison_groups\"/&gt;\\n          &lt;/operator&gt;\\n          &lt;operator activated=\"true\" class=\"support_vector_machine_libsvm\" compatibility=\"5.3.008\" expanded=\"true\" height=\"76\" name=\"SVM (2)\" width=\"90\" x=\"179\" y=\"165\"&gt;\\n            &lt;parameter key=\"kernel_type\" value=\"linear\"/&gt;\\n            &lt;list key=\"class_weights\"/&gt;\\n          &lt;/operator&gt;\\n          &lt;connect from_port=\"training\" to_op=\"Remap Binominals (5)\" to_port=\"example set input\"/&gt;\\n          &lt;connect from_op=\"Remap Binominals (5)\" from_port=\"example set output\" to_op=\"Nominal to Numerical\" to_port=\"example set input\"/&gt;\\n          &lt;connect from_op=\"Nominal to Numerical\" from_port=\"example set output\" to_op=\"SVM (2)\" to_port=\"training set\"/&gt;\\n          &lt;connect from_op=\"SVM (2)\" from_port=\"model\" to_port=\"model\"/&gt;\\n          &lt;portSpacing port=\"source_training\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"sink_model\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"sink_through 1\" spacing=\"0\"/&gt;\\n        &lt;/process&gt;\\n        &lt;process expanded=\"true\"&gt;\\n          &lt;operator activated=\"true\" class=\"remap_binominals\" compatibility=\"5.3.008\" expanded=\"true\" height=\"76\" name=\"Remap Binominals (8)\" width=\"90\" x=\"45\" y=\"165\"&gt;\\n            &lt;parameter key=\"attribute_filter_type\" value=\"single\"/&gt;\\n            &lt;parameter key=\"attribute\" value=\"REINTERNAMENTO\"/&gt;\\n            &lt;parameter key=\"negative_value\" value=\"N\"/&gt;\\n            &lt;parameter key=\"positive_value\" value=\"S\"/&gt;\\n          &lt;/operator&gt;\\n          &lt;operator activated=\"true\" class=\"nominal_to_numerical\" compatibility=\"5.3.008\" expanded=\"true\" height=\"94\" name=\"Nominal to Numerical (4)\" width=\"90\" x=\"179\" y=\"165\"&gt;\\n            &lt;list key=\"comparison_groups\"/&gt;\\n          &lt;/operator&gt;\\n          &lt;operator activated=\"true\" class=\"apply_model\" compatibility=\"5.3.008\" expanded=\"true\" height=\"76\" name=\"Apply Model (11)\" width=\"90\" x=\"45\" y=\"30\"&gt;\\n            &lt;list key=\"application_parameters\"/&gt;\\n          &lt;/operator&gt;\\n          &lt;operator activated=\"true\" class=\"performance\" compatibility=\"5.3.008\" expanded=\"true\" height=\"76\" name=\"Performance (11)\" width=\"90\" x=\"212\" y=\"30\"/&gt;\\n          &lt;connect from_port=\"model\" to_op=\"Apply Model (11)\" to_port=\"model\"/&gt;\\n          &lt;connect from_port=\"test set\" to_op=\"Remap Binominals (8)\" to_port=\"example set input\"/&gt;\\n          &lt;connect from_op=\"Remap Binominals (8)\" from_port=\"example set output\" to_op=\"Nominal to Numerical (4)\" to_port=\"example set input\"/&gt;\\n          &lt;connect from_op=\"Nominal to Numerical (4)\" from_port=\"example set output\" to_op=\"Apply Model (11)\" to_port=\"unlabelled data\"/&gt;\\n          &lt;connect from_op=\"Apply Model (11)\" from_port=\"labelled data\" to_op=\"Performance (11)\" to_port=\"labelled data\"/&gt;\\n          &lt;connect from_op=\"Performance (11)\" from_port=\"performance\" to_port=\"averagable 1\"/&gt;\\n          &lt;portSpacing port=\"source_model\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"source_test set\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"source_through 1\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"sink_averagable 1\" spacing=\"0\"/&gt;\\n          &lt;portSpacing port=\"sink_averagable 2\" spacing=\"0\"/&gt;\\n        &lt;/process&gt;\\n      &lt;/operator&gt;\\n      &lt;connect from_op=\"Read XRFF (4)\" from_port=\"output\" to_op=\"Validation (11)\" to_port=\"training\"/&gt;\\n      &lt;connect from_op=\"Validation (11)\" from_port=\"averagable 1\" to_port=\"result 1\"/&gt;\\n      &lt;portSpacing port=\"source_input 1\" spacing=\"0\"/&gt;\\n      &lt;portSpacing port=\"sink_result 1\" spacing=\"0\"/&gt;\\n      &lt;portSpacing port=\"sink_result 2\" spacing=\"0\"/&gt;\\n    &lt;/process&gt;\\n  &lt;/operator&gt;\\n&lt;/process&gt;\\n</code></pre>\\n-Reduce computation time of an SVM in Rapidminer-<machine-learning><classification><svm><rapidminer>',\n",
       " \"<p>I'm trying to import data from a txt file and keep getting a 'Wrong number of data values in row xxx' error. Looking at the text file, everything looks fine but I can't tell what/how Teradata is interpreting it. </p>\\n\\n<p>So is there a way to view or preview the data from Teradata's perspective? I tried running a SELECT statement, but since the import doesn't finish, nothing is even imported. Which brings me to my next question, is there a way to limit an external-file import to a certain # of rows? Like import just the first 50 rows from the text file? </p>\\n-Is it possible to view data as it is being imported in Teradata?-<import><teradata>\",\n",
       " \"<p>Eclipse won't work after my PC shutdown unexpectedly. Here is the error that corresponds to the log file written when I tried to run Eclipse. </p>\\n\\n<pre><code>!SESSION 2014-08-20 14:05:14.763 -----------------------------------------------\\neclipse.buildId=4.3.0.M20130911-1000\\njava.version=1.7.0_45\\njava.vendor=Oracle Corporation\\nBootLoader constants: OS=win32, ARCH=x86, WS=win32, NL=en_US\\nFramework arguments:  -product org.eclipse.epp.package.jee.product\\nCommand-line arguments:  -os win32 -ws win32 -arch x86 -product        \\norg.eclipse.epp.package.jee.product\\n\\n!ENTRY org.eclipse.osgi 4 0 2014-08-20 14:05:18.319\\n!MESSAGE An error occurred while automatically activating bundle       \\norg.eclipse.core.resources (82).\\n!STACK 0\\norg.osgi.framework.BundleException: Exception in org.eclipse.core.resources.ResourcesPlugin.start() of bundle org.eclipse.core.resources.\\n    at org.eclipse.osgi.framework.internal.core.BundleContextImpl.startActivator(BundleContextImpl.java:734)\\n    at org.eclipse.osgi.framework.internal.core.BundleContextImpl.start(BundleContextImpl.java:683)\\n    at org.eclipse.osgi.framework.internal.core.BundleHost.startWorker(BundleHost.java:381)\\n    at org.eclipse.osgi.framework.internal.core.AbstractBundle.start(AbstractBundle.java:300)\\n    at org.eclipse.osgi.framework.util.SecureAction.start(SecureAction.java:478)\\n    at org.eclipse.osgi.internal.loader.BundleLoader.setLazyTrigger(BundleLoader.java:263)\\n    at org.eclipse.core.runtime.internal.adaptor.EclipseLazyStarter.postFindLocalClass(EclipseLazyStarter.java:109)\\n    at org.eclipse.osgi.baseadaptor.loader.ClasspathManager.findLocalClass(ClasspathManager.java:469)\\n    at org.eclipse.osgi.internal.baseadaptor.DefaultClassLoader.findLocalClass(DefaultClassLoader.java:216)\\n    at org.eclipse.osgi.internal.loader.BundleLoader.findLocalClass(BundleLoader.java:395)\\n    at org.eclipse.osgi.internal.loader.SingleSourcePackage.loadClass(SingleSourcePackage.java:35)\\n    at org.eclipse.osgi.internal.loader.BundleLoader.findClassInternal(BundleLoader.java:461)\\n    at org.eclipse.osgi.internal.loader.BundleLoader.findClass(BundleLoader.java:421)\\n    at org.eclipse.osgi.internal.loader.BundleLoader.findClass(BundleLoader.java:412)\\n    at org.eclipse.osgi.internal.baseadaptor.DefaultClassLoader.loadClass(DefaultClassLoader.java:107)\\n    at java.lang.ClassLoader.loadClass(Unknown Source)\\n    at org.eclipse.ui.internal.ide.application.IDEApplication.start(IDEApplication.java:125)\\n    at org.eclipse.equinox.internal.app.EclipseAppHandle.run(EclipseAppHandle.java:196)\\n    at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.runApplication(EclipseAppLauncher.java:110)\\n    at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.start(EclipseAppLauncher.java:79)\\n    at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:354)\\n    at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:181)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\\n    at java.lang.reflect.Method.invoke(Unknown Source)\\n    at org.eclipse.equinox.launcher.Main.invokeFramework(Main.java:636)\\n    at org.eclipse.equinox.launcher.Main.basicRun(Main.java:591)\\n    at org.eclipse.equinox.launcher.Main.run(Main.java:1450)\\nCaused by: org.eclipse.core.internal.resources.ResourceException: Problems reading project tree.\\n    at org.eclipse.core.internal.resources.WorkspaceTreeReader_2.readTree(WorkspaceTreeReader_2.java:125)\\n    at org.eclipse.core.internal.resources.SaveManager.restoreTree(SaveManager.java:1000)\\n    at org.eclipse.core.internal.resources.SaveManager.restore(SaveManager.java:687)\\n    at org.eclipse.core.internal.resources.SaveManager.startup(SaveManager.java:1530)\\n    at org.eclipse.core.internal.resources.Workspace.startup(Workspace.java:2503)\\n    at org.eclipse.core.internal.resources.Workspace.open(Workspace.java:2252)\\n    at org.eclipse.core.resources.ResourcesPlugin.start(ResourcesPlugin.java:439)\\n    at org.eclipse.osgi.framework.internal.core.BundleContextImpl$1.run(BundleContextImpl.java:711)\\n    at java.security.AccessController.doPrivileged(Native Method)\\n    at org.eclipse.osgi.framework.internal.core.BundleContextImpl.startActivator(BundleContextImpl.java:702)\\n    ... 28 more\\nCaused by: java.io.IOException: Unknown format.\\n    at org.eclipse.core.internal.watson.ElementTreeReader.getReader(ElementTreeReader.java:76)\\n    at org.eclipse.core.internal.watson.ElementTreeReader.readDelta(ElementTreeReader.java:85)\\n    at org.eclipse.core.internal.watson.ElementTreeReaderImpl_1.readDeltaChain(ElementTreeReaderImpl_1.java:88)\\n    at org.eclipse.core.internal.watson.ElementTreeReader.readDeltaChain(ElementTreeReader.java:110)\\n    at org.eclipse.core.internal.resources.WorkspaceTreeReader_1.readTrees(WorkspaceTreeReader_1.java:233)\\n    at org.eclipse.core.internal.resources.WorkspaceTreeReader_2.readTree(WorkspaceTreeReader_2.java:104)\\n    ... 37 more\\nRoot exception:\\norg.eclipse.core.internal.resources.ResourceException(null)[567]: java.io.IOException: Unknown format.\\n    at org.eclipse.core.internal.watson.ElementTreeReader.getReader(ElementTreeReader.java:76)\\n    at org.eclipse.core.internal.watson.ElementTreeReader.readDelta(ElementTreeReader.java:85)\\n    at org.eclipse.core.internal.watson.ElementTreeReaderImpl_1.readDeltaChain(ElementTreeReaderImpl_1.java:88)\\n    at org.eclipse.core.internal.watson.ElementTreeReader.readDeltaChain(ElementTreeReader.java:110)\\n    at org.eclipse.core.internal.resources.WorkspaceTreeReader_1.readTrees(WorkspaceTreeReader_1.java:233)\\n    at org.eclipse.core.internal.resources.WorkspaceTreeReader_2.readTree(WorkspaceTreeReader_2.java:104)\\n    at org.eclipse.core.internal.resources.SaveManager.restoreTree(SaveManager.java:1000)\\n    at org.eclipse.core.internal.resources.SaveManager.restore(SaveManager.java:687)\\n    at org.eclipse.core.internal.resources.SaveManager.startup(SaveManager.java:1530)\\n    at org.eclipse.core.internal.resources.Workspace.startup(Workspace.java:2503)\\n    at org.eclipse.core.internal.resources.Workspace.open(Workspace.java:2252)\\n    at org.eclipse.core.resources.ResourcesPlugin.start(ResourcesPlugin.java:439)\\n    at org.eclipse.osgi.framework.internal.core.BundleContextImpl$1.run(BundleContextImpl.java:711)\\n    at java.security.AccessController.doPrivileged(Native Method)\\n    at org.eclipse.osgi.framework.internal.core.BundleContextImpl.startActivator(BundleContextImpl.java:702)\\n    at org.eclipse.osgi.framework.internal.core.BundleContextImpl.start(BundleContextImpl.java:683)\\n    at org.eclipse.osgi.framework.internal.core.BundleHost.startWorker(BundleHost.java:381)\\n    at org.eclipse.osgi.framework.internal.core.AbstractBundle.start(AbstractBundle.java:300)\\n    at org.eclipse.osgi.framework.util.SecureAction.start(SecureAction.java:478)\\n    at org.eclipse.osgi.internal.loader.BundleLoader.setLazyTrigger(BundleLoader.java:263)\\n    at org.eclipse.core.runtime.internal.adaptor.EclipseLazyStarter.postFindLocalClass(EclipseLazyStarter.java:109)\\n    at org.eclipse.osgi.baseadaptor.loader.ClasspathManager.findLocalClass(ClasspathManager.java:469)\\n    at org.eclipse.osgi.internal.baseadaptor.DefaultClassLoader.findLocalClass(DefaultClassLoader.java:216)\\n    at org.eclipse.osgi.internal.loader.BundleLoader.findLocalClass(BundleLoader.java:395)\\n    at org.eclipse.osgi.internal.loader.SingleSourcePackage.loadClass(SingleSourcePackage.java:35)\\n    at org.eclipse.osgi.internal.loader.BundleLoader.findClassInternal(BundleLoader.java:461)\\n    at org.eclipse.osgi.internal.loader.BundleLoader.findClass(BundleLoader.java:421)\\n    at org.eclipse.osgi.internal.loader.BundleLoader.findClass(BundleLoader.java:412)\\n    at org.eclipse.osgi.internal.baseadaptor.DefaultClassLoader.loadClass(DefaultClassLoader.java:107)\\n    at java.lang.ClassLoader.loadClass(Unknown Source)\\n    at org.eclipse.ui.internal.ide.application.IDEApplication.start(IDEApplication.java:125)\\n    at org.eclipse.equinox.internal.app.EclipseAppHandle.run(EclipseAppHandle.java:196)\\n    at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.runApplication(EclipseAppLauncher.java:110)\\n    at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.start(EclipseAppLauncher.java:79)\\n    at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:354)\\n    at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:181)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\\n    at java.lang.reflect.Method.invoke(Unknown Source)\\n    at org.eclipse.equinox.launcher.Main.invokeFramework(Main.java:636)\\n    at org.eclipse.equinox.launcher.Main.basicRun(Main.java:591)\\n    at org.eclipse.equinox.launcher.Main.run(Main.java:1450)\\n\\n!ENTRY org.eclipse.osgi 4 0 2014-08-20 14:05:18.336\\n!MESSAGE Application error\\n!STACK 1\\njava.lang.NoClassDefFoundError: org/eclipse/core/resources/IContainer\\n    at org.eclipse.ui.internal.ide.application.IDEApplication.start(IDEApplication.java:125)\\n    at org.eclipse.equinox.internal.app.EclipseAppHandle.run(EclipseAppHandle.java:196)\\n    at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.runApplication(EclipseAppLauncher.java:110)\\n    at org.eclipse.core.runtime.internal.adaptor.EclipseAppLauncher.start(EclipseAppLauncher.java:79)\\n    at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:354)\\n    at org.eclipse.core.runtime.adaptor.EclipseStarter.run(EclipseStarter.java:181)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\\n    at java.lang.reflect.Method.invoke(Unknown Source)\\n    at org.eclipse.equinox.launcher.Main.invokeFramework(Main.java:636)\\n    at org.eclipse.equinox.launcher.Main.basicRun(Main.java:591)\\n    at org.eclipse.equinox.launcher.Main.run(Main.java:1450)\\nCaused by: org.eclipse.core.runtime.internal.adaptor.EclipseLazyStarter$TerminatingClassNotFoundException: An error occurred while automatically activating bundle org.eclipse.core.resources (82).\\n    at org.eclipse.core.runtime.internal.adaptor.EclipseLazyStarter.postFindLocalClass(EclipseLazyStarter.java:124)\\n    at org.eclipse.osgi.baseadaptor.loader.ClasspathManager.findLocalClass(ClasspathManager.java:469)\\n    at org.eclipse.osgi.internal.baseadaptor.DefaultClassLoader.findLocalClass(DefaultClassLoader.java:216)\\n    at org.eclipse.osgi.internal.loader.BundleLoader.findLocalClass(BundleLoader.java:395)\\n    at org.eclipse.osgi.internal.loader.SingleSourcePackage.loadClass(SingleSourcePackage.java:35)\\n    at org.eclipse.osgi.internal.loader.BundleLoader.findClassInternal(BundleLoader.java:461)\\n    at org.eclipse.osgi.internal.loader.BundleLoader.findClass(BundleLoader.java:421)\\n    at org.eclipse.osgi.internal.loader.BundleLoader.findClass(BundleLoader.java:412)\\n    at org.eclipse.osgi.internal.baseadaptor.DefaultClassLoader.loadClass(DefaultClassLoader.java:107)\\n    at java.lang.ClassLoader.loadClass(Unknown Source)\\n    ... 13 more\\nCaused by: org.osgi.framework.BundleException: Exception in org.eclipse.core.resources.ResourcesPlugin.start() of bundle org.eclipse.core.resources.\\n    at org.eclipse.osgi.framework.internal.core.BundleContextImpl.startActivator(BundleContextImpl.java:734)\\n    at org.eclipse.osgi.framework.internal.core.BundleContextImpl.start(BundleContextImpl.java:683)\\n    at org.eclipse.osgi.framework.internal.core.BundleHost.startWorker(BundleHost.java:381)\\n    at org.eclipse.osgi.framework.internal.core.AbstractBundle.start(AbstractBundle.java:300)\\n    at org.eclipse.osgi.framework.util.SecureAction.start(SecureAction.java:478)\\n    at org.eclipse.osgi.internal.loader.BundleLoader.setLazyTrigger(BundleLoader.java:263)\\n    at org.eclipse.core.runtime.internal.adaptor.EclipseLazyStarter.postFindLocalClass(EclipseLazyStarter.java:109)\\n    ... 22 more\\nCaused by: org.eclipse.core.internal.resources.ResourceException: Problems reading project tree.\\n    at org.eclipse.core.internal.resources.WorkspaceTreeReader_2.readTree(WorkspaceTreeReader_2.java:125)\\n    at org.eclipse.core.internal.resources.SaveManager.restoreTree(SaveManager.java:1000)\\n    at org.eclipse.core.internal.resources.SaveManager.restore(SaveManager.java:687)\\n    at org.eclipse.core.internal.resources.SaveManager.startup(SaveManager.java:1530)\\n    at org.eclipse.core.internal.resources.Workspace.startup(Workspace.java:2503)\\n    at org.eclipse.core.internal.resources.Workspace.open(Workspace.java:2252)\\n    at org.eclipse.core.resources.ResourcesPlugin.start(ResourcesPlugin.java:439)\\n    at org.eclipse.osgi.framework.internal.core.BundleContextImpl$1.run(BundleContextImpl.java:711)\\n    at java.security.AccessController.doPrivileged(Native Method)\\n    at org.eclipse.osgi.framework.internal.core.BundleContextImpl.startActivator(BundleContextImpl.java:702)\\n    ... 28 more\\nCaused by: java.io.IOException: Unknown format.\\n    at org.eclipse.core.internal.watson.ElementTreeReader.getReader(ElementTreeReader.java:76)\\n    at org.eclipse.core.internal.watson.ElementTreeReader.readDelta(ElementTreeReader.java:85)\\n    at org.eclipse.core.internal.watson.ElementTreeReaderImpl_1.readDeltaChain(ElementTreeReaderImpl_1.java:88)\\n    at org.eclipse.core.internal.watson.ElementTreeReader.readDeltaChain(ElementTreeReader.java:110)\\n    at org.eclipse.core.internal.resources.WorkspaceTreeReader_1.readTrees(WorkspaceTreeReader_1.java:233)\\n    at org.eclipse.core.internal.resources.WorkspaceTreeReader_2.readTree(WorkspaceTreeReader_2.java:104)\\n    ... 37 more\\n</code></pre>\\n-Eclipse error upon start up-<eclipse><eclipse-kepler>\",\n",
       " '<p>I have already installed Kepler in which maven comes inbuilt.</p>\\n\\n<p>After that am I required to download Maven explicitly and set environment variable? or just downloading eclipse kepler version is fine?</p>\\n-Is maven required to install explicitly if already installed Eclipse Kepler?-<eclipse><selenium-webdriver><maven-2><eclipse-kepler>',\n",
       " '<p>I want to get an example from an id attribute. \\nActually this attribute was some regular attribute and i transformed it into an id attribute wiht the \"SetRole\" Operator now i want to get access on some examples in the \"ExecuteScript\" Operator. So my process is something like this: ExampleSet with some \"text\" regular Attribute -> transform it into a id Attribute with \"SetRole\" -> ScriptOperator\\nNow i wanted to use something like:</p>\\n\\n<pre><code>ExampleSet es = input[0];\\nes.remapIds();\\nExample e = es.getExampleFromId(\"Text\");\\n</code></pre>\\n\\n<p>but obviously getExampleFromId() only accepts double-value. But although the attribute type of \"text\" is \"Polynomial\", it seems like it contains something like strings. And the exampleSet after the SetRole Operator seems perfectly fine. So I wonder how i can access an Example from an id. (Casting doesnt work)\\nHelp appreciated</p>\\n\\n<p>EDIT: Okay, i get it now, the id there is not the value of the id-Attribute but just a intern identifier in the ExampleSet. But my question still remains how can i access a row of a ExampleSet by the id-Attribute.</p>\\n-RapidMiner: Access an Example of an ExampleSet by the Id-Attribute-<java><groovy><attributes><identifier><rapidminer>',\n",
       " '<p>I am trying to check for dates but after running the query below, it displays no result. Could someone recommend me the correct syntax?</p>\\n\\n<pre><code>SELECT TOP 10 * FROM MY_DATABASE.AGREEMENT\\nWHERE end_dt=12/31/9999\\n</code></pre>\\n-check for dates syntax - teradata SQL-<sql><teradata>',\n",
       " '<p>Recently the server that houses all of my SSIS packages was upgraded. This has caused a need for all of our existing packages that utalize an OLE DB connection to be migrated to an ADO.NET connection. One of the issues I am running into is that passing paramaters into the SQL Execution Task are no longer working. Even after following the instruction provided by micrtosfot <a href=\"http://work.http://msdn.microsoft.com/en-us/library/ms140355.aspx\" rel=\"nofollow noreferrer\">here</a>.</p>\\n\\n<p>Before I start let me share my setup.</p>\\n\\n<p><img src=\"https://i.stack.imgur.com/vdNX2.png\" alt=\"enter image description here\"></p>\\n\\n<p><img src=\"https://i.stack.imgur.com/SX2M9.png\" alt=\"enter image description here\"></p>\\n\\n<p><img src=\"https://i.stack.imgur.com/HTup6.png\" alt=\"enter image description here\"></p>\\n\\n<p>The SQL I am testing with is extremly simple. Even with such a simple statement I am receiving the following error message.</p>\\n\\n<p><img src=\"https://i.stack.imgur.com/rEmsV.png\" alt=\"enter image description here\"></p>\\n\\n<p>In the past if I ran into issues like this I would just set the SQL as a variable and through an expression update the parts of the statement that needs to be updated. However, the statement is over 4k characters long. Has anyone had this issue using an ADO.NET connection to teradata? If so, any suggestions on how to solve it. I have searched high/low on google w/o any luck. The most I have found is people asking the same question without any answers.</p>\\n\\n<p>Thanks</p>\\n-ADO.NET Execution Task - Parameter Mapping Failure (Teradata)-<ssis><teradata><ssis-2008>',\n",
       " '<p>How to  calculate following in Teradata:</p>\\n\\n<pre><code>Beginning Of Previous Month, \\nEnd Of Previous Month, \\nBeginning Of Six Months Ago, \\nBeginning Of Twelve Months Ago\\n</code></pre>\\n\\n<p>Thanks.</p>\\n-Time date calculations for different months in Teradata-<date><time><teradata>',\n",
       " \"<p>I'm working on source code that is split across several projects with a specifically defined build order. I want to see the projects sorted by the build order so I can always tell which classes can be used in which projects. Does anyone know how to do this in Eclipse Kepler?</p>\\n-Is there any way to reorder projects in the package/project explorer in Eclipse?-<java><eclipse><eclipse-kepler>\",\n",
       " '<p>In Teradata how to get Firstname and lastname from a string\\nexample:Samaira Oberoi \\nthe function should return firstname as : Samaira \\nLast name as :Oberoi\\nalso if there is no space it should return the whole set</p>\\n\\n<p>Thanks</p>\\n-Get Firstname and lastname from String in Teradata-<string><teradata>',\n",
       " '<p>I want to run a similar query at different times changing only one thing in the where clause and the table name that it\\'s pulling from. </p>\\n\\n<pre><code>declare ?tablename varchar(100);\\nset ?tablename = \\'some_table_this_time\\';\\n\\ndeclare ?id int;\\nset ?id = 12\\n\\nselect * from ?tablename\\nwhere my_id = ?id\\n</code></pre>\\n\\n<p>This syntax does not work in teradata. I\\'ve used the @variable syntax in other sql programs, but cannot find the correct way in teradata. </p>\\n\\n<p>Research: </p>\\n\\n<p><a href=\"http://forums.teradata.com/forum/database/using-parameters-with-queries\" rel=\"nofollow noreferrer\">http://forums.teradata.com/forum/database/using-parameters-with-queries</a></p>\\n\\n<p><a href=\"https://stackoverflow.com/questions/2687819/how-to-use-variables-in-teradata-sql-macros\">How to use variables in Teradata SQL Macros</a></p>\\n-User-Defined variable in query teradata-<sql><syntax><teradata>',\n",
       " \"<p>I tried to search but I have not found anything. I am trying to add two columns to a table withe a default value (Teradata). I am trying with this statement</p>\\n\\n<pre><code>ALTER TABLE TEST \\nadd (DWH_Change_dt date  default CURRENT_DATE, dwh_create_dt date  default current_date);\\n</code></pre>\\n\\n<p>This does not work with default clause. I get this error</p>\\n\\n<p>Syntax error, expected something like a 'BETWEEN' keyword or an 'IN' keyword or a 'LIKE' keyword or a 'CONTAINS' keyword between the word 'DWH_Change_dt' and the 'date'</p>\\n\\n<p>If I add one column at a time it works (without parenthesis). Anyone has any idea? What is wrong?</p>\\n\\n<p>Thanks,\\nUmberto</p>\\n-Adding multiple columns with default value (Teradata)-<teradata><ddl>\",\n",
       " '<p>How to hide the login credential in the SAS program to connect teradata database</p>\\n-How to hide the database login credential in the SAS program to connect teradata database-<sas><teradata>',\n",
       " '<p>I am new in rapid miner 5, just want to know how to find noise in my data and show them in chart and how to delete them? </p>\\n-How to detect and delete noise in rapidminer?-<data-mining><rapidminer>',\n",
       " ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.read_csv('Dataset/StackOverflowPostsDataset.csv')\n",
    "new_df[\"merged\"] = new_df[[\"Body\", \"Title\", \"Tags\"]].apply(\"-\".join, axis=1)\n",
    "#new_df.head()\n",
    "new_df.to_csv('Dataset/ConcatenatedDatasetSO.csv')\n",
    "\n",
    "data = new_df.merged.values.tolist()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed84875d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When dealing with small projects, what do you feel is the break even point '\n",
      " 'for storing data in simple text files, hash tables, etc., versus using a '\n",
      " 'real database? For small projects with simple data management requirements, '\n",
      " 'a real database is unnecessary complexity and violates YAGNI. However, at '\n",
      " 'some point the complexity of a database is obviously worth it. What are some '\n",
      " 'signs that your problem is too complex for simple ad-hoc techniques and '\n",
      " 'needs a real database? Note: To people used to enterprise environments, this '\n",
      " 'will probably sound like a weird question. However, my problem domain is '\n",
      " 'bioinformatics. Most of my programming is prototypes, not production code. '\n",
      " 'Im primarily a domain expert and secondarily a programmer. Most of my code '\n",
      " 'is algorithm-centric, not data management-centric. The purpose of this '\n",
      " 'question is largely for me to figure out how much work I might save in the '\n",
      " 'long run if I learn to use proper databases in my code instead of the more '\n",
      " 'ad-hoc techniques I typically use. -Databases versus plain text-']\n"
     ]
    }
   ],
   "source": [
    "# Remove Emails\n",
    "data = [re.sub('<[^<>]*>', '', sent) for sent in data]\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "print(data[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d98f4b6",
   "metadata": {},
   "source": [
    "# Tokenize words and Clean-up text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7b3a9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['when', 'dealing', 'with', 'small', 'projects', 'what', 'do', 'you', 'feel', 'is', 'the', 'break', 'even', 'point', 'for', 'storing', 'data', 'in', 'simple', 'text', 'files', 'hash', 'tables', 'etc', 'versus', 'using', 'real', 'database', 'for', 'small', 'projects', 'with', 'simple', 'data', 'management', 'requirements', 'real', 'database', 'is', 'unnecessary', 'complexity', 'and', 'violates', 'yagni', 'however', 'at', 'some', 'point', 'the', 'complexity', 'of', 'database', 'is', 'obviously', 'worth', 'it', 'what', 'are', 'some', 'signs', 'that', 'your', 'problem', 'is', 'too', 'complex', 'for', 'simple', 'ad', 'hoc', 'techniques', 'and', 'needs', 'real', 'database', 'note', 'to', 'people', 'used', 'to', 'enterprise', 'environments', 'this', 'will', 'probably', 'sound', 'like', 'weird', 'question', 'however', 'my', 'problem', 'domain', 'is', 'bioinformatics', 'most', 'of', 'my', 'programming', 'is', 'prototypes', 'not', 'production', 'code', 'im', 'primarily', 'domain', 'expert', 'and', 'secondarily', 'programmer', 'most', 'of', 'my', 'code', 'is', 'algorithm', 'centric', 'not', 'data', 'management', 'centric', 'the', 'purpose', 'of', 'this', 'question', 'is', 'largely', 'for', 'me', 'to', 'figure', 'out', 'how', 'much', 'work', 'might', 'save', 'in', 'the', 'long', 'run', 'if', 'learn', 'to', 'use', 'proper', 'databases', 'in', 'my', 'code', 'instead', 'of', 'the', 'more', 'ad', 'hoc', 'techniques', 'typically', 'use', 'databases', 'versus', 'plain', 'text']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38592eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=50) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=50)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "#print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d485455",
   "metadata": {},
   "source": [
    "# Remove Stopwords, Make Bigrams and Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c49abe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1408ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['deal', 'small', 'project', 'feel', 'break', 'even', 'point', 'store', 'datum', 'simple', 'text', 'file', 'hash', 'table', 'use', 'real', 'database', 'small', 'project', 'simple', 'data', 'management', 'requirement', 'real', 'database', 'unnecessary', 'complexity', 'violate', 'however', 'point', 'complexity', 'database', 'obviously', 'worth', 'sign', 'problem', 'complex', 'simple', 'ad_hoc', 'technique', 'need', 'real', 'database', 'note', 'people', 'use', 'enterprise', 'environment', 'probably', 'sound', 'weird', 'question', 'however', 'problem', 'domain', 'bioinformatic', 'programming', 'prototype', 'production', 'code', 'm', 'primarily', 'domain', 'expert', 'secondarily', 'management', 'centric', 'purpose', 'question', 'largely', 'figure', 'much', 'work', 'save', 'long', 'run', 'learn', 'use', 'proper', 'database', 'code', 'instead', 'technique', 'typically', 'use', 'database', 'plain_text']]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98003715",
   "metadata": {},
   "source": [
    "# Create the Dictionary and Corpus needed for Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfd3247d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 2), (5, 1), (6, 2), (7, 1), (8, 6), (9, 1), (10, 1), (11, 2), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 2), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 2), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 2), (34, 1), (35, 1), (36, 2), (37, 1), (38, 1), (39, 2), (40, 1), (41, 1), (42, 1), (43, 2), (44, 3), (45, 1), (46, 1), (47, 1), (48, 1), (49, 1), (50, 3), (51, 2), (52, 1), (53, 1), (54, 1), (55, 2), (56, 1), (57, 1), (58, 1), (59, 4), (60, 1), (61, 1), (62, 1), (63, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be414a5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('ad_hoc', 1),\n",
       "  ('bioinformatic', 1),\n",
       "  ('break', 1),\n",
       "  ('centric', 1),\n",
       "  ('code', 2),\n",
       "  ('complex', 1),\n",
       "  ('complexity', 2),\n",
       "  ('data', 1),\n",
       "  ('database', 6),\n",
       "  ('datum', 1),\n",
       "  ('deal', 1),\n",
       "  ('domain', 2),\n",
       "  ('enterprise', 1),\n",
       "  ('environment', 1),\n",
       "  ('even', 1),\n",
       "  ('expert', 1),\n",
       "  ('feel', 1),\n",
       "  ('figure', 1),\n",
       "  ('file', 1),\n",
       "  ('hash', 1),\n",
       "  ('however', 2),\n",
       "  ('instead', 1),\n",
       "  ('largely', 1),\n",
       "  ('learn', 1),\n",
       "  ('long', 1),\n",
       "  ('m', 1),\n",
       "  ('management', 2),\n",
       "  ('much', 1),\n",
       "  ('need', 1),\n",
       "  ('note', 1),\n",
       "  ('obviously', 1),\n",
       "  ('people', 1),\n",
       "  ('plain_text', 1),\n",
       "  ('point', 2),\n",
       "  ('primarily', 1),\n",
       "  ('probably', 1),\n",
       "  ('problem', 2),\n",
       "  ('production', 1),\n",
       "  ('programming', 1),\n",
       "  ('project', 2),\n",
       "  ('proper', 1),\n",
       "  ('prototype', 1),\n",
       "  ('purpose', 1),\n",
       "  ('question', 2),\n",
       "  ('real', 3),\n",
       "  ('requirement', 1),\n",
       "  ('run', 1),\n",
       "  ('save', 1),\n",
       "  ('secondarily', 1),\n",
       "  ('sign', 1),\n",
       "  ('simple', 3),\n",
       "  ('small', 2),\n",
       "  ('sound', 1),\n",
       "  ('store', 1),\n",
       "  ('table', 1),\n",
       "  ('technique', 2),\n",
       "  ('text', 1),\n",
       "  ('typically', 1),\n",
       "  ('unnecessary', 1),\n",
       "  ('use', 4),\n",
       "  ('violate', 1),\n",
       "  ('weird', 1),\n",
       "  ('work', 1),\n",
       "  ('worth', 1)]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "594e184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5fc103e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.316*\"process\" + 0.109*\"module\" + 0.105*\"line\" + 0.052*\"python\" + '\n",
      "  '0.028*\"usr_local\" + 0.027*\"flag\" + 0.026*\"node\" + 0.026*\"scale\" + '\n",
      "  '0.022*\"location\" + 0.021*\"traceback\"'),\n",
      " (1,\n",
      "  '0.409*\"file\" + 0.125*\"script\" + 0.081*\"command\" + 0.071*\"import\" + '\n",
      "  '0.025*\"load\" + 0.024*\"open\" + 0.024*\"project\" + 0.021*\"report\" + '\n",
      "  '0.020*\"merge\" + 0.016*\"copy\"'),\n",
      " (2,\n",
      "  '0.144*\"table\" + 0.104*\"teradata\" + 0.089*\"column\" + 0.081*\"select\" + '\n",
      "  '0.063*\"query\" + 0.061*\"row\" + 0.056*\"date\" + 0.034*\"sql\" + 0.025*\"insert\" + '\n",
      "  '0.020*\"varchar\"'),\n",
      " (3,\n",
      "  '0.216*\"com\" + 0.209*\"java\" + 0.063*\"action\" + 0.047*\"force\" + 0.032*\"jdbc\" '\n",
      "  '+ 0.025*\"weight\" + 0.021*\"stack\" + 0.021*\"play\" + 0.020*\"mark\" + '\n",
      "  '0.011*\"country\"'),\n",
      " (4,\n",
      "  '0.147*\"error\" + 0.130*\"run\" + 0.083*\"job\" + 0.041*\"user\" + 0.039*\"log\" + '\n",
      "  '0.038*\"test\" + 0.038*\"base\" + 0.037*\"execute\" + 0.035*\"fail\" + '\n",
      "  '0.020*\"message\"'),\n",
      " (5,\n",
      "  '0.475*\"thread\" + 0.100*\"memory\" + 0.076*\"convert\" + 0.036*\"calculate\" + '\n",
      "  '0.010*\"unit\" + 0.009*\"prefer\" + 0.002*\"development\" + 0.001*\"catalog\" + '\n",
      "  '0.000*\"snakemake\" + 0.000*\"quot\"'),\n",
      " (6,\n",
      "  '0.113*\"stop\" + 0.103*\"galaxy\" + 0.062*\"therefore\" + 0.000*\"ansible\" + '\n",
      "  '0.000*\"snakemake\" + 0.000*\"quot\" + 0.000*\"https\" + 0.000*\"role\" + '\n",
      "  '0.000*\"ansible_galaxy\" + 0.000*\"fastq_gz\"'),\n",
      " (7,\n",
      "  '0.163*\"version\" + 0.160*\"instal\" + 0.077*\"install\" + 0.045*\"window\" + '\n",
      "  '0.043*\"software\" + 0.037*\"previous\" + 0.036*\"support\" + 0.023*\"modify\" + '\n",
      "  '0.023*\"suppose\" + 0.019*\"declare\"'),\n",
      " (8,\n",
      "  '0.430*\"rule\" + 0.107*\"workflow\" + 0.089*\"output\" + 0.083*\"self\" + '\n",
      "  '0.080*\"run\" + 0.019*\"join\" + 0.012*\"print\" + 0.010*\"extract\" + '\n",
      "  '0.009*\"replicate\" + 0.009*\"produce\"'),\n",
      " (9,\n",
      "  '0.394*\"datum\" + 0.089*\"cluster\" + 0.070*\"dataset\" + 0.048*\"data\" + '\n",
      "  '0.042*\"model\" + 0.040*\"analysis\" + 0.030*\"knime\" + 0.027*\"fast\" + '\n",
      "  '0.024*\"raw\" + 0.022*\"limit\"'),\n",
      " (10,\n",
      "  '0.162*\"database\" + 0.158*\"teradata\" + 0.099*\"connect\" + 0.087*\"server\" + '\n",
      "  '0.060*\"connection\" + 0.044*\"driver\" + 0.031*\"client\" + 0.023*\"host\" + '\n",
      "  '0.022*\"odbc\" + 0.020*\"db\"'),\n",
      " (11,\n",
      "  '0.076*\"use\" + 0.029*\"try\" + 0.028*\"get\" + 0.020*\"work\" + 0.019*\"want\" + '\n",
      "  '0.019*\"m\" + 0.017*\"result\" + 0.016*\"way\" + 0.014*\"need\" + 0.013*\"follow\"'),\n",
      " (12,\n",
      "  '0.096*\"answer\" + 0.068*\"ask\" + 0.065*\"title\" + 0.055*\"whole\" + '\n",
      "  '0.051*\"partial\" + 0.048*\"detect\" + 0.045*\"explain\" + 0.035*\"frame\" + '\n",
      "  '0.032*\"recognize\" + 0.024*\"matrix\"'),\n",
      " (13,\n",
      "  '0.289*\"value\" + 0.084*\"map\" + 0.066*\"set\" + 0.058*\"template\" + '\n",
      "  '0.036*\"range\" + 0.034*\"attribute\" + 0.033*\"filter\" + 0.032*\"text\" + '\n",
      "  '0.031*\"separate\" + 0.027*\"regular\"'),\n",
      " (14,\n",
      "  '0.096*\"create\" + 0.044*\"name\" + 0.044*\"directory\" + 0.029*\"add\" + '\n",
      "  '0.027*\"new\" + 0.026*\"environment\" + 0.025*\"local\" + 0.024*\"index\" + '\n",
      "  '0.023*\"option\" + 0.022*\"update\"'),\n",
      " (15,\n",
      "  '0.310*\"info\" + 0.151*\"part\" + 0.085*\"parallel\" + 0.060*\"library\" + '\n",
      "  '0.041*\"amount\" + 0.040*\"properly\" + 0.037*\"mention\" + 0.007*\"coalesce\" + '\n",
      "  '0.007*\"price\" + 0.000*\"local_easybuild\"'),\n",
      " (16,\n",
      "  '0.179*\"main\" + 0.155*\"resource\" + 0.129*\"core\" + 0.038*\"plugin\" + '\n",
      "  '0.037*\"application\" + 0.037*\"org\" + 0.024*\"relevant\" + 0.023*\"property\" + '\n",
      "  '0.019*\"net\" + 0.017*\"jar\"'),\n",
      " (17,\n",
      "  '0.251*\"input\" + 0.233*\"output\" + 0.163*\"sample\" + 0.103*\"file\" + '\n",
      "  '0.055*\"read\" + 0.025*\"txt\" + 0.024*\"result\" + 0.024*\"log\" + 0.018*\"line\" + '\n",
      "  '0.011*\"list\"'),\n",
      " (18,\n",
      "  '0.143*\"return\" + 0.139*\"name\" + 0.132*\"task\" + 0.104*\"true\" + '\n",
      "  '0.073*\"parameter\" + 0.069*\"def\" + 0.055*\"trim\" + 0.040*\"class\" + '\n",
      "  '0.026*\"pass\" + 0.018*\"benchmark\"'),\n",
      " (19,\n",
      "  '0.267*\"variable\" + 0.086*\"loop\" + 0.070*\"timestamp\" + 0.052*\"accept\" + '\n",
      "  '0.046*\"minute\" + 0.044*\"interval\" + 0.042*\"hour\" + 0.031*\"oracle\" + '\n",
      "  '0.018*\"time\" + 0.011*\"ss\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 20 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8995f24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -14.197674254680775\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e7a8d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.3808045741859483\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8322fd",
   "metadata": {},
   "source": [
    "# Visualize the topics-keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd5827c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el136841398273506895368227318670\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el136841398273506895368227318670_data = {\"mdsDat\": {\"x\": [0.261888784739123, 0.23176163582117348, 0.15797520594100498, 0.0858262028797719, 0.24351593796203222, 0.15922331155895292, 0.17309651880759103, 0.032724713027761636, -0.04062526533598707, -0.015849547442349038, 0.0058681619674606335, -0.04400628141770729, -0.07532958683323078, -0.10366139571962954, -0.16397290087030605, -0.15555458872370276, -0.1591463060738018, -0.16614357006143698, -0.2008884591715656, -0.22670257105515457], \"y\": [0.20325858139113734, 0.10353621966981649, 0.17856737544587503, 0.12479199108373687, -0.2859991510278226, -0.2678876366670345, -0.12460475324139694, 0.06819002535666704, 0.034260749360924526, 0.11230798400283833, -0.03369586003887673, 0.001133345217431416, -0.015445445808525657, 0.0015041998189726292, -0.01750014442049842, -0.01953251464970317, -0.008793804943003417, -0.01959541587287495, -0.01865183040721746, -0.015843914270445316], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [32.028057959246, 10.720898153904015, 9.975132860082692, 7.429707207332252, 6.898692210860179, 6.080242138410959, 4.563009653981501, 3.4374307443194856, 2.9684334585796868, 2.866677506339944, 2.4938413766754675, 2.4128969044608217, 2.008157344496349, 1.8702145914901955, 1.3843982194234343, 0.775989365081826, 0.7678522030610232, 0.7114851750235436, 0.4507645371054544, 0.15611839012518314]}, \"tinfo\": {\"Term\": [\"file\", \"rule\", \"input\", \"output\", \"datum\", \"error\", \"run\", \"teradata\", \"value\", \"sample\", \"process\", \"table\", \"use\", \"name\", \"script\", \"create\", \"job\", \"column\", \"thread\", \"return\", \"database\", \"set\", \"select\", \"task\", \"workflow\", \"line\", \"command\", \"main\", \"version\", \"com\", \"want\", \"way\", \"number\", \"type\", \"function\", \"call\", \"different\", \"give\", \"multiple\", \"order\", \"check\", \"contain\", \"seem\", \"know\", \"question\", \"show\", \"possible\", \"expect\", \"single\", \"key\", \"second\", \"like\", \"solution\", \"think\", \"instead\", \"batch\", \"similar\", \"include\", \"match\", \"thank\", \"use\", \"m\", \"first\", \"need\", \"help\", \"also\", \"look\", \"get\", \"try\", \"code\", \"work\", \"example\", \"result\", \"problem\", \"write\", \"see\", \"list\", \"follow\", \"set\", \"issue\", \"however\", \"make\", \"find\", \"time\", \"error\", \"job\", \"test\", \"fail\", \"message\", \"complete\", \"home\", \"exit\", \"attempt\", \"monetdb\", \"cause\", \"late\", \"network\", \"manager\", \"fine\", \"url\", \"tutorial\", \"never\", \"receive\", \"common\", \"execute\", \"setup\", \"manage\", \"throw\", \"aggregate\", \"maybe\", \"queue\", \"prepare\", \"sub\", \"max\", \"base\", \"user\", \"program\", \"exception\", \"run\", \"sure\", \"log\", \"unable\", \"time\", \"follow\", \"try\", \"get\", \"work\", \"line\", \"host\", \"code\", \"directory\", \"add\", \"environment\", \"local\", \"index\", \"option\", \"update\", \"service\", \"require\", \"folder\", \"exist\", \"package\", \"default\", \"build\", \"reference\", \"remove\", \"provide\", \"tool\", \"dependency\", \"structure\", \"thing\", \"port\", \"final\", \"combine\", \"far\", \"machine\", \"section\", \"element\", \"detail\", \"normal\", \"new\", \"create\", \"object\", \"take\", \"name\", \"specify\", \"find\", \"method\", \"table\", \"column\", \"select\", \"query\", \"row\", \"date\", \"insert\", \"varchar\", \"cast\", \"field\", \"sum\", \"null\", \"decimal\", \"temp\", \"lock\", \"hope\", \"cant_find\", \"clause\", \"matching\", \"supply\", \"dd\", \"table_name\", \"dbc\", \"yyyy_mm\", \"primary\", \"interpret\", \"sel\", \"primary_key\", \"set_unicode\", \"literal\", \"sql\", \"integer\", \"teradata\", \"format\", \"view\", \"join\", \"case\", \"end\", \"syntax\", \"condition\", \"last\", \"group\", \"delete\", \"input\", \"sample\", \"txt\", \"int\", \"failure\", \"basic\", \"site\", \"ready\", \"reach\", \"reflect\", \"manual\", \"utf\", \"successful\", \"byte\", \"quiet\", \"incorrect\", \"sorry\", \"reader\", \"bat\", \"println\", \"troubleshoot\", \"stream\", \"transport\", \"clue\", \"unicode\", \"datatype\", \"ip_address\", \"special_character\", \"tera\", \"introduce\", \"output\", \"read\", \"file\", \"log\", \"line\", \"result\", \"list\", \"generate\", \"define\", \"advance\", \"write\", \"name\", \"script\", \"command\", \"import\", \"load\", \"open\", \"project\", \"report\", \"merge\", \"copy\", \"status\", \"export\", \"warning\", \"depend\", \"operation\", \"determine\", \"password\", \"csv\", \"amp\", \"exactly\", \"please_help\", \"creation\", \"bteq\", \"utility\", \"hard\", \"refer\", \"analyze\", \"buffer\", \"route\", \"building\", \"standard\", \"file\", \"format\", \"issue\", \"source\", \"rule\", \"workflow\", \"self\", \"extract\", \"replicate\", \"automatically\", \"custom\", \"api\", \"current_date\", \"dynamic\", \"suggest\", \"usage\", \"rd\", \"rest\", \"initial\", \"logic\", \"constant\", \"important\", \"compile\", \"nice\", \"float\", \"mismatch\", \"luck\", \"precision\", \"fairly\", \"ability\", \"obvious\", \"capacity\", \"spl\", \"half\", \"join\", \"output\", \"run\", \"produce\", \"print\", \"assume\", \"modify\", \"next\", \"format\", \"quite\", \"base\", \"return\", \"task\", \"true\", \"parameter\", \"def\", \"trim\", \"benchmark\", \"foo\", \"top\", \"provider\", \"initialize\", \"expected_somethe\", \"substre\", \"guy\", \"please_let\", \"anyone_know\", \"substr\", \"boolean\", \"logoff\", \"persist\", \"likely\", \"dual\", \"trim_leading\", \"highly_appreciate\", \"difficulty\", \"eliminate\", \"class\", \"gain\", \"forward\", \"name\", \"pass\", \"method\", \"space\", \"list\", \"write\", \"result\", \"value\", \"map\", \"template\", \"range\", \"attribute\", \"separate\", \"regular\", \"operator\", \"customer\", \"account\", \"maximum\", \"rapidminer\", \"datetime\", \"other\", \"mistake\", \"reduce\", \"company\", \"letter\", \"dot\", \"hadoop\", \"warehouse\", \"customerid\", \"sequential\", \"comma\", \"nominal\", \"populated\", \"rapid_miner\", \"binomial\", \"payment\", \"rdbms\", \"filter\", \"text\", \"set\", \"numeric\", \"example\", \"base\", \"integer\", \"database\", \"connect\", \"server\", \"connection\", \"driver\", \"client\", \"odbc\", \"mysql\", \"sys\", \"fetch\", \"ip\", \"functionality\", \"odbc_driver\", \"sas\", \"libname\", \"uid\", \"dbcname\", \"white\", \"odbc_ini\", \"assumption\", \"oreplace\", \"administrator\", \"uid_pwd\", \"username\", \"db\", \"host\", \"teradata\", \"defaultdatabase\", \"jdbc\", \"able\", \"system\", \"source\", \"user\", \"sql\", \"access\", \"string\", \"use\", \"close\", \"request\", \"process\", \"module\", \"python\", \"usr_local\", \"flag\", \"node\", \"scale\", \"location\", \"traceback\", \"runtime\", \"document\", \"stage\", \"cli\", \"testing\", \"integration\", \"strip\", \"pipe\", \"dynamically\", \"score\", \"pretty\", \"recent_call\", \"communicate\", \"minor\", \"possibly\", \"prompt\", \"series\", \"xml\", \"extension\", \"tune\", \"fly\", \"line\", \"none\", \"last\", \"datum\", \"cluster\", \"dataset\", \"model\", \"analysis\", \"knime\", \"fast\", \"raw\", \"limit\", \"performance\", \"speed\", \"million\", \"consume\", \"slow\", \"simulation\", \"comparison\", \"wish\", \"straightforward\", \"carry\", \"thanks_lot\", \"computation\", \"light\", \"offset\", \"inherit\", \"measure\", \"data\", \"algorithm\", \"quite\", \"retrieve\", \"regard\", \"take\", \"sure\", \"version\", \"instal\", \"install\", \"window\", \"software\", \"previous\", \"support\", \"suppose\", \"declare\", \"control\", \"product\", \"somehow\", \"random\", \"release\", \"enter\", \"week\", \"stored_procedure\", \"here\", \"engine\", \"login\", \"workaround\", \"language\", \"advice\", \"private\", \"recommend\", \"addition\", \"article\", \"optimizer\", \"treat\", \"didnt_work\", \"modify\", \"particular\", \"successfully\", \"main\", \"resource\", \"core\", \"plugin\", \"application\", \"org\", \"relevant\", \"property\", \"net\", \"jar\", \"org_eclipse\", \"web\", \"deprecate\", \"integrate\", \"fatal\", \"workbench\", \"manifest\", \"accomplish\", \"singleton\", \"upgrade\", \"eclipse\", \"overflow\", \"alpha\", \"seemingly\", \"bundle\", \"stack_trace\", \"vendor\", \"platform\", \"setting\", \"app\", \"start\", \"class\", \"com\", \"java\", \"action\", \"force\", \"weight\", \"stack\", \"play\", \"mark\", \"country\", \"realize\", \"automatic\", \"runnable\", \"annoying\", \"tdnetworkioif\", \"jdbc__statemachine\", \"tdsession\", \"jdbc\", \"jdbc__tdsession\", \"runbody_java\", \"greatly\", \"preparerequest\", \"org_apache\", \"unnamed_add\", \"spark\", \"scala\", \"opens_java\", \"ansible\", \"livy\", \"sun_security\", \"errorfactory_errorfactory\", \"run\", \"snakemake\", \"read\", \"info\", \"part\", \"parallel\", \"library\", \"amount\", \"properly\", \"mention\", \"coalesce\", \"price\", \"local_easybuild\", \"easyblock_py\", \"environment_variable\", \"libjpeg_turbo\", \"previously_undefine\", \"gcccore\", \"build_libjpegturbo\", \"easyconfig_py\", \"step\", \"filetools_py\", \"software_libjpeg\", \"ftree_vectorize\", \"software_gcccore\", \"debug_nextflow\", \"cmake\", \"march_native\", \"software_binutil\", \"errno_fpic\", \"easyblock\", \"apr_main\", \"arch_libjpeg\", \"nextflow\", \"false\", \"path\", \"quot\", \"snakemake\", \"param\", \"variable\", \"loop\", \"timestamp\", \"accept\", \"minute\", \"interval\", \"hour\", \"oracle\", \"ss\", \"almost\", \"equal\", \"translate\", \"quick\", \"macro\", \"activity\", \"docker_gwbridge\", \"images_section\", \"infrastructure_documentation\", \"oracle_provide\", \"security_impact\", \"variant\", \"docker\", \"risiko_schl\", \"day\", \"file_ending\", \"root_path\", \"docker_isolation\", \"sequences_split\", \"mar_main\", \"udp\", \"time\", \"subreads_bam\", \"path\", \"param\", \"snakemake\", \"nextflow\", \"quot\", \"bam\", \"wildcard\", \"fastq\", \"false\", \"expand\", \"pipeline\", \"fastq_gz\", \"thread\", \"memory\", \"convert\", \"calculate\", \"unit\", \"prefer\", \"development\", \"catalog\", \"snakemake\", \"quot\", \"basecalle\", \"wildcard\", \"shell\", \"bam\", \"fastq_gz\", \"cdirs_user\", \"pertoldi_legend\", \"lane\", \"nameerror\", \"tue\", \"fastq\", \"bash_provide\", \"sample_name\", \"min\", \"reason\", \"param\", \"snakefile\", \"prjna_srr\", \"shortread\", \"scheduler\", \"genome\", \"expand\", \"path\", \"miss\", \"count\", \"pipeline\", \"container\", \"site_package\", \"nextflow\", \"answer\", \"ask\", \"title\", \"whole\", \"partial\", \"detect\", \"explain\", \"frame\", \"recognize\", \"matrix\", \"cant_seem\", \"dimension\", \"grow\", \"vector\", \"association\", \"fp_growth\", \"chance\", \"rowname\", \"association_rule\", \"event\", \"logtime\", \"transcript_name\", \"pool\", \"gene_transcript\", \"element_text_size\", \"percontig\", \"polya_estimate\", \"logdate\", \"plot\", \"fasttext\", \"param\", \"path\", \"snakemake\", \"step\", \"wildcard\", \"fastq_gz\", \"expand\", \"quot\", \"pipeline\", \"stop\", \"galaxy\", \"therefore\", \"ansible\", \"https\", \"ansible_galaxy\", \"crash\", \"msg_merovingian\", \"proxy\", \"versions_page\", \"role\", \"community_general\", \"parallelism\", \"healthy\", \"delay\", \"api_collection\", \"collection_install\", \"shutdown\", \"namespace\", \"deploy\", \"paper\", \"configured_module\", \"google_cloud\", \"experiment\", \"packet\", \"timeout\", \"accepting_connection\", \"serve\", \"mapped_l\", \"failure_action\", \"community\", \"ansible_collection\", \"root\", \"lane\", \"fastq_gz\", \"yml\", \"quot\", \"wildcard\", \"param\", \"path\", \"snakemake\", \"bam\", \"docker\"], \"Freq\": [17781.0, 10900.0, 9624.0, 11176.0, 5283.0, 8743.0, 9787.0, 6805.0, 4768.0, 6241.0, 4380.0, 5936.0, 13671.0, 5165.0, 4228.0, 5477.0, 4921.0, 3656.0, 1880.0, 2728.0, 2573.0, 3469.0, 3343.0, 2525.0, 2705.0, 2716.0, 2724.0, 1865.0, 1822.0, 1662.0, 3414.995439845072, 2842.233680174214, 1802.6089634761267, 1792.6688361255665, 1730.800849703587, 1711.3916298163908, 1617.9904978778845, 1312.10053943767, 1274.8489285209248, 1203.5409370393832, 1191.9155591652545, 1178.7039607820964, 1147.4867168684336, 1060.2554448817723, 1033.6339394754627, 1010.7672875911286, 983.5885622536393, 974.0617091143811, 901.6907402299178, 882.3435073309017, 880.3487362179584, 851.2822253313498, 838.0670635250742, 799.3096162952014, 769.8952987382833, 768.0674284197177, 741.7709403498282, 739.5617837577113, 729.3775204884635, 1643.128873434651, 13575.402337155338, 3407.912680612757, 1609.1996159178066, 2529.9268254027897, 1747.725120623448, 1989.8530918196848, 1387.4076032301796, 4895.421065593495, 5154.327455972699, 2204.5150263308115, 3636.6231207177775, 1918.901028298348, 3086.0013232262395, 1694.2250461245533, 1481.6086822233676, 1519.3098645450282, 1913.0986152791102, 2225.4737551818985, 2209.117311592264, 1379.8052471851795, 1476.7650786335992, 1378.8316868414106, 1450.4806067579425, 1416.80211478419, 8741.919949500612, 4920.412224595645, 2290.7862205467536, 2101.514349260455, 1198.751810200137, 987.3559788015538, 963.0636730926277, 561.2659163841566, 529.2771044852083, 521.3288340868768, 469.88288748975015, 439.1062386995167, 430.91624581207043, 406.6373383085887, 337.2215132967234, 305.46327258546415, 305.141022165904, 296.99200060235285, 266.9659564521193, 244.35106168154257, 2184.473374374809, 220.3650422778221, 217.4631102586555, 207.04658805630797, 175.36973520172342, 172.8901101798353, 172.41715075369248, 170.8560656425297, 166.5295826057765, 163.20377630442954, 2248.826702716264, 2456.6783065512454, 447.80475312362285, 299.4496697658368, 7729.102165318216, 498.14760854538326, 2325.5225799671443, 302.89930544421026, 963.3994326758555, 1060.9358162719366, 1148.4415764959804, 925.4238698472801, 739.9680737712819, 566.6082774328864, 348.9465242152509, 307.6533105147296, 2427.7679628618034, 1599.0344856632744, 1442.5017328682054, 1384.5916573508732, 1346.1065567837807, 1276.3159587032885, 1210.1276025414577, 1142.2848291279201, 1133.6382922377531, 1114.2447021764067, 1088.3625960556217, 1064.2744873503066, 1060.7953072328332, 871.4987952282427, 664.0846850513833, 651.7626060086261, 644.927204335101, 620.0253081735252, 578.8455980914146, 485.92598873264967, 466.3718608350223, 443.3112975216719, 425.1749613318564, 417.4931226260425, 416.12098870478707, 390.5322125694255, 354.88703148813295, 334.9439477717346, 320.45344649181476, 308.87262896963506, 1499.0660780344617, 5314.498216925956, 698.0073622552283, 1087.565688301118, 2449.593468853334, 658.918876875181, 1092.71642347147, 579.0881083019767, 5935.310161909411, 3654.918871581178, 3342.4488128611965, 2606.382625659092, 2536.5931838980255, 2315.594745999444, 1041.8884130806382, 833.1854631757026, 749.2415116776751, 714.8579141230774, 696.4906931763411, 647.9766541262475, 378.755918098078, 291.1408100470726, 288.58006065344046, 233.78753630444461, 167.21139701730513, 166.5172627818275, 136.1111632034229, 126.46871823541701, 104.8400487199062, 104.45717720941296, 94.14344553386965, 66.57801841701425, 60.47346557155846, 59.69809140921895, 58.337053500592084, 42.15295373800078, 34.5120848620192, 30.380695280404606, 1406.1549422354294, 210.5766202105484, 4286.006617674952, 711.4133104659979, 505.65206869360156, 583.7211777569763, 792.5591174315344, 471.4056908734903, 264.4600686245298, 253.07878932985327, 280.9051559775277, 353.43997284391156, 206.75000050363482, 9623.477628235332, 6240.503669473319, 969.1329417645178, 321.4617680891834, 228.65598287784388, 185.43094757012162, 127.53082760085778, 112.76121894329704, 86.13245892189967, 76.20607382469667, 70.48296760967546, 62.748475367758786, 59.42341017722891, 56.96883860880542, 49.17911886359665, 41.279484070425084, 41.097018218606216, 40.808840963629095, 37.680561684554846, 36.188633701309946, 32.689634897672775, 30.64081545841745, 26.080728795492508, 23.71542443584851, 21.317990306997736, 21.184446491264126, 21.14109584640868, 13.515767842222983, 7.198287671684068, 29.69032317345414, 8919.097697507987, 2117.352504721459, 3947.015055965439, 916.6368972519236, 687.9529793247417, 917.7101743424042, 426.310594043453, 167.38326350949592, 92.28987929217348, 58.83829584269789, 68.41333298868888, 59.72146287376916, 4227.123392728936, 2723.160934548989, 2410.011297833896, 831.8203357180179, 818.345210813127, 812.8225196035603, 723.6094814425085, 674.3966083414646, 535.35015654294, 462.2513497685513, 427.56385117007915, 253.75199496392526, 222.397360999934, 170.60919118326126, 161.21521683909353, 145.22405838155737, 143.59745997158393, 140.1676850413825, 138.057022850723, 135.7578575491343, 119.91282754643586, 119.53932960314009, 104.64744225001608, 99.2665138123225, 86.68229500994904, 76.82273434326471, 73.36004766977638, 70.36257761231936, 68.63598539721605, 65.48403702462035, 13833.337073970943, 230.443338302134, 159.82180096210226, 109.63662340301198, 10899.236230358038, 2704.617176541267, 2116.765761964384, 264.9060412539866, 236.4830265086223, 196.93223381509546, 184.88969130545746, 175.20211263797418, 146.9790284951399, 144.61328535452589, 116.91890632046074, 109.69231351064647, 105.47117329207126, 100.54704099445712, 99.82172289856547, 78.41532306257862, 76.7951165815053, 67.20665905335949, 67.1970544977224, 51.469904561606405, 46.33144368285204, 41.25249917160409, 35.85434633875739, 31.992907649766483, 23.133944752923036, 23.026950180222965, 20.535928686978384, 17.69166527386421, 16.064405908434768, 13.37783843693763, 492.3632175946297, 2256.722378578029, 2022.9965856345684, 227.30926500318614, 306.2391372283645, 100.35631943113658, 133.31770218292183, 125.77179184174895, 126.4934735477989, 60.586593636864784, 71.36175393175796, 2726.968967882168, 2524.142372795948, 1994.0472664785245, 1402.274738679061, 1325.957192219276, 1058.0126509364325, 336.84995347105627, 256.6992253161393, 174.7895632321396, 160.23373658867112, 127.85613209476578, 111.67819844503677, 110.46646547797386, 98.6034182906707, 77.41503463851909, 51.489285606821404, 44.93362258937978, 41.508200066355656, 39.73491549764907, 39.5528887294172, 36.69310909668068, 21.972163380006645, 21.2230439822503, 21.09997846454619, 20.217505292262118, 17.871166819653677, 763.6684826676553, 6.7115917000017715, 115.44817862472573, 2655.225268667758, 489.87083712153736, 200.5246048483957, 73.31436716007879, 163.21135867551143, 96.01801737319701, 38.82717861798873, 4767.413653762558, 1380.2078728163074, 950.2734405618055, 593.5245194882547, 565.2682967718154, 514.7548984300795, 452.51090615199246, 415.85510812386264, 304.80364218027563, 256.4148692270432, 253.4617519203008, 210.04333138900958, 146.1317099505426, 137.61181022020023, 120.70058415210013, 98.43136533208587, 92.74958115178518, 85.74685513455042, 81.03364181716957, 78.4329953323568, 47.402559731724395, 35.84109280282337, 30.962602383191587, 24.19555200901317, 19.812255405597487, 19.658864045582195, 16.93326557526984, 9.226730803985078, 6.482983054898677, 6.314709075071927, 545.864498066397, 529.9094770809264, 1082.9223485506743, 56.75646221717423, 284.7547952190133, 91.16233724390162, 24.560665113152712, 2572.4793985808637, 1579.9615221635765, 1378.4599244073077, 955.3208934526273, 705.4791205820424, 487.2764714828199, 350.4490020516849, 297.742812815278, 258.96398195496045, 201.12326979765447, 123.16197221355485, 93.7968734693623, 78.87342058387833, 51.46120646580175, 39.11585174914546, 28.014738070830234, 27.81781371771022, 19.680753028842283, 19.27048978759827, 15.222593507585545, 14.900716010094074, 13.612866677617372, 8.331869250704676, 290.1137107784861, 318.13938198288884, 370.0396642797326, 2518.4881730315915, 0.4134189063928246, 64.92743731063808, 160.991385775856, 62.7206503132065, 94.47923006569839, 188.73491686164974, 127.56634872834348, 100.4928466486569, 91.29868211734345, 69.19937752967607, 34.80397666500966, 33.158295290705766, 4379.418212514623, 1514.0614506954485, 720.9074759770801, 394.8928493970659, 380.69679237722386, 366.5198609701011, 365.8936455641714, 304.13492562921675, 284.614205110385, 257.12295646980925, 216.5119891883536, 155.75326004474508, 132.5747467354532, 110.95444174663712, 100.9976374407893, 97.02925992798961, 96.10969479012954, 81.35970479137562, 79.79992848038339, 66.9005673544703, 64.51747342225767, 61.155463007451, 44.917811356190036, 35.0226845703074, 28.847999216548462, 21.026922207352836, 130.1648937222869, 185.36622569633985, 10.670168205260765, 3.2964796630026485, 1460.244815663322, 181.7152856051739, 219.56706707272318, 5280.20745524974, 1196.854291271148, 941.6039941429218, 557.0185115871504, 531.4215502459991, 401.9832771355834, 357.68871239540215, 320.0614584197176, 290.1820129737339, 117.16244799090578, 110.26533760000997, 96.00830856172625, 87.5720001733184, 82.56351956350221, 62.987529443210065, 54.94255718232737, 52.83733213829779, 50.583350419137524, 46.562958881574275, 39.363078367828855, 38.63413195186183, 34.95483128554086, 34.15401317594458, 28.825986970660477, 15.653285899460743, 646.1339676189134, 3.780509536839111, 84.56954019372282, 72.7793859212253, 13.09204753939715, 135.6236714330047, 37.667191712319905, 1821.6320508174915, 1784.4928020312173, 861.3291727196109, 496.6596377709717, 482.1269442719462, 410.9483999347657, 405.64909938010766, 253.05432120292716, 209.7684141771767, 199.18405000713216, 184.89841416813684, 172.38569294550612, 168.30748531943345, 162.26375911169796, 156.60863899451644, 142.82295873826078, 139.65389919182982, 138.71813738848417, 136.04689268793894, 108.31348047396757, 96.22502004162847, 91.0820660207954, 81.52561925808186, 56.53633032502893, 56.272463723924496, 43.785789865004794, 43.64148757396503, 39.841178247720435, 29.902706081893623, 28.958828183970557, 256.20344903437797, 124.83788020937436, 71.96199687013694, 1864.0487730853945, 1608.197481042304, 1340.3657944344484, 394.1947852395964, 388.3975304092314, 387.8156060635796, 253.03332185077696, 243.87459450965915, 198.62744097499834, 177.67231896366582, 170.85330919603626, 158.95203303743298, 140.8273949920125, 136.73373607150148, 122.18078838395832, 89.20860151538994, 78.43629904177776, 77.40268844120456, 74.59195746207862, 58.04714704061593, 44.72183151297696, 23.622019599293104, 22.924565443983056, 22.892724519668352, 22.30035509268932, 19.69246604444372, 7.676617602458574, 38.98649953793005, 49.195684619134994, 37.85305457420539, 126.1211269631339, 71.26836984440482, 1661.2384358583708, 1609.8102110078298, 483.57868184277714, 364.2819411705196, 189.6331900193093, 161.04995657474487, 158.8131357021792, 157.13091355563685, 83.70165966922222, 68.98476271115682, 57.203899115127626, 28.749792522134317, 20.4119522375378, 19.501159911409694, 16.25784558525084, 9.092814253696094, 247.55744277202658, 2.04460303390754, 1.7871165570721443, 1.3894957587386363, 0.5758827076363624, 0.09060427486544648, 0.09055136994375781, 0.09055260826313614, 0.09053604836500906, 0.09054244285038639, 0.0905447585775854, 0.09051941150918048, 0.09050725918850167, 0.09050103960699132, 34.443930801620525, 0.09058281066288884, 0.09122575727796255, 1337.8909485414283, 651.1280751529592, 364.96633553059945, 258.39154404308283, 176.4243313607546, 170.57292653371096, 159.28383959463886, 32.01234923175804, 30.648156123027274, 0.04445507353899421, 0.04425879375290963, 0.04425586437945392, 0.044252848732616645, 0.044229056886892694, 0.044229056886892694, 0.04419932002087577, 0.044196347510729275, 0.04421164927140422, 0.04418742605877246, 0.04418742605877246, 0.04418445354862597, 0.04418445354862597, 0.0441948220404826, 0.044181979071182126, 0.04418147711696214, 0.04418147711696214, 0.04417850460681565, 0.04417558699791197, 0.04417935557607922, 0.044172555665005324, 0.044202080769085696, 0.044186775086893414, 0.04420710815432027, 0.0442100845859841, 0.044266401496556314, 0.04419256324649265, 1139.512146288793, 367.0927188423949, 298.2727579384145, 223.13101394496445, 197.42875947152754, 187.82203138330928, 177.81094148553717, 133.24252487755112, 47.43399973988099, 45.34495467368212, 41.42878739612854, 37.74009648421843, 27.37325092283173, 12.233850044296089, 0.04897032106803172, 0.048962339094246056, 0.04895888166175066, 0.04895888166175066, 0.04895888166175066, 0.04895888166175066, 0.048959211495378165, 0.04897143862196963, 0.04895569197655289, 0.04896034845129416, 0.04895436876164724, 0.048954361000856005, 0.0489544036852078, 0.048953127035049566, 0.04895501290731979, 0.04895337926076472, 77.74289300262963, 0.0489548421699126, 0.04899017705240764, 0.04899311451189026, 0.049085390319679824, 0.04897807797887156, 0.04899344822591338, 0.048965668473686066, 0.04897351851402073, 0.048965237749772496, 0.04896430645482425, 0.048965773244367745, 0.04896375155825091, 0.04896308801060028, 1879.0742576391972, 394.9096959049798, 301.2840785984079, 142.53604032391715, 40.46526823787451, 36.545020950453186, 8.34250709347979, 2.088590419480616, 0.04509775054373191, 0.044949574710172004, 0.044867689863080366, 0.04489973691858786, 0.044865363548156845, 0.04487421217416424, 0.04488024908723471, 0.04485685649852925, 0.04485685649852925, 0.04485417422506256, 0.044852858257115095, 0.044851477569432516, 0.04487120270648111, 0.04485189105662912, 0.04484868023865894, 0.04485029463649613, 0.044852757581971575, 0.04490118592154641, 0.044865546920739685, 0.04484666673578851, 0.04484558088245481, 0.0448504528402931, 0.044853760737865955, 0.044872155524803725, 0.044888752541321464, 0.0448695056112046, 0.04485461647658589, 0.04486077563804492, 0.044854792658087055, 0.04485452658806489, 0.04485388658179536, 240.00507537051467, 170.4447240706065, 162.88929701484705, 138.42700289581566, 128.73161017752028, 120.87554290544267, 113.30933904418842, 87.03753064219111, 79.29254377653376, 60.749384416452365, 40.055239993584564, 20.092968959116178, 12.81070942990975, 11.493801293261134, 6.579149947853811, 5.5233938994241125, 4.883970981474239, 3.2459490307092578, 1.4605984392087312, 0.04286153373935506, 0.04285939244700245, 0.04285930132817894, 0.0428653971774721, 0.0428583673602379, 0.04285829902112027, 0.042858075780002655, 0.042858075780002655, 0.04285782520323799, 0.04285983892923768, 0.042860075838178815, 0.04289016782964469, 0.04288196257958716, 0.04294407828157744, 0.04286699175688361, 0.04287194862088284, 0.04286716943858947, 0.04286630836570725, 0.042874067133529564, 0.04286375248270765, 97.88514517466365, 89.50994036864174, 54.00776682963164, 0.02448108623017678, 0.02445591224231737, 0.024454986008716175, 0.02445000770034859, 0.0244491272261587, 0.024449270816035547, 0.024448622294723634, 0.024455045969324088, 0.024448649119206124, 0.02444816154478914, 0.02444795168266144, 0.024448008487447884, 0.024447429394208298, 0.02444760927603204, 0.024447784424123577, 0.024448063714323596, 0.02444977890329208, 0.02444683452186136, 0.024447023871149512, 0.024447031760703182, 0.024448327225416268, 0.024446635705108806, 0.02444850237350781, 0.024446676730787903, 0.02444673037975288, 0.024446894482469277, 0.024446487381499755, 0.024447042806078324, 0.024447165883115622, 0.024448974168817444, 0.024447697639033174, 0.024451853855908045, 0.024447670814550685, 0.024456199422071064, 0.024450818746466162, 0.024451382060598405, 0.024450736695107965, 0.024460006920673592, 0.02444779231367725, 0.02444773393098007], \"Total\": [17781.0, 10900.0, 9624.0, 11176.0, 5283.0, 8743.0, 9787.0, 6805.0, 4768.0, 6241.0, 4380.0, 5936.0, 13671.0, 5165.0, 4228.0, 5477.0, 4921.0, 3656.0, 1880.0, 2728.0, 2573.0, 3469.0, 3343.0, 2525.0, 2705.0, 2716.0, 2724.0, 1865.0, 1822.0, 1662.0, 3416.185297441147, 2843.3618332022124, 1803.737116504125, 1793.7969891535647, 1731.9290027315851, 1712.5234996650063, 1619.1186509058828, 1313.2356210616144, 1275.977081548923, 1204.6690900724554, 1193.043712216093, 1179.8321138100946, 1148.6148698964319, 1061.3835979097705, 1034.762092503461, 1011.8954406191269, 984.7167152816377, 975.1898621423794, 902.8188932579161, 883.4716603736441, 881.4768892866643, 852.4103783593481, 839.1952165530726, 800.4377693231997, 771.0234517662816, 769.1955814549563, 742.8990933778265, 740.6899367896311, 730.5056735281789, 1645.7062449567702, 13671.624874410763, 3425.330992462816, 1621.6718318453393, 2586.2131538923654, 1782.3764824747361, 2054.143224356118, 1426.819787078223, 5821.915544307271, 6411.669047175035, 2513.2387645216404, 4531.414070289325, 2204.700152875176, 4097.766400622098, 1947.0782062704898, 1647.057616827399, 1708.4002912246037, 2503.6381500125976, 3311.4936804427557, 3469.0413398921746, 1540.699228337349, 1823.6063581676142, 1591.2084531649616, 2550.4625741250748, 2458.965923816357, 8743.055226938397, 4921.547502037025, 2291.921497984537, 2102.6496266982385, 1199.88708763792, 988.4912562436797, 964.1989505386902, 562.4011938258612, 530.4123819229914, 522.4641115246599, 471.01816493452947, 440.2415161462544, 432.05152326941476, 407.7726157564978, 338.3567907446225, 306.5985500493304, 306.27629960804506, 298.127278078017, 268.1012339065326, 245.4863391412539, 2194.685572683289, 221.5003197156053, 218.59838769643866, 208.18186549409114, 176.5050126688902, 174.02538761761846, 173.55242819147566, 171.9913431050735, 167.66486004355968, 164.33905375992055, 2412.3572584497306, 2646.468334304438, 461.5940917861898, 309.8532521384852, 9787.542514585135, 536.8887916354197, 3243.2482628398448, 322.0648714414033, 2458.965923816357, 3311.4936804427557, 6411.669047175035, 5821.915544307271, 4531.414070289325, 2716.703303189835, 720.0412993865261, 2513.2387645216404, 2428.8975961032174, 1600.1641188964893, 1443.6313661171882, 1385.7212906069205, 1347.2361900213534, 1277.4540253894318, 1211.2572357746726, 1143.4144623683785, 1134.7679254871082, 1115.3743354096216, 1089.492229292758, 1065.4041205885956, 1061.9249404660482, 872.6284324370472, 665.2143182889561, 652.892239241841, 646.056837586292, 621.154941417576, 579.9752313297037, 487.05562197336525, 467.50149407111974, 444.4409307621304, 426.3045945694293, 418.6227558667661, 417.25062194306, 391.66184580264047, 356.016664744299, 336.0735810250168, 321.58307975076735, 310.00226224608815, 1516.6844380131672, 5477.123457299274, 740.9489866341644, 1224.2577069072709, 5165.559262610681, 909.091876100269, 2550.4625741250748, 780.6782672010855, 5936.4153063193435, 3656.024015991109, 3343.553957279449, 2607.487770069023, 2537.6983283079567, 2316.6998904093753, 1042.9935574905694, 834.2906075856337, 750.3466560876062, 715.9630585881446, 697.5958375862722, 649.0817985361786, 379.8610625080092, 292.24595448861606, 289.6852052165789, 234.89268076136503, 168.3165414532986, 167.62240719175873, 137.21630771537974, 127.573862736755, 105.94519317831225, 105.56232162658766, 95.24858994380088, 67.68316282694548, 61.578610142049484, 60.80323584496708, 59.442197910523305, 43.25809815248794, 35.6172292791939, 31.485839690335816, 1534.7462688274636, 236.1585997055532, 6805.519768570235, 1069.355629639834, 826.4387340386716, 1077.1445568788704, 1662.232607531713, 1291.1102101759873, 529.7174637882799, 494.8910691200354, 795.4755000372677, 1629.814401667148, 495.47177074374207, 9624.62413920801, 6241.650180445996, 970.2794527371937, 322.60827909202783, 229.80249393122892, 186.57747276664526, 128.67733865728675, 113.9077302056532, 87.27896997294867, 77.35258494405909, 71.62947865504873, 63.89498637974725, 60.56992118246086, 58.11534963657322, 50.32563011788122, 42.42599507134, 42.243529270548954, 41.95535194354856, 38.827072736530894, 37.335144686544524, 33.836146013883976, 31.787326459596212, 27.22724029539144, 24.86193550365614, 22.464501306221965, 22.330957473326738, 22.28760685933222, 14.662278831786193, 8.344798666048165, 34.98126819549502, 11176.921604176025, 2870.1988685247434, 17781.44266805928, 3243.2482628398448, 2716.703303189835, 4097.766400622098, 2503.6381500125976, 1176.5674347890388, 1340.9500891292118, 511.1250022334826, 1647.057616827399, 5165.559262610681, 4228.260422668842, 2724.2979644956663, 2411.148327776684, 832.9573656579227, 819.4822407530318, 813.9595495434651, 724.7465113852957, 675.5336382857272, 536.4871864867663, 463.38837973381106, 428.70088112170583, 254.88902494902888, 223.53439094706417, 171.74622114848978, 162.35224679766088, 146.36108835935173, 144.73448991148885, 141.30471500247347, 139.19405279568593, 136.89488750592645, 121.04985750165811, 120.67635954304501, 105.7844722092348, 100.40354377983071, 87.81932496145532, 77.95976430019655, 74.49707767939216, 71.49960773415383, 69.77301539042925, 66.62106696452527, 17781.44266805928, 1069.355629639834, 1540.699228337349, 872.2382942009069, 10900.384250265055, 2705.765196473768, 2117.9137818757426, 266.05406116100204, 237.6310464269872, 198.08025377395188, 186.0377112235573, 176.35013257265373, 148.12704841997837, 145.76130526154137, 118.0669262549848, 110.8403334458461, 106.61919321597395, 101.69506092106533, 100.9697428357489, 79.56334298648132, 77.94313649357878, 68.35467899491714, 68.34507443891988, 52.61792451300854, 47.47946358986751, 42.400519147985314, 37.00236629306831, 33.140927585253834, 24.28196469525423, 24.17497008723842, 21.683948635028162, 18.839685297015656, 17.212427033528574, 14.52585834395309, 1077.1445568788704, 11176.921604176025, 9787.542514585135, 615.7937085096839, 943.1063953807765, 209.3365918028486, 390.60685593022026, 633.5147256171601, 1069.355629639834, 146.24286769344775, 2412.3572584497306, 2728.09789147146, 2525.2712963852405, 1995.1761900764639, 1403.4036622683532, 1327.0861158085681, 1059.1415745300824, 337.9788771342943, 257.8281489085822, 175.91848683373016, 161.362660182689, 128.985055761793, 112.80712208692016, 111.59538906726607, 99.73234194173041, 78.54395826008474, 52.61820922841179, 46.062546178671994, 42.63712365564787, 40.86383913536742, 40.68181234426637, 37.82203273306894, 23.101087005963503, 22.351967617708983, 22.22890210723937, 21.346428909708507, 19.000090444833702, 835.9922550729129, 7.840515336138108, 156.70103537346674, 5165.559262610681, 1365.7264473877633, 780.6782672010855, 266.2059808807217, 2503.6381500125976, 1647.057616827399, 4097.766400622098, 4768.522826524162, 1381.3170455892598, 951.3826133234086, 594.6336922498577, 566.3774695334184, 515.8640712145941, 453.6200789407548, 416.9642808854657, 305.9128149418787, 257.5240484788036, 254.5709247215234, 211.15250415061269, 147.2408827363889, 138.72098301293974, 121.8097569586654, 99.54053809874698, 93.85875395391908, 86.85602790480068, 82.14281458838661, 79.54216810095606, 48.511732553789486, 36.95026556442647, 32.071775170388115, 25.304724770616268, 20.921428167200585, 20.768036832979266, 18.04243834159857, 10.335903612391984, 7.592155816501776, 7.423881836675027, 747.083638180835, 874.9913631798848, 3469.0413398921746, 115.81206533279297, 2204.700152875176, 2412.3572584497306, 236.1585997055532, 2573.5922348243053, 1581.0743584070183, 1379.5747030394618, 956.433729696069, 706.5919568254841, 488.38930772626173, 351.56183829512673, 298.85564905871985, 260.07681826175195, 202.23610607250785, 124.27480845699668, 94.90970972894333, 79.98625682732016, 52.5740427092436, 40.22868808338751, 29.127574333863212, 28.930649961152056, 20.79358928386726, 20.383326031040106, 16.335429794567425, 16.013552253535913, 14.725702921059211, 9.4447055463609, 407.0108710079751, 514.7283708620573, 720.0412993865261, 6805.519768570235, 1.5262551498346641, 313.50725420610803, 1119.6946695312213, 525.1467123150219, 872.2382942009069, 2646.468334304438, 1534.7462688274636, 1153.0637367468669, 1171.4788916581008, 13671.624874410763, 378.7966948264075, 547.513942727563, 4380.560348145462, 1515.2035863302083, 722.0496116151618, 396.03498502790427, 381.8389280080622, 367.6619966056651, 367.03578124312963, 305.2770612600551, 285.75634074122337, 258.2650921366715, 217.65412484266506, 156.89539569345612, 133.71688236629151, 112.0965773885906, 102.13977312533805, 98.17139561406293, 97.25183044819119, 82.50184043331959, 80.94206412280484, 68.04270303760991, 65.65960905309598, 62.297599023398696, 46.059947220431475, 36.16482023419639, 29.990134882021223, 22.169057862290575, 140.89455830554957, 201.8524638742735, 11.812303868194698, 4.438615293840962, 2716.703303189835, 558.6677455940453, 795.4755000372677, 5283.014600964822, 1197.986008002478, 942.7357108674794, 558.150228311708, 532.5532669749147, 403.11499387885914, 358.82042913439136, 321.19317517360076, 291.3137296982917, 118.29416471546354, 111.39705435469217, 97.14002533525499, 88.70371691045092, 83.69523628805997, 64.11924621559794, 56.074273906885125, 53.96904887225637, 51.71506720485708, 47.694675622907226, 40.49479511495072, 39.76584868077747, 36.086548033999314, 35.2857299184846, 29.957703746655262, 16.78500264693002, 741.1602526553287, 4.9122262613968575, 146.24286769344775, 174.28015255127295, 74.07285925380684, 1224.2577069072709, 536.8887916354197, 1822.7627384048403, 1785.623489618566, 862.4598603069599, 497.7903253583207, 483.2576318671382, 412.079087529355, 406.77978696745663, 254.18500881318764, 210.89910176452565, 200.3147375944811, 186.02910175548578, 173.5163805608245, 169.43817291133834, 163.3944466990469, 157.73932658885693, 143.95364632560972, 140.78458677917877, 139.84882499128213, 137.17758030960115, 109.44416809789116, 97.35570762897741, 92.21275366461592, 82.6563068454308, 57.66701791237788, 57.40315131442415, 44.91647749238372, 44.77217517710872, 40.97186583506939, 31.03339367360044, 30.08951577831565, 390.60685593022026, 237.4140169232595, 457.5298200885269, 1865.1682548466374, 1609.3169628071425, 1341.4852762024636, 395.3142670008394, 389.51701217831743, 388.9350878248226, 254.15280362698152, 244.9940762709021, 199.7469227362413, 178.79180072490877, 171.97279098551118, 160.07151480255635, 141.9468767967179, 137.85321783274443, 123.30027015973604, 90.3280833345841, 79.55578080736356, 78.52217020921036, 75.71143931207354, 59.166628801858906, 45.841313274219935, 24.741501369922705, 24.044047205226022, 24.01220631345298, 23.41983686650706, 20.811947819678995, 8.796099403301437, 129.3387849394482, 203.9306040591368, 157.61961830530115, 1656.1422309657135, 835.9922550729129, 1662.3409765280544, 1610.9127516775134, 484.6812225124609, 365.384481843354, 190.73573076418594, 162.15249728985597, 159.91567637186293, 158.23345424734885, 84.80420033890597, 70.08730343179357, 58.30643979231197, 29.852333583469246, 21.51449293724707, 20.60370058109343, 17.36038625493458, 10.195354923379831, 313.50725420610803, 3.147143703591277, 2.8896572267558818, 2.4920364284223733, 1.6784233773200994, 1.1932067441607779, 1.1932290291540535, 1.1932702651536957, 1.1931089545113889, 1.1932063898602079, 1.1934201800604851, 1.1931577916448108, 1.1931170595667104, 1.1930570864463137, 9787.542514585135, 1.1982239101768544, 2870.1988685247434, 1339.0398025633697, 652.2769291749006, 366.1151895525409, 259.5403980650243, 177.57318538269604, 171.72178056737417, 160.4326936633688, 33.16120325369949, 31.797010161855955, 1.1935928173636847, 1.1932147278459475, 1.1932267792071791, 1.1932032889294324, 1.1931574456984964, 1.1931594856966798, 1.1931001570796007, 1.1930944311813658, 1.1937143427843762, 1.193077232471196, 1.193077232471196, 1.1930715203793227, 1.1930715203793227, 1.193373196153765, 1.1930734288563756, 1.1930657980671209, 1.1930657980671209, 1.1930600697333462, 1.1930633410467235, 1.1931724605082497, 1.193048622295392, 1.1939890121299772, 1.1935227750003357, 1.1943767263454999, 1.194866338468059, 1.1982239101768544, 1.1944948269871714, 1140.6562047560578, 368.2367773096598, 299.41681640567936, 224.2750724122293, 198.57281793879238, 188.96608985057412, 178.954999952802, 134.38658337550302, 48.57805825736792, 46.48901316598493, 42.57284587525188, 38.88415495148329, 28.51730939009658, 13.37790851156094, 1.1930772107117467, 1.1930483429766425, 1.1930375545611294, 1.1930375545611294, 1.1930375545611294, 1.1930375545611294, 1.193056718435359, 1.1933847651712441, 1.1930262638128355, 1.193164883449458, 1.1930240952909565, 1.1930240044504086, 1.193026863163459, 1.1930180825514722, 1.1930657258470527, 1.1930301031224093, 2458.965923816357, 1.1930802544562729, 1.1943767263454999, 1.1944948269871714, 1.1982239101768544, 1.1939890121299772, 1.194866338468059, 1.1935719009215757, 1.1941483278753704, 1.193604306064317, 1.1935227750003357, 1.1938033528331728, 1.1938190896602812, 1.1937683074684449, 1880.222423689432, 396.0578619669363, 302.4322446486426, 143.68420637415187, 41.61343428810923, 37.69318700068791, 9.490673143714506, 3.2367564697153317, 1.1982239101768544, 1.194866338468059, 1.193099393181515, 1.1941483278753704, 1.1933305813337611, 1.1935719009215757, 1.1937683074684449, 1.1931548072700369, 1.1931548072700369, 1.1930957714725339, 1.193062642295324, 1.1930541804100887, 1.193604306064317, 1.1931032215315698, 1.1930451795902217, 1.1931057539470529, 1.193183367755825, 1.1944948269871714, 1.1935506185545244, 1.19305723563969, 1.1930300102781046, 1.1931611783902805, 1.193257276140713, 1.1938033528331728, 1.1943767263454999, 1.1939323202083774, 1.1933278723075944, 1.1938190896602812, 1.1934831786727664, 1.1936524836478057, 1.1939890121299772, 241.1552281939922, 171.59487690580582, 164.03944983832457, 139.57715573256024, 129.8817630159594, 122.02569572892021, 114.45949186766596, 88.18768346566866, 80.44269661084715, 61.89953724385142, 41.205392817062105, 21.24312178259371, 13.960862289187673, 12.643954125439373, 7.729302771331346, 6.673546745403431, 6.0341238092945995, 4.396101854186792, 2.610751262686266, 1.1930821236398583, 1.1930308623684247, 1.1930302311214853, 1.1932161838947746, 1.193027481010334, 1.1930311151907094, 1.193029133505314, 1.193029133505314, 1.1930262899936284, 1.1930883260564062, 1.1931171662811277, 1.1944948269871714, 1.1943767263454999, 1.1982239101768544, 1.1937143427843762, 1.1941483278753704, 1.1937683074684449, 1.1938033528331728, 1.194866338468059, 1.1938190896602812, 99.05370293089659, 90.6784981131529, 55.176324578500676, 1.1934201800604851, 1.19314481372213, 1.1931664010516092, 1.1930612681568258, 1.193020978069038, 1.1930302001481423, 1.193011886837019, 1.1933265681684666, 1.1930196318915074, 1.1930204104681938, 1.1930112098734904, 1.1930189683589951, 1.193008850350597, 1.193025356516309, 1.193044472560772, 1.193067585472943, 1.193151702664466, 1.1930115896497182, 1.1930211288317134, 1.1930223072939832, 1.1930866129051612, 1.1930073885067076, 1.1930988967112608, 1.1930125553333697, 1.1930170802829183, 1.1930254643101073, 1.193006013368035, 1.1930351757177475, 1.1930488986549421, 1.1931840583036746, 1.1930957714725339, 1.1937683074684449, 1.1931566203201454, 1.194866338468059, 1.1941483278753704, 1.1944948269871714, 1.1943767263454999, 1.1982239101768544, 1.1935719009215757, 1.1933847651712441], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic16\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic17\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic18\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic19\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\", \"Topic20\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.9534, -4.137, -4.5923, -4.5979, -4.633, -4.6443, -4.7004, -4.9099, -4.9387, -4.9963, -5.006, -5.0172, -5.044, -5.1231, -5.1485, -5.1709, -5.1981, -5.2079, -5.2851, -5.3067, -5.309, -5.3426, -5.3582, -5.4056, -5.4431, -5.4455, -5.4803, -5.4833, -5.4971, -4.685, -2.5733, -3.9555, -4.7058, -4.2534, -4.6233, -4.4935, -4.8541, -3.5933, -3.5417, -4.3911, -3.8905, -4.5298, -4.0547, -4.6543, -4.7884, -4.7633, -4.5328, -4.3816, -4.389, -4.8596, -4.7917, -4.8603, -4.8097, -4.8332, -1.919, -2.4938, -3.2583, -3.3445, -3.9059, -4.0999, -4.1248, -4.6647, -4.7234, -4.7385, -4.8424, -4.9102, -4.929, -4.987, -5.1742, -5.2731, -5.2741, -5.3012, -5.4078, -5.4963, -3.3058, -5.5996, -5.6129, -5.662, -5.828, -5.8423, -5.845, -5.8541, -5.8797, -5.8999, -3.2767, -3.1883, -4.8906, -5.293, -2.0422, -4.784, -3.2432, -5.2815, -4.1244, -4.028, -3.9488, -4.1647, -4.3883, -4.6552, -5.14, -5.2659, -3.1281, -3.5457, -3.6487, -3.6897, -3.7178, -3.7711, -3.8243, -3.882, -3.8896, -3.9069, -3.9304, -3.9528, -3.956, -4.1526, -4.4244, -4.4431, -4.4537, -4.4931, -4.5618, -4.7368, -4.7778, -4.8285, -4.8703, -4.8885, -4.8918, -4.9553, -5.051, -5.1088, -5.1531, -5.1899, -3.6102, -2.3446, -4.3746, -3.9311, -3.1191, -4.4322, -3.9264, -4.5614, -1.9395, -2.4244, -2.5137, -2.7625, -2.7896, -2.8808, -3.6794, -3.9029, -4.0091, -4.0561, -4.0821, -4.1543, -4.6913, -4.9544, -4.9632, -5.1738, -5.5089, -5.5131, -5.7147, -5.7882, -5.9758, -5.9794, -6.0834, -6.4298, -6.526, -6.5389, -6.562, -6.8869, -7.0869, -7.2144, -3.3796, -5.2784, -2.2651, -4.0609, -4.4024, -4.2588, -3.9529, -4.4725, -5.0505, -5.0945, -4.9902, -4.7605, -5.2967, -1.3821, -1.8152, -3.6776, -4.7812, -5.1218, -5.3314, -5.7057, -5.8288, -6.0982, -6.2206, -6.2987, -6.4149, -6.4694, -6.5115, -6.6586, -6.8337, -6.8381, -6.8451, -6.9249, -6.9653, -7.067, -7.1317, -7.2929, -7.3879, -7.4945, -7.5008, -7.5028, -7.9502, -8.5802, -7.1632, -1.4581, -2.8961, -2.2733, -3.7333, -4.0203, -3.7322, -4.4989, -5.4338, -6.0291, -6.4793, -6.3285, -6.4644, -2.0785, -2.5182, -2.6404, -3.7041, -3.7205, -3.7272, -3.8435, -3.9139, -4.1448, -4.2917, -4.3697, -4.8914, -5.0233, -5.2884, -5.345, -5.4495, -5.4607, -5.4849, -5.5001, -5.5169, -5.641, -5.6441, -5.7772, -5.83, -5.9655, -6.0863, -6.1324, -6.1741, -6.1989, -6.246, -0.8929, -4.9878, -5.3537, -5.7306, -0.8443, -2.238, -2.4831, -4.5613, -4.6748, -4.8578, -4.9209, -4.9748, -5.1504, -5.1666, -5.3792, -5.443, -5.4823, -5.5301, -5.5373, -5.7787, -5.7996, -5.9329, -5.9331, -6.1997, -6.3049, -6.421, -6.5612, -6.6752, -6.9994, -7.004, -7.1185, -7.2676, -7.3641, -7.5471, -3.9415, -2.419, -2.5284, -4.7144, -4.4163, -5.532, -5.248, -5.3062, -5.3005, -6.0366, -5.8729, -1.9465, -2.0238, -2.2595, -2.6116, -2.6676, -2.8933, -4.0378, -4.3095, -4.6939, -4.7808, -5.0065, -5.1418, -5.1527, -5.2663, -5.5083, -5.9161, -6.0523, -6.1315, -6.1752, -6.1798, -6.2549, -6.7677, -6.8024, -6.8082, -6.8509, -6.9743, -3.2193, -7.9536, -5.1086, -1.9732, -3.6633, -4.5565, -5.5627, -4.7624, -5.2929, -6.1983, -1.2412, -2.4808, -2.854, -3.3247, -3.3735, -3.4671, -3.5959, -3.6804, -3.9911, -4.164, -4.1755, -4.3634, -4.7262, -4.7863, -4.9174, -5.1214, -5.1808, -5.2594, -5.3159, -5.3485, -5.8521, -6.1317, -6.278, -6.5246, -6.7245, -6.7322, -6.8815, -7.4886, -7.8416, -7.8679, -3.4084, -3.438, -2.7233, -5.672, -4.0591, -5.1981, -6.5096, -1.8232, -2.3107, -2.4471, -2.8138, -3.117, -3.487, -3.8167, -3.9796, -4.1192, -4.372, -4.8624, -5.1347, -5.308, -5.735, -6.0093, -6.3431, -6.3502, -6.6962, -6.7173, -6.9531, -6.9745, -7.0649, -7.5558, -4.0056, -3.9134, -3.7623, -1.8445, -10.5592, -5.5026, -4.5945, -5.5372, -5.1275, -4.4355, -4.8272, -5.0658, -5.1617, -5.4389, -6.1261, -6.1746, -1.1519, -2.214, -2.956, -3.5579, -3.5945, -3.6325, -3.6342, -3.8191, -3.8854, -3.987, -4.1589, -4.4883, -4.6494, -4.8274, -4.9214, -4.9615, -4.9711, -5.1377, -5.157, -5.3333, -5.3696, -5.4231, -5.7317, -5.9805, -6.1745, -6.4907, -4.6677, -4.3142, -7.1691, -8.3437, -2.2502, -4.3341, -4.1449, -0.9318, -2.4161, -2.656, -3.1809, -3.228, -3.5071, -3.6239, -3.735, -3.833, -4.74, -4.8007, -4.9391, -5.0311, -5.09, -5.3606, -5.4973, -5.5363, -5.5799, -5.6627, -5.8307, -5.8494, -5.9495, -5.9727, -6.1423, -6.7529, -3.0325, -8.1737, -5.066, -5.2161, -6.9315, -4.5937, -5.8748, -1.8124, -1.833, -2.5615, -3.112, -3.1417, -3.3015, -3.3144, -3.7863, -3.9739, -4.0257, -4.1001, -4.1702, -4.1941, -4.2307, -4.2662, -4.3583, -4.3808, -4.3875, -4.4069, -4.6349, -4.7532, -4.8082, -4.919, -5.2851, -5.2897, -5.5406, -5.5439, -5.635, -5.922, -5.9541, -3.774, -4.4929, -5.0438, -1.7183, -1.8659, -2.0481, -3.2719, -3.2867, -3.2882, -3.7152, -3.7521, -3.9573, -4.0688, -4.108, -4.1802, -4.3012, -4.3307, -4.4433, -4.7578, -4.8865, -4.8997, -4.9367, -5.1875, -5.4483, -6.0866, -6.1166, -6.118, -6.1442, -6.2685, -7.2106, -5.5856, -5.353, -5.6151, -4.4115, -4.9823, -1.5327, -1.5641, -2.7668, -3.0501, -3.7029, -3.8663, -3.8803, -3.8909, -4.5207, -4.7141, -4.9014, -5.5894, -5.9319, -5.9775, -6.1594, -6.7405, -3.4363, -8.2328, -8.3674, -8.619, -9.4998, -11.3492, -11.3498, -11.3498, -11.35, -11.3499, -11.3499, -11.3502, -11.3503, -11.3504, -5.4086, -11.3495, -11.3424, -1.1703, -1.8904, -2.4693, -2.8146, -3.1962, -3.2299, -3.2984, -4.903, -4.9465, -11.4824, -11.4868, -11.4869, -11.4869, -11.4875, -11.4875, -11.4881, -11.4882, -11.4879, -11.4884, -11.4884, -11.4885, -11.4885, -11.4882, -11.4885, -11.4885, -11.4885, -11.4886, -11.4887, -11.4886, -11.4888, -11.4881, -11.4884, -11.488, -11.4879, -11.4866, -11.4883, -1.3202, -2.4529, -2.6606, -2.9508, -3.0732, -3.1231, -3.1778, -3.4664, -4.4992, -4.5443, -4.6346, -4.7278, -5.049, -5.8544, -11.3751, -11.3753, -11.3753, -11.3753, -11.3753, -11.3753, -11.3753, -11.3751, -11.3754, -11.3753, -11.3754, -11.3754, -11.3754, -11.3755, -11.3754, -11.3754, -4.0052, -11.3754, -11.3747, -11.3746, -11.3728, -11.3749, -11.3746, -11.3752, -11.375, -11.3752, -11.3752, -11.3752, -11.3752, -11.3752, -0.7438, -2.3037, -2.5743, -3.3227, -4.5819, -4.6838, -6.161, -7.5458, -11.3812, -11.3845, -11.3864, -11.3856, -11.3864, -11.3862, -11.3861, -11.3866, -11.3866, -11.3867, -11.3867, -11.3867, -11.3863, -11.3867, -11.3868, -11.3867, -11.3867, -11.3856, -11.3864, -11.3868, -11.3868, -11.3867, -11.3867, -11.3863, -11.3859, -11.3863, -11.3866, -11.3865, -11.3866, -11.3866, -11.3867, -2.3452, -2.6875, -2.7328, -2.8956, -2.9682, -3.0311, -3.0958, -3.3596, -3.4528, -3.7191, -4.1356, -4.8255, -5.2756, -5.3841, -5.942, -6.1169, -6.2399, -6.6485, -7.4471, -10.9757, -10.9757, -10.9757, -10.9756, -10.9758, -10.9758, -10.9758, -10.9758, -10.9758, -10.9757, -10.9757, -10.975, -10.9752, -10.9738, -10.9756, -10.9754, -10.9756, -10.9756, -10.9754, -10.9756, -2.1818, -2.2712, -2.7764, -10.4754, -10.4765, -10.4765, -10.4767, -10.4767, -10.4767, -10.4768, -10.4765, -10.4768, -10.4768, -10.4768, -10.4768, -10.4768, -10.4768, -10.4768, -10.4768, -10.4767, -10.4768, -10.4768, -10.4768, -10.4768, -10.4768, -10.4768, -10.4768, -10.4768, -10.4768, -10.4768, -10.4768, -10.4768, -10.4767, -10.4768, -10.4766, -10.4768, -10.4764, -10.4767, -10.4766, -10.4767, -10.4763, -10.4768, -10.4768], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.1382, 1.1382, 1.1379, 1.1379, 1.1379, 1.1379, 1.1379, 1.1377, 1.1377, 1.1376, 1.1376, 1.1376, 1.1376, 1.1375, 1.1375, 1.1374, 1.1374, 1.1374, 1.1373, 1.1373, 1.1373, 1.1372, 1.1372, 1.1371, 1.1371, 1.1371, 1.137, 1.137, 1.137, 1.137, 1.1315, 1.1335, 1.1308, 1.1166, 1.1189, 1.1068, 1.1105, 0.9652, 0.9203, 1.0075, 0.9186, 0.9997, 0.855, 0.9995, 1.0327, 1.0213, 0.8695, 0.7411, 0.6873, 1.0283, 0.9276, 0.9953, 0.5742, 0.5872, 2.2328, 2.2327, 2.2325, 2.2324, 2.232, 2.2318, 2.2318, 2.231, 2.2308, 2.2308, 2.2306, 2.2304, 2.2303, 2.2302, 2.2296, 2.2293, 2.2293, 2.2292, 2.2287, 2.2283, 2.2283, 2.2278, 2.2278, 2.2275, 2.2265, 2.2264, 2.2264, 2.2264, 2.2262, 2.226, 2.1628, 2.1586, 2.2026, 2.1988, 1.9969, 2.1581, 1.9003, 2.1716, 1.2959, 1.0947, 0.5133, 0.3938, 0.4208, 0.6655, 1.5086, 0.1326, 2.3046, 2.3044, 2.3043, 2.3043, 2.3042, 2.3042, 2.3041, 2.3041, 2.3041, 2.3041, 2.304, 2.304, 2.304, 2.3038, 2.3034, 2.3033, 2.3033, 2.3033, 2.3031, 2.3028, 2.3027, 2.3025, 2.3024, 2.3024, 2.3024, 2.3022, 2.3019, 2.3017, 2.3016, 2.3014, 2.2934, 2.2749, 2.2454, 2.1867, 1.559, 1.9832, 1.4575, 2.0064, 2.5995, 2.5994, 2.5994, 2.5993, 2.5992, 2.5992, 2.5986, 2.5984, 2.5982, 2.5981, 2.5981, 2.598, 2.5968, 2.5959, 2.5959, 2.595, 2.5931, 2.5931, 2.5916, 2.591, 2.5892, 2.5892, 2.588, 2.5832, 2.5816, 2.5813, 2.5809, 2.5738, 2.5682, 2.564, 2.5122, 2.485, 2.1373, 2.1921, 2.1084, 1.987, 1.859, 1.5921, 1.905, 1.929, 1.5588, 1.0712, 1.7257, 2.6737, 2.6737, 2.6727, 2.6703, 2.6688, 2.6677, 2.6649, 2.6637, 2.6606, 2.6589, 2.6577, 2.6557, 2.6547, 2.6539, 2.6508, 2.6464, 2.6463, 2.6461, 2.6439, 2.6426, 2.6394, 2.6371, 2.6308, 2.6266, 2.6215, 2.6211, 2.621, 2.5924, 2.526, 2.5098, 2.4482, 2.3696, 1.1686, 1.4102, 1.3004, 1.1775, 0.9035, 0.7238, -0.0024, 0.512, -0.5073, -1.7862, 2.7999, 2.7997, 2.7997, 2.7988, 2.7987, 2.7987, 2.7986, 2.7984, 2.798, 2.7977, 2.7975, 2.7957, 2.795, 2.7935, 2.7931, 2.7923, 2.7922, 2.792, 2.7919, 2.7918, 2.7907, 2.7907, 2.7893, 2.7887, 2.7871, 2.7854, 2.7847, 2.7841, 2.7837, 2.7829, 2.5491, 1.2653, 0.5342, 0.7262, 3.0871, 3.0868, 3.0866, 3.0829, 3.0823, 3.0814, 3.081, 3.0807, 3.0794, 3.0793, 3.0774, 3.0768, 3.0764, 3.0758, 3.0758, 3.0727, 3.0723, 3.0703, 3.0702, 3.0651, 3.0627, 3.0597, 3.0557, 3.0519, 3.0388, 3.0385, 3.0328, 3.0243, 3.0182, 3.0049, 2.3043, 1.4873, 1.5107, 2.0906, 1.9624, 2.352, 2.0122, 1.4704, 0.9526, 2.206, -0.4334, 3.37, 3.37, 3.3699, 3.3696, 3.3696, 3.3694, 3.3671, 3.3661, 3.364, 3.3634, 3.3617, 3.3604, 3.3603, 3.3591, 3.356, 3.3488, 3.3456, 3.3436, 3.3424, 3.3423, 3.3401, 3.3203, 3.3186, 3.3183, 3.3161, 3.3092, 3.28, 3.215, 3.0649, 2.705, 2.3451, 2.0112, 2.0809, 0.64, 0.5282, -1.2886, 3.5169, 3.5163, 3.516, 3.5153, 3.5152, 3.515, 3.5147, 3.5145, 3.5135, 3.5128, 3.5128, 3.5119, 3.5096, 3.5091, 3.508, 3.5059, 3.5052, 3.5043, 3.5035, 3.5031, 3.494, 3.4867, 3.4819, 3.4723, 3.4627, 3.4622, 3.4537, 3.4036, 3.3592, 3.3553, 3.2033, 3.0156, 2.3529, 2.8039, 1.4704, 0.2414, 1.2538, 3.5516, 3.5513, 3.5512, 3.5509, 3.5504, 3.5497, 3.5488, 3.5483, 3.5477, 3.5465, 3.543, 3.5402, 3.538, 3.5306, 3.524, 3.5131, 3.5128, 3.497, 3.4959, 3.4815, 3.48, 3.4734, 3.4267, 3.2134, 3.0709, 2.8863, 2.5579, 2.2459, 1.9775, 1.6126, 1.427, 1.3293, 0.9114, 1.0645, 1.1119, 1.0001, -1.7341, 1.1647, 0.7479, 3.6911, 3.6906, 3.6898, 3.6885, 3.6884, 3.6882, 3.6882, 3.6876, 3.6873, 3.6869, 3.6861, 3.684, 3.6828, 3.6811, 3.6801, 3.6796, 3.6795, 3.6774, 3.6771, 3.6744, 3.6738, 3.6728, 3.6662, 3.6593, 3.6525, 3.6385, 3.6121, 3.6061, 3.5897, 3.3939, 3.0705, 2.5682, 2.4041, 3.7238, 3.7234, 3.7231, 3.7223, 3.7222, 3.7215, 3.7212, 3.7208, 3.7204, 3.7147, 3.7141, 3.7126, 3.7115, 3.7107, 3.7065, 3.704, 3.7031, 3.7022, 3.7003, 3.696, 3.6955, 3.6925, 3.6917, 3.6858, 3.6545, 3.5871, 3.4625, 3.1766, 2.8511, 1.9913, 1.5241, 1.0673, 3.9073, 3.9073, 3.9066, 3.9057, 3.9056, 3.9052, 3.9052, 3.9035, 3.9026, 3.9023, 3.9019, 3.9014, 3.9013, 3.901, 3.9008, 3.9001, 3.8999, 3.8998, 3.8997, 3.8976, 3.8963, 3.8956, 3.8942, 3.8882, 3.8881, 3.8825, 3.8824, 3.88, 3.8708, 3.8697, 3.4862, 3.2652, 2.0582, 3.9785, 3.9784, 3.9783, 3.9763, 3.9762, 3.9762, 3.9747, 3.9745, 3.9735, 3.9728, 3.9726, 3.9721, 3.9712, 3.971, 3.97, 3.9666, 3.9649, 3.9648, 3.9642, 3.96, 3.9544, 3.9328, 3.9314, 3.9314, 3.9301, 3.9238, 3.843, 2.7799, 2.5571, 2.5526, 1.4041, 1.517, 4.2792, 4.2792, 4.2776, 4.2769, 4.2741, 4.2731, 4.273, 4.2729, 4.2668, 4.264, 4.2608, 4.2423, 4.2273, 4.2249, 4.2143, 4.1655, 4.0437, 3.8486, 3.7994, 3.6957, 3.2102, 1.702, 1.7014, 1.7014, 1.7013, 1.7013, 1.7012, 1.7011, 1.701, 1.701, -1.3696, 1.6976, -6.0766, 4.8579, 4.857, 4.8556, 4.8544, 4.8523, 4.8521, 4.8516, 4.8235, 4.822, 1.5685, 1.5644, 1.5644, 1.5643, 1.5638, 1.5638, 1.5632, 1.5631, 1.5629, 1.5629, 1.5629, 1.5629, 1.5629, 1.5629, 1.5628, 1.5628, 1.5628, 1.5627, 1.5627, 1.5627, 1.5626, 1.5625, 1.5625, 1.5623, 1.562, 1.5604, 1.5619, 4.8683, 4.8662, 4.8655, 4.8642, 4.8636, 4.8633, 4.8629, 4.8608, 4.8455, 4.8444, 4.8421, 4.8395, 4.8284, 4.7799, 1.6763, 1.6761, 1.6761, 1.6761, 1.6761, 1.6761, 1.676, 1.676, 1.676, 1.676, 1.676, 1.676, 1.676, 1.6759, 1.6759, 1.6759, 1.4152, 1.6759, 1.6756, 1.6755, 1.6743, 1.6756, 1.6752, 1.6757, 1.6754, 1.6757, 1.6758, 1.6756, 1.6755, 1.6755, 4.945, 4.9427, 4.9418, 4.9375, 4.9176, 4.9146, 4.8166, 4.5075, 1.6658, 1.6653, 1.665, 1.6648, 1.6647, 1.6647, 1.6647, 1.6647, 1.6647, 1.6647, 1.6647, 1.6647, 1.6646, 1.6646, 1.6646, 1.6646, 1.6646, 1.6646, 1.6646, 1.6645, 1.6645, 1.6645, 1.6645, 1.6645, 1.6644, 1.6643, 1.6645, 1.6642, 1.6644, 1.6642, 1.6639, 5.3972, 5.3953, 5.3949, 5.3937, 5.3931, 5.3925, 5.3919, 5.3889, 5.3876, 5.3832, 5.3737, 5.3463, 5.316, 5.3066, 5.2409, 5.2128, 5.1905, 5.0987, 4.8212, 2.0757, 2.0757, 2.0757, 2.0756, 2.0756, 2.0756, 2.0756, 2.0756, 2.0756, 2.0756, 2.0756, 2.0751, 2.0751, 2.0733, 2.0753, 2.075, 2.0752, 2.0752, 2.0745, 2.0751, 6.4504, 6.4493, 6.4409, 2.5756, 2.5748, 2.5748, 2.5747, 2.5747, 2.5747, 2.5746, 2.5746, 2.5746, 2.5746, 2.5746, 2.5746, 2.5746, 2.5746, 2.5746, 2.5746, 2.5746, 2.5746, 2.5746, 2.5746, 2.5746, 2.5746, 2.5746, 2.5746, 2.5746, 2.5746, 2.5746, 2.5746, 2.5746, 2.5745, 2.5745, 2.5741, 2.5745, 2.5734, 2.5738, 2.5735, 2.5736, 2.5708, 2.5741, 2.5743]}, \"token.table\": {\"Topic\": [7, 1, 3, 10, 17, 1, 10, 14, 9, 15, 3, 13, 10, 3, 4, 5, 12, 13, 2, 12, 17, 14, 1, 3, 16, 6, 12, 6, 15, 19, 8, 7, 3, 14, 14, 13, 19, 19, 19, 4, 7, 10, 2, 9, 15, 7, 2, 7, 9, 5, 5, 1, 8, 9, 8, 6, 6, 3, 6, 14, 5, 18, 1, 4, 19, 7, 12, 1, 4, 4, 18, 2, 19, 1, 8, 14, 4, 11, 10, 1, 10, 5, 12, 16, 1, 2, 4, 15, 3, 9, 6, 2, 11, 9, 12, 7, 2, 12, 1, 4, 10, 10, 7, 12, 1, 13, 18, 6, 14, 15, 3, 4, 6, 6, 7, 7, 9, 9, 4, 12, 10, 12, 5, 4, 9, 4, 12, 2, 4, 10, 4, 10, 4, 4, 13, 8, 3, 1, 5, 1, 3, 4, 6, 3, 14, 3, 19, 6, 18, 13, 1, 8, 19, 3, 11, 9, 10, 8, 7, 11, 14, 3, 8, 1, 4, 13, 13, 3, 17, 2, 6, 1, 9, 2, 14, 2, 6, 3, 2, 1, 8, 19, 6, 11, 14, 7, 2, 5, 7, 3, 12, 14, 10, 4, 5, 6, 1, 9, 3, 1, 3, 6, 13, 2, 1, 8, 11, 7, 11, 3, 1, 2, 3, 4, 8, 15, 4, 6, 7, 2, 8, 19, 19, 1, 10, 8, 20, 1, 5, 1, 2, 1, 15, 1, 4, 19, 8, 9, 7, 6, 1, 3, 4, 13, 8, 2, 4, 2, 10, 17, 1, 2, 3, 6, 7, 1, 5, 3, 16, 12, 7, 8, 5, 4, 13, 13, 1, 5, 4, 9, 14, 11, 4, 17, 5, 12, 10, 5, 1, 6, 14, 15, 10, 15, 15, 15, 2, 4, 7, 1, 12, 1, 13, 2, 4, 11, 2, 9, 10, 16, 12, 1, 8, 12, 2, 5, 7, 11, 1, 5, 8, 4, 6, 3, 11, 4, 2, 5, 7, 13, 8, 1, 2, 17, 7, 1, 2, 3, 17, 14, 1, 3, 2, 2, 14, 5, 9, 15, 1, 4, 19, 2, 9, 2, 12, 18, 16, 6, 2, 3, 8, 12, 11, 17, 7, 9, 12, 7, 13, 11, 2, 1, 10, 3, 5, 8, 1, 4, 14, 2, 2, 1, 3, 1, 7, 7, 11, 9, 1, 11, 3, 4, 1, 1, 9, 1, 3, 7, 10, 10, 10, 12, 6, 6, 9, 13, 3, 17, 1, 10, 14, 14, 9, 5, 7, 14, 3, 16, 8, 16, 19, 4, 13, 1, 8, 6, 9, 12, 8, 11, 1, 14, 15, 6, 8, 14, 9, 3, 1, 11, 7, 18, 2, 15, 11, 13, 16, 4, 4, 1, 7, 5, 13, 1, 2, 3, 11, 1, 7, 13, 2, 5, 6, 11, 16, 14, 3, 8, 11, 4, 1, 2, 17, 5, 7, 12, 13, 9, 9, 9, 12, 7, 9, 5, 1, 5, 5, 5, 15, 2, 11, 19, 13, 9, 6, 3, 5, 6, 12, 9, 13, 14, 3, 7, 6, 1, 10, 3, 14, 7, 1, 4, 5, 8, 1, 12, 8, 6, 4, 19, 7, 2, 7, 15, 15, 15, 11, 5, 10, 11, 11, 6, 1, 3, 1, 2, 1, 14, 4, 4, 7, 9, 9, 11, 10, 3, 1, 2, 9, 4, 1, 14, 2, 1, 1, 12, 1, 14, 5, 12, 13, 1, 13, 5, 1, 2, 3, 6, 10, 1, 8, 5, 1, 3, 7, 12, 7, 4, 10, 17, 15, 14, 11, 6, 1, 3, 14, 6, 20, 13, 12, 5, 1, 10, 11, 3, 2, 8, 8, 5, 3, 6, 13, 7, 4, 4, 13, 13, 2, 12, 2, 4, 10, 1, 3, 6, 10, 4, 4, 3, 12, 8, 15, 15, 4, 9, 5, 4, 10, 2, 11, 1, 9, 1, 4, 12, 20, 3, 1, 18, 2, 1, 2, 17, 17, 19, 3, 8, 11, 17, 5, 13, 8, 8, 5, 8, 1, 2, 3, 11, 2, 5, 1, 10, 10, 2, 14, 5, 18, 3, 14, 2, 7, 1, 2, 3, 10, 2, 10, 2, 10, 11, 5, 6, 9, 4, 17, 19, 14, 13, 1, 4, 1, 9, 6, 1, 14, 13, 15, 10, 19, 13, 12, 1, 2, 3, 13, 14, 7, 1, 5, 8, 11, 14, 4], \"Freq\": [0.9513972475250893, 0.674290965693436, 0.18129942521293713, 0.1437891993068122, 0.9943146940116214, 0.9123519944942527, 0.0867254747618111, 0.9806147715332527, 0.9940819178332814, 0.9985944936984981, 0.9992725003124728, 0.9795959624719208, 0.950718622740828, 0.6808510608546462, 0.18782098230473, 0.11543164537478197, 0.013695279959719893, 0.9920598092211147, 0.9914732582030773, 0.8142947387082586, 0.9679706437159994, 0.956577726024465, 0.9687737332063473, 0.030669721202009993, 0.9911406365814434, 0.9907666562828379, 0.9970833584709042, 0.9876889789391765, 0.9296059199877726, 0.9952096075103007, 0.9692462124397418, 0.9923440229221404, 0.7549821607200132, 0.2410867403979874, 0.9961054019955797, 0.9827532351498633, 0.9907055680532858, 0.9056444296584689, 0.3830315106200793, 0.5159155361701573, 0.47769957052792344, 0.9182494852378148, 0.9973372003159677, 0.9975679302099478, 0.9775935591854773, 0.9945463833301392, 0.932283140120502, 0.029431793218566312, 0.03772243919562725, 0.9915452131318221, 0.9786985554604344, 0.9984456730072542, 0.9971037328054518, 0.8707511541815913, 0.9850570676203794, 0.9943952606326034, 0.9799042093189879, 0.998133876485666, 0.9889209978083406, 0.9393746047591994, 0.9808080026439123, 0.9952381239983314, 0.9991103773669067, 0.9921781814078927, 0.9707467218571696, 0.9554299722220589, 0.9854349439673392, 0.522790851330006, 0.47706921185810675, 0.998205288080275, 0.6179025264065966, 0.9978383743763449, 0.8286207174434012, 0.9991251685035462, 0.9138840645519689, 0.0849290164701437, 0.9962868496987596, 0.9946388043633282, 0.9971553273090072, 0.9054989251085938, 0.09239784950087691, 0.9653311181854933, 0.9991769453099689, 0.9649830784240332, 0.8773539669716541, 0.12255103030715168, 0.9997199099386027, 0.9991933204156135, 0.9961235841959758, 0.9484394798819821, 0.9995235600097412, 0.9939453284999348, 0.9791709625452608, 0.9908505715477461, 0.9808419470813118, 0.9803193653681367, 0.9984913814520257, 0.9807410452389598, 0.4869758519355007, 0.5112236121978493, 0.9993204883746892, 0.998500962846088, 0.9878996851293446, 0.9920666581405901, 0.9992947184600635, 0.9934366407071722, 0.9952642462105634, 0.997227917974136, 0.9988928121472431, 0.9905169751534462, 0.9702173123226057, 0.029577569551423058, 0.9913270653652465, 0.9949252599574709, 0.9923913395156373, 0.9944220383236693, 0.9970160944645221, 0.9742825782193746, 0.12682817199550234, 0.8716063735010054, 0.9993813181424935, 0.9992196000862189, 0.9403985487448757, 0.9996978933644912, 0.9915724307452672, 0.0003785717343341708, 0.9994293786422108, 0.12239465233767627, 0.2564459382313217, 0.617801578466366, 0.9868912501010505, 0.9678316953680014, 0.9910784703868402, 0.9977332172391556, 0.9957368155814645, 0.9991815785007242, 0.9991289963811922, 0.9306834088138419, 0.0686080718035845, 0.27650414834805265, 0.3047600467193865, 0.4177836402047219, 0.9931357723500026, 0.9983184948649139, 0.9933293580099412, 0.995077229336835, 0.9915944283472983, 0.9916709080143117, 0.8429328329885903, 0.963790850396442, 0.9993090988697728, 0.9369248638541062, 0.9414812099974729, 0.9996304512365373, 0.9969946591035759, 0.9860874673687139, 0.9977469927160842, 0.9523361387419016, 0.9947770414090671, 0.9817962796292596, 0.9816472693705948, 0.9968055179412129, 0.947363911359399, 0.6343377920374161, 0.3648023199629096, 0.9914156503785573, 0.9953129850060539, 0.99956265419829, 0.9630551859309412, 0.9998793068428591, 0.9914216680116456, 0.8704131477912808, 0.12926927937494267, 0.9649729281084504, 0.02904600786948513, 0.9951311601004309, 0.004100815220194083, 0.9986303442533713, 0.9975085511175229, 0.9987798661690704, 0.9928451141028289, 0.9872488349908685, 0.9983651045459203, 0.9165109825720519, 0.07431170128962583, 0.9960381692487521, 0.9996910437716346, 0.9965078972055496, 0.9472050671622637, 0.9970027080193767, 0.9977135383947605, 0.9894544419241618, 0.993887807194702, 0.9986548767054494, 0.2219729902506717, 0.7779458763966408, 0.26770764313217243, 0.7308418657508307, 0.9969397595380202, 0.5685243197491013, 0.428549711369495, 0.0007841714755160019, 0.0015683429510320037, 0.9959900590686044, 0.9921859456417149, 0.006783123307681084, 0.9978029269764646, 0.9688399261910946, 0.6758864648988189, 0.9987678258625909, 0.6719022334665943, 0.32039922234069956, 0.0069455062335872664, 0.00030197853189509855, 0.996787981017248, 0.9962108904122875, 0.6648863860561238, 0.21508279717708645, 0.1178279671491865, 0.2552631506528831, 0.7338815581270388, 0.8990721469258677, 0.9865323203990156, 0.9994636023011798, 0.9904149983016342, 0.892798457741158, 0.9925175413436355, 0.8567294743974762, 0.1419383156987882, 0.8407885622432948, 0.15888241472421813, 0.9990591017774741, 0.40127824320492267, 0.7822976645044944, 0.21658907887849924, 0.9311745743719687, 0.9926569262540903, 0.9806119428502538, 0.89495571911671, 0.9860209736928364, 0.9807131193590446, 0.014587266077422861, 0.004488389562283957, 0.9939304102745586, 0.944716023251587, 0.9987565320020103, 0.9961996229151476, 0.4846944200247227, 0.5138594137797919, 0.9946634631440648, 0.8099335656430321, 0.11131788342961105, 0.07841604596273093, 0.9995237423747618, 0.9801816201197013, 0.9990685214482304, 0.9663886475981962, 0.9990824251675322, 0.9992234715044473, 0.9680314701435624, 0.9903957085705722, 0.9923630241040312, 0.9998312516744011, 0.9990473982476364, 0.9990907995845681, 0.9983073295649489, 0.9986726061782725, 0.9950147618760613, 0.8934673573737252, 0.1058610613002044, 0.9938106788788954, 0.9888410450653791, 0.9867895872019851, 0.9948874962098329, 0.8576018408578875, 0.114346912114385, 0.9897420203432636, 0.9422276753417751, 0.89569720982416, 0.10384895186367073, 0.9955713812283425, 0.9994333947157827, 0.2073317255914827, 0.7910502761028878, 0.9216384799878575, 0.6354968785561824, 0.9996855659655048, 0.5421742107597851, 0.45676320495516143, 0.9983342302422902, 0.9972340550567707, 0.9986964204906734, 0.9868483087596887, 0.36959026391916056, 0.35324783728327935, 0.2765641430687596, 0.9971799203375404, 0.9901442890556895, 0.9694574160400001, 0.9940649005838452, 0.9698904967863479, 0.9983454232900558, 0.9782657706720715, 0.9954903268731883, 0.2087088418283488, 0.2532481184795485, 0.00036809319546446, 0.5374160653781116, 0.7640880532158268, 0.17015238404074345, 0.06510525492638776, 0.9528092722014374, 0.9988506426650463, 0.9994794836365655, 0.9958167139883227, 0.9976346558117575, 0.717182223343986, 0.2827412290655353, 0.980350964052039, 0.9868045221322397, 0.978860548748104, 0.9720919295913577, 0.026632655605242677, 0.9966413531024911, 0.9729107515684453, 0.9949403451809616, 0.004671081432774468, 0.9983101601298842, 0.897001200870063, 0.9993736464023545, 0.8666369244438888, 0.1326036193311534, 0.9926880169918806, 0.9981052779744307, 0.9804441513668161, 0.9772512841689672, 0.9990465291125847, 0.9922048453456579, 0.9979388612809715, 0.9911358370179829, 0.9854677872581225, 0.9918518834733177, 0.9938291274887663, 0.9941078274172759, 0.9532319021067537, 0.997329021669504, 0.991069814819821, 0.9977297380932517, 0.9992606907374373, 0.7416627621463713, 0.25746842001972475, 0.9882641029655853, 0.9769876588143093, 0.9920793895402281, 0.9669692924490557, 0.9933522816325774, 0.9979392137575805, 0.3404958156283865, 0.6553904421117815, 0.9992056603211168, 0.9971976801997227, 0.9992342483552001, 0.9971369152250766, 0.4742952070521337, 0.011615392825766539, 0.5139811325401694, 0.9782642997512552, 0.021266615211983808, 0.9962606546022861, 0.9975662086284115, 0.99621880263596, 0.01120865987276149, 0.9883400676040868, 0.8002971036010071, 0.19889040444522071, 0.9692514570275659, 0.9981994423906284, 0.9559576832022803, 0.6730297264614585, 0.3257750271701741, 0.9967669195739851, 0.998333340206707, 0.9995913392825482, 0.5008113777552753, 0.4921766988284602, 0.05668406429812293, 0.942035163811662, 0.9684582985073431, 0.9955574293765764, 0.9876696714355647, 0.9321344304195718, 0.9635623261455883, 0.9981912472542789, 0.995655094222745, 0.9976873777211372, 0.9762796783778022, 0.9988617786937667, 0.9896821294159356, 0.9994445860045972, 0.9367065946712658, 0.9975957740659188, 0.9943433436188569, 0.9948026391013067, 0.7979835875978231, 0.20193395640859813, 0.9700300576413636, 0.9986820770058409, 0.9969539926657949, 0.9989998157293645, 0.9980423511582482, 0.9932110329003538, 0.47174973681609667, 0.5265064026965365, 0.6406846712777804, 0.358783415915557, 0.9907004766457467, 0.7902893651048127, 0.989059775530125, 0.9832403645516923, 0.9871279497524926, 0.6881153247393396, 0.30153368162735106, 0.9942740049466218, 0.993462958900582, 0.9803427495343158, 0.9966753868743204, 0.9630183228604623, 0.9967578801537035, 0.9992721609468844, 0.9677913445538167, 0.9655734564966886, 0.9816097534900603, 0.9942360871938313, 0.5957972306109542, 0.9846757552087023, 0.9973813581858164, 0.9749344306965044, 0.9743643102952804, 0.9709164709910951, 0.6743671796894314, 0.3244596807939717, 0.9642389309656082, 0.9884332858447548, 0.8700215505183813, 0.11555776202280742, 0.013866931442736891, 0.9996438017008207, 0.6284572165191488, 0.3686299435396557, 0.9944680603960642, 0.9705496841747131, 0.028163272085426943, 0.9988211336251251, 0.9669846472542943, 0.9957968024499316, 0.9959424477275813, 0.9983641724306481, 0.99155529425986, 0.9985463441870512, 0.9994294239512449, 0.9992635094491941, 0.9910549900819429, 0.9467933889084392, 0.9736589464498288, 0.41711435888871756, 0.5812249263203441, 0.9915121080059633, 0.9989343149267912, 0.942222978853411, 0.9945418400068293, 0.9962851789333448, 0.9848133045548936, 0.8082025188438683, 0.9853461839278685, 0.2620027511844576, 0.7375795535339051, 0.9772293188045712, 0.9920310043575239, 0.9844864422148628, 0.9958924698313155, 0.989954112389512, 0.9820655364423398, 0.9755561971373587, 0.9845235104393476, 0.990670334099984, 0.9981745457733389, 0.9825140304614607, 0.8100132842774853, 0.17550287826012181, 0.9986330434441907, 0.9914657644294649, 0.9954641317721858, 0.9986334050426497, 0.9931362233533388, 0.9989699689842332, 0.9369624405259452, 0.06027243769465145, 0.9993232752972124, 0.9991816635022318, 0.9931652440662302, 0.7530931971943305, 0.013177910773977267, 0.22402448315761356, 0.009517380003428026, 0.5737888023168912, 0.4188658256913306, 0.9995975615556567, 0.9790263501902053, 0.9997248182338433, 0.6824227689681115, 0.9998730090395647, 0.7896772850265988, 0.20669131163265747, 0.003473803556851386, 0.6921236129606039, 0.9714483431894508, 0.9951015751830602, 0.9998958319631509, 0.9700604589616836, 0.9971779829213885, 0.9883612540276272, 0.9997019051470708, 0.9983245286352777, 0.9971443338332794, 0.8891358821480655, 0.11004446730996466, 0.9985940719219686, 0.9578461762222205, 0.975737809818301, 0.9995352378638706, 0.9995685462347134, 0.9983250021414368, 0.9665819816741018, 0.947266236140818, 0.9988585590646215, 0.9987629486814005, 0.6367753461446674, 0.05073447755611654, 0.3121899954163308, 0.9826704858383114, 0.7551588478369942, 0.24027781522086178, 0.9932265573362078, 0.999115085824896, 0.9987897503364307, 0.982544301724407, 0.9990929595470017, 0.9906032784670614, 0.9947361465169035, 0.9916932394375813, 0.9973975954352151, 0.9985757586202865, 0.991260879486286, 0.9705628461442044, 0.5354041471291257, 0.07452091983595968, 0.15477421812083933, 0.12611232587623947, 0.10776871483969554, 0.7212460041836137, 0.27422374117397813, 0.9548311119039391, 0.23209975311310294, 0.7248992289172267, 0.04179995553695692, 0.9874587854877751, 0.9295609485421868, 0.9161123428396898, 0.08340140816748243, 0.9675149992820354, 0.9928925097724778, 0.9609864570719686, 0.9942930403438636, 0.9756673521096793, 0.7221601971363294, 0.2010696869953158, 0.0760804221063357, 0.9970038529351802, 0.9893623065093115, 0.9944270406503419, 0.9861729425580265, 0.9752314350627457, 0.9210579957380137, 0.07767959000200116, 0.9880678520791536, 0.9978326459530674, 0.9960345892193095, 0.9769325348505381, 0.985703808368781, 0.9740808448845157, 0.7103362135764532, 0.12895334338772532, 0.15736679193078346, 0.9909633773925764, 0.9977123751314315, 0.987663125478903, 0.9980830242985524, 0.995338006679778, 0.9275663931873855, 0.07077815851630652, 0.4983788869485277, 0.4983788869485277, 0.995859614597914, 0.30467676222263035, 0.4893870493201, 0.08569033937511479, 0.1199664751251607, 0.9997615890657385, 0.9852000069483678, 0.8887017772985999, 0.11108772216232499, 0.9994965703736227, 0.9706994101026976, 0.8827549474870501, 0.9957366236573016, 0.9985467326141491, 0.8388458823434959, 0.6297829035474893, 0.36999378234544517, 0.9995979364976736, 0.990217565833529, 0.39314673775731757, 0.6057202645679602, 0.9983555722869355, 0.0012152837155044863, 0.9630867347097939, 0.978680628195394, 0.9967882582405366, 0.9982037712632983, 0.9993498515526513, 0.9943229181308076, 0.5762584939773349, 0.3916280378970879, 0.03172065104462394, 0.9952680800541286, 0.9936634154811599, 0.9981406548663362, 0.9947789066955864, 0.9973531969955189, 0.9772618190472064, 0.954926012255485, 0.9667005908387155, 0.9989221700313399, 0.9395146037775284, 0.9752883790742338, 0.9994104831030393, 0.8038468551758514, 0.17904854282923505, 0.016844288001356608, 0.9312323931674436, 0.9958328489351661, 0.9986813564550046, 0.9995556971282794, 0.9612884230956261, 0.8470354063163406, 0.9408042505347499, 0.05588936141790594, 0.9348081986660285, 0.9612280429214596, 0.9989620406487244, 0.9802823174907296, 0.9947861787047811, 0.992418522935456, 0.9929324513144289, 0.0005851535624689084, 0.0013897397108636573, 0.005046949476294335, 0.9284071032143163, 0.07141593101648587, 0.2850046725108899, 0.7125116812772248, 0.9973866323253453, 0.9859928543621862, 0.9925842404574919, 0.999680650260141, 0.9984530479261075, 0.9994247129386385, 0.8699810115467145, 0.9094940419838097, 0.9995815481692873, 0.3872035358703628, 0.6122655910950112, 0.9996530347923355, 0.9688377950197247, 0.9965121097340042, 0.9995210482231596, 0.9933060244736358, 0.9933753235853945, 0.9961426694346244, 0.9618349062764759, 0.9887004737681989, 0.998412332827578, 0.9820443588963353, 0.8026192141314913, 0.16330443180019344, 0.033984976347607825, 0.9860746980121184, 0.9852971159626542, 0.9997171977543488, 0.8997863735056598, 0.04128574453332312, 0.05828575698822088, 0.9226758049666958, 0.07097506192051506, 0.9899064582916698], \"Term\": [\"ability\", \"able\", \"able\", \"able\", \"accept\", \"access\", \"access\", \"accomplish\", \"account\", \"action\", \"add\", \"addition\", \"administrator\", \"advance\", \"advance\", \"advance\", \"advance\", \"advice\", \"aggregate\", \"algorithm\", \"almost\", \"alpha\", \"also\", \"also\", \"amount\", \"amp\", \"analysis\", \"analyze\", \"annoying\", \"answer\", \"anyone_know\", \"api\", \"app\", \"app\", \"application\", \"article\", \"ask\", \"association\", \"association_rule\", \"assume\", \"assume\", \"assumption\", \"attempt\", \"attribute\", \"automatic\", \"automatically\", \"base\", \"base\", \"base\", \"basic\", \"bat\", \"batch\", \"benchmark\", \"binomial\", \"boolean\", \"bteq\", \"buffer\", \"build\", \"building\", \"bundle\", \"byte\", \"calculate\", \"call\", \"cant_find\", \"cant_seem\", \"capacity\", \"carry\", \"case\", \"case\", \"cast\", \"catalog\", \"cause\", \"chance\", \"check\", \"class\", \"class\", \"clause\", \"cli\", \"client\", \"close\", \"close\", \"clue\", \"cluster\", \"coalesce\", \"code\", \"code\", \"column\", \"com\", \"combine\", \"comma\", \"command\", \"common\", \"communicate\", \"company\", \"comparison\", \"compile\", \"complete\", \"computation\", \"condition\", \"condition\", \"connect\", \"connection\", \"constant\", \"consume\", \"contain\", \"control\", \"convert\", \"copy\", \"core\", \"country\", \"create\", \"create\", \"creation\", \"csv\", \"current_date\", \"custom\", \"customer\", \"customerid\", \"data\", \"data\", \"database\", \"dataset\", \"datatype\", \"date\", \"datetime\", \"datum\", \"datum\", \"db\", \"db\", \"db\", \"dbc\", \"dbcname\", \"dd\", \"decimal\", \"declare\", \"def\", \"default\", \"define\", \"define\", \"delete\", \"delete\", \"delete\", \"depend\", \"dependency\", \"deprecate\", \"detail\", \"detect\", \"determine\", \"development\", \"didnt_work\", \"different\", \"difficulty\", \"dimension\", \"directory\", \"document\", \"dot\", \"driver\", \"dual\", \"dynamic\", \"dynamically\", \"eclipse\", \"element\", \"eliminate\", \"end\", \"end\", \"engine\", \"enter\", \"environment\", \"equal\", \"error\", \"exactly\", \"example\", \"example\", \"exception\", \"exception\", \"execute\", \"execute\", \"exist\", \"exit\", \"expect\", \"expected_somethe\", \"explain\", \"export\", \"extension\", \"extension\", \"extract\", \"fail\", \"failure\", \"fairly\", \"far\", \"fast\", \"fatal\", \"fetch\", \"field\", \"file\", \"file\", \"filter\", \"filter\", \"final\", \"find\", \"find\", \"find\", \"find\", \"fine\", \"first\", \"first\", \"flag\", \"float\", \"fly\", \"folder\", \"follow\", \"follow\", \"follow\", \"follow\", \"foo\", \"force\", \"format\", \"format\", \"format\", \"forward\", \"forward\", \"fp_growth\", \"frame\", \"function\", \"functionality\", \"gain\", \"galaxy\", \"generate\", \"generate\", \"get\", \"get\", \"give\", \"greatly\", \"group\", \"group\", \"grow\", \"guy\", \"hadoop\", \"half\", \"hard\", \"help\", \"help\", \"help\", \"here\", \"highly_appreciate\", \"home\", \"hope\", \"host\", \"host\", \"hour\", \"however\", \"however\", \"however\", \"import\", \"important\", \"include\", \"incorrect\", \"index\", \"info\", \"inherit\", \"initial\", \"initialize\", \"input\", \"insert\", \"instal\", \"install\", \"instead\", \"int\", \"integer\", \"integer\", \"integrate\", \"integration\", \"interpret\", \"interval\", \"introduce\", \"introduce\", \"ip\", \"ip_address\", \"issue\", \"issue\", \"jar\", \"java\", \"jdbc\", \"jdbc\", \"jdbc__statemachine\", \"jdbc__tdsession\", \"job\", \"join\", \"join\", \"key\", \"knime\", \"know\", \"language\", \"last\", \"last\", \"last\", \"late\", \"letter\", \"libname\", \"library\", \"light\", \"like\", \"likely\", \"limit\", \"line\", \"line\", \"line\", \"line\", \"list\", \"list\", \"list\", \"literal\", \"load\", \"local\", \"location\", \"lock\", \"log\", \"log\", \"logic\", \"login\", \"logoff\", \"look\", \"look\", \"loop\", \"luck\", \"m\", \"m\", \"machine\", \"macro\", \"main\", \"make\", \"make\", \"manage\", \"manager\", \"manifest\", \"manual\", \"map\", \"mark\", \"match\", \"matching\", \"matrix\", \"max\", \"maximum\", \"maybe\", \"measure\", \"memory\", \"mention\", \"merge\", \"message\", \"method\", \"method\", \"million\", \"minor\", \"minute\", \"mismatch\", \"mistake\", \"model\", \"modify\", \"modify\", \"module\", \"monetdb\", \"multiple\", \"mysql\", \"name\", \"name\", \"name\", \"need\", \"need\", \"net\", \"network\", \"never\", \"new\", \"new\", \"next\", \"next\", \"nice\", \"node\", \"nominal\", \"none\", \"none\", \"normal\", \"null\", \"number\", \"numeric\", \"numeric\", \"object\", \"object\", \"obvious\", \"odbc\", \"odbc_driver\", \"odbc_ini\", \"offset\", \"open\", \"operation\", \"operator\", \"optimizer\", \"option\", \"oracle\", \"order\", \"oreplace\", \"org\", \"org_eclipse\", \"other\", \"output\", \"output\", \"overflow\", \"package\", \"parallel\", \"parameter\", \"part\", \"partial\", \"particular\", \"particular\", \"pass\", \"pass\", \"password\", \"payment\", \"performance\", \"persist\", \"pipe\", \"platform\", \"platform\", \"play\", \"please_help\", \"please_let\", \"plugin\", \"populated\", \"port\", \"possible\", \"possibly\", \"precision\", \"prefer\", \"prepare\", \"preparerequest\", \"pretty\", \"previous\", \"price\", \"primary\", \"primary_key\", \"print\", \"print\", \"println\", \"private\", \"problem\", \"problem\", \"problem\", \"process\", \"produce\", \"produce\", \"product\", \"program\", \"program\", \"project\", \"prompt\", \"properly\", \"property\", \"provide\", \"provider\", \"python\", \"query\", \"question\", \"queue\", \"quick\", \"quiet\", \"quite\", \"quite\", \"random\", \"range\", \"rapid_miner\", \"rapidminer\", \"raw\", \"rd\", \"rdbms\", \"reach\", \"read\", \"read\", \"reader\", \"ready\", \"realize\", \"receive\", \"recent_call\", \"recognize\", \"recommend\", \"reduce\", \"refer\", \"reference\", \"reflect\", \"regard\", \"regard\", \"regular\", \"release\", \"relevant\", \"remove\", \"replicate\", \"report\", \"request\", \"request\", \"require\", \"resource\", \"rest\", \"result\", \"result\", \"result\", \"result\", \"retrieve\", \"retrieve\", \"return\", \"route\", \"row\", \"rowname\", \"rule\", \"run\", \"run\", \"run\", \"runbody_java\", \"runnable\", \"runtime\", \"sample\", \"sas\", \"scale\", \"score\", \"script\", \"second\", \"section\", \"see\", \"see\", \"seem\", \"seemingly\", \"sel\", \"select\", \"self\", \"separate\", \"sequential\", \"series\", \"server\", \"service\", \"set\", \"set\", \"set\", \"set_unicode\", \"setting\", \"setting\", \"setup\", \"show\", \"similar\", \"simulation\", \"single\", \"singleton\", \"site\", \"slow\", \"software\", \"solution\", \"somehow\", \"sorry\", \"source\", \"source\", \"source\", \"source\", \"source\", \"space\", \"space\", \"special_character\", \"specify\", \"specify\", \"specify\", \"speed\", \"spl\", \"sql\", \"sql\", \"ss\", \"stack\", \"stack_trace\", \"stage\", \"standard\", \"start\", \"start\", \"start\", \"status\", \"stop\", \"stored_procedure\", \"straightforward\", \"stream\", \"string\", \"string\", \"strip\", \"structure\", \"sub\", \"substr\", \"substre\", \"successful\", \"successfully\", \"successfully\", \"successfully\", \"suggest\", \"sum\", \"supply\", \"support\", \"suppose\", \"sure\", \"sure\", \"syntax\", \"syntax\", \"sys\", \"system\", \"system\", \"system\", \"system\", \"table\", \"table_name\", \"take\", \"take\", \"task\", \"tdnetworkioif\", \"tdsession\", \"temp\", \"template\", \"tera\", \"teradata\", \"teradata\", \"test\", \"testing\", \"text\", \"text\", \"thank\", \"thank\", \"thanks_lot\", \"therefore\", \"thing\", \"think\", \"thread\", \"throw\", \"time\", \"time\", \"time\", \"timestamp\", \"title\", \"tool\", \"top\", \"traceback\", \"translate\", \"transport\", \"treat\", \"trim\", \"trim_leading\", \"troubleshoot\", \"true\", \"try\", \"try\", \"try\", \"tune\", \"tutorial\", \"txt\", \"type\", \"uid\", \"uid_pwd\", \"unable\", \"unable\", \"unicode\", \"unit\", \"update\", \"upgrade\", \"url\", \"usage\", \"use\", \"use\", \"use\", \"use\", \"user\", \"user\", \"username\", \"username\", \"usr_local\", \"utf\", \"utility\", \"value\", \"varchar\", \"variable\", \"vector\", \"vendor\", \"version\", \"view\", \"view\", \"want\", \"warehouse\", \"warning\", \"way\", \"web\", \"week\", \"weight\", \"white\", \"whole\", \"window\", \"wish\", \"work\", \"work\", \"work\", \"workaround\", \"workbench\", \"workflow\", \"write\", \"write\", \"write\", \"xml\", \"xml\", \"yyyy_mm\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [12, 5, 15, 3, 18, 2, 9, 19, 14, 11, 1, 10, 8, 17, 4, 16, 20, 6, 13, 7]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el136841398273506895368227318670\", ldavis_el136841398273506895368227318670_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el136841398273506895368227318670\", ldavis_el136841398273506895368227318670_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el136841398273506895368227318670\", ldavis_el136841398273506895368227318670_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "11     0.261889  0.203259       1        1  32.028058\n",
       "4      0.231762  0.103536       2        1  10.720898\n",
       "14     0.157975  0.178567       3        1   9.975133\n",
       "2      0.085826  0.124792       4        1   7.429707\n",
       "17     0.243516 -0.285999       5        1   6.898692\n",
       "1      0.159223 -0.267888       6        1   6.080242\n",
       "8      0.173097 -0.124605       7        1   4.563010\n",
       "18     0.032725  0.068190       8        1   3.437431\n",
       "13    -0.040625  0.034261       9        1   2.968433\n",
       "10    -0.015850  0.112308      10        1   2.866678\n",
       "0      0.005868 -0.033696      11        1   2.493841\n",
       "9     -0.044006  0.001133      12        1   2.412897\n",
       "7     -0.075330 -0.015445      13        1   2.008157\n",
       "16    -0.103661  0.001504      14        1   1.870215\n",
       "3     -0.163973 -0.017500      15        1   1.384398\n",
       "15    -0.155555 -0.019533      16        1   0.775989\n",
       "19    -0.159146 -0.008794      17        1   0.767852\n",
       "5     -0.166144 -0.019595      18        1   0.711485\n",
       "12    -0.200888 -0.018652      19        1   0.450765\n",
       "6     -0.226703 -0.015844      20        1   0.156118, topic_info=           Term          Freq         Total Category  logprob  loglift\n",
       "18         file  17781.000000  17781.000000  Default  30.0000  30.0000\n",
       "1093       rule  10900.000000  10900.000000  Default  29.0000  29.0000\n",
       "151       input   9624.000000   9624.000000  Default  28.0000  28.0000\n",
       "202      output  11176.000000  11176.000000  Default  27.0000  27.0000\n",
       "9         datum   5283.000000   5283.000000  Default  26.0000  26.0000\n",
       "...         ...           ...           ...      ...      ...      ...\n",
       "1791      param      0.024451      1.194495  Topic20 -10.4766   2.5735\n",
       "1688       path      0.024451      1.194377  Topic20 -10.4767   2.5736\n",
       "2910  snakemake      0.024460      1.198224  Topic20 -10.4763   2.5708\n",
       "9965        bam      0.024448      1.193572  Topic20 -10.4768   2.5741\n",
       "8119     docker      0.024448      1.193385  Topic20 -10.4768   2.5743\n",
       "\n",
       "[804 rows x 6 columns], token_table=      Topic      Freq     Term\n",
       "term                          \n",
       "1084      7  0.951397  ability\n",
       "87        1  0.674291     able\n",
       "87        3  0.181299     able\n",
       "87       10  0.143789     able\n",
       "787      17  0.994315   accept\n",
       "...     ...       ...      ...\n",
       "175       5  0.041286    write\n",
       "175       8  0.058286    write\n",
       "699      11  0.922676      xml\n",
       "699      14  0.070975      xml\n",
       "612       4  0.989906  yyyy_mm\n",
       "\n",
       "[705 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[12, 5, 15, 3, 18, 2, 9, 19, 14, 11, 1, 10, 8, 17, 4, 16, 20, 6, 13, 7])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afbf531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=15, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a955294b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -11.065073129111033\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91ad8bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.42665830974002067\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5106334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el168631405316247652322282582577\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el168631405316247652322282582577_data = {\"mdsDat\": {\"x\": [-0.27715834881813045, -0.276117967751839, -0.19294167780422988, -0.13839376253043809, -0.04363182528278679, -0.01826202444022538, -0.05089275386886668, -0.03896304100426812, 0.07347781163694889, 0.05781423005250214, 0.12051523213072089, 0.14647222338850688, 0.16509965036723157, 0.23584516555944948, 0.2371370883654232], \"y\": [0.13955761129542593, 0.045182194195978555, -0.23479191968413965, 0.1898104856113358, -0.28609038206715265, -0.14024948078605767, 0.08348435868681779, 0.09810369069015035, 0.03702609409007695, -0.038168569253052016, -0.02848680307858526, 0.039965441998235415, 0.002387208550597055, 0.04607955843052528, 0.046190511319844856], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [27.329699422295473, 19.661416094688875, 11.601035598434402, 8.897018219766476, 5.9252226057040165, 5.006788304541331, 4.54923907996738, 4.175831986488592, 3.261934772246993, 3.115456105844456, 2.3591820971539113, 1.9719497551553051, 1.5489052474145049, 0.3041180727330699, 0.2922026375652189]}, \"tinfo\": {\"Term\": [\"file\", \"snakemake\", \"rule\", \"output\", \"sample\", \"input\", \"quot\", \"process\", \"job\", \"table\", \"name\", \"teradata\", \"use\", \"path\", \"run\", \"workflow\", \"error\", \"script\", \"column\", \"value\", \"info\", \"thread\", \"nextflow\", \"true\", \"try\", \"select\", \"date\", \"datum\", \"query\", \"param\", \"test\", \"find\", \"directory\", \"call\", \"add\", \"new\", \"variable\", \"environment\", \"option\", \"require\", \"folder\", \"exist\", \"project\", \"specify\", \"ve\", \"reference\", \"provide\", \"fix\", \"documentation\", \"keep\", \"specific\", \"available\", \"program\", \"structure\", \"information\", \"loop\", \"node\", \"modify\", \"final\", \"combine\", \"try\", \"m\", \"fail\", \"follow\", \"get\", \"error\", \"use\", \"create\", \"work\", \"change\", \"also\", \"command\", \"instal\", \"make\", \"possible\", \"however\", \"user\", \"name\", \"execute\", \"see\", \"help\", \"list\", \"set\", \"want\", \"need\", \"way\", \"self\", \"return\", \"task\", \"step\", \"pass\", \"check\", \"complete\", \"print\", \"trim\", \"do\", \"python\", \"batch\", \"next\", \"long\", \"collection\", \"else\", \"target\", \"parse\", \"empty\", \"luigi\", \"size\", \"even\", \"_\", \"mean\", \"save\", \"note\", \"request\", \"understand\", \"much\", \"looks_like\", \"write\", \"single\", \"time\", \"import\", \"datum\", \"run\", \"message\", \"start\", \"first\", \"string\", \"multiple\", \"number\", \"line\", \"expect\", \"open\", \"result\", \"access\", \"way\", \"want\", \"code\", \"base\", \"use\", \"snakemake\", \"rule\", \"output\", \"input\", \"param\", \"wildcard\", \"expand\", \"pipeline\", \"fastq_gz\", \"fastq\", \"snakefile\", \"bam\", \"def\", \"cluster\", \"false\", \"config\", \"shell\", \"genome\", \"submit\", \"fastqc\", \"analysis\", \"filename\", \"prefix\", \"vcf\", \"bed\", \"touch\", \"wait\", \"os_path\", \"pair\", \"checkpoint\", \"log\", \"read\", \"run\", \"result\", \"line\", \"produce\", \"table\", \"column\", \"query\", \"row\", \"sql\", \"count\", \"insert\", \"varchar\", \"processing\", \"field\", \"network\", \"null\", \"limit\", \"decimal\", \"move\", \"interval\", \"hour\", \"drop\", \"temp\", \"maximum\", \"queue\", \"somehow\", \"integer\", \"performance\", \"clause\", \"strip\", \"matching\", \"equivalent\", \"table_name\", \"dbc\", \"select\", \"teradata\", \"minute\", \"view\", \"value\", \"join\", \"remove\", \"datum\", \"data\", \"database\", \"result\", \"need\", \"number\", \"want\", \"create\", \"file\", \"sample\", \"script\", \"site_package\", \"txt\", \"lib_python\", \"merge\", \"failure\", \"warning\", \"operation\", \"determine\", \"csv\", \"amp\", \"bteq\", \"utility\", \"reach\", \"analyze\", \"buffer\", \"building\", \"quiet\", \"recommend\", \"incorrect\", \"sorry\", \"mclient\", \"instruction\", \"delimiter\", \"price\", \"evaluation\", \"google\", \"panel\", \"copy\", \"miss\", \"split\", \"part\", \"dont_know\", \"import\", \"command\", \"contain\", \"extension\", \"define\", \"info\", \"cause\", \"path\", \"conflict\", \"resource\", \"core\", \"package\", \"ansible\", \"install\", \"class\", \"download\", \"execution\", \"argument\", \"dependency\", \"software\", \"ansible_galaxy\", \"configuration\", \"enable\", \"https\", \"plugin\", \"org\", \"debug\", \"occur\", \"api\", \"title\", \"property\", \"detect\", \"xml\", \"public\", \"lib\", \"release\", \"jar\", \"module\", \"main\", \"base\", \"version\", \"local\", \"miss\", \"container\", \"connect\", \"server\", \"docker\", \"image\", \"session\", \"connection\", \"host\", \"driver\", \"mode\", \"manager\", \"port\", \"deploy\", \"client\", \"username\", \"teradatasql\", \"swarm\", \"odbc\", \"common\", \"stop\", \"galaxy\", \"relevant\", \"net\", \"random\", \"enter\", \"provider\", \"week\", \"therefore\", \"engine\", \"hostname\", \"database\", \"machine\", \"teradata\", \"window\", \"late\", \"version\", \"user\", \"local\", \"run\", \"error\", \"quot\", \"nextflow\", \"channel\", \"role\", \"model\", \"scale\", \"range\", \"append\", \"attribute\", \"regular\", \"operator\", \"context\", \"label\", \"gt\", \"document\", \"properly\", \"mention\", \"control\", \"rapidminer\", \"component\", \"datetime\", \"course\", \"process\", \"hand\", \"width\", \"validation\", \"training\", \"warehouse\", \"dimension\", \"customerid\", \"value\", \"set\", \"retrieve\", \"filter\", \"example\", \"datum\", \"result\", \"list\", \"read\", \"date\", \"group\", \"partition\", \"cast\", \"html\", \"item\", \"sum\", \"month\", \"day\", \"correctly\", \"never\", \"min\", \"expression\", \"calculate\", \"aggregate\", \"maybe\", \"max\", \"top\", \"forward\", \"transaction\", \"expected_somethe\", \"reflect\", \"else_end\", \"language\", \"supply\", \"period\", \"third\", \"advice\", \"dd\", \"vary\", \"case\", \"order\", \"part\", \"end\", \"format\", \"statement\", \"select\", \"store\", \"last\", \"job\", \"workflow\", \"service\", \"template\", \"status\", \"current\", \"timestamp\", \"pull\", \"extract\", \"replicate\", \"accept\", \"tag\", \"automatically\", \"custom\", \"basic\", \"encounter\", \"current_date\", \"suggest\", \"pop\", \"logic\", \"constant\", \"refer\", \"important\", \"compile\", \"conversion\", \"please_let\", \"float\", \"ss\", \"country\", \"mismatch\", \"type\", \"form\", \"parameter\", \"level\", \"kind\", \"assume\", \"convert\", \"sure\", \"time\", \"quite\", \"com\", \"java\", \"worker\", \"spark\", \"action\", \"jdbc\", \"event\", \"org_apache\", \"site\", \"internal\", \"repository\", \"weight\", \"stack\", \"play\", \"mark\", \"scala\", \"unknown_source\", \"connector\", \"area\", \"errorfactory_errorfactory\", \"hadoop_mapre\", \"automatic\", \"transport\", \"avail\", \"ip_address\", \"jdbc_teradriver\", \"sqoop\", \"sun_reflect\", \"stack_trace\", \"tdnetworkioif\", \"info\", \"url\", \"cause\", \"run\", \"hive\", \"main\", \"error\", \"thread\", \"true\", \"none\", \"monetdb\", \"memory\", \"usr_local\", \"flag\", \"condition\", \"location\", \"traceback\", \"runtime\", \"stage\", \"cli\", \"accord\", \"necessary\", \"substre\", \"dynamically\", \"score\", \"tab\", \"consume\", \"proceed\", \"pretty\", \"end_date\", \"recent_call\", \"cleanup\", \"communicate\", \"standard\", \"improve\", \"accomplish\", \"huge\", \"stand\", \"name\", \"bit\", \"update\", \"home\", \"map\", \"record\", \"previous\", \"share\", \"perl\", \"usr_bin\", \"place\", \"mysql\", \"sys\", \"account\", \"fetch\", \"usage\", \"ip\", \"mistake\", \"functionality\", \"realize\", \"thread\", \"elegant\", \"capacity\", \"sequential\", \"white\", \"populated\", \"oreplace\", \"administrator\", \"dbi\", \"suppose\", \"normal\", \"remove\", \"guess\", \"ask\", \"partial\", \"sub\", \"explain\", \"frame\", \"integration\", \"recognize\", \"other\", \"internet\", \"company\", \"constraint\", \"rowname\", \"pool\", \"fasttext\", \"saledate\", \"recipe\", \"wazuh_indexer\", \"bold\", \"newsaledate\", \"pickle\", \"multiprocesse\", \"se\", \"outs_be\", \"volume_wazuh\", \"metabolome\", \"df_input\", \"num_date\", \"totalsaledate\", \"vpc\", \"gcp\", \"predict\", \"root_ca\", \"pem_clustervolume\", \"filebeat\", \"pem_usr\", \"nginx\", \"metaflow\", \"upstream\", \"yml\", \"full\", \"directive\", \"parallel\", \"library\", \"customer\", \"reduce\", \"hadoop\", \"hive\", \"sale\", \"item_cust\", \"family\", \"file_b\", \"adult\", \"unstable\", \"cust\", \"invoice_dt\", \"element_text_size\", \"buy\", \"percontig\", \"polya_estimate\", \"purchase\", \"family_size\", \"lifestage\", \"lifestyle\", \"total_qty\", \"cdirs_user\", \"pertoldi_legend\", \"cust_nm\", \"hjust_axis\", \"gender\", \"aminoacid\", \"p__firmicute\", \"sbatch\", \"c__clostridia\", \"active\", \"stable\", \"local_easybuild\", \"schedule\", \"profile\", \"maptask\", \"tuple\", \"easyblock_py\", \"environment_variable\", \"dataframe\", \"libjpeg_turbo\", \"root\"], \"Freq\": [14097.0, 8803.0, 8613.0, 8603.0, 5699.0, 7358.0, 4439.0, 4376.0, 3932.0, 5498.0, 4529.0, 6184.0, 11655.0, 3383.0, 9340.0, 2773.0, 8413.0, 3207.0, 3386.0, 3745.0, 2176.0, 1913.0, 2285.0, 1818.0, 5569.0, 3019.0, 1786.0, 5300.0, 2414.0, 2652.0, 2183.2335225745173, 2184.7899297572912, 2107.705133885974, 1451.866261065894, 1388.228370686459, 1315.4316498520986, 1280.8359922204916, 1252.325667461252, 1108.0618543367254, 984.1814596427388, 967.3449666043282, 944.8785365253112, 798.9136361136964, 797.7146863733994, 784.8968494093665, 576.5344465507583, 559.9092105072834, 546.4508847627882, 531.7075861367945, 492.89184463162144, 454.49670784808137, 449.0189112006482, 441.768500420812, 421.86434367501766, 412.9491924182679, 412.5864668097245, 396.39781677017737, 389.1866874165125, 369.1273142342716, 362.4511689124685, 5505.66036668225, 2836.0185562035354, 1962.7459935543502, 2838.213507541714, 4834.236632224718, 7922.623678499078, 10682.647136000154, 4477.060064319533, 3658.3474579972694, 1335.38732148352, 1644.2277232382794, 2385.8920219266774, 1492.7231871023873, 1282.121849022471, 813.7140713595757, 1420.8789789782484, 2091.830063476232, 3344.1930677385976, 1785.5569910798463, 1323.0555700703453, 1363.1287061380126, 1690.720384833734, 1843.6227881749453, 1604.277628901554, 1469.1997295554875, 1295.617122643068, 2731.9707269102933, 2470.6573899832465, 2286.890843785617, 1851.7270293288582, 1226.9155018654035, 1067.0637132721165, 993.0106426937249, 964.3835649544084, 958.5554072955163, 950.9167989299043, 822.8130145105706, 687.6281843800491, 615.8629012224708, 606.1409111609979, 598.5968239096123, 585.4886073001067, 566.384506382837, 538.8794726130442, 530.809465405323, 501.0396894594665, 497.8789298534417, 496.88533895895637, 495.1790751424063, 487.6384472213584, 487.08144416448874, 486.16716350524496, 483.5124835786191, 474.040124899013, 438.87201308871977, 431.84972512076746, 1426.1102543889951, 794.3552913402848, 1992.1528466659574, 2034.2132163215106, 4030.491009180108, 6507.701184782978, 1068.6835955752347, 1270.2875681889925, 1243.0525656755578, 930.2826056807809, 938.5154049043923, 1299.576802452943, 1748.3899134820228, 787.5122780798317, 743.8824916460862, 1838.6743654947752, 809.6204004153268, 1127.3834458999836, 1035.6060893565075, 942.7207296548781, 900.3141217305283, 912.2149303071776, 8802.186653813304, 8612.620412296777, 8602.39547314711, 7357.701479637986, 2651.6064785333674, 2186.8015884888405, 1492.1749819575468, 1479.4815772844177, 1380.996538679013, 1092.9583459028686, 1058.0069680659753, 969.3415811266808, 735.4824184642888, 694.160204744928, 609.8672520064773, 559.5029430551683, 556.7539400690104, 446.0364655273289, 383.0754661875908, 345.6122784985246, 308.20984491008926, 296.38593827696843, 280.34154272827743, 252.81591288480695, 251.82886886749097, 244.04347400810474, 212.01073937462013, 209.0913457500079, 200.13038605857614, 195.780749653893, 1788.178205420758, 1513.682173715688, 1889.1565634998829, 1006.9394844301896, 793.763430977163, 281.3899411831871, 5497.05827301819, 3385.055083438731, 2413.947003629611, 2349.307202232827, 1431.744749947743, 1292.0660971884442, 964.98078678742, 771.686708660461, 668.5562510340985, 662.0891309122728, 607.3156874952468, 600.1618228909012, 385.2473332321841, 350.80536629996095, 339.7421318407453, 312.1401003593371, 295.54451201781046, 291.7331169724604, 269.64311236283544, 245.74455630576452, 242.82506352076408, 225.0084503033546, 218.81013935699104, 155.559492305387, 154.24808935098542, 154.0626025023697, 126.00495132405777, 109.44596541371932, 96.77144128872547, 87.21455801568331, 2637.639700123536, 4915.960101269162, 283.9902064503515, 624.8273955547394, 2252.4425283363576, 690.6662833890906, 361.33112053063434, 1109.5310159542962, 384.67580583651767, 571.9394607722425, 523.1581346058864, 455.6198810669356, 407.34790420402, 451.7310914469599, 351.9969020473204, 14096.61777192585, 5698.602508193998, 3206.2021334478795, 1124.1514743225348, 884.9837904930309, 672.0762381047155, 511.5315903031511, 208.8363204283901, 192.45266037670206, 129.37074908337883, 122.31108930455261, 108.91077481582236, 106.28448264588702, 90.67082653007361, 79.35317146734349, 78.66008360406782, 58.280474904094326, 55.60247097157651, 52.018480178142276, 45.180500278077226, 38.32760706136464, 37.724696662515356, 37.514254471857654, 32.59853248103636, 32.510493703500075, 29.4727553538028, 29.452345841814086, 29.40360831046226, 26.109171958757845, 25.44818356269448, 381.79059542482213, 1378.4381873493232, 332.23699021522464, 298.12473501421715, 111.02635008951432, 326.82729287714903, 224.37135872074577, 106.02989458497844, 59.54278309854213, 69.0972326329689, 73.50587886114106, 57.88299342985932, 3382.4654168333914, 1244.1504697854948, 1197.5997551043642, 998.1664107109884, 919.5682409618495, 779.8192475372255, 756.7996679097498, 705.5287765905862, 662.7193291516268, 581.6866592528398, 533.3430132765512, 500.1533158808211, 423.62279491994076, 349.6105910558159, 338.41026325637534, 321.4179123629855, 316.16319865052446, 293.56462105505784, 288.8158922452434, 262.4147716487467, 244.7562638690879, 213.30548959966092, 208.09320254898924, 181.61506622597724, 154.37904074429017, 147.2537452295672, 146.4559555767513, 146.24401658839304, 142.56298872860992, 132.325040120316, 1561.7650359556849, 1317.4772142515747, 1357.7342218984647, 854.1470280368276, 583.2447994584498, 402.944571106765, 1423.6272796980863, 1262.7843485263857, 1101.7541199066923, 1019.3147102978357, 868.9936770378283, 770.7120955779047, 763.5464379842763, 682.9941749347447, 563.8672067630083, 463.53034420711276, 451.3553980044252, 448.2789267753428, 446.4078356282264, 389.4648208147264, 360.3868684722027, 318.56261383564123, 289.749175586336, 280.1105136793924, 271.17974949255193, 256.5124477319225, 234.60399027362845, 220.10583534103168, 173.10348662145034, 172.98701425795818, 160.97371686455185, 160.2219963441172, 146.84885026580324, 141.27095993962908, 139.93751322655703, 136.02955056169014, 1605.762324124223, 256.7222745486594, 1267.6736688354233, 245.37446457463236, 240.13004186419678, 378.73135400344654, 436.51628920816853, 339.06320939319295, 333.54429092564396, 325.0497537010086, 4438.22071357331, 2284.2543674979, 1576.3143282022922, 675.0414737156028, 464.57500819175874, 367.6583645322723, 361.9045125427535, 359.5772307107307, 344.68012462514577, 275.8776972339454, 253.58613706809717, 244.2896944178591, 229.7751815543949, 229.49514622169437, 217.55551491570108, 197.47474067334954, 184.40926317292008, 163.38764022693024, 128.08717579168967, 123.57502842984059, 89.10909478505931, 75.90871022528505, 4319.177151248078, 48.19978778232992, 48.110886990700166, 40.704647721026305, 39.917001077392406, 28.899672265893944, 23.976588341909164, 21.86273918035997, 1492.082362395666, 727.2358761643183, 82.5011933760001, 154.88356856052297, 267.4569079328617, 159.42430506853788, 108.52826691585702, 93.71108294447785, 88.90695028936155, 1785.0740623131653, 1604.3833100547274, 734.4750593632414, 577.5951633517426, 574.0161211989969, 569.6949647537834, 536.9469012326172, 521.0417464782633, 497.9621675349883, 369.0241811503665, 348.420364627427, 299.27705112928146, 225.72750532174527, 215.3087533897164, 205.806098372584, 202.82125067914927, 191.40125888592226, 184.75364401720879, 169.09917480915286, 169.04220465773173, 118.0896035506758, 111.09344951557912, 100.83523525024376, 99.099544636947, 97.64401212828322, 96.60730729282264, 94.28722952628168, 88.55527269129816, 80.89277044511276, 80.57275011452987, 1178.1015440792717, 870.8225414415818, 387.40587480711963, 381.9066600307418, 309.7373786118141, 243.3702918945553, 381.3267967295216, 148.51841642749963, 144.28648789944538, 3931.139376923899, 2772.9475892886308, 831.3461489279151, 522.8033126798038, 380.91183078005525, 320.6116010022383, 281.0638852297469, 275.63246710245016, 271.61859535550064, 242.47599503505924, 210.21626300778925, 206.322919622638, 201.95521749093902, 189.5901941524883, 184.02536615442096, 154.35025489361078, 150.7233120769928, 119.84365881198292, 87.765323903864, 80.41427880401793, 78.68340866131535, 71.40408245091011, 68.96083584237022, 68.89956403404834, 63.61403423552535, 55.720700178800215, 47.518080930430045, 44.56031557678175, 42.67442397034571, 42.33687684474771, 1208.258972480761, 102.69650021337057, 825.3217944589628, 137.7868108304811, 173.79939879711446, 121.45322673502655, 159.54047119896208, 156.3257044253798, 267.97575620866155, 78.72481005686083, 1391.2468047704513, 1348.1987114843255, 1004.930792296401, 512.547507632025, 404.9857283756086, 268.6333233458858, 221.40796951581362, 213.62497383139717, 207.92581050628755, 204.7908323948698, 166.34408385700036, 158.68777504215018, 134.92997652949106, 132.98301286968388, 131.54896194487597, 110.77973806094802, 74.7921517247782, 73.21814947210356, 61.5122253445636, 49.82323736039896, 48.479937793875806, 47.89408274665324, 42.99327743912251, 42.795671804468995, 34.424678493509504, 30.83595885120449, 30.518725871156114, 23.652239855836722, 20.331109142386822, 16.3549351614629, 1898.3091628408336, 318.58669950920074, 118.64826600017616, 457.3434325809761, 41.069894360270055, 97.90633405967264, 148.04410023305735, 79.885142124408, 1817.7033438470157, 547.1935072329009, 527.563694236888, 514.4719819305066, 453.4692199408631, 437.17523591273203, 385.1248912368733, 349.2610561915092, 326.802968499323, 295.2889484639539, 178.87211357087187, 152.24727042235568, 149.71437684220814, 133.90973195097945, 100.70366365362054, 93.45189704364176, 91.66182380408887, 88.22546974210175, 83.46433964043118, 78.58562242482556, 76.89111486452299, 75.0305383384842, 74.08585144509404, 70.13141711052825, 69.28406638708248, 68.35480878435183, 63.59448994498776, 61.467599984433704, 60.942044873622145, 48.10598734164105, 1064.2016073836664, 102.40254896901102, 856.6668246874325, 748.4292914445065, 738.5941850082152, 373.47950872345376, 295.84738727938435, 287.5911469735775, 203.86177123975762, 182.40097444823155, 169.38719962378656, 166.60684666607406, 145.01345974595384, 137.22783344149366, 112.5819078795357, 109.41202432882247, 68.91409205116943, 64.6187153263053, 52.53017785611057, 34.213246748206906, 1832.7640914832834, 18.352863978565466, 17.622053271744527, 16.634926296881655, 11.029997621996744, 10.441604880272338, 8.363667706310551, 7.6419529331920195, 7.009819355685705, 117.82525352706342, 79.46221308540669, 174.34340292052497, 25.1655074174404, 116.64769247216152, 88.10103687946943, 84.62342003273852, 77.54795680248829, 59.57264328406639, 58.2658371685115, 54.26574841933392, 48.17671317329442, 40.244774230821065, 32.47866316273113, 26.53350803045865, 2.2298907474832386, 0.04084940215389991, 0.040105483517969186, 0.03963581537353778, 0.03944625324201055, 0.03977222596903724, 0.03944625324201055, 0.03942089161098088, 0.03961298345070559, 0.039628177068713064, 0.039517712119501516, 0.039409143682996944, 0.03966605814262044, 0.03946985053220748, 0.03947695092824172, 0.0393673773707401, 0.0393673773707401, 0.03939481071905423, 0.03939481071905423, 0.03960733694528788, 0.03959528164085225, 0.03955989031620364, 0.03955989031620364, 0.039524502065319544, 0.03952459120449053, 0.03955489852262805, 0.039499383261686706, 0.039618196555326404, 0.03953000103004044, 0.03951003078197356, 243.61353681861303, 172.48355077913055, 107.1588208332079, 34.62853622832979, 27.590311303847425, 31.874409162601143, 0.04022441734839794, 0.03999680688712251, 0.04002224099542162, 0.03997535091907873, 0.03989459496722889, 0.03979099498057952, 0.03978323362008136, 0.0397321527634695, 0.03968867674067907, 0.03964055807759066, 0.039660168212182635, 0.039660168212182635, 0.039611704009072046, 0.039592852881195455, 0.039592852881195455, 0.039592852881195455, 0.039592852881195455, 0.040071281098569174, 0.040071281098569174, 0.0395557353854798, 0.03956790607292762, 0.03959287650786364, 0.0395307797172114, 0.03975231812476378, 0.04008242993261807, 0.03973914035058465, 0.03974818641116526, 0.03969151194086104, 0.04043841884879991, 0.03967476063311922, 0.039901053907643445, 0.03964224147769871, 0.03977859393311691, 0.039703216001612245, 0.039692988607622486, 0.039694376674378244, 0.03968094491351615, 0.039678180593338726], \"Total\": [14097.0, 8803.0, 8613.0, 8603.0, 5699.0, 7358.0, 4439.0, 4376.0, 3932.0, 5498.0, 4529.0, 6184.0, 11655.0, 3383.0, 9340.0, 2773.0, 8413.0, 3207.0, 3386.0, 3745.0, 2176.0, 1913.0, 2285.0, 1818.0, 5569.0, 3019.0, 1786.0, 5300.0, 2414.0, 2652.0, 2184.218909784468, 2185.7781882390514, 2108.69052115385, 1452.8516482835028, 1389.213758027244, 1316.4170369505334, 1281.8213794631833, 1253.3110562290804, 1109.0472421390866, 985.1668478388857, 968.3303535949632, 945.8639245840066, 799.8990234495792, 798.7000733233101, 785.8824410469837, 577.5198339032939, 560.8945988547392, 547.4362732229552, 532.6929732485905, 493.87723338627944, 455.482095156038, 450.004299680279, 442.75388745373283, 422.8497315881079, 413.9345803874746, 413.57185460322415, 397.3832054277319, 390.1720758623071, 370.1127015230151, 363.43655665728494, 5569.633182497935, 2911.5140591542, 2006.186599577965, 2932.4917594278395, 5062.783305049067, 8413.789997031578, 11655.319784231882, 4829.933859726432, 3939.4351475713875, 1394.08403063976, 1749.367366634559, 2611.192161935571, 1589.9351874441563, 1358.043905422242, 836.5580369468996, 1555.7343047186598, 2533.510462080524, 4529.89783402242, 2110.8234763146575, 1477.0444620420662, 1575.6152230383695, 2287.6042617008875, 2755.9591382922226, 3096.441380100448, 2356.3664917363517, 2493.911370911565, 2732.9520582955506, 2471.6387220615634, 2287.872175780632, 1852.7083620451783, 1227.8968357330216, 1068.0450471996198, 993.9919754387836, 965.3648969235013, 959.5367396296717, 951.8981315736114, 823.794346865048, 688.6095168957227, 616.8442340175906, 607.1222443299996, 599.5781579468181, 586.4699402481245, 567.3658386165832, 539.8608053638312, 531.7907981136656, 502.02102047560254, 498.86026236590993, 497.86667202924195, 496.1604072277742, 488.62074768574183, 488.06277786818043, 487.1484967029769, 484.49381673066705, 475.02145772931846, 439.85334623122594, 432.83105792165, 1471.5436523365793, 813.3858243115537, 2261.048443151617, 2386.3080215012005, 5300.251012906409, 9340.014868621522, 1202.7804979072266, 1469.2081950378522, 1453.2290972279905, 1059.2513426124976, 1103.9482839435307, 1735.3788605189038, 2591.9308083708897, 906.928294117127, 843.8081206946907, 3478.057542104636, 1031.3802544061402, 2493.911370911565, 3096.441380100448, 2213.719480390997, 2366.7749945866562, 11655.319784231882, 8803.197999975753, 8613.631757938985, 8603.40681987781, 7358.712825349079, 2652.617824998917, 2187.8129337822893, 1493.1863271905083, 1480.4929244525595, 1382.0078837087926, 1093.969691456848, 1059.0183136543728, 970.3529263428197, 736.4937672568462, 695.1715523988587, 610.8786084656155, 560.5142911883557, 557.7652860637739, 447.0478110871783, 384.0868141055599, 346.62362437990686, 309.22119269084396, 297.3972850857999, 281.3528891864618, 253.82725834438676, 252.84021449066358, 245.05481965885892, 213.02208850705688, 210.10269183617618, 201.14173215323729, 196.79209499147288, 2354.5986270594635, 2352.2625770310115, 9340.014868621522, 3478.057542104636, 2591.9308083708897, 462.9531561777057, 5498.008518029105, 3386.0053286419366, 2414.8972488044265, 2350.257447514856, 1432.69499509466, 1293.0163455066295, 965.9310317609302, 772.6369535624232, 669.5065072409437, 663.0393775451853, 608.265940830572, 601.1120684122407, 386.1975808861694, 351.75561165331334, 340.69237990716704, 313.09034852259896, 296.4947594989677, 292.6833646739834, 270.5933600451796, 246.69481575991279, 243.7753171249816, 225.9587002990979, 219.76038502984855, 156.50973797002595, 155.19833407617168, 155.01287138141436, 126.95520561461483, 110.39677596930665, 97.72168614727318, 88.16480313455914, 3019.8264384977288, 6184.498585377451, 316.50075698778386, 800.303473451889, 3745.4069818344624, 1219.9553404003914, 643.8126776210937, 5300.251012906409, 785.2895516007695, 2178.566600841067, 3478.057542104636, 2356.3664917363517, 1735.3788605189038, 3096.441380100448, 4829.933859726432, 14097.619906420545, 5699.604641441392, 3207.2042685268298, 1125.1536082025616, 885.9859239521054, 673.0783718585933, 512.5337243429232, 209.8384611193823, 193.45479931415505, 130.37288772595852, 123.31322408984917, 109.91290911979486, 107.28662116137683, 91.67296100543821, 80.35530990965205, 79.66222570376712, 59.2826104991164, 56.604612741923795, 53.02062162861634, 46.182647316933426, 39.329742381640244, 38.726831723713076, 38.51639885276442, 33.60066773115142, 33.5126320857778, 30.474888895584215, 30.454482138828325, 30.405744319551992, 27.111309105332367, 26.45034015535571, 429.2862071549886, 1782.3118913681226, 500.2613355412891, 796.3025815597788, 202.0332960677867, 2386.3080215012005, 2611.192161935571, 1035.669691569905, 194.72180370017153, 1148.0990624457213, 2176.868487105019, 479.9829143668802, 3383.451154241947, 1245.1362113790954, 1198.5854950156797, 999.1521496343335, 920.5539781992635, 780.8049837254873, 757.785404642023, 706.5145140083821, 663.7050670405773, 582.6723979138171, 534.3287511227971, 501.13905276861095, 424.60853288703834, 350.5963271326663, 339.3960012322426, 322.4036494146203, 317.1489377758094, 294.5503582282087, 289.80162936036555, 263.400509564745, 245.74200328770638, 214.2912263088775, 209.0789414333476, 182.60080356103649, 155.36478021985332, 148.23948185448913, 147.44169677999184, 147.22975533156207, 143.54872625687588, 133.3107769546604, 1642.2558003580443, 1416.2682735081253, 2366.7749945866562, 1658.6387578880078, 1248.1622123598786, 1782.3118913681226, 1424.6005901799754, 1263.7576581311216, 1102.7274298925522, 1020.2880201855056, 869.9669873663694, 771.6854063116451, 764.5197476829773, 683.9674870831004, 564.8405164463492, 464.50365628651133, 452.32870971226265, 449.25223648276614, 447.3811454835388, 390.43813093024136, 361.36018037142304, 319.5359239924722, 290.722484872073, 281.08382275334236, 272.15306267882715, 257.485759488715, 235.5773006078155, 221.0791598271314, 174.07679727136346, 173.96032774064335, 161.94702963072103, 161.19530747065167, 147.82216151807512, 142.24427852138095, 140.91082378396996, 137.0028602167505, 2178.566600841067, 376.29902216907016, 6184.498585377451, 474.12592800958794, 453.4147535444062, 1658.6387578880078, 2533.510462080524, 1248.1622123598786, 9340.014868621522, 8413.789997031578, 4439.211301313706, 2285.244952211798, 1577.3049147371169, 676.0320582543602, 465.5655938400218, 368.64895444467066, 362.89509876695143, 360.56781828059655, 345.6707100590121, 276.86828583628966, 254.5767212192696, 245.28028461878816, 230.7657663877548, 230.4857314834689, 218.54610192816688, 198.46532813436156, 185.39985163277788, 164.37822568956452, 129.07775988755967, 124.56561571385129, 90.09968217444212, 76.89929688632547, 4376.979099923253, 49.1903794948831, 49.10147172082674, 41.695232192134114, 40.907585635831936, 29.89026098089756, 24.967172774831077, 22.853323727122767, 3745.4069818344624, 2755.9591382922226, 174.73068922511712, 537.1532966989867, 1838.011555931213, 5300.251012906409, 3478.057542104636, 2287.6042617008875, 2352.2625770310115, 1786.042497395741, 1605.3517461880108, 735.4434955726448, 578.5635983909202, 574.9845617713848, 570.6634014345174, 537.9153367447282, 522.0101814110325, 498.93060353601464, 369.99261973827385, 349.3888050722914, 300.24548978495125, 226.6988434654503, 216.27718913075913, 206.77453434460364, 203.78968893525774, 192.36969892645484, 185.72208135094746, 170.06761327428092, 170.01064008946886, 119.05804084698089, 112.06189014263134, 101.80367022264475, 100.06798170029349, 98.61245143980265, 97.5757424445239, 95.2556667886107, 89.52371056976406, 81.86120943999221, 81.54118974234686, 1470.5154270692474, 1194.8581158602788, 796.3025815597788, 1121.5763678509402, 917.2837944996666, 608.4006307222196, 3019.8264384977288, 472.78285708731613, 753.7294223712689, 3932.136625137227, 2773.944838072168, 832.3433970367686, 523.8005612397709, 381.90907896437596, 321.6088495883402, 282.06113241482456, 276.6297159028348, 272.6158435172689, 243.47324367447655, 211.21351332181933, 207.32016762959145, 202.95246656989335, 190.58744221075912, 185.0226151676646, 155.3475052869853, 151.72055971342647, 120.84090917478376, 88.76257141489347, 81.41152664069291, 79.68065901645494, 72.40133192090639, 69.95808453159889, 69.8968138836929, 64.61128154072901, 56.7179500841986, 48.515328544116024, 45.557572059494355, 43.67167102831716, 43.33412813480349, 1326.8395292343982, 107.85352424812912, 969.4942695698113, 163.25828683332296, 304.16548507149173, 189.63122916206854, 425.84206261072495, 470.2768936750234, 2261.048443151617, 115.93122577689067, 1392.204532177093, 1349.156437316466, 1005.8885189634345, 513.5052334191291, 405.9434554328292, 269.59104911396344, 222.36569601499016, 214.5826994443277, 208.88353879093862, 205.74856003716684, 167.30181296963315, 159.6455095361204, 135.88770461248927, 133.9407408881489, 132.5066913416915, 111.73746364201547, 75.74987740869949, 74.1758757503316, 62.46997281800891, 50.780962971953805, 49.43766358301542, 48.851812123850344, 43.95100585651584, 43.75340438337084, 35.38241363035019, 31.793684469844635, 31.476451412102435, 24.609965468590683, 21.288835090889258, 17.312660824404418, 2176.868487105019, 379.04597138649024, 479.9829143668802, 9340.014868621522, 73.86273855665799, 1416.2682735081253, 8413.789997031578, 1913.5471487539958, 1818.684202479188, 548.1743654062994, 528.544551269566, 515.4528402209597, 454.4500776615549, 438.1560943281785, 386.1057494698243, 350.24191429332444, 327.7838275842822, 296.2698084507037, 179.85297363312094, 153.22812830468754, 150.69523513215495, 134.8905908975538, 101.68452137256148, 94.43275496349717, 92.64268235634609, 89.20632728200435, 84.44519934391056, 79.56648078706468, 77.8719743386522, 76.01139664144902, 75.06670918239774, 71.11228189348206, 70.26501430696004, 69.33566675541266, 64.5753486056641, 62.44845977824171, 61.92290349773592, 49.08685284508247, 4529.89783402242, 173.26301150116618, 857.665753111518, 749.42821958249, 739.5931134228631, 374.47843588536705, 296.8463158558478, 288.5900751891222, 204.86069974579468, 183.3999036847811, 170.38612999667174, 167.60577680916927, 146.01238881259727, 138.22676124670627, 113.58083608949988, 110.41095390420413, 69.91302075610683, 65.61764574426371, 53.52910551051169, 35.212177540423355, 1913.5471487539958, 19.351793542322785, 18.62099195950854, 17.633854061775278, 12.028925547565414, 11.440537240881751, 9.362594250734384, 8.640880135698147, 8.00875739580461, 197.3440774005083, 258.10435923229295, 643.8126776210937, 281.4733197353381, 117.66733399407042, 89.1206780834372, 85.64305917343613, 78.5675971496129, 60.5922836155057, 59.28547906592341, 55.285389216805214, 49.196353658591576, 41.26441451613529, 33.498303277900085, 27.55314757625737, 3.24952963033452, 1.080948662463275, 1.0684202903616047, 1.0603778675247506, 1.0593766590587583, 1.068403901324599, 1.0596707704285027, 1.0592851239327807, 1.0644555899987205, 1.0650921850685378, 1.062319503419974, 1.0596584237687152, 1.0668776049853355, 1.0618588061720184, 1.062059571054901, 1.059194012073073, 1.059194012073073, 1.0599484639140195, 1.0602921353158326, 1.066287177466729, 1.066102379354299, 1.0653513511184132, 1.0657516944144394, 1.0648426353781888, 1.0655814625315532, 1.0707941127570337, 1.0654040611992082, 1.082841593118343, 1.07285156277875, 1.0770781253208852, 244.63298629292427, 173.50299955067683, 108.17826854406633, 35.647985260875686, 28.609759251126714, 73.86273855665799, 1.0668894296722997, 1.0611290580372497, 1.061919711111196, 1.061958217035231, 1.0605132177417236, 1.0609251404143327, 1.0608039043292017, 1.060232560388289, 1.0608200357738689, 1.0598850426556172, 1.0606511043302225, 1.0606511043302225, 1.059932873255559, 1.0596249856695847, 1.0596249856695847, 1.0596249856695847, 1.0596249856695847, 1.0726650916192113, 1.0726650916192113, 1.0596349326409682, 1.0601729534968272, 1.0609004152814157, 1.0593112820516617, 1.0657820915769323, 1.0758009090779386, 1.0655771446991051, 1.0679597310704747, 1.066258415286635, 1.1128866720248531, 1.0658265203170052, 1.0837338817935325, 1.064593649172857, 1.0835532094043483, 1.0781710726486442, 1.0796525195644397, 1.087974354194038, 1.0771200959869391, 1.080092568647936], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic13\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic14\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\", \"Topic15\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.2421, -4.2414, -4.2773, -4.6501, -4.6949, -4.7488, -4.7754, -4.7979, -4.9203, -5.0389, -5.0561, -5.0796, -5.2474, -5.2489, -5.2651, -5.5737, -5.6029, -5.6272, -5.6546, -5.7304, -5.8115, -5.8236, -5.8399, -5.886, -5.9074, -5.9082, -5.9483, -5.9666, -6.0195, -6.0378, -3.3172, -3.9805, -4.3486, -3.9798, -3.4472, -2.9532, -2.6543, -3.524, -3.7259, -4.7337, -4.5257, -4.1534, -4.6223, -4.7744, -5.2291, -4.6717, -4.2849, -3.8157, -4.4432, -4.743, -4.7132, -4.4978, -4.4112, -4.5503, -4.6382, -4.7639, -3.6886, -3.7891, -3.8664, -4.0775, -4.4891, -4.6287, -4.7006, -4.7299, -4.7359, -4.7439, -4.8886, -5.0681, -5.1784, -5.1943, -5.2068, -5.2289, -5.2621, -5.3119, -5.327, -5.3847, -5.391, -5.393, -5.3965, -5.4118, -5.4129, -5.4148, -5.4203, -5.4401, -5.5172, -5.5333, -4.3387, -4.9238, -4.0044, -3.9835, -3.2997, -2.8206, -4.6272, -4.4544, -4.476, -4.7659, -4.7571, -4.4316, -4.1349, -4.9325, -4.9895, -4.0846, -4.9048, -4.5737, -4.6586, -4.7526, -4.7986, -4.7855, -1.9911, -2.0128, -2.014, -2.1703, -3.1909, -3.3836, -3.7658, -3.7744, -3.8433, -4.0772, -4.1097, -4.1972, -4.4733, -4.5311, -4.6606, -4.7468, -4.7517, -4.9734, -5.1256, -5.2285, -5.343, -5.3821, -5.4378, -5.5411, -5.5451, -5.5765, -5.7172, -5.731, -5.7748, -5.7968, -3.5849, -3.7515, -3.5299, -4.1591, -4.397, -5.4341, -2.1965, -2.6813, -3.0194, -3.0466, -3.5418, -3.6444, -3.9363, -4.1599, -4.3033, -4.313, -4.3994, -4.4112, -4.8545, -4.9482, -4.9802, -5.065, -5.1196, -5.1326, -5.2113, -5.3041, -5.3161, -5.3923, -5.4202, -5.7614, -5.7699, -5.7711, -5.9721, -6.113, -6.2361, -6.3401, -2.9308, -2.3082, -5.1595, -4.371, -3.0887, -4.2708, -4.9186, -3.7967, -4.856, -4.4594, -4.5485, -4.6868, -4.7988, -4.6953, -4.9448, -0.8482, -1.754, -2.3291, -3.3772, -3.6164, -3.8916, -4.1645, -5.0604, -5.1421, -5.5393, -5.5954, -5.7114, -5.7358, -5.8947, -6.028, -6.0368, -6.3367, -6.3837, -6.4503, -6.5913, -6.7558, -6.7716, -6.7772, -6.9177, -6.9204, -7.0185, -7.0192, -7.0208, -7.1396, -7.1653, -4.4571, -3.1732, -4.5961, -4.7044, -5.6922, -4.6125, -4.9886, -5.7382, -6.3152, -6.1664, -6.1046, -6.3435, -2.1072, -3.1073, -3.1454, -3.3276, -3.4096, -3.5744, -3.6044, -3.6746, -3.7372, -3.8676, -3.9543, -4.0186, -4.1847, -4.3767, -4.4093, -4.4608, -4.4773, -4.5514, -4.5677, -4.6636, -4.7332, -4.8708, -4.8955, -5.0316, -5.1941, -5.2414, -5.2468, -5.2482, -5.2737, -5.3482, -2.8799, -3.05, -3.0199, -3.4834, -3.8649, -4.2347, -2.8767, -2.9966, -3.133, -3.2108, -3.3703, -3.4904, -3.4997, -3.6112, -3.8029, -3.9988, -4.0254, -4.0323, -4.0364, -4.1729, -4.2505, -4.3739, -4.4687, -4.5025, -4.5349, -4.5905, -4.6798, -4.7436, -4.9838, -4.9845, -5.0564, -5.0611, -5.1483, -5.187, -5.1965, -5.2248, -2.7563, -4.5897, -2.9927, -4.6349, -4.6565, -4.2008, -4.0589, -4.3115, -4.3279, -4.3537, -1.654, -2.3182, -2.6892, -3.5373, -3.9109, -4.1449, -4.1606, -4.1671, -4.2094, -4.4321, -4.5163, -4.5537, -4.6149, -4.6161, -4.6696, -4.7664, -4.8349, -4.9559, -5.1993, -5.2352, -5.5622, -5.7225, -1.6812, -6.1767, -6.1785, -6.3457, -6.3652, -6.6882, -6.875, -6.9672, -2.7441, -3.4628, -5.6392, -5.0094, -4.4631, -4.9805, -5.365, -5.5118, -5.5644, -2.3178, -2.4245, -3.2059, -3.4462, -3.4524, -3.4599, -3.5191, -3.5492, -3.5945, -3.8942, -3.9516, -4.1037, -4.3857, -4.433, -4.4781, -4.4927, -4.5507, -4.586, -4.6746, -4.6749, -5.0336, -5.0947, -5.1915, -5.2089, -5.2237, -5.2344, -5.2587, -5.3214, -5.4119, -5.4159, -2.7334, -3.0356, -3.8456, -3.8599, -4.0693, -4.3105, -3.8614, -4.8043, -4.8332, -1.4824, -1.8314, -3.036, -3.4999, -3.8165, -3.9889, -4.1205, -4.14, -4.1547, -4.2682, -4.411, -4.4296, -4.451, -4.5142, -4.544, -4.7199, -4.7436, -4.9729, -5.2844, -5.3719, -5.3937, -5.4907, -5.5256, -5.5264, -5.6063, -5.7387, -5.898, -5.9622, -6.0055, -6.0134, -2.6622, -5.1273, -3.0433, -4.8334, -4.6012, -4.9596, -4.6868, -4.7072, -4.1682, -5.3931, -2.2431, -2.2745, -2.5684, -3.2416, -3.4772, -3.8877, -4.081, -4.1168, -4.1439, -4.159, -4.367, -4.4141, -4.5763, -4.5908, -4.6017, -4.7735, -5.1663, -5.1876, -5.3618, -5.5725, -5.5999, -5.612, -5.72, -5.7246, -5.9423, -6.0523, -6.0627, -6.3176, -6.4689, -6.6865, -1.9323, -3.7171, -4.7049, -3.3556, -5.7658, -4.897, -4.4835, -5.1004, -1.7964, -2.9969, -3.0335, -3.0586, -3.1848, -3.2214, -3.3482, -3.4459, -3.5124, -3.6138, -4.1151, -4.2762, -4.293, -4.4046, -4.6896, -4.7643, -4.7836, -4.8218, -4.8773, -4.9376, -4.9593, -4.9838, -4.9965, -5.0514, -5.0635, -5.077, -5.1492, -5.1832, -5.1918, -5.4283, -2.3318, -4.6728, -2.3072, -2.4423, -2.4555, -3.1374, -3.3704, -3.3987, -3.7428, -3.8541, -3.9281, -3.9446, -4.0834, -4.1386, -4.3366, -4.3651, -4.8274, -4.8918, -5.0989, -5.5277, -1.5467, -6.1505, -6.1911, -6.2488, -6.6596, -6.7145, -6.9364, -7.0266, -7.113, -4.2911, -4.685, -3.8992, -5.8348, -2.6732, -2.9539, -2.9942, -3.0815, -3.3452, -3.3674, -3.4385, -3.5575, -3.7374, -3.9518, -4.154, -6.6304, -10.6302, -10.6486, -10.6604, -10.6652, -10.657, -10.6652, -10.6658, -10.661, -10.6606, -10.6634, -10.6661, -10.6596, -10.6646, -10.6644, -10.6672, -10.6672, -10.6665, -10.6665, -10.6611, -10.6614, -10.6623, -10.6623, -10.6632, -10.6632, -10.6624, -10.6638, -10.6608, -10.6631, -10.6636, -1.8968, -2.2421, -2.7181, -3.8477, -4.0749, -3.9306, -10.6057, -10.6114, -10.6107, -10.6119, -10.6139, -10.6165, -10.6167, -10.618, -10.6191, -10.6203, -10.6198, -10.6198, -10.621, -10.6215, -10.6215, -10.6215, -10.6215, -10.6095, -10.6095, -10.6225, -10.6221, -10.6215, -10.6231, -10.6175, -10.6092, -10.6178, -10.6176, -10.619, -10.6004, -10.6194, -10.6138, -10.6203, -10.6168, -10.6187, -10.619, -10.619, -10.6193, -10.6194], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.2967, 1.2967, 1.2967, 1.2965, 1.2965, 1.2964, 1.2964, 1.2964, 1.2963, 1.2962, 1.2962, 1.2962, 1.296, 1.296, 1.2959, 1.2955, 1.2954, 1.2954, 1.2953, 1.2952, 1.295, 1.295, 1.295, 1.2949, 1.2948, 1.2948, 1.2947, 1.2947, 1.2945, 1.2945, 1.2856, 1.2709, 1.2753, 1.2645, 1.251, 1.237, 1.2101, 1.2213, 1.2232, 1.2542, 1.2352, 1.207, 1.2341, 1.2397, 1.2695, 1.2065, 1.1056, 0.9937, 1.1298, 1.1871, 1.1523, 0.9948, 0.8952, 0.6396, 0.8248, 0.6423, 1.6262, 1.6261, 1.6261, 1.626, 1.6257, 1.6256, 1.6255, 1.6255, 1.6255, 1.6255, 1.6253, 1.6251, 1.6249, 1.6249, 1.6249, 1.6248, 1.6248, 1.6247, 1.6247, 1.6246, 1.6245, 1.6245, 1.6245, 1.6245, 1.6245, 1.6245, 1.6245, 1.6244, 1.6243, 1.6242, 1.5952, 1.6028, 1.4999, 1.4669, 1.3526, 1.2652, 1.5083, 1.481, 1.4703, 1.4967, 1.4642, 1.3373, 1.2328, 1.4853, 1.5005, 0.9891, 1.3844, 0.8326, 0.5312, 0.7729, 0.66, -0.9211, 2.154, 2.154, 2.154, 2.1539, 2.1537, 2.1536, 2.1534, 2.1534, 2.1533, 2.1532, 2.1531, 2.153, 2.1527, 2.1526, 2.1524, 2.1523, 2.1523, 2.1518, 2.1514, 2.1512, 2.1508, 2.1507, 2.1505, 2.1501, 2.1501, 2.1499, 2.1493, 2.1493, 2.149, 2.1489, 1.8789, 1.7132, 0.5559, 0.9145, 0.9707, 1.6562, 2.4193, 2.4192, 2.4191, 2.419, 2.4188, 2.4187, 2.4185, 2.4182, 2.418, 2.418, 2.4179, 2.4179, 2.417, 2.4167, 2.4167, 2.4164, 2.4162, 2.4162, 2.4159, 2.4156, 2.4155, 2.4152, 2.4151, 2.4134, 2.4133, 2.4133, 2.4119, 2.4108, 2.4097, 2.4086, 2.2841, 2.1899, 2.3111, 2.1719, 1.9109, 1.8505, 1.8418, 0.8556, 1.7058, 1.0821, 0.5251, 0.7762, 0.9701, 0.4945, -0.1995, 2.8259, 2.8258, 2.8256, 2.8251, 2.8248, 2.8245, 2.824, 2.8212, 2.8208, 2.8182, 2.8178, 2.8168, 2.8166, 2.815, 2.8134, 2.8133, 2.8089, 2.8081, 2.8069, 2.804, 2.8001, 2.7997, 2.7996, 2.7957, 2.7956, 2.7925, 2.7925, 2.7924, 2.7883, 2.7873, 2.7087, 2.569, 2.4167, 1.8435, 2.2273, 0.8379, 0.3717, 0.5469, 1.6411, 0.0156, -0.5623, 0.7106, 2.9941, 2.9936, 2.9936, 2.9934, 2.9933, 2.9931, 2.9931, 2.993, 2.9929, 2.9927, 2.9925, 2.9924, 2.9921, 2.9916, 2.9915, 2.9913, 2.9913, 2.991, 2.991, 2.9906, 2.9904, 2.9898, 2.9896, 2.989, 2.988, 2.9877, 2.9877, 2.9877, 2.9875, 2.987, 2.9441, 2.9221, 2.4387, 2.3307, 2.2336, 1.5075, 3.0895, 3.0894, 3.0893, 3.0893, 3.0891, 3.0889, 3.0889, 3.0888, 3.0885, 3.0881, 3.0881, 3.088, 3.088, 3.0877, 3.0875, 3.0872, 3.0869, 3.0867, 3.0866, 3.0864, 3.0861, 3.0858, 3.0846, 3.0846, 3.0842, 3.0842, 3.0836, 3.0833, 3.0833, 3.0831, 2.7851, 2.7078, 1.5053, 2.4315, 2.4546, 1.6133, 1.3317, 1.787, -0.2421, -0.1634, 3.1756, 3.1754, 3.1752, 3.1744, 3.1737, 3.1732, 3.1731, 3.1731, 3.173, 3.1723, 3.172, 3.1718, 3.1716, 3.1715, 3.1713, 3.1709, 3.1705, 3.1698, 3.1682, 3.1679, 3.1648, 3.1629, 3.1626, 3.1555, 3.1555, 3.1518, 3.1513, 3.1422, 3.1354, 3.1315, 2.2555, 1.8436, 2.4254, 1.9322, 1.2484, -0.3281, -0.2914, -0.0192, -0.0997, 3.4223, 3.4222, 3.4215, 3.4212, 3.4212, 3.4212, 3.421, 3.421, 3.4209, 3.4202, 3.4201, 3.4196, 3.4186, 3.4184, 3.4182, 3.4181, 3.4178, 3.4176, 3.4171, 3.4171, 3.4147, 3.4142, 3.4133, 3.4131, 3.413, 3.4129, 3.4126, 3.412, 3.4109, 3.4109, 3.2011, 3.1065, 2.7023, 2.3455, 2.3372, 2.5066, 1.3536, 2.2649, 1.7696, 3.4685, 3.4684, 3.4676, 3.4669, 3.4662, 3.4657, 3.4653, 3.4652, 3.4651, 3.4647, 3.4641, 3.464, 3.4639, 3.4635, 3.4634, 3.4624, 3.4622, 3.4605, 3.4575, 3.4565, 3.4562, 3.4549, 3.4544, 3.4544, 3.4532, 3.4511, 3.448, 3.4467, 3.4457, 3.4455, 3.3752, 3.4198, 3.3078, 3.2992, 2.9091, 3.0232, 2.487, 2.3674, 1.3361, 3.0818, 3.7462, 3.7461, 3.7459, 3.745, 3.7445, 3.7433, 3.7425, 3.7424, 3.7423, 3.7422, 3.7411, 3.7408, 3.7398, 3.7397, 3.7396, 3.7382, 3.7341, 3.7339, 3.7314, 3.7278, 3.7273, 3.7271, 3.7248, 3.7247, 3.7194, 3.7163, 3.716, 3.7072, 3.7008, 3.6899, 3.6099, 3.5731, 2.3493, 0.7302, 3.1599, 1.0751, -0.2933, 0.5707, 3.9256, 3.9244, 3.9243, 3.9242, 3.924, 3.9239, 3.9236, 3.9233, 3.9232, 3.9228, 3.9207, 3.9197, 3.9196, 3.9188, 3.9165, 3.9157, 3.9155, 3.9151, 3.9145, 3.9137, 3.9135, 3.9132, 3.913, 3.9123, 3.9121, 3.9119, 3.9108, 3.9103, 3.9102, 3.906, 2.4777, 3.4002, 4.1665, 4.1663, 4.1663, 4.165, 4.1643, 4.1642, 4.1627, 4.1622, 4.1617, 4.1616, 4.1608, 4.1604, 4.1588, 4.1585, 4.1532, 4.1523, 4.1488, 4.1388, 4.1245, 4.1146, 4.1125, 4.1093, 4.0809, 4.0763, 4.0548, 4.0448, 4.0344, 3.6519, 2.9895, 2.8612, 1.7531, 5.7868, 5.784, 5.7835, 5.7824, 5.7785, 5.7782, 5.7769, 5.7746, 5.7705, 5.7646, 5.7578, 5.419, 2.5198, 2.5131, 2.5089, 2.505, 2.5048, 2.5047, 2.5045, 2.5044, 2.5042, 2.504, 2.5038, 2.5035, 2.5033, 2.5033, 2.5032, 2.5032, 2.5032, 2.5028, 2.5026, 2.5025, 2.5023, 2.5019, 2.5018, 2.5012, 2.497, 2.5007, 2.4875, 2.4945, 2.4901, 5.8313, 5.8296, 5.826, 5.8065, 5.7992, 4.9951, 2.5574, 2.5572, 2.5571, 2.5559, 2.5552, 2.5522, 2.5521, 2.5514, 2.5497, 2.5494, 2.5492, 2.5492, 2.5486, 2.5485, 2.5485, 2.5485, 2.5485, 2.5482, 2.5482, 2.5475, 2.5473, 2.5473, 2.5472, 2.5467, 2.5456, 2.5465, 2.5445, 2.5447, 2.5205, 2.5447, 2.5337, 2.545, 2.5308, 2.5339, 2.5323, 2.5246, 2.5343, 2.5315]}, \"token.table\": {\"Topic\": [2, 10, 2, 6, 7, 12, 12, 13, 11, 1, 13, 9, 9, 1, 2, 5, 3, 5, 6, 6, 6, 8, 11, 6, 14, 4, 10, 8, 11, 10, 11, 1, 3, 2, 4, 6, 10, 2, 3, 10, 12, 5, 5, 5, 9, 1, 13, 2, 9, 9, 2, 5, 11, 1, 2, 8, 2, 3, 6, 4, 12, 12, 7, 3, 1, 2, 2, 4, 11, 1, 1, 5, 7, 12, 14, 10, 2, 8, 12, 3, 6, 6, 7, 7, 11, 10, 14, 12, 1, 2, 4, 5, 7, 8, 8, 10, 4, 10, 4, 5, 6, 9, 4, 10, 8, 1, 4, 5, 10, 10, 10, 15, 8, 2, 4, 4, 7, 9, 8, 2, 4, 8, 9, 4, 13, 9, 6, 4, 3, 1, 5, 5, 6, 7, 6, 5, 8, 1, 2, 7, 8, 1, 4, 5, 8, 6, 7, 4, 12, 13, 2, 9, 2, 6, 10, 2, 4, 9, 12, 7, 7, 1, 4, 1, 5, 7, 11, 11, 5, 2, 11, 1, 2, 3, 4, 8, 1, 2, 5, 11, 6, 1, 3, 2, 4, 9, 14, 9, 5, 6, 10, 1, 7, 10, 5, 3, 3, 3, 3, 13, 4, 5, 3, 2, 8, 1, 1, 1, 2, 4, 8, 1, 12, 10, 1, 1, 2, 4, 9, 4, 10, 2, 4, 9, 10, 9, 14, 13, 7, 3, 1, 2, 4, 9, 5, 9, 8, 4, 6, 13, 15, 11, 8, 1, 2, 4, 11, 15, 13, 7, 7, 4, 1, 2, 3, 9, 6, 12, 7, 2, 5, 11, 10, 12, 5, 2, 5, 11, 1, 3, 4, 1, 7, 6, 5, 4, 14, 11, 14, 4, 13, 11, 9, 6, 11, 11, 11, 10, 2, 4, 1, 2, 10, 8, 9, 2, 9, 12, 1, 7, 9, 10, 6, 5, 15, 4, 2, 3, 6, 1, 2, 8, 1, 6, 7, 12, 1, 3, 7, 10, 2, 2, 1, 2, 1, 2, 1, 7, 6, 11, 1, 2, 7, 13, 11, 4, 9, 4, 9, 5, 2, 12, 8, 5, 2, 6, 7, 9, 2, 4, 10, 5, 6, 13, 7, 8, 1, 6, 7, 12, 9, 4, 2, 2, 3, 4, 13, 1, 3, 4, 10, 12, 12, 1, 2, 4, 9, 7, 4, 9, 1, 2, 8, 1, 12, 2, 6, 13, 2, 4, 2, 4, 9, 6, 7, 2, 6, 5, 8, 1, 1, 2, 9, 13, 6, 11, 3, 14, 3, 6, 3, 5, 15, 3, 3, 8, 10, 2, 5, 6, 9, 14, 9, 2, 6, 4, 9, 13, 3, 13, 11, 10, 6, 10, 13, 7, 1, 2, 3, 12, 13, 5, 2, 12, 3, 8, 4, 2, 3, 1, 1, 8, 6, 1, 7, 6, 10, 2, 4, 4, 5, 3, 10, 8, 7, 8, 8, 5, 2, 3, 8, 11, 13, 12, 14, 5, 13, 15, 10, 1, 9, 8, 6, 7, 4, 6, 13, 10, 11, 2, 1, 6, 2, 3, 4, 8, 4, 8, 2, 8, 4, 14, 3, 1, 2, 3, 6, 7, 11, 12, 5, 2, 11, 8, 12, 5, 1, 2, 4, 9, 2, 13, 7, 10, 7, 1, 2, 4, 8, 13, 3, 2, 4, 11, 5, 2, 3, 3, 6, 4, 5, 11, 1, 1, 2, 5, 4, 11, 10, 11, 11, 12, 12, 12, 2, 6, 11, 2, 9, 10, 2, 7, 1, 2, 9, 2, 4, 7, 4, 1, 14, 3, 12, 10, 9, 11, 9, 6, 13, 3, 4, 10, 7, 13, 12, 4, 4, 10, 2, 2, 11, 4, 10, 4, 7, 7, 1, 7, 9, 11, 13, 2, 10, 10, 6, 9, 3, 12, 8, 9, 11, 2, 12, 1, 2, 4, 5, 4, 10, 2, 11, 13, 6, 11, 13, 1, 2, 4, 1, 2, 7, 8, 7, 13, 12, 5, 8, 4, 8, 4, 1, 9, 3, 1, 1, 6, 7, 2, 4, 3, 1, 2, 3, 4, 8, 5, 1, 2, 4, 7, 11, 13, 8, 3, 1, 7, 1, 2, 11, 10, 2, 3, 6], \"Freq\": [0.9976612256623663, 0.994254565900003, 0.7853553493385337, 0.028117660655330216, 0.18615830502839317, 0.9768055163668523, 0.9953864823161446, 0.9911250091108135, 0.9976758944621407, 0.9991262985841951, 0.9258316137206356, 0.9941500350417676, 0.9962542082512307, 0.9397683021621324, 0.05945006291049986, 0.9880076271631154, 0.9960507471036602, 0.9783644733536908, 0.9989690335714221, 0.9982991061613699, 0.9939744322195612, 0.9984252108707198, 0.992476820513784, 0.9975132329675224, 0.9943286384468943, 0.35331733225616746, 0.638080555268601, 0.9980596850138168, 0.9825633464385967, 0.9953069475528383, 0.982780668293387, 0.9977682442567937, 0.9986057378649655, 0.3802642845469051, 0.04563171414562861, 0.5737765537941079, 0.9944730260852819, 0.999114858449139, 0.9966768953571877, 0.4040100618909585, 0.5887003758982539, 0.9926591112792976, 0.9893186665779272, 0.980750477884524, 0.9940946655729516, 0.9994138091906982, 0.9666509732210351, 0.1985698311115029, 0.8010796611279124, 0.9990258661407533, 0.6312724701871338, 0.12083763455727314, 0.24792549159164662, 0.9576180277937438, 0.04160437873560834, 0.9991726934184223, 0.9990215326569232, 0.9959749653993611, 0.9992717573408888, 0.9922786924015335, 0.9843587933917219, 0.9919849683065667, 0.9963166227468231, 0.9983147290840427, 0.5736950915640345, 0.4259798986967595, 0.9990357254693901, 0.9997030930124555, 0.9991348022871257, 0.996047297304108, 0.9137588702898658, 0.08578457122587173, 0.995763183160691, 0.9819965267290248, 0.9552722636286906, 0.987169459752698, 0.9990020287252865, 0.9954592950019964, 0.9971361486552774, 0.9990824655206108, 0.9958868070714618, 0.9990874802541989, 0.9994004719763739, 0.9993201644763887, 0.9841474638696619, 0.991457663316836, 0.9799243416845044, 0.9828859502359055, 0.46829602521709385, 0.2751842622409727, 0.15255829275113572, 0.10234923437734422, 0.9995784150420017, 0.9947803199071709, 0.9916155215583884, 0.9905390896736249, 0.6222964410217139, 0.3757261530697141, 0.10948406731137118, 0.8898492279349742, 0.9988468726862519, 0.997317190437539, 0.999213973195187, 0.984619983332407, 0.988305525242255, 0.9269278068858644, 0.07287884476743897, 0.9916942502286071, 0.9981068630756911, 0.9952507444291829, 0.9969177286606874, 0.9891080846465354, 0.9626608480537986, 0.5093662575601853, 0.49026502290167834, 0.26255795887955463, 0.7371819614695188, 0.9994163087399873, 0.9877948273744959, 0.7603413480204472, 0.20942404374756735, 0.029998579239516404, 0.9981348036592278, 0.9867883430445437, 0.8740432072105159, 0.9894796394301564, 0.9946829656212159, 0.9978518845804284, 0.997971785610067, 0.9389433675728349, 0.060099343564495, 0.9516031411750963, 0.9977270724316573, 0.9969128214331742, 0.9912156396197256, 0.9893505007306246, 0.9612622228574448, 0.9996725355632213, 0.9990564835208504, 0.9987375915819618, 0.9975012049020834, 0.9986991132164472, 0.326678826136933, 0.5494143894121146, 0.11879230041343018, 0.9989376801902068, 0.998511940234675, 0.9976651741900514, 0.9848277754466551, 0.9301463433161178, 0.9974935795558377, 0.992105685179256, 0.9985129526188293, 0.9956462979957923, 0.9913258646510226, 0.5911322839927331, 0.06776177011078086, 0.3405920550305038, 0.9866941447449011, 0.9935361687660961, 0.9941522259909275, 0.9989539259048548, 0.987347674268178, 0.9416683804558076, 0.002020492549255172, 0.038627063441642996, 0.017590170428809733, 0.9846209499338339, 0.9537671466030171, 0.9982592286691746, 0.9938583331895847, 0.5070697172672617, 0.3106618117592344, 0.00544066220243843, 0.031011774553899054, 0.1452656808051061, 0.8461152815669004, 0.13786088854197537, 0.004263738820885836, 0.011369970189028897, 0.9988460103546615, 0.9990866291000722, 0.999205506259396, 0.8688669270894224, 0.1312121374665498, 0.9911132348604599, 0.9927756839943566, 0.9969173046727219, 0.30813190336089286, 0.6881612508393273, 0.9977409841287164, 0.9784732887822848, 0.015950659827322015, 0.005483039315641943, 0.9960042543444633, 0.9985617298536246, 0.9991136029961153, 0.9992707105938586, 0.9982008601374979, 0.9948861435652561, 0.9984324044990609, 0.9999560275830488, 0.9953016212458133, 0.7092947252514159, 0.288558221558975, 0.9969936143276458, 0.9996439765740007, 0.10528278045894131, 0.8553365758853859, 0.028213032672003882, 0.010321841221464834, 0.9973763645318947, 0.9973614555562644, 0.9893780262943613, 0.9986261366381585, 0.9677776555981608, 0.019778401700032883, 0.005456110813802174, 0.006820138517252718, 0.03708733699603132, 0.9549989276478065, 0.6246703620361499, 0.01744280243032879, 0.33795429708762037, 0.01962315273411989, 0.9937224186679265, 0.9902250983101397, 0.9901155547908831, 0.9975494217552965, 0.9976561543056657, 0.9548107648966718, 0.032788288575268415, 0.009678470724025015, 0.0023702377283326566, 0.959009389734198, 0.9991579750723039, 0.9935539112382085, 0.3268515825461012, 0.5826484732343543, 0.0888183648223101, 0.978687019147052, 0.9709196697655158, 0.9758005628924467, 0.8650589179835616, 0.008885418086404888, 0.12566519865058343, 0.555083670077438, 0.4332360351823906, 0.9980942543326089, 0.9985854779629564, 0.9926800052556284, 0.9983313044055019, 0.9133950416147536, 0.062349978210155595, 0.023782981379131516, 0.9982876726840254, 0.9963772926882021, 0.9850959266183364, 0.9988884780912243, 0.8523627216910719, 0.1370317649916325, 0.010057377247092292, 0.9863048775847181, 0.9910902748790792, 0.9812318309719092, 0.09371259734266087, 0.03399378531057306, 0.871894655668482, 0.9977422026770516, 0.9999031317886705, 0.9990361301890955, 0.9390319880900422, 0.06037981972983526, 0.9989635526928708, 0.9847033177082096, 0.9965399358499246, 0.9783171345466568, 0.9963617726557522, 0.9693582344263997, 0.9965174636403068, 0.9869406192690211, 0.9609293575957636, 0.998837490834615, 0.9901675094496958, 0.999142844162115, 0.9978076085392821, 0.9750364110646748, 0.9997109395614687, 0.4328027285217748, 0.5664141769101257, 0.9982237824969888, 0.42411123658451766, 0.5720570167884191, 0.996681629169952, 0.9893274383859153, 0.7310315660313851, 0.19104999184849264, 0.07695069116119843, 0.4675630829009566, 0.5293166976237245, 0.1531315836085155, 0.8452863415190056, 0.9916473723073664, 0.9983978509729624, 0.9913373281466651, 0.9968990461218803, 0.6744007187054014, 0.30633533790165257, 0.018904825638766973, 0.7392012807069619, 0.2194435499201034, 0.04109102329181219, 0.2603828226665572, 0.4670867249680088, 0.2715993134891166, 0.9964541242991142, 0.1439735826327902, 0.7593650907003802, 0.09640708925558517, 0.9826618330482533, 0.9981515348177729, 0.9980799485008296, 0.9986172787222845, 0.9979661798331966, 0.9740636460549543, 0.025759793178463178, 0.31623786666799686, 0.6829674935602957, 0.9299085665018565, 0.06919592977766283, 0.9440048255298502, 0.055226491353150366, 0.997062512982853, 0.9991980544273619, 0.9961761075115452, 0.9924760421599848, 0.9928798613601901, 0.9971835007648114, 0.9961249809085845, 0.9821233394539199, 0.9987295920431503, 0.9971814294003367, 0.9924495536514745, 0.9989586551721891, 0.8887739715268101, 0.08812913094653123, 0.022447986184493802, 0.9958517618837728, 0.10110560336269629, 0.8973122298439296, 0.9692129923405105, 0.7731531202107571, 0.22611081817484407, 0.9905872004815457, 0.9989157108244576, 0.9987851468246253, 0.9969960027003041, 0.9511307554276581, 0.048713482992453684, 0.9989697154795031, 0.9980648243137674, 0.9979677270523171, 0.9980599301141219, 0.8505833232021509, 0.08424307673887117, 0.06522044650751317, 0.9963857044745004, 0.7382064943903212, 0.006401910387954341, 0.0101547544084793, 0.0101547544084793, 0.23488388457873857, 0.9933976796185126, 0.6234174544374581, 0.1689041163145734, 0.19351828401870722, 0.014004612659248548, 0.9938142401041256, 0.9979187708112616, 0.996025044156741, 0.9989235653210505, 0.9986313659575093, 0.9994552215461221, 0.9965192151836334, 0.9978576790882423, 0.5269186479628595, 0.16272487657676543, 0.3060777440372493, 0.9976424094280287, 0.9981499815580511, 0.749115959388419, 0.23453091959314348, 0.01613480527913518, 0.9969805597831085, 0.996144129737792, 0.8817170417694954, 0.11732525152577962, 0.9894695304376144, 0.9977345877639265, 0.9990557281067065, 0.11967948169062925, 0.1506455014287641, 0.7289568430247418, 0.8544640284259349, 0.9972338686910254, 0.9972844994221965, 0.9947516529819809, 0.9756820664617967, 0.9998364810699688, 0.9993982121500934, 0.9943237430591109, 0.9451674289692624, 0.9974125063732561, 0.9997670885745038, 0.14337372005476398, 0.005157328059523883, 0.8509591298214408, 0.9984055049833612, 0.3742296042997683, 0.13813844454018293, 0.48599616397318907, 0.987425162066339, 0.9980372447627389, 0.9992696163823191, 0.9995711023520681, 0.996743091026556, 0.994099533038642, 0.9957986097535414, 0.9989916031154884, 0.9918647721108589, 0.9929764395664014, 0.9873417483683244, 0.9981315309493451, 0.9914088629617425, 0.8740848256903427, 0.9972126204811578, 0.9730347017772641, 0.026298235183169302, 0.9951914864269789, 0.9888024626824007, 0.9971489763873007, 0.9522407857011657, 0.9985861336704379, 0.9928804091690232, 0.013022680414672177, 0.9867536265082304, 0.9992434618103548, 0.39096828174668, 0.606972857297332, 0.9982972764890481, 0.9988760788259221, 0.9926167046499451, 0.9967097430607108, 0.9984050499745124, 0.9925847253905372, 0.9902219195012175, 0.9977236143963074, 0.9990357461567065, 0.9996284525957074, 0.9968195421336111, 0.9743919548652683, 0.3105289343639125, 0.6814384948541412, 0.9997271359186377, 0.9944796163980841, 0.9975334503827888, 0.9916503053004754, 0.9916870800694211, 0.3094892581757901, 0.6436356275798711, 0.03783591205720511, 0.008927574755070868, 0.9655750474666958, 0.9857898502010279, 0.9767499291401481, 0.9661899035916139, 0.9960520132971833, 0.9818226680657081, 0.9806449427969468, 0.9990998856268182, 0.9905240743192911, 0.996863902871118, 0.9961774216241114, 0.995118672298306, 0.5607221052774315, 0.1661974107054991, 0.270264948250064, 0.9939490530776914, 0.9922187754781269, 0.998980757413997, 0.9988155835314136, 0.9995115116792965, 0.5287434085656868, 0.2895294249187855, 0.15037129020111703, 0.03133933199220221, 0.520801471130001, 0.4750167264152756, 0.9997415795213669, 0.9984733590045638, 0.9994649745643034, 0.6154736923553185, 0.9999266560311911, 0.0008565296857156617, 0.6967868993296908, 0.20224807203961065, 0.015310468132167455, 0.03576011437862888, 0.048929258296507176, 0.9957140133267579, 0.9998939151959776, 0.9978224566257182, 0.9934000323796667, 0.9982396411630999, 0.993062783373715, 0.9996245114355055, 0.8957076337234329, 0.10358523655305006, 0.8735601378840581, 0.12616619125618883, 0.9996516373960309, 0.9640547063872283, 0.9993403357231958, 0.9983860062546887, 0.9991118060468176, 0.669095551664335, 0.002177100493484821, 0.06458731464004969, 0.2637920097939108, 0.9979553171094483, 0.9986279424645182, 0.976166508276731, 0.022129719331210527, 0.9957701846873491, 0.9989747104802832, 0.9982755444143215, 0.9990384362184834, 0.9998639130943372, 0.9985668378284799, 0.995757187938199, 0.9865927535245846, 0.9990161085296735, 0.9967460956823555, 0.9991234840878415, 0.33382551905456354, 0.6636531277012879, 0.9995149036626501, 0.9848632424962859, 0.9877611550772237, 0.9934673661975472, 0.9394595765627014, 0.9952573837624675, 0.9778585755230109, 0.9807362239678991, 0.864411187120611, 0.12319561013293748, 0.011570858410275895, 0.5982899780493377, 0.39940786996150845, 0.9976196455794109, 0.9996176613331651, 0.9981134510518966, 0.12902329068317758, 0.554165609163812, 0.31515525101300756, 0.8779785897710387, 0.06891659898202776, 0.05192346498645928, 0.9934658885266233, 0.9979904644022912, 0.9924914035107752, 0.9971703946460886, 0.993268185134555, 0.993041187950949, 0.9982983628050699, 0.9752146962835372, 0.9937893092519201, 0.40031604211597444, 0.5979404173377846, 0.2700536677605962, 0.3955116708934716, 0.3317194659106536, 0.9975148641413446, 0.9930664183989439, 0.986477110774992, 0.9998165666666761, 0.9926148823692467, 0.9936322276569344, 0.9975926668057534, 0.9996187829941442, 0.9241791404730777, 0.9978071891894149, 0.9984716296640155, 0.7948906337570079, 0.20502874768183196, 0.9983228051926806, 0.9994419470598813, 0.9912525232345712, 0.9868179308282283, 0.04180717472892785, 0.9579068909765595, 0.8810072185907714, 0.11852908362566603, 0.9962379346429626, 0.9948395499520378, 0.9961120328520172, 0.9956955767679765, 0.9976087057434807, 0.9778137569909049, 0.9940554303604938, 0.9783621367023878, 0.9994406262860983, 0.99962379258683, 0.9885749778463155, 0.008259071736456685, 0.0030522656417339925, 0.9988872013364417, 0.08893313577120164, 0.9104341356916236, 0.9978496598149457, 0.9901006122471511, 0.9992237615771613, 0.15829214535780314, 0.8415865728189866, 0.9872208883782644, 0.9165771680029489, 0.07824753133190016, 0.005147863903414484, 0.8257317391466563, 0.0011841277330018971, 0.17248793977394303, 0.0007894184886679315, 0.9962359428478672, 0.9923669333698932, 0.9968091596132704, 0.9831335364000724, 0.9833258587233561, 0.6012697714620571, 0.39835457327770385, 0.9991756107969126, 0.9993592091095194, 0.9933629893792707, 0.9967408608918418, 0.9988771335241845, 0.25623421494211596, 0.5148800460248636, 0.22850062932485163, 0.21866705044423412, 0.7809537515865504, 0.9952019599741038, 0.518014004821227, 0.3345776240615905, 0.001291805498307299, 0.1459740213087248, 0.9702156839156905, 0.9924799006315032, 0.5196656204852584, 0.45190058201148636, 0.02806835913114822, 0.9944381714512097, 0.9959566069976159, 0.9144623895545132, 0.9775674397889882, 0.999628426283739, 0.48088490110034504, 0.5167403542525637, 0.9285595175377138, 0.0710761795818917, 0.9991166824685999, 0.9996593882981377, 0.969050423842838, 0.0299005740877173, 0.9916386522741236], \"Term\": [\"_\", \"accept\", \"access\", \"access\", \"access\", \"accomplish\", \"accord\", \"account\", \"action\", \"add\", \"administrator\", \"advice\", \"aggregate\", \"also\", \"also\", \"amp\", \"analysis\", \"analyze\", \"ansible\", \"ansible_galaxy\", \"api\", \"append\", \"area\", \"argument\", \"ask\", \"assume\", \"assume\", \"attribute\", \"automatic\", \"automatically\", \"avail\", \"available\", \"bam\", \"base\", \"base\", \"base\", \"basic\", \"batch\", \"bed\", \"bit\", \"bit\", \"bteq\", \"buffer\", \"building\", \"calculate\", \"call\", \"capacity\", \"case\", \"case\", \"cast\", \"cause\", \"cause\", \"cause\", \"change\", \"change\", \"channel\", \"check\", \"checkpoint\", \"class\", \"clause\", \"cleanup\", \"cli\", \"client\", \"cluster\", \"code\", \"code\", \"collection\", \"column\", \"com\", \"combine\", \"command\", \"command\", \"common\", \"communicate\", \"company\", \"compile\", \"complete\", \"component\", \"condition\", \"config\", \"configuration\", \"conflict\", \"connect\", \"connection\", \"connector\", \"constant\", \"constraint\", \"consume\", \"contain\", \"contain\", \"contain\", \"contain\", \"container\", \"context\", \"control\", \"conversion\", \"convert\", \"convert\", \"copy\", \"copy\", \"core\", \"correctly\", \"count\", \"country\", \"course\", \"create\", \"create\", \"csv\", \"current\", \"current_date\", \"custom\", \"customer\", \"customerid\", \"data\", \"data\", \"database\", \"database\", \"date\", \"datetime\", \"datum\", \"datum\", \"datum\", \"day\", \"dbc\", \"dbi\", \"dd\", \"debug\", \"decimal\", \"def\", \"define\", \"define\", \"delimiter\", \"dependency\", \"deploy\", \"detect\", \"determine\", \"dimension\", \"directory\", \"do\", \"docker\", \"document\", \"documentation\", \"dont_know\", \"dont_know\", \"dont_know\", \"download\", \"driver\", \"drop\", \"dynamically\", \"elegant\", \"else\", \"else_end\", \"empty\", \"enable\", \"encounter\", \"end\", \"end\", \"end\", \"end_date\", \"engine\", \"enter\", \"environment\", \"equivalent\", \"error\", \"error\", \"error\", \"error\", \"errorfactory_errorfactory\", \"evaluation\", \"even\", \"event\", \"example\", \"example\", \"example\", \"example\", \"example\", \"execute\", \"execute\", \"execute\", \"execute\", \"execution\", \"exist\", \"expand\", \"expect\", \"expect\", \"expected_somethe\", \"explain\", \"expression\", \"extension\", \"extension\", \"extract\", \"fail\", \"fail\", \"fail\", \"failure\", \"false\", \"fastq\", \"fastq_gz\", \"fastqc\", \"fetch\", \"field\", \"file\", \"filename\", \"filter\", \"filter\", \"final\", \"find\", \"first\", \"first\", \"first\", \"first\", \"fix\", \"flag\", \"float\", \"folder\", \"follow\", \"follow\", \"follow\", \"follow\", \"form\", \"form\", \"format\", \"format\", \"format\", \"format\", \"forward\", \"frame\", \"functionality\", \"galaxy\", \"genome\", \"get\", \"get\", \"get\", \"get\", \"google\", \"group\", \"gt\", \"guess\", \"guess\", \"guess\", \"hadoop\", \"hadoop_mapre\", \"hand\", \"help\", \"help\", \"help\", \"hive\", \"hive\", \"home\", \"host\", \"hostname\", \"hour\", \"however\", \"however\", \"however\", \"html\", \"https\", \"huge\", \"image\", \"import\", \"import\", \"import\", \"important\", \"improve\", \"incorrect\", \"info\", \"info\", \"info\", \"information\", \"input\", \"insert\", \"instal\", \"instal\", \"install\", \"instruction\", \"integer\", \"integration\", \"internal\", \"internet\", \"interval\", \"ip\", \"ip_address\", \"item\", \"jar\", \"java\", \"jdbc\", \"jdbc_teradriver\", \"job\", \"join\", \"join\", \"keep\", \"kind\", \"kind\", \"label\", \"language\", \"last\", \"last\", \"last\", \"late\", \"late\", \"level\", \"level\", \"lib\", \"lib_python\", \"library\", \"limit\", \"line\", \"line\", \"line\", \"list\", \"list\", \"list\", \"local\", \"local\", \"local\", \"location\", \"log\", \"log\", \"log\", \"logic\", \"long\", \"looks_like\", \"loop\", \"luigi\", \"m\", \"m\", \"machine\", \"machine\", \"main\", \"main\", \"make\", \"make\", \"manager\", \"map\", \"mark\", \"matching\", \"max\", \"maximum\", \"maybe\", \"mclient\", \"mean\", \"memory\", \"mention\", \"merge\", \"message\", \"message\", \"message\", \"min\", \"minute\", \"minute\", \"mismatch\", \"miss\", \"miss\", \"mistake\", \"mode\", \"model\", \"modify\", \"module\", \"module\", \"monetdb\", \"month\", \"move\", \"much\", \"multiple\", \"multiple\", \"multiple\", \"mysql\", \"name\", \"name\", \"name\", \"name\", \"name\", \"necessary\", \"need\", \"need\", \"need\", \"need\", \"net\", \"network\", \"never\", \"new\", \"next\", \"nextflow\", \"node\", \"none\", \"normal\", \"normal\", \"normal\", \"note\", \"null\", \"number\", \"number\", \"number\", \"occur\", \"odbc\", \"open\", \"open\", \"operation\", \"operator\", \"option\", \"order\", \"order\", \"order\", \"oreplace\", \"org\", \"org_apache\", \"os_path\", \"other\", \"output\", \"package\", \"pair\", \"panel\", \"parallel\", \"param\", \"parameter\", \"parameter\", \"parameter\", \"parse\", \"part\", \"part\", \"part\", \"partial\", \"partition\", \"pass\", \"path\", \"performance\", \"period\", \"perl\", \"pipeline\", \"place\", \"play\", \"please_let\", \"plugin\", \"pop\", \"populated\", \"port\", \"possible\", \"possible\", \"prefix\", \"pretty\", \"previous\", \"price\", \"print\", \"proceed\", \"process\", \"process\", \"processing\", \"produce\", \"produce\", \"program\", \"project\", \"properly\", \"property\", \"provide\", \"provider\", \"public\", \"pull\", \"python\", \"query\", \"queue\", \"quiet\", \"quite\", \"quite\", \"quot\", \"random\", \"range\", \"rapidminer\", \"reach\", \"read\", \"read\", \"read\", \"read\", \"realize\", \"recent_call\", \"recognize\", \"recommend\", \"record\", \"reduce\", \"refer\", \"reference\", \"reflect\", \"regular\", \"release\", \"relevant\", \"remove\", \"remove\", \"remove\", \"replicate\", \"repository\", \"request\", \"require\", \"resource\", \"result\", \"result\", \"result\", \"result\", \"retrieve\", \"retrieve\", \"return\", \"role\", \"row\", \"rowname\", \"rule\", \"run\", \"run\", \"run\", \"run\", \"run\", \"run\", \"runtime\", \"sample\", \"save\", \"scala\", \"scale\", \"score\", \"script\", \"see\", \"see\", \"select\", \"select\", \"self\", \"sequential\", \"server\", \"service\", \"session\", \"set\", \"set\", \"set\", \"set\", \"share\", \"shell\", \"single\", \"single\", \"site\", \"site_package\", \"size\", \"snakefile\", \"snakemake\", \"software\", \"somehow\", \"sorry\", \"spark\", \"specific\", \"specify\", \"split\", \"split\", \"sql\", \"sqoop\", \"ss\", \"stack\", \"stack_trace\", \"stage\", \"stand\", \"standard\", \"start\", \"start\", \"start\", \"statement\", \"statement\", \"status\", \"step\", \"stop\", \"store\", \"store\", \"store\", \"string\", \"string\", \"string\", \"strip\", \"structure\", \"sub\", \"submit\", \"substre\", \"suggest\", \"sum\", \"sun_reflect\", \"supply\", \"suppose\", \"suppose\", \"sure\", \"sure\", \"sure\", \"swarm\", \"sys\", \"tab\", \"table\", \"table_name\", \"tag\", \"target\", \"task\", \"tdnetworkioif\", \"temp\", \"template\", \"teradata\", \"teradata\", \"teradatasql\", \"test\", \"therefore\", \"third\", \"thread\", \"thread\", \"time\", \"time\", \"timestamp\", \"title\", \"top\", \"touch\", \"traceback\", \"training\", \"transaction\", \"transport\", \"trim\", \"true\", \"try\", \"try\", \"try\", \"txt\", \"type\", \"type\", \"understand\", \"unknown_source\", \"update\", \"url\", \"url\", \"usage\", \"use\", \"use\", \"use\", \"user\", \"user\", \"user\", \"user\", \"username\", \"usr_bin\", \"usr_local\", \"utility\", \"validation\", \"value\", \"value\", \"varchar\", \"variable\", \"vary\", \"vcf\", \"ve\", \"version\", \"version\", \"version\", \"view\", \"view\", \"wait\", \"want\", \"want\", \"want\", \"want\", \"warehouse\", \"warning\", \"way\", \"way\", \"way\", \"week\", \"weight\", \"white\", \"width\", \"wildcard\", \"window\", \"window\", \"work\", \"work\", \"worker\", \"workflow\", \"write\", \"write\", \"xml\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [15, 12, 10, 3, 2, 6, 8, 14, 5, 9, 4, 1, 11, 13, 7]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el168631405316247652322282582577\", ldavis_el168631405316247652322282582577_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el168631405316247652322282582577\", ldavis_el168631405316247652322282582577_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el168631405316247652322282582577\", ldavis_el168631405316247652322282582577_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "14    -0.277158  0.139558       1        1  27.329699\n",
       "11    -0.276118  0.045182       2        1  19.661416\n",
       "9     -0.192942 -0.234792       3        1  11.601036\n",
       "2     -0.138394  0.189810       4        1   8.897018\n",
       "1     -0.043632 -0.286090       5        1   5.925223\n",
       "5     -0.018262 -0.140249       6        1   5.006788\n",
       "7     -0.050893  0.083484       7        1   4.549239\n",
       "13    -0.038963  0.098104       8        1   4.175832\n",
       "4      0.073478  0.037026       9        1   3.261935\n",
       "8      0.057814 -0.038169      10        1   3.115456\n",
       "3      0.120515 -0.028487      11        1   2.359182\n",
       "0      0.146472  0.039965      12        1   1.971950\n",
       "10     0.165100  0.002387      13        1   1.548905\n",
       "12     0.235845  0.046080      14        1   0.304118\n",
       "6      0.237137  0.046191      15        1   0.292203, topic_info=                       Term          Freq         Total Category  logprob  \\\n",
       "18                     file  14097.000000  14097.000000  Default  30.0000   \n",
       "2910              snakemake   8803.000000   8803.000000  Default  29.0000   \n",
       "1093                   rule   8613.000000   8613.000000  Default  28.0000   \n",
       "202                  output   8603.000000   8603.000000  Default  27.0000   \n",
       "111                  sample   5699.000000   5699.000000  Default  26.0000   \n",
       "...                     ...           ...           ...      ...      ...   \n",
       "24307          easyblock_py      0.039703      1.078171  Topic15 -10.6187   \n",
       "3580   environment_variable      0.039693      1.079653  Topic15 -10.6190   \n",
       "3248              dataframe      0.039694      1.087974  Topic15 -10.6190   \n",
       "24341         libjpeg_turbo      0.039681      1.077120  Topic15 -10.6193   \n",
       "1427                   root      0.039678      1.080093  Topic15 -10.6194   \n",
       "\n",
       "       loglift  \n",
       "18     30.0000  \n",
       "2910   29.0000  \n",
       "1093   28.0000  \n",
       "202    27.0000  \n",
       "111    26.0000  \n",
       "...        ...  \n",
       "24307   2.5339  \n",
       "3580    2.5323  \n",
       "3248    2.5246  \n",
       "24341   2.5343  \n",
       "1427    2.5315  \n",
       "\n",
       "[641 rows x 6 columns], token_table=      Topic      Freq      Term\n",
       "term                           \n",
       "866       2  0.997661         _\n",
       "787      10  0.994255    accept\n",
       "64        2  0.785355    access\n",
       "64        6  0.028118    access\n",
       "64        7  0.186158    access\n",
       "...     ...       ...       ...\n",
       "3494     11  0.999117    worker\n",
       "1095     10  0.999659  workflow\n",
       "175       2  0.969050     write\n",
       "175       3  0.029901     write\n",
       "699       6  0.991639       xml\n",
       "\n",
       "[673 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[15, 12, 10, 3, 2, 6, 8, 14, 5, 9, 4, 1, 11, 13, 7])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d52ecece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=12, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "168b9743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -8.92758251942754\n",
      "\n",
      "Coherence Score:  0.4304558381590035\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2996d87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el168631405316247688164684389898\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el168631405316247688164684389898_data = {\"mdsDat\": {\"x\": [0.2421836496782575, 0.212187861393287, 0.13255740214680486, 0.21719036485626894, 0.19377874211678983, 0.0037477600322650606, -0.05263789541638343, -0.13952674938860213, -0.19263866082254932, -0.15196146022390036, -0.2253893701781644, -0.23949164419407318], \"y\": [0.16538935933039195, -0.25664619206142275, 0.25940484684300036, -0.19819601805178474, 0.06063736474302218, -0.07395074813796625, 0.12421688187666698, 0.07462483208374442, -0.04398668939737709, -0.07206518358464699, -0.0231890931208482, -0.016239360522779748], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [36.092097649579294, 14.650906647931619, 12.357408639055226, 10.769401850529361, 9.375638585534848, 5.023238744653347, 3.8087857003694015, 2.895834839790957, 1.9445418041813747, 1.8952951365691244, 0.8703929106031809, 0.31645749120227035]}, \"tinfo\": {\"Term\": [\"file\", \"snakemake\", \"rule\", \"output\", \"process\", \"input\", \"job\", \"table\", \"quot\", \"teradata\", \"error\", \"value\", \"sample\", \"run\", \"nextflow\", \"column\", \"use\", \"date\", \"select\", \"would_require\", \"group\", \"user\", \"set\", \"query\", \"main\", \"java\", \"row\", \"name\", \"param\", \"directory\", \"self\", \"expand\", \"pipeline\", \"task\", \"also\", \"type\", \"write\", \"change\", \"different\", \"new\", \"environment\", \"add\", \"pass\", \"seem\", \"able\", \"map\", \"know\", \"question\", \"dataset\", \"possible\", \"expect\", \"ve\", \"python\", \"key\", \"image\", \"solution\", \"think\", \"instead\", \"batch\", \"text\", \"m\", \"thread\", \"make\", \"use\", \"start\", \"function\", \"see\", \"work\", \"multiple\", \"way\", \"step\", \"want\", \"however\", \"try\", \"datum\", \"get\", \"code\", \"time\", \"create\", \"need\", \"run\", \"example\", \"return\", \"list\", \"find\", \"import\", \"follow\", \"snakemake\", \"rule\", \"input\", \"sample\", \"param\", \"wildcard\", \"workflow\", \"fastq_gz\", \"fastq\", \"snakefile\", \"bam\", \"genome\", \"template\", \"directive\", \"fastqc\", \"yaml\", \"vcf\", \"bed\", \"touch\", \"smk\", \"rerun\", \"conda_environment\", \"barcode\", \"optional\", \"checkpoint\", \"regular\", \"replicate\", \"config_yaml\", \"ref\", \"glob_wildcard\", \"output\", \"trim\", \"config\", \"log\", \"folder\", \"file\", \"read\", \"path\", \"result\", \"script\", \"run\", \"job\", \"parameter\", \"name\", \"list\", \"error\", \"line\", \"table\", \"column\", \"query\", \"row\", \"sql\", \"insert\", \"tmp\", \"varchar\", \"monetdb\", \"field\", \"sum\", \"int\", \"final\", \"condition\", \"timestamp\", \"character\", \"traceback\", \"min\", \"decimal\", \"interval\", \"summary\", \"directly\", \"duplicate\", \"drop\", \"temporary\", \"temp\", \"individual\", \"maximum\", \"sub\", \"region\", \"null\", \"select\", \"teradata\", \"cast\", \"value\", \"join\", \"view\", \"convert\", \"format\", \"take\", \"name\", \"datum\", \"result\", \"end\", \"error\", \"create\", \"set\", \"miss\", \"module\", \"site_package\", \"package\", \"home\", \"lib_python\", \"shell\", \"project\", \"ansible\", \"install\", \"download\", \"merge\", \"session\", \"argument\", \"reason\", \"singularity\", \"conda\", \"zip\", \"prefix\", \"software\", \"executor\", \"root\", \"skip\", \"dir\", \"ansible_galaxy\", \"miniconda\", \"enable\", \"library\", \"https\", \"master\", \"core\", \"command\", \"txt\", \"instal\", \"line\", \"file\", \"version\", \"error\", \"base\", \"script\", \"path\", \"require\", \"import\", \"message\", \"fail\", \"run\", \"execute\", \"quot\", \"directory\", \"channel\", \"variable\", \"connect\", \"service\", \"server\", \"connection\", \"host\", \"profile\", \"pool\", \"env\", \"driver\", \"dataframe\", \"loop\", \"mode\", \"manager\", \"port\", \"deploy\", \"machine\", \"client\", \"locally\", \"username\", \"exception\", \"teradatasql\", \"procedure\", \"context\", \"swarm\", \"odbc\", \"installation\", \"local\", \"user\", \"database\", \"docker\", \"access\", \"error\", \"test\", \"window\", \"teradata\", \"create\", \"fail\", \"name\", \"version\", \"try\", \"get\", \"use\", \"log\", \"execute\", \"would_require\", \"conflict\", \"container\", \"def\", \"false\", \"worker\", \"role\", \"class\", \"luigi\", \"yml\", \"scheduler\", \"block\", \"url\", \"warn\", \"page\", \"environment_variable\", \"foo\", \"airflow\", \"def_require\", \"argo\", \"property\", \"partial\", \"event\", \"org_apache\", \"completely\", \"argo_workflow\", \"public\", \"static\", \"clear\", \"http\", \"info\", \"resource\", \"true\", \"name\", \"run\", \"complete\", \"com\", \"return\", \"require\", \"default\", \"dependency\", \"nextflow\", \"debug_nextflow\", \"usr_local\", \"flag\", \"attribute\", \"location\", \"flow\", \"channel_frompath\", \"runtime\", \"nf\", \"false_false\", \"label\", \"lot\", \"dsl\", \"groovy\", \"overlap\", \"mode_copy\", \"stage\", \"emit\", \"cli\", \"accord\", \"rapidminer\", \"nextflow_enable\", \"component\", \"testing\", \"subprocess\", \"cf\", \"flatten\", \"pipe\", \"enough\", \"process\", \"true\", \"value\", \"set\", \"parameter\", \"name\", \"example\", \"datum\", \"date\", \"group\", \"partition\", \"item\", \"month\", \"day\", \"apr\", \"wait\", \"extract\", \"operator\", \"running\", \"year\", \"calculate\", \"desire\", \"aggregate\", \"max\", \"forward\", \"nest\", \"overwrite\", \"divide\", \"exact\", \"patient\", \"course\", \"supply\", \"cant_figure\", \"period\", \"third\", \"direct\", \"dd\", \"rank\", \"count\", \"order\", \"case\", \"transaction\", \"total\", \"last\", \"select\", \"end\", \"store\", \"java\", \"execution\", \"spark\", \"util\", \"assign\", \"jdbc\", \"upload\", \"pop\", \"play\", \"mark\", \"pyspark\", \"executor_executor\", \"widget\", \"re\", \"unknown_source\", \"country\", \"ui_internal\", \"area\", \"realize\", \"classloader\", \"execution_datasource\", \"widget_sendevent\", \"errorfactory_errorfactory\", \"database_terajdbc\", \"hadoop_mapre\", \"automatic\", \"sql_sqlexception\", \"avail\", \"dispatch\", \"sqlstate_hy\", \"com\", \"internal\", \"action\", \"main\", \"cause\", \"run\", \"cluster\", \"model\", \"memory\", \"sbatch\", \"panda\", \"wrapper\", \"os_path\", \"tsv\", \"raw\", \"trigger\", \"limit\", \"plot\", \"token\", \"sqlalchemy\", \"hpc\", \"pickle\", \"per_second\", \"average\", \"resources_mem_mb\", \"storage\", \"submission\", \"trouble\", \"couple\", \"speed\", \"metadata\", \"steps_done\", \"slurm\", \"preprocesse\", \"analyze\", \"incomplete\", \"job\", \"jobid\", \"time\", \"take\", \"count\", \"submit\", \"record\", \"range\", \"append\", \"section\", \"perl\", \"setup\", \"certain\", \"sys\", \"account\", \"turn\", \"destination\", \"datetime\", \"ip\", \"future\", \"company\", \"comparison\", \"efficient\", \"today\", \"express\", \"dozen\", \"sequential\", \"populated\", \"mongo\", \"association\", \"fp_growth\", \"cube\", \"payment\", \"association_rule\", \"update\", \"scale\", \"share\", \"galaxy\", \"customer\", \"entire\", \"reduce\", \"hadoop\", \"cust\", \"buy\", \"meteor\", \"tera\", \"sqlserver\", \"meteor_app\", \"rdbms\", \"uvicorn\", \"item_cust\", \"home_sibyl\", \"adult\", \"unstable\", \"invoice_dt\", \"microservice\", \"family\", \"featureset\", \"nbconvert\", \"getprecision\", \"getmetadata\", \"sar\", \"cust_nm\", \"entry_point\", \"pointplot\", \"cdirs_user\", \"pertoldi_legend\"], \"Freq\": [12004.0, 7703.0, 7537.0, 7553.0, 4157.0, 6439.0, 2987.0, 5170.0, 4183.0, 5466.0, 6915.0, 3544.0, 4175.0, 7766.0, 2188.0, 3184.0, 10353.0, 1540.0, 2859.0, 1642.0, 1384.0, 2163.0, 2616.0, 2271.0, 1380.0, 1195.0, 2210.0, 3909.0, 2321.0, 1846.0, 2297.9628402087014, 2049.9625051697462, 2032.5638125600453, 1923.5909007970095, 1547.0933157346167, 1349.9394481678569, 1260.6417859810788, 1233.889580131146, 1218.4036137413575, 1167.6368042842773, 1111.5826824007247, 1232.1020841553943, 1032.0254304724924, 864.1018520957492, 822.7326339365028, 804.0253272988929, 798.4149936833435, 778.3700300603793, 750.2747041607495, 740.6831974251827, 733.5051044218835, 696.7121598531356, 692.1120262123886, 664.4427726386231, 662.2946677234031, 631.1043524486956, 601.918608244857, 579.7664893428133, 578.3904041356303, 567.7414909461982, 2565.8696478548936, 2027.7688296244128, 1190.7761969680275, 9549.072640096521, 1223.35240295002, 1284.1184878683407, 1259.2438952093833, 3154.6542556415898, 944.5323538473583, 2007.918388446331, 1487.8477370321252, 2372.180203552771, 1254.5193251030553, 3878.809675637567, 3498.8657841392965, 3556.746250456148, 1725.6905078224777, 1694.1027896364903, 2895.432506285814, 1670.8336841942469, 3764.402462332243, 1385.1413803057558, 1475.3393741733353, 1328.641435017562, 1312.1186070312651, 1324.6652580333107, 1357.491644346479, 7702.128129614917, 7536.255318565719, 6438.170249402323, 4174.942073538328, 2320.2241718516016, 1913.5115031261328, 1870.0963367197085, 1208.4152076710243, 956.3746986317825, 925.7894009461412, 848.2060840141069, 390.29945435849834, 352.50265792278503, 317.76641555194806, 302.4224959083354, 223.25696775008456, 221.22958192194488, 220.35621557854557, 213.54915910104293, 212.16714092398192, 194.96697188858866, 191.66983293074145, 185.6719453096575, 175.1401078013059, 171.31976027227086, 167.7978448166075, 163.49445242317879, 159.40859606697376, 158.11878003795448, 157.43809144888098, 7483.561748313189, 478.9947175342045, 438.18877051300404, 1427.2151791202425, 482.77526148406645, 5713.3575353150945, 1307.711794031867, 1205.7093224506655, 1014.0318889317106, 946.8997788290742, 1527.132473867162, 670.8220778680449, 401.3767146260335, 660.499910451509, 420.2156349241543, 467.76722718641884, 362.21772598650443, 5170.044486603602, 3183.687390393602, 2270.350890357416, 2209.5582577699356, 1346.5882101912907, 907.5895735871884, 877.0253942713981, 725.7989086367419, 691.1934190716186, 622.7228277772941, 606.7180958029505, 529.2334840418991, 513.3338503063135, 504.5145509451646, 466.40318621018446, 444.0724483216338, 427.90365822040667, 338.0130738604028, 329.96303477496883, 293.54906894505024, 287.23729142033113, 278.2751326339404, 276.6581481099181, 274.37326060159023, 262.92999042757236, 253.6082370740414, 251.72473403186703, 230.93975878512592, 220.75784333270136, 212.7673876266179, 557.40748576715, 2688.842804657518, 4324.707026652371, 533.8802842052442, 2107.3363918623145, 829.1782300487846, 578.4399870301197, 411.235659478557, 516.2501585688213, 514.9205875124578, 924.0151461512887, 864.7343678751638, 728.1660010650108, 501.50794836374826, 687.290236109001, 603.9826639434402, 476.9463908469243, 1688.6597981421799, 1262.0585088920798, 1122.725987780693, 712.1280455954668, 707.3990113406769, 671.2284977366149, 664.0921456295824, 615.7342928154619, 603.899000713298, 586.079687998472, 513.2188660499122, 510.858863061466, 510.0191774868818, 413.03092375334427, 384.5166684569717, 374.46356955780965, 369.39107063213885, 350.8375694418649, 334.41228971055574, 328.04972232246627, 327.1651461438913, 324.9619398212773, 305.99494907630583, 304.8555748326218, 270.7451804925195, 259.15080965795556, 248.90530255236544, 248.14291464101532, 244.8794151014867, 242.11408701952814, 767.7400082400616, 1951.927725441868, 843.2361956529434, 1129.3677908873365, 1767.8078125044067, 6290.1345176593295, 849.0705809189699, 2785.0387074326895, 1050.7025107420773, 1212.519233465919, 975.8356834668327, 510.5667815524466, 675.5391660520404, 496.4636640645685, 574.1066310632044, 814.3604748600052, 556.8033234413017, 4182.719281477275, 1845.5839142175453, 1485.5211338380836, 1121.5550166765925, 949.458968687449, 868.4017061168753, 828.3882830711395, 574.1001947060863, 513.557477224948, 513.3107653030987, 473.81968258197065, 455.06312368256226, 423.9648554749255, 386.512797057357, 361.2556579933353, 348.5329158280286, 339.3901164791495, 337.0549650929927, 335.6541896256038, 296.91088418972697, 292.8339029694871, 273.9355029091099, 270.9745959650275, 255.97662163155488, 239.525327285686, 232.3549375955878, 230.33357369312037, 217.86055153995886, 210.61990838617578, 208.70353653855236, 1043.3019735079945, 2117.9602802604627, 1360.5786478702662, 636.7282465260653, 633.3158106732925, 2974.291975547449, 862.0512393615445, 322.718212672386, 1141.2379218343822, 862.9207715274016, 504.25898494446335, 598.5264701198288, 443.6470813865194, 519.9734912381781, 503.32832062097987, 520.6512333888788, 412.48656511679167, 398.61288879020486, 1641.9138393941728, 1093.414586510262, 1069.3301250495308, 995.8351374453771, 825.8403060232703, 638.3985630338277, 635.5826784633974, 620.1812252816574, 415.3675129845082, 320.8025467777971, 303.09002299787494, 264.19322262178173, 254.68426381991642, 245.51203829284532, 223.86952994060886, 199.16568357134292, 192.78329608534108, 188.5975512312065, 168.8301980339176, 163.30770994039622, 159.63942540235632, 144.38403554245272, 140.65949881924573, 135.7057452725224, 129.8685152852842, 128.99181042251234, 128.78310025859986, 128.6875313312584, 121.52441082520055, 120.4048129780473, 1255.814333153199, 711.8785257936993, 741.0106070861068, 1070.6440970716317, 1391.971447623153, 351.04363314219364, 383.14049573175856, 473.8318075349433, 281.515204428529, 237.81531486117854, 194.75307136690986, 2187.796063437272, 581.5908163013821, 380.03315455482453, 366.3985021035563, 330.11533837169213, 292.6901030409978, 267.0793878982562, 256.72862491186225, 247.39747579194423, 240.52269514979434, 223.0954007811286, 220.0867406377727, 216.89899616218273, 197.672659144497, 180.92405615775374, 179.9869480159897, 176.3200356776502, 149.89149682839437, 144.67513841172013, 127.5878992911321, 125.44477071541435, 122.68805262321206, 118.75479028227024, 118.34655842124978, 106.77111956171444, 106.56431322713753, 103.75106708465489, 103.42841569585107, 92.46495013936824, 89.8790086481611, 4055.5916434579326, 769.7531040242877, 1436.1926420559962, 751.693929333468, 266.1373719864364, 286.2565092459454, 216.2079466604814, 158.17804975836012, 1540.0351148498385, 1384.1395663001717, 633.6529499603236, 491.48474640004423, 449.5326949534152, 429.62031218527375, 350.11209466151575, 348.5185333910784, 344.11980077085997, 289.87868554242334, 272.2331339408845, 212.20396261147414, 185.7612454801004, 179.26785044855544, 177.54279115152326, 165.1057003034247, 145.9030771972329, 143.86135866373647, 135.91110036729228, 92.60691203284182, 92.0041115919543, 87.54136827992967, 86.77856733420427, 84.22137653645633, 83.77787977325721, 83.36113218127993, 81.3324585517323, 78.12705366414008, 69.87760925932386, 66.26031209150557, 812.5502082495982, 623.9718179811698, 373.17923122552406, 108.47602644741345, 153.51349364215386, 149.70219380170926, 170.10097690571544, 133.99954614821206, 95.98695079541092, 1194.5836828530992, 713.1340527441099, 454.1610091063401, 300.8719363684359, 299.490223594334, 238.0429780808478, 225.1501720103402, 127.7173941310628, 117.82495053235525, 116.57156769058255, 115.58998911464094, 80.47125988307987, 75.43122832961812, 68.2473080513414, 66.28679739614152, 62.14251943682608, 55.29816078338136, 54.558002075037635, 51.12483676826193, 50.14920023145153, 47.43546441945426, 45.215758092888, 44.16730264638576, 43.00649184354213, 42.97805961396508, 42.41649414948385, 40.05891339589584, 37.959625184102606, 37.28602888616227, 34.41106306234469, 698.4023843013106, 136.32279507276664, 231.71012283485163, 521.0459700979031, 85.02705801392895, 108.10764866282432, 710.2218232232719, 330.5408760992097, 320.30935391740445, 304.3948711024294, 290.1177551042873, 235.3697929642831, 213.92562707396272, 204.17533836107359, 189.91652386810816, 187.22371140547156, 172.20872893495564, 143.79836873901243, 140.4389146328299, 125.8587939718114, 110.24289246556765, 109.44344231098363, 87.61827949135343, 86.87205670471887, 84.67981885151733, 78.52182332996277, 78.2729385521693, 78.02698633067153, 66.88608726865537, 65.4501056639296, 62.25689513370857, 57.06180790461265, 51.310559902850905, 50.45935356240559, 49.90807711427244, 48.345774911517395, 2315.5240106715637, 104.49911316487344, 131.81888607886734, 99.40525307132317, 71.82780751852923, 422.1750300298139, 326.1502260073995, 277.3760626911659, 275.5915150867367, 219.3808599425235, 178.02556818615093, 149.55747568708645, 141.66942395114518, 126.57388982286191, 119.84270417705696, 94.11704065759461, 93.44114663053594, 68.2792100785838, 60.19417797821925, 46.80476955042596, 43.37624012056566, 35.134744813717376, 33.49169534909926, 32.968073409024285, 18.139317211583126, 15.949623926670103, 14.503686573111887, 9.24758356458892, 7.4115010674678565, 6.0150503405159315, 5.085230435923578, 4.868308472686171, 3.0526357253551, 1.360929666092404, 333.63397546653925, 182.66493306614916, 162.7899122657313, 92.98956982995753, 92.34358029811352, 49.39722452164835, 29.837475461122054, 23.778009871676343, 14.312750394825073, 8.369614931990574, 5.594254803684504, 3.9763033676114024, 3.04043035667485, 3.0170682438733167, 1.9295677986368034, 0.05855787512344087, 0.056548269668388156, 0.05515486432369311, 0.05174674874866879, 0.0515255961941905, 0.05120430240591904, 0.05058203477757914, 0.05178870641914369, 0.04891485227463056, 0.0480529480715597, 0.04770442241818508, 0.04770438083793534, 0.047449637838623424, 0.04764727836111048, 0.04744963144166192, 0.04797228238705162, 0.051701234367600185, 0.051701234367600185], \"Total\": [12004.0, 7703.0, 7537.0, 7553.0, 4157.0, 6439.0, 2987.0, 5170.0, 4183.0, 5466.0, 6915.0, 3544.0, 4175.0, 7766.0, 2188.0, 3184.0, 10353.0, 1540.0, 2859.0, 1642.0, 1384.0, 2163.0, 2616.0, 2271.0, 1380.0, 1195.0, 2210.0, 3909.0, 2321.0, 1846.0, 2298.832501727944, 2050.8321581768814, 2033.4334774614867, 1924.460571845852, 1547.9629816797706, 1350.8091129843763, 1261.511450662323, 1234.759245964735, 1219.2732715140467, 1168.506463277088, 1112.4523773036383, 1233.1224427208335, 1032.895100739547, 864.9715115291077, 823.6023000279264, 804.8949956044519, 799.2846575134594, 779.2396884774766, 751.1443676681445, 741.5528519008385, 734.3747726144146, 697.5818166087612, 692.9816856572635, 665.3124365257057, 663.1643358728844, 631.9740112459243, 602.7882638377267, 580.6361485741122, 579.2600692985363, 568.6111512902384, 2580.718425556579, 2054.427170029556, 1201.0373887260937, 10353.674021053981, 1248.3926285895297, 1315.1352163025606, 1298.1903678048602, 3468.2740131191854, 969.6548576962728, 2216.2290819267364, 1597.9757812504733, 2685.607059652764, 1345.7817311161232, 5015.265415969346, 4522.438708310848, 4621.491254763855, 1981.069705301376, 1965.1712374607457, 4387.80124236755, 2083.791085337144, 7766.810975654315, 1617.9776648494956, 2141.3521816692864, 1774.112053293091, 1868.3166998524753, 2001.0034193854715, 2560.9071750033117, 7703.027337352826, 7537.154516673673, 6439.069448990715, 4175.841267705563, 2321.1233837921222, 1914.4106958975924, 1870.995573477697, 1209.3143959543174, 957.2738940822148, 926.688597664493, 849.1052764024155, 391.1986498079158, 353.4019610166933, 318.6656502712949, 303.32170216314887, 224.15629895375614, 222.1287763626866, 221.25542689653398, 214.4483595437201, 213.06634069683648, 195.8661892545729, 192.569096780368, 186.57114005715377, 176.03935900530823, 172.21895650514278, 168.69715262178264, 164.39369716057706, 160.30778747954326, 159.0179935210042, 158.33727981483543, 7553.137702055292, 564.9308421227524, 518.6069034386863, 2076.010699104352, 582.9967526818194, 12004.320591137135, 2045.5094607509839, 2182.3735447139743, 3045.233625529296, 2965.1160069523107, 7766.810975654315, 2987.1846867553954, 939.5558529115145, 3909.3790822947717, 1774.112053293091, 6915.0088438833045, 2130.8540741467195, 5170.867967475346, 3184.5108740320925, 2271.1743734239685, 2210.381741825769, 1347.4116943543115, 908.4130548643327, 877.8489605636071, 726.6223875674856, 692.0169057344023, 623.5463270568044, 607.5415911995226, 530.0569890503928, 514.1573871332503, 505.3380726087418, 467.22667158627837, 444.89593586056645, 428.7272696663869, 338.8366524189731, 330.7865178027533, 294.37261970060143, 288.0608301311465, 279.09867427949933, 277.4816470445968, 275.19680338356676, 263.7535818086983, 254.43175293408441, 252.54827639122263, 231.76351363185694, 221.5813516941926, 213.5908940015408, 562.6664307890187, 2859.6698917904105, 5466.688139101204, 625.4064360506045, 3544.2709304058694, 1159.8079198081184, 731.9518582529201, 477.7987116747668, 908.8457395306523, 1048.3068115209778, 3909.3790822947717, 4522.438708310848, 3045.233625529296, 1069.4129206599916, 6915.0088438833045, 4387.80124236755, 2616.383943680257, 1689.5428500365347, 1262.9388838296309, 1123.6063557180792, 713.0084193432673, 708.2793892921774, 672.1088644985572, 664.9725311205278, 616.6146739214524, 604.7793656345578, 586.9600573409915, 514.0992474430301, 511.7392512021373, 510.89957690679256, 413.91130619250777, 385.3970782791628, 375.3439549436362, 370.27145033691795, 351.71796489114615, 335.29266230760373, 328.930118202795, 328.04559635458656, 325.84233834813733, 306.87534090170567, 305.7359558825529, 271.6255438615383, 260.0311884309811, 249.78568392969422, 249.02331142093388, 245.75980014038936, 242.99456303544926, 774.5738548695437, 2078.8272424863603, 890.8803763349554, 1226.6260375938077, 2130.8540741467195, 12004.320591137135, 1293.517751613577, 6915.0088438833045, 1903.029273560549, 2965.1160069523107, 2182.3735447139743, 792.8821637870675, 2001.0034193854715, 934.3761023967293, 1685.2983008963415, 7766.810975654315, 1791.4352615416524, 4183.590092126126, 1846.454690013385, 1486.3919373702915, 1122.4257869909982, 950.3297124367515, 869.2724551385974, 829.2590305859503, 574.9709376592901, 514.4282276400797, 514.181580446452, 474.69047671804753, 455.9339103602064, 424.8356067953634, 387.3835783422674, 362.12644190743043, 349.40368868535927, 340.2608784601016, 337.925707991814, 336.5249313701203, 297.78163294270803, 293.7046567610696, 274.80628727627175, 271.84536870172275, 256.8473870365997, 240.39607767026715, 233.22569559807565, 231.20433371034528, 218.73128954469715, 211.49064260804101, 209.57429510994032, 1053.5996016147528, 2163.5406675286686, 1655.8194880049878, 767.1405146507511, 853.847903904712, 6915.0088438833045, 1765.955125042095, 385.66105238269625, 5466.688139101204, 4387.80124236755, 1685.2983008963415, 3909.3790822947717, 1293.517751613577, 5015.265415969346, 4621.491254763855, 10353.674021053981, 2076.010699104352, 1791.4352615416524, 1642.7851255321286, 1094.2855536751033, 1070.200961715247, 996.7059813010284, 826.7111520957749, 639.2693901217802, 636.453493653806, 621.0520526533984, 416.2383267496962, 321.6733743943458, 303.96087431787055, 265.06410410687664, 255.55512349746377, 246.38288244410163, 224.74038340867676, 200.0365646613835, 193.65414308719375, 189.4683852720518, 169.70101106569763, 164.17853283066168, 160.5102644975412, 145.2549533827027, 141.53032379280577, 136.5765993649051, 130.73942569900325, 129.86263304496933, 129.65395340717595, 129.55841856297008, 122.39527301686847, 121.27563758846553, 1435.3112926249214, 1058.5541361304424, 1511.5529670371905, 3909.3790822947717, 7766.810975654315, 781.1751226992658, 1082.3018431744706, 2141.3521816692864, 792.8821637870675, 805.028760204664, 411.3740142710217, 2188.6655019698665, 582.4602723497034, 380.9026193101402, 367.26796775461173, 330.9847997580444, 293.55958354081, 267.9488759749046, 257.5980624485812, 248.26701879678185, 241.3921344145638, 223.96489572062902, 220.95618137345508, 217.76849186193874, 198.5421077300581, 181.79349059710665, 180.85643331784175, 177.189477060005, 150.76103751418208, 145.5445786171806, 128.4573614266457, 126.3142648247361, 123.55748951367985, 119.624301145428, 119.21604416582706, 107.64060103879025, 107.43377589298515, 104.62056237891866, 104.29790166978626, 93.33444937063307, 90.74857923492911, 4157.742336600014, 1511.5529670371905, 3544.2709304058694, 2616.383943680257, 939.5558529115145, 3909.3790822947717, 1617.9776648494956, 4522.438708310848, 1540.888760425653, 1384.9932289336675, 634.5066218686825, 492.33841956574116, 450.3863362336207, 430.47396722592924, 350.9657973165134, 349.37222341573965, 344.9734762309764, 290.7323367891987, 273.08685703725615, 213.05760536127065, 186.61490436087942, 180.121533472566, 178.39645438473678, 165.95943120522168, 146.75675313101328, 144.7150248203357, 136.76481471869113, 93.46060361225226, 92.85779039860368, 88.39502514149179, 87.63224938251139, 85.07508977858508, 84.63155611009985, 84.21477720368658, 82.18614237841946, 78.98076089998598, 70.7312682934501, 67.11396226060886, 885.1710707055273, 1009.5972646364334, 1308.956616672962, 158.28690744457913, 366.97606281201536, 630.7566773154867, 2859.6698917904105, 1069.4129206599916, 415.3779085420275, 1195.4228290723197, 713.9732493982405, 455.0001556928053, 301.7111455767667, 300.32943544426166, 238.88212433582513, 225.9893426961701, 128.55656923874565, 118.66413744970639, 117.41077061134717, 116.42914320239424, 81.31044915429923, 76.27040286173806, 69.08649262527356, 67.12594439770142, 62.98167133716921, 56.13732255203592, 55.39741231802748, 51.96406641008993, 50.98834899781563, 48.27460982337601, 46.057429650438515, 45.00644692268719, 43.845650981838084, 43.81720519885607, 43.25571288943911, 40.89805906037541, 38.798850853269336, 38.12519897670598, 35.250207305114735, 1082.3018431744706, 169.5203415201419, 325.00084667781806, 1380.9171445053937, 401.9090925380828, 7766.810975654315, 711.1122491439417, 331.43130776935726, 321.19979794476393, 305.28530131972155, 291.00820403071924, 236.2602318926667, 214.81605645075962, 205.06578489603567, 190.80697009768284, 188.11414522656423, 173.09916720472856, 144.68879281426936, 141.32935065944864, 126.74926319952061, 111.13355935090144, 110.33388750260019, 88.5087047231851, 87.76256726523069, 85.57025328087808, 79.41225794871745, 79.16336031703077, 78.9174516580835, 67.77656792533546, 66.3405547789351, 63.14735418106191, 57.9522266083482, 52.20098791913219, 51.349824437137336, 50.79853101419102, 49.23623221369634, 2987.1846867553954, 217.11061780288665, 1965.1712374607457, 1048.3068115209778, 885.1710707055273, 423.060811908663, 327.03596917219113, 278.26181717549275, 276.4772799772042, 220.26663786563972, 178.91133051469748, 150.44323056502603, 142.55518440013722, 127.45969447353849, 120.72845695368667, 95.00281908151005, 94.32690810216643, 69.16499497897404, 61.07993094895329, 47.690542854129916, 44.262004458047734, 36.020494066299285, 34.377457838730045, 33.853834195537274, 19.02508821144209, 16.83566793995059, 15.38945491673691, 10.133340311817417, 8.297277137333575, 6.900809556127296, 5.97098618799176, 5.754076336373626, 3.938382569395921, 2.2466627142806685, 932.1503243931105, 183.57368858689946, 163.69864398076072, 93.8982807357117, 93.25228562402931, 50.30594305328026, 30.746208231964346, 24.686729751331935, 15.221452373100336, 9.278330333994717, 6.502962553549829, 4.88501186081827, 3.949175197763784, 3.925787334092651, 2.838270988575796, 0.9806915040821489, 0.9898405317018332, 0.9734317360462341, 0.9797649567737233, 0.9807226375812391, 0.9752688014707506, 0.9670547674274046, 1.0145559343350357, 0.9834211828967877, 0.9670336976871358, 0.9630433611282203, 0.9633892260237233, 0.9606403045835595, 0.9655636839571754, 0.9644317968260151, 0.9787909598011747, 1.1352035663472924, 1.1352035663472924], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic11\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\", \"Topic12\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.469, -4.5832, -4.5917, -4.6468, -4.8647, -5.001, -5.0694, -5.0909, -5.1035, -5.1461, -5.1953, -5.0923, -5.2695, -5.4471, -5.4962, -5.5192, -5.5262, -5.5516, -5.5884, -5.6012, -5.611, -5.6624, -5.669, -5.7098, -5.7131, -5.7613, -5.8087, -5.8462, -5.8485, -5.8671, -4.3587, -4.5941, -5.1264, -3.0446, -5.0994, -5.051, -5.0705, -4.1522, -5.3581, -4.6039, -4.9037, -4.4372, -5.0743, -3.9455, -4.0486, -4.0322, -4.7554, -4.7739, -4.2379, -4.7877, -3.9754, -4.9752, -4.9121, -5.0169, -5.0294, -5.0199, -4.9954, -2.358, -2.3797, -2.5372, -2.9704, -3.5578, -3.7505, -3.7735, -4.2102, -4.4441, -4.4766, -4.5641, -5.3403, -5.4422, -5.5459, -5.5954, -5.8989, -5.908, -5.912, -5.9434, -5.9498, -6.0344, -6.0514, -6.0832, -6.1416, -6.1637, -6.1845, -6.2104, -6.2357, -6.2439, -6.2482, -2.3868, -5.1355, -5.2246, -4.0437, -5.1277, -2.6567, -4.1312, -4.2124, -4.3855, -4.454, -3.9761, -4.7987, -5.3123, -4.8142, -5.2665, -5.1592, -5.415, -2.5863, -3.0712, -3.4093, -3.4364, -3.9316, -4.3262, -4.3604, -4.5497, -4.5986, -4.7029, -4.7289, -4.8655, -4.896, -4.9134, -4.9919, -5.041, -5.0781, -5.3139, -5.338, -5.4549, -5.4767, -5.5084, -5.5142, -5.5225, -5.5651, -5.6012, -5.6086, -5.6948, -5.7399, -5.7768, -4.8137, -3.2401, -2.7649, -4.8568, -3.4838, -4.4165, -4.7766, -5.1178, -4.8904, -4.893, -4.3082, -4.3745, -4.5464, -4.9194, -4.6042, -4.7334, -4.9696, -3.5677, -3.8589, -3.9759, -4.4312, -4.4378, -4.4903, -4.501, -4.5766, -4.596, -4.626, -4.7587, -4.7633, -4.765, -4.9759, -5.0474, -5.0739, -5.0876, -5.1391, -5.1871, -5.2063, -5.209, -5.2157, -5.2759, -5.2796, -5.3982, -5.442, -5.4824, -5.4854, -5.4987, -5.51, -4.356, -3.4229, -4.2622, -3.97, -3.5219, -2.2527, -4.2553, -3.0674, -4.0422, -3.899, -4.1161, -4.7639, -4.4839, -4.7919, -4.6466, -4.297, -4.6772, -2.5221, -3.3403, -3.5573, -3.8384, -4.0049, -4.0942, -4.1413, -4.508, -4.6195, -4.6199, -4.7, -4.7404, -4.8112, -4.9037, -4.9712, -5.0071, -5.0337, -5.0406, -5.0447, -5.1674, -5.1812, -5.2479, -5.2588, -5.3157, -5.3822, -5.4126, -5.4213, -5.477, -5.5108, -5.5199, -3.9107, -3.2026, -3.6452, -4.4045, -4.4099, -2.8631, -4.1015, -5.0841, -3.821, -4.1005, -4.6377, -4.4664, -4.7658, -4.6071, -4.6396, -4.6058, -4.8386, -4.8728, -2.8332, -3.2397, -3.262, -3.3332, -3.5204, -3.7778, -3.7822, -3.8068, -4.2076, -4.466, -4.5228, -4.6601, -4.6968, -4.7334, -4.8257, -4.9427, -4.9752, -4.9972, -5.1079, -5.1412, -5.1639, -5.2643, -5.2904, -5.3263, -5.3703, -5.377, -5.3787, -5.3794, -5.4367, -5.4459, -3.1013, -3.6689, -3.6288, -3.2608, -2.9983, -4.3759, -4.2884, -4.0759, -4.5966, -4.7653, -4.9651, -2.2694, -3.5943, -4.0198, -4.0563, -4.1606, -4.2809, -4.3725, -4.412, -4.449, -4.4772, -4.5524, -4.566, -4.5806, -4.6734, -4.7619, -4.7671, -4.7877, -4.9501, -4.9855, -5.1112, -5.1282, -5.1504, -5.183, -5.1864, -5.2893, -5.2913, -5.318, -5.3211, -5.4332, -5.4616, -1.6522, -3.314, -2.6903, -3.3377, -4.376, -4.3031, -4.5838, -4.8963, -2.3464, -2.4532, -3.2345, -3.4886, -3.5778, -3.6231, -3.8277, -3.8323, -3.845, -4.0165, -4.0793, -4.3284, -4.4615, -4.4971, -4.5068, -4.5794, -4.703, -4.7171, -4.774, -5.1576, -5.1642, -5.2139, -5.2226, -5.2525, -5.2578, -5.2628, -5.2874, -5.3277, -5.4392, -5.4924, -2.9858, -3.2499, -3.7639, -4.9995, -4.6522, -4.6773, -4.5496, -4.7882, -5.1218, -2.2022, -2.7181, -3.1693, -3.5811, -3.5857, -3.8153, -3.871, -4.4379, -4.5185, -4.5292, -4.5377, -4.8998, -4.9645, -5.0646, -5.0938, -5.1583, -5.275, -5.2885, -5.3535, -5.3727, -5.4284, -5.4763, -5.4998, -5.5264, -5.5271, -5.5402, -5.5974, -5.6512, -5.6691, -5.7494, -2.7389, -4.3727, -3.8423, -3.0319, -4.8448, -4.6046, -2.6965, -3.4614, -3.4928, -3.5438, -3.5918, -3.8009, -3.8965, -3.9431, -4.0155, -4.0298, -4.1134, -4.2937, -4.3173, -4.4269, -4.5594, -4.5667, -4.7891, -4.7977, -4.8232, -4.8987, -4.9019, -4.905, -5.0591, -5.0808, -5.1308, -5.218, -5.3242, -5.3409, -5.3519, -5.3837, -1.5147, -4.6129, -4.3807, -4.6629, -4.9878, -2.4385, -2.6965, -2.8585, -2.865, -3.0931, -3.302, -3.4762, -3.5304, -3.6431, -3.6977, -3.9394, -3.9466, -4.2603, -4.3863, -4.6379, -4.714, -4.9247, -4.9726, -4.9884, -5.5858, -5.7145, -5.8095, -6.2595, -6.4809, -6.6896, -6.8576, -6.9012, -7.3679, -8.1757, -2.6739, -2.2645, -2.3797, -2.9397, -2.9466, -3.5723, -4.0764, -4.3034, -4.811, -5.3475, -5.7504, -6.0918, -6.3602, -6.3679, -6.8149, -10.3099, -10.3448, -10.3698, -10.4335, -10.4378, -10.4441, -10.4563, -10.4327, -10.4898, -10.5076, -10.5149, -10.5149, -10.5202, -10.5161, -10.5202, -10.5093, -10.4344, -10.4344], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.0187, 1.0187, 1.0187, 1.0186, 1.0185, 1.0185, 1.0184, 1.0184, 1.0184, 1.0184, 1.0183, 1.0183, 1.0183, 1.0181, 1.018, 1.018, 1.018, 1.018, 1.0179, 1.0179, 1.0179, 1.0178, 1.0178, 1.0178, 1.0178, 1.0177, 1.0177, 1.0176, 1.0176, 1.0176, 1.0133, 1.006, 1.0105, 0.9382, 0.9988, 0.9952, 0.9886, 0.9243, 0.9928, 0.9204, 0.9477, 0.895, 0.9489, 0.7621, 0.7625, 0.7572, 0.8811, 0.8707, 0.6034, 0.7982, 0.2948, 0.8637, 0.6465, 0.73, 0.6657, 0.6066, 0.3844, 1.9206, 1.9205, 1.9205, 1.9205, 1.9203, 1.9202, 1.9202, 1.9199, 1.9197, 1.9197, 1.9196, 1.9184, 1.9181, 1.9178, 1.9177, 1.9166, 1.9166, 1.9166, 1.9165, 1.9164, 1.9161, 1.916, 1.9158, 1.9155, 1.9154, 1.9153, 1.9152, 1.915, 1.915, 1.915, 1.9114, 1.7557, 1.7522, 1.5459, 1.732, 1.1782, 1.4733, 1.3273, 0.821, 0.7792, 0.2942, 0.4271, 1.0702, 0.1425, 0.4804, -0.7728, 0.1486, 2.0908, 2.0907, 2.0906, 2.0905, 2.0903, 2.09, 2.09, 2.0898, 2.0897, 2.0896, 2.0896, 2.0894, 2.0893, 2.0893, 2.0892, 2.0891, 2.089, 2.0885, 2.0884, 2.0881, 2.0881, 2.088, 2.0879, 2.0879, 2.0878, 2.0877, 2.0876, 2.0874, 2.0872, 2.0871, 2.0815, 2.0293, 1.8566, 1.9327, 1.571, 1.7553, 1.8555, 1.9409, 1.5253, 1.38, 0.6485, 0.4365, 0.6601, 1.3337, -0.2178, 0.1079, 0.3888, 2.2279, 2.2278, 2.2277, 2.2272, 2.2272, 2.2272, 2.2271, 2.227, 2.227, 2.227, 2.2267, 2.2267, 2.2267, 2.2263, 2.2262, 2.2261, 2.2261, 2.226, 2.2258, 2.2258, 2.2258, 2.2258, 2.2256, 2.2256, 2.2252, 2.2251, 2.2249, 2.2249, 2.2249, 2.2248, 2.2196, 2.1655, 2.1735, 2.1459, 2.0417, 1.5822, 1.8075, 1.319, 1.6345, 1.3342, 1.4236, 1.7883, 1.1426, 1.5961, 1.1516, -0.0268, 1.0599, 2.3668, 2.3666, 2.3665, 2.3663, 2.3661, 2.3661, 2.366, 2.3655, 2.3654, 2.3654, 2.3652, 2.3651, 2.365, 2.3648, 2.3646, 2.3646, 2.3645, 2.3645, 2.3645, 2.3641, 2.3641, 2.3639, 2.3638, 2.3637, 2.3634, 2.3633, 2.3633, 2.3631, 2.3629, 2.3629, 2.3572, 2.3458, 2.1707, 2.1807, 2.0683, 1.5234, 1.6499, 2.1889, 0.8005, 0.7408, 1.1604, 0.4904, 1.297, 0.1006, 0.1498, -0.623, 0.7511, 0.8643, 2.9906, 2.9903, 2.9903, 2.9902, 2.99, 2.9897, 2.9897, 2.9897, 2.989, 2.9884, 2.9882, 2.9878, 2.9877, 2.9876, 2.9872, 2.9867, 2.9866, 2.9865, 2.986, 2.9858, 2.9857, 2.9851, 2.9849, 2.9847, 2.9844, 2.9844, 2.9844, 2.9844, 2.984, 2.9839, 2.8575, 2.5943, 2.2782, 1.696, 1.272, 2.1912, 1.9527, 1.4828, 1.9556, 1.7717, 2.2433, 3.2675, 3.2664, 3.2656, 3.2655, 3.2652, 3.2649, 3.2646, 3.2645, 3.2644, 3.2643, 3.264, 3.2639, 3.2639, 3.2635, 3.2631, 3.263, 3.2629, 3.2621, 3.2619, 3.2611, 3.261, 3.2608, 3.2606, 3.2605, 3.2597, 3.2597, 3.2595, 3.2595, 3.2585, 3.2582, 3.243, 2.593, 2.3645, 2.0206, 2.0065, 0.6536, 1.2552, -0.0852, 3.5413, 3.5413, 3.5406, 3.5402, 3.54, 3.5399, 3.5395, 3.5395, 3.5394, 3.539, 3.5388, 3.5379, 3.5373, 3.5371, 3.5371, 3.5367, 3.5361, 3.536, 3.5356, 3.5327, 3.5327, 3.5322, 3.5321, 3.5318, 3.5318, 3.5317, 3.5315, 3.531, 3.5298, 3.5291, 3.4563, 3.0607, 2.287, 3.164, 2.6704, 2.1036, 0.7198, 1.4649, 2.0769, 3.9394, 3.939, 3.9383, 3.9374, 3.9373, 3.9366, 3.9364, 3.9336, 3.933, 3.933, 3.9329, 3.9298, 3.9291, 3.9279, 3.9276, 3.9267, 3.9251, 3.9249, 3.9239, 3.9235, 3.9226, 3.9217, 3.9213, 3.9208, 3.9208, 3.9206, 3.9194, 3.9183, 3.9179, 3.9161, 3.5021, 3.7222, 3.6018, 2.9655, 2.3869, -0.3343, 3.9645, 3.9631, 3.963, 3.9629, 3.9627, 3.962, 3.9616, 3.9614, 3.9611, 3.9611, 3.9606, 3.9596, 3.9595, 3.9587, 3.9577, 3.9577, 3.9557, 3.9556, 3.9553, 3.9545, 3.9545, 3.9544, 3.9526, 3.9523, 3.9516, 3.9503, 3.9486, 3.9483, 3.9481, 3.9475, 3.7111, 3.2346, 1.2639, 1.6101, 1.4543, 4.7419, 4.7413, 4.7408, 4.7408, 4.74, 4.739, 4.7381, 4.7377, 4.737, 4.7366, 4.7346, 4.7345, 4.7311, 4.7294, 4.7252, 4.7238, 4.7191, 4.7179, 4.7175, 4.6963, 4.6899, 4.6847, 4.6525, 4.6311, 4.6066, 4.5834, 4.5768, 4.4892, 4.2427, 3.7165, 5.7508, 5.7502, 5.746, 5.7459, 5.7375, 5.7257, 5.7182, 5.6942, 5.6527, 5.6052, 5.5499, 5.4942, 5.4925, 5.3698, 2.9375, 2.8933, 2.8851, 2.8148, 2.8095, 2.8088, 2.8051, 2.7807, 2.7548, 2.7538, 2.7507, 2.7503, 2.7478, 2.7468, 2.7439, 2.74, 2.6667, 2.6667]}, \"token.table\": {\"Topic\": [1, 1, 3, 5, 7, 11, 1, 9, 1, 8, 6, 1, 10, 4, 4, 11, 8, 9, 6, 6, 4, 9, 11, 11, 7, 9, 9, 10, 2, 2, 1, 3, 4, 1, 2, 6, 12, 8, 8, 1, 3, 8, 3, 8, 1, 4, 9, 11, 7, 1, 5, 7, 3, 2, 6, 9, 6, 7, 5, 10, 1, 3, 5, 3, 6, 9, 4, 5, 11, 11, 1, 2, 6, 6, 7, 4, 2, 3, 2, 6, 2, 6, 5, 5, 6, 5, 1, 3, 4, 6, 8, 10, 9, 10, 8, 1, 2, 3, 4, 5, 11, 12, 12, 3, 5, 9, 5, 1, 8, 11, 1, 3, 7, 8, 8, 7, 3, 6, 6, 1, 3, 4, 5, 6, 4, 6, 5, 8, 11, 1, 4, 8, 2, 3, 5, 9, 8, 5, 6, 4, 11, 5, 3, 7, 3, 11, 7, 4, 1, 2, 3, 8, 7, 12, 5, 1, 6, 2, 3, 4, 5, 9, 6, 8, 1, 3, 7, 5, 1, 3, 4, 5, 9, 9, 9, 4, 9, 1, 1, 11, 8, 1, 3, 4, 5, 6, 7, 2, 2, 2, 3, 2, 4, 3, 1, 3, 4, 5, 7, 7, 7, 2, 5, 1, 2, 3, 4, 5, 6, 1, 2, 3, 8, 11, 1, 3, 11, 12, 2, 1, 3, 4, 5, 2, 7, 8, 12, 9, 4, 5, 1, 2, 5, 10, 6, 4, 1, 1, 4, 10, 3, 4, 5, 6, 2, 3, 4, 5, 4, 5, 1, 3, 6, 9, 3, 11, 8, 9, 9, 2, 10, 2, 10, 1, 3, 1, 1, 7, 3, 4, 8, 4, 4, 10, 2, 4, 1, 2, 7, 5, 7, 5, 7, 2, 4, 5, 5, 7, 6, 1, 5, 5, 1, 9, 1, 2, 4, 5, 5, 1, 9, 4, 8, 3, 10, 4, 1, 4, 5, 10, 12, 12, 3, 4, 4, 5, 7, 10, 4, 3, 11, 8, 1, 3, 1, 2, 3, 5, 6, 7, 1, 3, 8, 8, 1, 7, 7, 7, 3, 6, 5, 8, 2, 1, 8, 6, 10, 1, 2, 7, 8, 4, 6, 10, 2, 2, 3, 7, 6, 8, 1, 2, 4, 8, 11, 10, 8, 11, 10, 7, 1, 9, 10, 5, 9, 11, 5, 1, 4, 10, 5, 2, 7, 5, 4, 6, 6, 9, 1, 3, 1, 5, 11, 8, 7, 10, 12, 9, 1, 2, 9, 4, 11, 12, 2, 3, 2, 2, 4, 6, 2, 1, 6, 10, 1, 2, 3, 7, 8, 1, 3, 6, 6, 4, 3, 2, 1, 2, 4, 5, 6, 9, 8, 7, 2, 10, 12, 6, 1, 2, 4, 11, 1, 4, 1, 3, 8, 1, 11, 5, 5, 4, 1, 3, 5, 7, 11, 12, 4, 4, 4, 4, 10, 2, 2, 2, 4, 1, 9, 10, 3, 9, 10, 12, 9, 7, 1, 9, 6, 1, 3, 10, 10, 1, 3, 8, 3, 10, 11, 7, 3, 3, 8, 5, 11, 3, 3, 4, 7, 10, 1, 3, 2, 3, 12, 3, 5, 5, 1, 2, 3, 5, 6, 7, 1, 1, 8, 1, 9, 1, 3, 10, 3, 3, 11, 10, 1, 8, 2, 3, 3, 8, 10, 2, 3, 10, 6, 7, 1, 3, 4, 5, 10, 11, 1, 4, 1, 9, 9, 3, 4, 11, 9, 6, 1, 3, 4, 5, 1, 3, 4, 5, 5, 7, 9, 3, 7, 3, 5, 2, 1, 4, 5, 1, 3, 8, 1, 3, 6, 1, 3, 9, 9, 2, 1, 5, 1, 4, 5, 6, 2, 6, 10, 1, 2, 8, 6, 4], \"Freq\": [0.9992687004056375, 0.2295490790615951, 0.028108050497338177, 0.7413498318672944, 0.9895952778844126, 0.9939661536967533, 0.2861531006785141, 0.7138442941657557, 0.9990897556625788, 0.997777677891054, 0.9975278974833757, 0.9993779039349342, 0.9842804309839601, 0.9987113223783022, 0.997697035953816, 0.998273709950982, 0.997248172545878, 0.9928261573709256, 0.9928216386738133, 0.9933573421026307, 0.9977983056300378, 0.995573409438555, 0.8694632059035076, 0.44510464060475485, 0.9970246375097458, 0.9709700105359795, 0.9794104506782829, 0.9913110191623485, 0.9986983046353234, 0.996938754530959, 0.4056690092610061, 0.041512761310387926, 0.5522773688255407, 0.9978246915930832, 0.994325893316411, 0.9959854839248713, 0.8622240976578444, 0.9967049557858985, 0.9925375812624994, 0.39267917167985184, 0.3216302164926413, 0.2849597880089197, 0.8538447467412881, 0.1455053781899948, 0.46527934667783327, 0.32096810546224863, 0.21149061212628784, 0.9961054773106051, 0.9940684473032071, 0.9993851060705021, 0.9997363162699975, 0.997678311541258, 0.9979861900540102, 0.9929220538210242, 0.9983060153349408, 0.9806161796323711, 0.9967705205672935, 0.9964395856993618, 0.9976007981322447, 0.9984359021444497, 0.8712464762755167, 0.08530744756116009, 0.04341089047490987, 0.999839575353233, 0.35387540214902796, 0.644921751174991, 0.9389909657261034, 0.06061109717289397, 0.9714878602200702, 0.9716690708233773, 0.4378019602291688, 0.11137067409338504, 0.44932306444572584, 0.9943442791258269, 0.9897996601519878, 0.9965661669681499, 0.9970447138721481, 0.9993309971540506, 0.8445703231017319, 0.15425941974460858, 0.9918420215255597, 0.9988252118738241, 0.9986007883165707, 0.9983113274155336, 0.9988778166361183, 0.9947910417983165, 0.13813348254677923, 0.8601948685867615, 0.9915129398853118, 0.007746194842853998, 0.9184665280034477, 0.08134020912207655, 0.9844133806498421, 0.9885422359215512, 0.9927851973792018, 0.659783759584773, 0.0006837137405023555, 0.1376543664211409, 0.00501390076368394, 0.19668165268451093, 0.8689491949199851, 0.9197545448909388, 0.9865709927038333, 0.17755558630018636, 0.8219494998454205, 0.980713002021834, 0.9990098229152902, 0.9984765010330876, 0.9994232157125947, 0.9831562920039509, 0.7736976055794669, 0.19126848494605284, 0.03493690245257381, 0.9988989642533238, 0.9896613151284632, 0.9992097789814117, 0.997622279747138, 0.9992916854977565, 0.9958691403115669, 0.15775833892904964, 0.05465643238486759, 0.18135998018615154, 0.31054791127765674, 0.29564161153632923, 0.5250696264390068, 0.47402119053521447, 0.9984401412163338, 0.9937734625563976, 0.9859328782331205, 0.998955712764485, 0.9975928383024874, 0.9875822809401908, 0.997911132653525, 0.996063491586495, 0.9997537497043149, 0.9704867382490656, 0.9950716816021946, 0.8303563530209339, 0.16946048020835386, 0.9978617991594086, 0.9503632440998927, 0.9980331055542482, 0.9956510999806248, 0.997269557897536, 0.9982642201755442, 0.9599313641749803, 0.9962583380133108, 0.9968545678145615, 0.38338792442022035, 0.021507127467475778, 0.4694164342901235, 0.12530239481051106, 0.9917510638597306, 0.974039984661512, 0.9979516540906804, 0.9995933513084535, 0.9948181240607777, 0.06767887222790338, 0.09934911371916585, 0.4027471349459635, 0.4300789871918475, 0.9776377165605612, 0.9962529316785699, 0.9907623216649728, 0.8560068720904332, 0.009888888053030275, 0.1334999887159087, 0.9967008150389361, 0.4264736864353813, 0.032376274624675544, 0.3109238787231772, 0.22272644095250935, 0.007814962840438925, 0.9986368545333306, 0.9735966830588695, 0.996812649320077, 0.9838833856173583, 0.999594233894976, 0.9994896711755492, 0.9461191348996963, 0.9971781128172166, 0.358393525750757, 0.0011867335289760166, 0.34059252281611674, 0.2990568493019562, 0.9991397816589602, 0.9956917546496545, 0.998669248069868, 0.9989131065017379, 0.9956425730380546, 0.9991238388663387, 0.47591198157586223, 0.5239780087716039, 0.9977489633287125, 0.702236403551709, 0.0037466881287057646, 0.25852148088069776, 0.03532591664208292, 0.9965475678089658, 0.9875558218429409, 0.9964587424692408, 0.8284780280133148, 0.1698122666114248, 0.5298903502811441, 0.07887829827324326, 0.1468229710432647, 0.12378426016147581, 0.12026988053544022, 0.9966221064173194, 0.4071097920215548, 0.02420652817425461, 0.5677531153597899, 0.9948434868251841, 0.8373826102722347, 0.9763254637875977, 0.02281134261185976, 0.9855203398241437, 0.9904334698284837, 0.9969359561734061, 0.7696649855895383, 0.10256429664589294, 0.018825092422347436, 0.10883932745334209, 0.9915542327340771, 0.9956352089698018, 0.9992828636899314, 0.9721822307673261, 0.981349673144434, 0.998193665788502, 0.9991675658195426, 0.9325434957117203, 0.06390337898901031, 0.0037153127319192046, 0.9898000265849287, 0.9894815016945592, 0.9969083627999562, 0.9982442724828502, 0.6621677840045476, 0.33783050716005597, 0.9748918193347774, 0.9978290234284819, 0.12401490945874927, 0.0006967129744873554, 0.8750714959561184, 0.999833912493228, 0.9995453006073384, 0.9204109201975572, 0.07826346177056288, 0.9983643566048758, 0.9972597063507285, 0.9989043937831387, 0.9980058954560973, 0.1887679066302368, 0.8022636031785064, 0.9987341903571725, 0.98231938163362, 0.9972815049312591, 0.9996462932930201, 0.9963072819354829, 0.22462621845079933, 0.7753119551893461, 0.5158660646513571, 0.47901848860483154, 0.28452987289015585, 0.7147735291695128, 0.9980273380540438, 0.9983927409323033, 0.995672529424108, 0.19341848352558785, 0.567572271329184, 0.2378096108921162, 0.9983501712934787, 0.995890700291893, 0.9936500722535045, 0.16988493224012044, 0.8297142546976047, 0.7491071364591216, 0.23673814696225062, 0.014091556366800631, 0.9899396301986942, 0.009491271622231009, 0.9970659795150132, 0.9980937991052429, 0.6873760335703698, 0.1136795682709231, 0.1984575513882217, 0.9968893685269236, 0.9964710603661343, 0.9970249574099387, 0.994296772010916, 0.005424845989147632, 0.9973751472346234, 0.622050355025225, 0.3772854889035416, 0.9916427341727138, 0.0008326135467445119, 0.0008326135467445119, 0.0058282948272115834, 0.9962943772266507, 0.998888059176241, 0.9965014230874363, 0.9959070564253565, 0.9942188810948908, 0.9967056348952763, 0.996264636676483, 0.9985554143044515, 0.05565210825342917, 0.5308354941096322, 0.41310988049660885, 0.9818305264576548, 0.9226563970793169, 0.7641779201708531, 0.9975308089812593, 0.996034366349655, 0.9996787000480497, 0.9988446353074343, 0.9932869768580999, 0.9986986510952749, 0.9992565880727468, 0.9985305189425638, 0.8436502582881702, 0.9991422114692656, 0.9745735737818626, 0.024751074889698098, 0.09438839064524798, 0.1688247637557281, 0.23635466925801935, 0.15322126286315324, 0.2739565484581588, 0.07315739762748218, 0.8019038049246875, 0.1929176119567471, 0.0047989455710633604, 0.9950590837321597, 0.9995665721217599, 0.99969593253548, 0.9947811511586677, 0.9983755294450052, 0.9899293249446697, 0.008886259649413551, 0.997680074153681, 0.9974810617997072, 0.9940958714506744, 0.38134017740097836, 0.6180682355797675, 0.9957781979666623, 0.9962011384798571, 0.009135276321153836, 0.9908464925726855, 0.9952645681321348, 0.99440781080818, 0.9985856838209622, 0.9967056058308381, 0.9965354790113999, 0.9995160172009956, 0.4267974051328329, 0.2884341565860292, 0.2831124931803829, 0.991360340191661, 0.9992015499110309, 0.9991334059587406, 0.5526093380856395, 0.4472194974888757, 0.99553113830943, 0.7617340233303307, 0.9942524893480692, 0.9855752488574725, 0.9949062448304657, 0.9879104458947958, 0.9857024991347628, 0.9997868248623368, 0.9944032168102358, 0.9952394874483919, 0.9985454169571266, 0.9956706277863402, 0.8881572830929476, 0.9972606168458884, 0.9992544672986944, 0.9961446746293009, 0.9737131635417801, 0.9947445945227754, 0.024292029621679867, 0.9755294271835003, 0.9977020171640025, 0.999003147431534, 0.9968209852551267, 0.9949561630016617, 0.9963141255652097, 0.9985833887423268, 0.9994829223868892, 0.9984091050599606, 0.9998589507783668, 0.9954653599681735, 0.9834019297462537, 0.9954880152075433, 0.9957707514706108, 0.7046543505007503, 0.9842734435634659, 0.36030143792608976, 0.639449499060143, 0.9814474409588789, 0.9989696904788801, 0.9968322470008011, 0.975730072913883, 0.9935982494907425, 0.9972335243770433, 0.9958674310090719, 0.9915221983284692, 0.6444841659185457, 0.3556644516419372, 0.9955776478938533, 0.32686094002221483, 0.6726155759994711, 0.9933358467573274, 0.36975816586298477, 0.3329793784947305, 0.2390621178936527, 0.035465259247959464, 0.022329978045011516, 0.6888171000671953, 0.08966297166976374, 0.22135546130972922, 0.9992874677280776, 0.9974148898132527, 0.9998272959740186, 0.9998468232711537, 0.48462618850884315, 0.1966057890151444, 0.104804919619075, 0.020600475600800982, 0.17922413772696855, 0.013905321030540664, 0.9960201049254161, 0.9948965480677924, 0.9997985393477312, 0.9957898355598344, 0.9968748866391717, 0.9968388223647965, 0.2714902209938888, 0.31938042146734497, 0.4090902336218474, 0.9942495246764862, 0.969811540143278, 0.02927151590583365, 0.9988768282930033, 0.940318324055384, 0.05944742100759214, 0.9996378589012822, 0.9746933911016332, 0.9984817402771475, 0.9985361837579514, 0.9982392294935162, 0.4708789025310424, 0.1823126919702169, 0.05924206971778537, 0.2874195898566103, 0.9970538351020424, 0.9957321333654857, 0.9985374867757484, 0.9964194043198643, 0.9994603486221012, 0.9971475684584704, 0.9769930040214427, 0.9949952644169464, 0.9992569265811314, 0.9998666319996238, 0.9971722923766393, 0.9984587795880971, 0.9978018563723733, 0.9797928313472476, 0.9996944554095556, 0.9780415237053265, 0.9940886188952344, 0.7596522944078923, 0.9645333346753586, 0.9949520278798129, 0.9796597416485716, 0.01922472101354515, 0.9956898318985064, 0.9311780675646952, 0.06821129661596222, 0.983568765790769, 0.9948086358533756, 0.6548253876926614, 0.11314997507924664, 0.23111484271505695, 0.9973763509891621, 0.9853043085542127, 0.9974925309109177, 0.9959623880908994, 0.9991085528836745, 0.9963173398803874, 0.98736304855648, 0.9966566761151576, 0.9963934130280382, 0.999832142789024, 0.4912683904560266, 0.3214707720071475, 0.09157624365782244, 0.09443800127212938, 0.9997606748339821, 0.998303069765839, 0.9988625953983478, 0.9971428565878401, 0.8188311746145842, 0.7911554290183247, 0.20871869237223317, 0.9983523954545946, 0.3352293564004853, 0.16195202015293714, 0.0039638606331138465, 0.48812112367773364, 0.01019278448514989, 0.9940487043679791, 0.9989251858869606, 0.9986923039398475, 0.9855676109853415, 0.987136477547084, 0.012655595865988257, 0.8620113950929111, 0.0707317496563841, 0.06716971909814892, 0.9973745685747911, 0.9990329081633109, 0.9747788037654586, 0.9905939519763882, 0.5804193286282815, 0.41964589957162135, 0.9979092423711048, 0.9983036542859687, 0.3095644535045094, 0.6823053260915718, 0.9940772916081225, 0.8478913953434308, 0.1504608947895441, 0.988374540246707, 0.4902243031896138, 0.5094098697112046, 0.7734386275248147, 0.06958754344061881, 0.05323746160070264, 0.10368344581410252, 0.9948027171057522, 0.9894443229032008, 0.05275680242655702, 0.9462549882039907, 0.9994010160454213, 0.9797403491949284, 0.9832263902161208, 0.3583113058695285, 0.2821433336637305, 0.3583113058695285, 0.9956221710087444, 0.9978277739461198, 0.9222813061896972, 0.022117752551831675, 0.005215539903052011, 0.05032030165722403, 0.00277323190178513, 0.004159847852677695, 0.013866159508925651, 0.9789508613301511, 0.9968902589521387, 0.9976303147723821, 0.9976429588790722, 0.5944805127408018, 0.4051608999980026, 0.9991434511540869, 0.9996206546607063, 0.9949183695099298, 0.9991659521579997, 0.6563497091098511, 0.3432500245521483, 0.20903014081444146, 0.7896694208545567, 0.99893459356299, 0.8832267518341601, 0.11654720629177576, 0.9984459860185763, 0.9060435206699359, 0.09385311369489376, 0.9833434358011582, 0.9770410624634489, 0.9997854713732678, 0.16076292800880662, 0.8375229959168474, 0.9096743763802436, 0.02335455609724239, 0.06689206190815104, 0.9980143111161033, 0.9994678910566066, 0.999522076551628, 0.9946659161274369, 0.9995945731114414, 0.9948415504754802, 0.9950360591001794, 0.997906651753153, 0.9979586914436732], \"Term\": [\"able\", \"access\", \"access\", \"access\", \"accord\", \"account\", \"action\", \"action\", \"add\", \"aggregate\", \"airflow\", \"also\", \"analyze\", \"ansible\", \"ansible_galaxy\", \"append\", \"apr\", \"area\", \"argo\", \"argo_workflow\", \"argument\", \"assign\", \"association\", \"association_rule\", \"attribute\", \"automatic\", \"avail\", \"average\", \"bam\", \"barcode\", \"base\", \"base\", \"base\", \"batch\", \"bed\", \"block\", \"buy\", \"calculate\", \"cant_figure\", \"case\", \"case\", \"case\", \"cast\", \"cast\", \"cause\", \"cause\", \"cause\", \"certain\", \"cf\", \"change\", \"channel\", \"channel_frompath\", \"character\", \"checkpoint\", \"class\", \"classloader\", \"clear\", \"cli\", \"client\", \"cluster\", \"code\", \"code\", \"code\", \"column\", \"com\", \"com\", \"command\", \"command\", \"company\", \"comparison\", \"complete\", \"complete\", \"complete\", \"completely\", \"component\", \"conda\", \"conda_environment\", \"condition\", \"config\", \"config\", \"config_yaml\", \"conflict\", \"connect\", \"connection\", \"container\", \"context\", \"convert\", \"convert\", \"core\", \"core\", \"count\", \"count\", \"country\", \"couple\", \"course\", \"create\", \"create\", \"create\", \"create\", \"create\", \"cube\", \"cust\", \"customer\", \"database\", \"database\", \"database_terajdbc\", \"dataframe\", \"dataset\", \"date\", \"datetime\", \"datum\", \"datum\", \"datum\", \"day\", \"dd\", \"debug_nextflow\", \"decimal\", \"def\", \"def_require\", \"default\", \"default\", \"default\", \"default\", \"default\", \"dependency\", \"dependency\", \"deploy\", \"desire\", \"destination\", \"different\", \"dir\", \"direct\", \"directive\", \"directly\", \"directory\", \"dispatch\", \"divide\", \"docker\", \"docker\", \"download\", \"dozen\", \"driver\", \"drop\", \"dsl\", \"duplicate\", \"efficient\", \"emit\", \"enable\", \"end\", \"end\", \"end\", \"end\", \"enough\", \"entire\", \"env\", \"environment\", \"environment_variable\", \"error\", \"error\", \"error\", \"error\", \"errorfactory_errorfactory\", \"event\", \"exact\", \"example\", \"example\", \"example\", \"exception\", \"execute\", \"execute\", \"execute\", \"execute\", \"execute\", \"execution\", \"execution_datasource\", \"executor\", \"executor_executor\", \"expand\", \"expect\", \"express\", \"extract\", \"fail\", \"fail\", \"fail\", \"fail\", \"false\", \"false_false\", \"fastq\", \"fastq_gz\", \"fastqc\", \"field\", \"file\", \"file\", \"final\", \"find\", \"find\", \"find\", \"find\", \"flag\", \"flatten\", \"flow\", \"folder\", \"folder\", \"follow\", \"follow\", \"follow\", \"follow\", \"follow\", \"foo\", \"format\", \"format\", \"format\", \"forward\", \"fp_growth\", \"function\", \"function\", \"future\", \"galaxy\", \"genome\", \"get\", \"get\", \"get\", \"get\", \"glob_wildcard\", \"groovy\", \"group\", \"hadoop\", \"hadoop_mapre\", \"home\", \"host\", \"however\", \"however\", \"however\", \"hpc\", \"http\", \"https\", \"image\", \"import\", \"import\", \"incomplete\", \"individual\", \"info\", \"info\", \"info\", \"input\", \"insert\", \"instal\", \"instal\", \"install\", \"installation\", \"instead\", \"int\", \"internal\", \"internal\", \"interval\", \"ip\", \"item\", \"java\", \"jdbc\", \"job\", \"job\", \"jobid\", \"jobid\", \"join\", \"join\", \"key\", \"know\", \"label\", \"last\", \"last\", \"last\", \"lib_python\", \"library\", \"limit\", \"line\", \"line\", \"list\", \"list\", \"list\", \"local\", \"local\", \"locally\", \"location\", \"log\", \"log\", \"log\", \"loop\", \"lot\", \"luigi\", \"m\", \"m\", \"machine\", \"main\", \"main\", \"make\", \"make\", \"make\", \"make\", \"manager\", \"map\", \"mark\", \"master\", \"max\", \"maximum\", \"memory\", \"merge\", \"message\", \"message\", \"message\", \"metadata\", \"meteor\", \"meteor_app\", \"min\", \"miniconda\", \"miss\", \"mode\", \"mode_copy\", \"model\", \"module\", \"monetdb\", \"mongo\", \"month\", \"multiple\", \"multiple\", \"name\", \"name\", \"name\", \"name\", \"name\", \"name\", \"need\", \"need\", \"need\", \"nest\", \"new\", \"nextflow\", \"nextflow_enable\", \"nf\", \"null\", \"null\", \"odbc\", \"operator\", \"optional\", \"order\", \"order\", \"org_apache\", \"os_path\", \"output\", \"output\", \"overlap\", \"overwrite\", \"package\", \"page\", \"panda\", \"param\", \"parameter\", \"parameter\", \"parameter\", \"partial\", \"partition\", \"pass\", \"path\", \"path\", \"patient\", \"payment\", \"per_second\", \"period\", \"perl\", \"pickle\", \"pipe\", \"pipeline\", \"play\", \"plot\", \"pool\", \"pop\", \"populated\", \"port\", \"possible\", \"prefix\", \"preprocesse\", \"procedure\", \"process\", \"process\", \"profile\", \"project\", \"property\", \"public\", \"pyspark\", \"python\", \"query\", \"question\", \"quot\", \"range\", \"rank\", \"rapidminer\", \"raw\", \"rdbms\", \"re\", \"read\", \"read\", \"realize\", \"reason\", \"record\", \"reduce\", \"ref\", \"region\", \"regular\", \"replicate\", \"require\", \"require\", \"rerun\", \"resource\", \"resource\", \"resources_mem_mb\", \"result\", \"result\", \"result\", \"result\", \"result\", \"return\", \"return\", \"return\", \"role\", \"root\", \"row\", \"rule\", \"run\", \"run\", \"run\", \"run\", \"run\", \"run\", \"running\", \"runtime\", \"sample\", \"sbatch\", \"scale\", \"scheduler\", \"script\", \"script\", \"script\", \"section\", \"see\", \"see\", \"seem\", \"select\", \"select\", \"self\", \"sequential\", \"server\", \"service\", \"session\", \"set\", \"set\", \"set\", \"set\", \"setup\", \"share\", \"shell\", \"singularity\", \"site_package\", \"skip\", \"slurm\", \"smk\", \"snakefile\", \"snakemake\", \"software\", \"solution\", \"spark\", \"speed\", \"sql\", \"sql_sqlexception\", \"sqlalchemy\", \"sqlserver\", \"sqlstate_hy\", \"stage\", \"start\", \"start\", \"static\", \"step\", \"step\", \"steps_done\", \"storage\", \"store\", \"store\", \"store\", \"sub\", \"submission\", \"submit\", \"subprocess\", \"sum\", \"summary\", \"supply\", \"swarm\", \"sys\", \"table\", \"take\", \"take\", \"take\", \"take\", \"task\", \"temp\", \"template\", \"temporary\", \"tera\", \"teradata\", \"teradata\", \"teradatasql\", \"test\", \"test\", \"test\", \"test\", \"test\", \"testing\", \"text\", \"think\", \"third\", \"thread\", \"thread\", \"time\", \"time\", \"time\", \"timestamp\", \"tmp\", \"today\", \"token\", \"total\", \"total\", \"touch\", \"traceback\", \"transaction\", \"transaction\", \"trigger\", \"trim\", \"trim\", \"trouble\", \"true\", \"true\", \"try\", \"try\", \"try\", \"try\", \"tsv\", \"turn\", \"txt\", \"txt\", \"type\", \"ui_internal\", \"unknown_source\", \"update\", \"update\", \"update\", \"upload\", \"url\", \"use\", \"use\", \"use\", \"use\", \"user\", \"user\", \"user\", \"user\", \"username\", \"usr_local\", \"util\", \"value\", \"value\", \"varchar\", \"variable\", \"vcf\", \"ve\", \"version\", \"version\", \"view\", \"view\", \"wait\", \"want\", \"want\", \"warn\", \"way\", \"way\", \"widget\", \"widget_sendevent\", \"wildcard\", \"window\", \"window\", \"work\", \"work\", \"work\", \"worker\", \"workflow\", \"would_require\", \"wrapper\", \"write\", \"yaml\", \"year\", \"yml\", \"zip\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [12, 9, 3, 2, 8, 6, 1, 5, 4, 10, 11, 7]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el168631405316247688164684389898\", ldavis_el168631405316247688164684389898_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el168631405316247688164684389898\", ldavis_el168631405316247688164684389898_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el168631405316247688164684389898\", ldavis_el168631405316247688164684389898_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "11     0.242184  0.165389       1        1  36.092098\n",
       "8      0.212188 -0.256646       2        1  14.650907\n",
       "2      0.132557  0.259405       3        1  12.357409\n",
       "1      0.217190 -0.198196       4        1  10.769402\n",
       "7      0.193779  0.060637       5        1   9.375639\n",
       "5      0.003748 -0.073951       6        1   5.023239\n",
       "0     -0.052638  0.124217       7        1   3.808786\n",
       "4     -0.139527  0.074625       8        1   2.895835\n",
       "3     -0.192639 -0.043987       9        1   1.944542\n",
       "9     -0.151961 -0.072065      10        1   1.895295\n",
       "10    -0.225389 -0.023189      11        1   0.870393\n",
       "6     -0.239492 -0.016239      12        1   0.316457, topic_info=                  Term          Freq         Total Category  logprob  loglift\n",
       "18                file  12004.000000  12004.000000  Default  30.0000  30.0000\n",
       "2910         snakemake   7703.000000   7703.000000  Default  29.0000  29.0000\n",
       "1093              rule   7537.000000   7537.000000  Default  28.0000  28.0000\n",
       "202             output   7553.000000   7553.000000  Default  27.0000  27.0000\n",
       "344            process   4157.000000   4157.000000  Default  26.0000  26.0000\n",
       "...                ...           ...           ...      ...      ...      ...\n",
       "23393          cust_nm      0.047647      0.965564  Topic12 -10.5161   2.7468\n",
       "20482      entry_point      0.047450      0.964432  Topic12 -10.5202   2.7439\n",
       "25384        pointplot      0.047972      0.978791  Topic12 -10.5093   2.7400\n",
       "25577       cdirs_user      0.051701      1.135204  Topic12 -10.4344   2.6667\n",
       "25585  pertoldi_legend      0.051701      1.135204  Topic12 -10.4344   2.6667\n",
       "\n",
       "[527 rows x 6 columns], token_table=      Topic      Freq    Term\n",
       "term                         \n",
       "87        1  0.999269    able\n",
       "64        1  0.229549  access\n",
       "64        3  0.028108  access\n",
       "64        5  0.741350  access\n",
       "1052      7  0.989595  accord\n",
       "...     ...       ...     ...\n",
       "175       1  0.999595   write\n",
       "4446      2  0.994842    yaml\n",
       "324       8  0.995036    year\n",
       "9138      6  0.997907     yml\n",
       "3385      4  0.997959     zip\n",
       "\n",
       "[598 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[12, 9, 3, 2, 8, 6, 1, 5, 4, 10, 11, 7])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a9f15c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -8.230815871407843\n",
      "\n",
      "Coherence Score:  0.4283454164390454\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el168631405316247683685416992945\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el168631405316247683685416992945_data = {\"mdsDat\": {\"x\": [-0.2658361065358947, -0.2064021907597453, -0.13367226688403105, -0.18485854640704327, 0.0946850779573841, 0.13117315812091657, -0.0018802154424411507, 0.1425339953707565, 0.18784535791923546, 0.2364117366608628], \"y\": [-0.11555850300122138, 0.12730882239923055, -0.19298894430530575, 0.05708339065381947, -0.05012461097137173, 0.15426656162076188, 0.25982069934633634, -0.23264614494261765, -0.003381203375494842, -0.0037800674241363565], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [22.34381087186541, 18.1851869016904, 16.091886937697414, 12.908425124174084, 7.746939085776865, 7.3844415510261285, 6.6429979453059325, 5.088798757280929, 2.9509562822175046, 0.6565565429653274]}, \"tinfo\": {\"Term\": [\"snakemake\", \"rule\", \"file\", \"value\", \"output\", \"quot\", \"input\", \"path\", \"table\", \"process\", \"use\", \"error\", \"name\", \"sample\", \"datum\", \"teradata\", \"run\", \"thread\", \"base\", \"column\", \"self\", \"select\", \"nextflow\", \"user\", \"job\", \"info\", \"task\", \"expand\", \"param\", \"query\", \"require\", \"package\", \"config\", \"load\", \"project\", \"ve\", \"merge\", \"next\", \"provide\", \"fix\", \"copy\", \"solve\", \"parse\", \"empty\", \"reason\", \"good\", \"even\", \"save\", \"zip\", \"status\", \"software\", \"export\", \"thing\", \"skip\", \"already\", \"append\", \"previous\", \"bash\", \"answer\", \"tell\", \"problem\", \"include\", \"seem\", \"change\", \"add\", \"possible\", \"check\", \"also\", \"use\", \"find\", \"m\", \"new\", \"work\", \"see\", \"make\", \"way\", \"still\", \"question\", \"instead\", \"import\", \"try\", \"different\", \"option\", \"script\", \"however\", \"want\", \"get\", \"code\", \"file\", \"set\", \"need\", \"issue\", \"create\", \"thank\", \"help\", \"run\", \"command\", \"follow\", \"snakemake\", \"rule\", \"sample\", \"param\", \"wildcard\", \"workflow\", \"fastq_gz\", \"fastq\", \"snakefile\", \"bam\", \"site_package\", \"txt\", \"trim\", \"lib_python\", \"genome\", \"submit\", \"directive\", \"fastqc\", \"singularity\", \"sbatch\", \"vcf\", \"bed\", \"touch\", \"smk\", \"jobid\", \"conda_environment\", \"miniconda\", \"barcode\", \"os_path\", \"input\", \"job\", \"output\", \"channel\", \"log\", \"folder\", \"directory\", \"file\", \"read\", \"line\", \"result\", \"miss\", \"run\", \"pipeline\", \"script\", \"error\", \"name\", \"list\", \"table\", \"column\", \"select\", \"query\", \"row\", \"date\", \"sql\", \"insert\", \"varchar\", \"cast\", \"field\", \"dataframe\", \"panda\", \"sum\", \"month\", \"null\", \"day\", \"convert\", \"final\", \"condition\", \"separate\", \"timestamp\", \"apr\", \"character\", \"procedure\", \"unique\", \"assign\", \"min\", \"decimal\", \"minute\", \"teradata\", \"case\", \"format\", \"end\", \"number\", \"value\", \"datum\", \"result\", \"join\", \"key\", \"get\", \"want\", \"create\", \"need\", \"name\", \"try\", \"time\", \"use\", \"expect\", \"expand\", \"instal\", \"conflict\", \"container\", \"connect\", \"service\", \"server\", \"docker\", \"install\", \"session\", \"connection\", \"host\", \"profile\", \"pool\", \"env\", \"driver\", \"late\", \"network\", \"mode\", \"dir\", \"manager\", \"port\", \"deploy\", \"int\", \"machine\", \"client\", \"enable\", \"https\", \"master\", \"username\", \"user\", \"database\", \"version\", \"error\", \"local\", \"fail\", \"access\", \"module\", \"message\", \"execute\", \"create\", \"name\", \"run\", \"teradata\", \"command\", \"try\", \"get\", \"follow\", \"use\", \"base\", \"order\", \"python\", \"function\", \"partition\", \"split\", \"statement\", \"none\", \"monetdb\", \"item\", \"record\", \"df\", \"usr_local\", \"flag\", \"pattern\", \"finish\", \"sequence\", \"location\", \"fast\", \"traceback\", \"rename\", \"err\", \"stat\", \"length\", \"dplyr\", \"appear\", \"approach\", \"collect\", \"d\", \"reshape\", \"else\", \"type\", \"remove\", \"replace\", \"group\", \"start\", \"current\", \"list\", \"count\", \"string\", \"return\", \"map\", \"call\", \"last\", \"path\", \"thread\", \"main\", \"def\", \"com\", \"java\", \"worker\", \"class\", \"debug_nextflow\", \"template\", \"target\", \"html\", \"prefix\", \"executor\", \"spark\", \"scheduler\", \"happen\", \"regular\", \"plugin\", \"action\", \"url\", \"org\", \"warn\", \"debug\", \"context\", \"page\", \"gt\", \"util\", \"api\", \"groovy\", \"info\", \"configuration\", \"node\", \"resource\", \"map\", \"core\", \"false\", \"run\", \"start\", \"import\", \"self\", \"nextflow\", \"task\", \"would_require\", \"parameter\", \"ansible\", \"tmp\", \"luigi\", \"yaml\", \"metaflow\", \"combine\", \"ansible_galaxy\", \"rerun\", \"wait\", \"extract\", \"pair\", \"tsv\", \"flow\", \"channel_frompath\", \"runtime\", \"nf\", \"running\", \"input_file\", \"dsl\", \"token\", \"def_require\", \"fa\", \"control\", \"aggregate\", \"pickle\", \"process\", \"role\", \"documentation\", \"pass\", \"test\", \"complete\", \"variable\", \"shell\", \"return\", \"run\", \"pipeline\", \"output\", \"miss\", \"fail\", \"time\", \"execute\", \"quot\", \"cluster\", \"dataset\", \"image\", \"batch\", \"collection\", \"model\", \"memory\", \"analysis\", \"tuple\", \"size\", \"parallel\", \"processing\", \"knime\", \"wrapper\", \"pull\", \"ggplot\", \"section\", \"raw\", \"limit\", \"achieve\", \"volume\", \"move\", \"foo\", \"plot\", \"airflow\", \"cpu\", \"sqlalchemy\", \"external\", \"apache_airflow\", \"response\", \"true\", \"filter\", \"datum\", \"request\", \"time\", \"execution\", \"number\", \"data\", \"object\", \"read\", \"attribute\", \"operator\", \"false_false\", \"label\", \"tag\", \"cell\", \"upload\", \"window_size\", \"site\", \"rapidminer\", \"ready\", \"extra\", \"weight\", \"play\", \"mark\", \"matrix\", \"inputs_parameter\", \"arguments_parameter\", \"outputs_artifact\", \"essentially\", \"whalesay\", \"png\", \"represent\", \"width\", \"threshold\", \"validation\", \"confirm\", \"area\", \"portion\", \"realize\", \"range\", \"value\", \"false\", \"pop\", \"_\", \"name\", \"apply\", \"text\", \"word\", \"set\", \"process\", \"example\", \"scale\", \"share\", \"specie\", \"customer\", \"partial\", \"python_site\", \"entire\", \"other\", \"escape\", \"keplergl\", \"symlink\", \"reduce\", \"hide\", \"hadoop\", \"asset\", \"php\", \"yesterday\", \"warehouse\", \"cust\", \"valueerror\", \"subject\", \"quantity\", \"custid\", \"buy\", \"dist\", \"safety\", \"java_home\", \"mapper\", \"bzip\", \"decompress\", \"library\", \"lib\"], \"Freq\": [7575.0, 7412.0, 11642.0, 3915.0, 7514.0, 3481.0, 6351.0, 3230.0, 4474.0, 3375.0, 9703.0, 6437.0, 4023.0, 4106.0, 4192.0, 4859.0, 6925.0, 2191.0, 2054.0, 2755.0, 1758.0, 2520.0, 1669.0, 2061.0, 2627.0, 1580.0, 1472.0, 1922.0, 2282.0, 1965.0, 809.8137564069244, 760.2768531771751, 712.5553362396313, 672.7641453577672, 657.3907412853459, 645.8573350765142, 545.4397808513478, 480.2118199718222, 460.7286336298185, 449.6501290814504, 432.99746765477994, 424.7653576926858, 420.2049230187934, 413.9040566187235, 410.53477227352937, 396.0690613472107, 387.46654343714806, 379.8257449299973, 374.58239408857276, 373.87161205075483, 350.2261492270952, 345.81712682348996, 333.19261569784686, 326.63982911153175, 325.80156938910784, 318.4706775161206, 298.5535756358809, 248.0117377790732, 253.54094504296233, 240.8587798942021, 1360.6140172557114, 514.1396443984036, 785.9125866449373, 1112.5936653383133, 1103.0656189689048, 672.5635342169134, 800.6294661883855, 1350.4710717237317, 8202.596652306996, 1650.9967358124188, 2184.8766320968625, 1020.8190981294337, 2671.4073272210244, 1094.803446242187, 996.9331910017938, 1754.5801882174985, 497.3084853067802, 678.4276233348072, 523.6365295560194, 1557.1292220556452, 3198.8858202829356, 948.539077518805, 797.1879155820769, 1977.1269721570754, 1000.4645252109112, 1670.1318011213816, 2409.310201394626, 1315.7299627945677, 4577.362554147923, 1503.4938968125607, 1300.3755213890627, 894.9962530899471, 1846.6527236742054, 923.6365864428151, 906.0201532514171, 1194.4774673599984, 939.5715860836095, 816.9177611265167, 7575.087059859171, 7411.947616256041, 4106.081279377366, 2281.959050977194, 1881.9543431904335, 1839.2549952174593, 1188.4893704216727, 940.6056421535625, 910.5274730373103, 834.2220260498632, 809.9802946163351, 637.6830455205155, 505.0939936117079, 484.251549536699, 383.8679504095321, 329.6896554644452, 312.5310744904991, 297.4396906701645, 270.1791884174609, 255.9495072857958, 217.5857752078399, 216.73037760341325, 210.03640141079194, 208.6752795866465, 197.7910573330988, 188.54585113905748, 186.9483945247564, 182.61724865214893, 179.96058493791398, 6318.452527152079, 2580.007535173301, 6901.9668059331725, 909.0069125716951, 1586.8387583303434, 502.0736229950379, 1027.3625661723065, 7064.0590409175775, 1429.6643008439041, 1162.3041673495825, 1355.3124889143955, 724.3925402488354, 1876.2236803538135, 722.9500404903763, 974.2197401555077, 1219.013694827409, 780.3797121050462, 607.2452513380825, 4473.425063437408, 2754.722847283925, 2519.2107268788686, 1964.4512785367667, 1911.8490921028806, 1745.2873542821474, 1165.1646355890675, 785.3168961577388, 628.0180536038857, 564.7563969355207, 538.835279365656, 531.3235363850816, 528.2277299762251, 524.99131675604, 509.45590875009293, 488.42251211059664, 486.88512214865, 444.98770657427224, 444.19902602343285, 436.5511729847499, 406.5373021625433, 403.57585707443207, 396.4985504188212, 384.2580550615215, 319.3617036879244, 300.6082233858047, 295.3761261046818, 292.5355312593544, 285.5201590520264, 267.1238791746587, 3848.018291082939, 1140.0289509149388, 742.1185865314961, 857.394187694023, 963.5472089447712, 1982.1836419468443, 1736.9845158277083, 1165.1919492012178, 594.5266936249847, 514.2552768070118, 1043.5601040892059, 804.0407936527963, 907.4871025580177, 714.0755843543103, 837.5996465665992, 813.7034003169975, 629.833118077159, 761.9917375670598, 535.2430397286472, 1921.1465437488082, 1310.592721154679, 1039.6983854661166, 1016.8976641006985, 902.01698313017, 825.0162860642574, 787.0009668805397, 728.115291037959, 632.6118031064713, 550.5290859523172, 545.4204463632258, 487.903887934499, 487.69243491080874, 450.03332272403566, 432.29414747694557, 402.7881174658081, 348.1665383032948, 341.72298251599597, 331.1076132115512, 329.0260150045587, 322.42915494009384, 320.22210774289755, 318.89303647900476, 316.4918038640648, 282.08093877348017, 278.2129616418199, 268.67666741072594, 264.3307750498786, 261.46677393553256, 257.4387625023383, 2044.0706559272303, 1304.7387021421641, 1137.8092484959145, 4654.129748074548, 812.0816134241512, 1110.2561613375988, 642.7310209866673, 872.1449600320641, 540.0703274878792, 722.8231873106381, 1161.9138912078602, 1094.0616853311735, 1258.5375357510243, 1010.9367662892878, 581.9056953418774, 664.4582116164274, 649.1449513068973, 524.5757740157223, 536.234981789507, 2053.161906362398, 915.3483020055944, 698.9930899048685, 1314.8643856806925, 534.8426842193362, 527.8607719818884, 486.52381374199416, 462.01806709327144, 445.46571666645946, 414.8819912958119, 410.63469041610625, 398.3558191812953, 382.8610244640584, 369.1427469459915, 330.8628984302413, 312.1158593256172, 306.95173622073617, 294.8999107312315, 287.83104480409185, 275.9388949927993, 259.17902230933026, 256.418172505344, 250.9606959544882, 240.53703721292985, 228.51313392023025, 220.39831813559758, 207.17159555812793, 204.2160119838892, 200.84786943287878, 183.23577842969226, 463.3031696643911, 1250.1668316201276, 445.4437878198659, 277.790639761682, 943.3829893839138, 942.7981666890207, 305.13170208742116, 798.027449336801, 507.23819510938114, 525.8291665727097, 647.4268659346263, 371.49899549907235, 388.5214330468273, 309.0017626628394, 3229.178816568904, 2190.5900127450723, 1325.2579795622753, 1081.5977858482017, 959.9206573366591, 930.2240102068915, 693.3586986776022, 673.5897468478569, 620.9475421865615, 592.5418592376152, 509.95507873669317, 443.04592957830005, 412.2501114543299, 403.37446999387754, 353.65608921936496, 329.20373807525203, 328.934677340647, 282.10800206810876, 280.2903995526816, 279.39381703802866, 276.650628017384, 275.75056808093746, 266.65982324382105, 250.5410370013279, 249.8007474002575, 243.18365318135187, 234.68355416790016, 234.35457478085073, 203.6570989094734, 193.1148170742071, 1546.9545479471235, 300.24441900601624, 331.58214172840064, 477.42880381598167, 466.98075628998055, 458.50234339595625, 430.3208968325606, 715.9551383464184, 329.1844402949686, 308.6063955900929, 1757.6033466325578, 1669.06994899551, 1471.2687241240035, 1271.9117535650457, 817.3528813056491, 531.9784649253849, 428.1106111711703, 322.36724095027796, 268.19319979048817, 249.15205650487692, 246.07808714733883, 238.50866040749335, 234.03432742250212, 222.8005654385301, 219.96506629121495, 210.2954206875283, 209.68205688099764, 203.8282268416548, 195.84777673889585, 188.8032972882546, 183.48287117235483, 174.06548078023167, 157.63630304787839, 150.7367551824036, 144.2274844247076, 131.03718901169034, 129.4659996537845, 119.40131578848948, 113.48210152832438, 112.37773728751169, 3043.0908170290245, 480.3422906395773, 348.42085910735824, 710.8076090242305, 1172.0412408105335, 519.8198187305229, 652.4282314766809, 450.7421113395992, 826.4717456338288, 1880.0779614356275, 671.9748133525918, 611.9980257245116, 355.0351846383628, 302.00370007970827, 231.70509190297327, 228.0208354411987, 3480.581005760331, 782.8815031752338, 615.9146524378378, 543.6933523484851, 474.82497751673253, 413.34652642609757, 364.36686238338376, 353.0949935965669, 347.6191423975362, 344.39404142444164, 343.79218233454145, 331.3707376689015, 329.5031706246384, 262.94682478067705, 259.45501504090083, 239.59237471779957, 237.36980166211913, 224.4994183062749, 209.34548597164087, 189.84318425526237, 189.2471976036491, 183.39597119582464, 167.44366375911014, 160.61322801729338, 158.52346950197224, 157.12069409790777, 141.38012361207242, 138.7776459333141, 137.21481366867587, 124.25484271170099, 168.05035510204016, 1117.8288981003261, 357.0456698706468, 1613.0504074819949, 269.14664491865045, 503.9207761903394, 253.96805097659174, 287.88601683310185, 227.79327684155407, 213.9288977207843, 217.1381194100806, 483.77597441541775, 355.93362169091193, 326.9932722521556, 322.52275267308573, 320.92478816482605, 282.09422923502933, 240.6018977510746, 199.59123707348374, 196.77852817254677, 179.8129712707733, 174.36345753390808, 156.87321205069222, 150.4650142876804, 125.92664918676124, 124.5550662925064, 101.72423351532031, 99.74515625852142, 86.28454015445358, 87.30157381445717, 84.74182531258866, 77.02328046578165, 70.4159517879583, 69.58026622449184, 67.5628440499712, 60.31189035435503, 57.17381085378921, 56.62777754564607, 56.66624844726715, 56.134268070439596, 54.73698827841641, 474.08807216052946, 1932.4796454033483, 640.5538815551178, 100.53975580003234, 270.0679436424679, 1126.5337953991595, 172.80255519821338, 255.11102821920156, 133.35044515686315, 344.7070360789371, 331.57016805458255, 207.24519417457023, 264.06522037003424, 235.33629338716972, 153.97364963628422, 133.5074614870209, 110.25227655050784, 87.34068268877465, 71.41152213173726, 60.29483146215959, 56.729931362707525, 50.742025835138335, 47.57705897873875, 43.12603108278711, 35.10277449442892, 34.38463354143797, 30.29351532571836, 26.0533348203874, 22.77487160439382, 20.819248296163792, 20.702753566944246, 17.08564616135473, 16.339330726641492, 12.390720857181732, 12.110818380955365, 12.098786679610889, 7.991507846187075, 7.133847573200395, 6.907363217814368, 6.3055297021159324, 5.378976153555765, 5.132396775903052, 69.07434991818153, 8.318413376229135], \"Total\": [7575.0, 7412.0, 11642.0, 3915.0, 7514.0, 3481.0, 6351.0, 3230.0, 4474.0, 3375.0, 9703.0, 6437.0, 4023.0, 4106.0, 4192.0, 4859.0, 6925.0, 2191.0, 2054.0, 2755.0, 1758.0, 2520.0, 1669.0, 2061.0, 2627.0, 1580.0, 1472.0, 1922.0, 2282.0, 1965.0, 810.6742165173305, 761.137231661535, 713.4156970831301, 673.6244455609508, 658.2510507132923, 646.7177195350051, 546.3000723321564, 481.07221471404245, 461.58898976947415, 450.5105377047439, 433.85773957440693, 425.62568909292537, 421.0652726139104, 414.76440344363954, 411.3966379623649, 396.9293468775382, 388.3268422231524, 380.68627457425663, 375.4427629372829, 374.7320222546711, 351.08667202916746, 346.6774488159576, 334.052895819954, 327.5004472518948, 326.6618131908116, 319.3311828366589, 299.4139102074032, 248.87206890676953, 254.43443292365836, 241.7196918912447, 1373.3635941219272, 517.3362530436337, 799.7751516742437, 1159.1308148452908, 1158.122387892648, 693.9812521243038, 833.2545043608551, 1449.7469498680584, 9703.139121313221, 1812.6617093568068, 2443.0356751926256, 1110.0895223675855, 3182.5447600201505, 1217.430130685131, 1099.7456799366794, 2085.5892873223474, 510.96950359645695, 730.8356428533957, 543.866285946748, 1966.5443297805332, 4790.214576077659, 1112.3423619875982, 913.8854556145238, 2952.145821441567, 1245.1286382978426, 2605.472563495534, 4367.055123129984, 1907.6503388834317, 11642.220687915471, 2536.2622471361683, 2021.7243425927877, 1122.2726000327189, 4105.6941493476415, 1249.530125604547, 1368.2369489101523, 6925.80003435159, 1986.7162439413432, 2382.214067981391, 7575.976657871438, 7412.8371834194995, 4106.970825997381, 2282.8487096920085, 1882.8438935301235, 1840.144825704871, 1189.3788778494504, 941.495197217816, 911.4170253954265, 835.111555745565, 810.8700636864488, 638.5726066197334, 505.9836020339337, 485.1412534586333, 384.75752283509905, 330.57940005407715, 313.42096979682407, 298.3293020610458, 271.06882131488356, 256.83949449858824, 218.47532810363282, 217.61999640556283, 210.92598268068198, 209.5648466252661, 198.68063468486508, 189.43560010712474, 187.83819354829453, 183.50680854713102, 180.85017412060589, 6351.0532412278635, 2627.818997792334, 7514.779791204318, 968.4464580851435, 1959.686019160662, 556.5465088725255, 1244.591437217935, 11642.220687915471, 1908.933904216217, 1857.6804329918525, 2841.7694326763103, 1392.598915374383, 6925.80003435159, 1395.7397863456583, 2952.145821441567, 6437.146091835542, 4023.6879853394007, 1619.6126365646057, 4474.243326790644, 2755.5411370806796, 2520.0290939207316, 1965.269554859807, 1912.6673863862754, 1746.105642773638, 1165.9828892217497, 786.1351627732588, 628.8363173693651, 565.5746227669565, 539.6536581607551, 532.1419796822479, 529.0463053393582, 525.8097075924895, 510.2741514029104, 489.240861373821, 487.70347544334425, 445.8060770820396, 445.01772580328077, 437.3696850084346, 407.3559935088548, 404.3941973417227, 397.31794677978155, 385.07636643563046, 320.1801722933054, 301.42712574918255, 296.19493562823556, 293.35428412164197, 286.33844338972483, 267.942266710546, 4859.681768550955, 1397.229529370705, 859.7262466557572, 1046.9099296204952, 1440.169218823237, 3915.338157230303, 4192.314868382686, 2841.7694326763103, 858.7513258963959, 693.7046912655697, 4367.055123129984, 2605.472563495534, 4105.6941493476415, 2021.7243425927877, 4023.6879853394007, 4790.214576077659, 1834.665366616133, 9703.139121313221, 875.6625786045935, 1922.0060661014459, 1311.4520249791658, 1040.5589209346276, 1017.7569464365824, 902.8761635132767, 825.8754868453475, 787.8601654530764, 728.9744728344713, 633.4710292795054, 551.3884210234215, 546.2796211155018, 488.76310616343636, 488.55186349154536, 450.8931766579415, 433.153737999147, 403.64729742845407, 349.0258491618853, 342.582171928995, 331.9670099841354, 329.885455905328, 323.2885200524728, 321.0812902820047, 319.752205701684, 317.3513354185276, 282.94018263523674, 279.0721898706121, 269.53596433429783, 265.19008038224416, 262.326163436056, 258.29816993125615, 2061.3959477453823, 1542.721969462043, 1336.5710818530008, 6437.146091835542, 1027.5652982376212, 1537.3210333976608, 847.9621543519442, 1357.7757334092316, 829.6263205078194, 1555.7415096214268, 4105.6941493476415, 4023.6879853394007, 6925.80003435159, 4859.681768550955, 1986.7162439413432, 4790.214576077659, 4367.055123129984, 2382.214067981391, 9703.139121313221, 2054.014219444373, 916.2004981094292, 699.8453386743136, 1316.6302124839917, 535.6948859364279, 528.712977135627, 487.37594803299345, 462.8703337905718, 446.31779169284914, 415.73418172162656, 411.4867772278159, 399.20810372073754, 383.7132429284659, 369.9949528372706, 331.71514917823504, 312.9683357128072, 307.8039844101433, 295.7521199881096, 288.68333319723735, 276.7911971787337, 260.03126140851424, 257.27040717643547, 251.81291670254657, 241.38926415879442, 229.36952708300734, 221.25050706303836, 208.0237128803174, 205.06838263437285, 201.7001084761592, 184.08988940351185, 485.30812548895557, 1403.040115512071, 484.650117400035, 293.2773475352465, 1246.204104773323, 1272.7302520209766, 334.8921596419909, 1619.6126365646057, 878.6038943845081, 945.6544715604065, 1889.5707299740461, 839.227474073578, 1095.1788767189494, 603.0670719233593, 3230.025114587196, 2191.4363820992353, 1326.1043511235305, 1082.4441675121986, 960.7669069484044, 931.0701276423907, 694.2049711995687, 674.43599106819, 621.7938452717317, 593.3882366358436, 510.80141247659844, 443.892465454445, 413.09641179643523, 404.22078451840054, 354.50223439060545, 330.05004082824536, 329.78120609238346, 282.9545347738222, 281.1367332347421, 280.2401812669664, 277.4969888968974, 276.5967827790516, 267.5061369333892, 251.38735917179827, 250.64749280317372, 244.02996766931074, 235.52989695870002, 235.20088045771732, 204.50333126962175, 193.96119462759398, 1580.2653665820692, 314.45233857661907, 374.9677142838548, 867.9876548795371, 839.227474073578, 887.5430917251105, 1071.5776624604046, 6925.80003435159, 1272.7302520209766, 1966.5443297805332, 1758.4793929562568, 1669.9460583956118, 1472.144790146258, 1272.8006739495797, 818.2290519377225, 532.8544693242327, 428.9868180379061, 323.24321676763975, 269.06946279014323, 250.0283163753691, 246.95434068655786, 239.38465327490013, 234.9107649856483, 223.6767682709469, 220.8413163160264, 211.17155120419875, 210.55840458506788, 204.70431301737082, 196.72389779334742, 189.6796169745975, 184.35901965322205, 174.9417817214575, 158.51244832630206, 151.6132603003974, 145.10371788613872, 131.91315344253732, 130.342487981415, 120.27744579173984, 114.3583715104716, 113.25416210887082, 3375.3936186870255, 506.04106117579204, 366.8277533870179, 850.8927996349157, 1552.9796691833187, 628.6146036983617, 1029.2062240171033, 639.4892598208233, 1889.5707299740461, 6925.80003435159, 1395.7397863456583, 7514.779791204318, 1392.598915374383, 1537.3210333976608, 1834.665366616133, 1555.7415096214268, 3481.4518859441414, 783.7521224184927, 616.7853331916247, 544.5640369465415, 475.69559674396606, 414.2172423233736, 365.23750277756557, 353.9656225607954, 348.48980784220333, 345.26479739117707, 344.66287701668955, 332.24153235433835, 330.3739848258683, 263.81761641005045, 260.32564411528324, 240.4630676837552, 238.2432279221996, 225.37023376400137, 210.2162432616517, 190.71379399021959, 190.11788213325997, 184.26665951402674, 168.31428634617657, 161.483859424874, 159.39406577248593, 157.99125543013193, 142.25082432641372, 139.64828399909385, 138.08556551961618, 125.12551745324393, 171.5738658464039, 1287.7397619263895, 386.2950630125348, 4192.314868382686, 354.41144610895714, 1834.665366616133, 479.00590835145385, 1440.169218823237, 621.2876299648443, 528.7198504342758, 1908.933904216217, 484.58330288541435, 356.74084337290293, 327.80072206556673, 323.330000133229, 321.7322409056563, 282.9015906025807, 241.4093520962657, 200.4085185668339, 197.58647533195574, 180.62016987024958, 175.17106998081516, 157.6808040543426, 151.2726596929034, 126.73414023255134, 125.3626016766015, 102.53160713541148, 100.55529087162411, 87.09473549920904, 88.12325674390836, 85.54935532175399, 77.8424296242823, 71.22328409537606, 70.38761177284131, 68.3701351485338, 61.11923664137249, 57.981040362107656, 57.43538073209361, 57.47813490328217, 56.941933468564805, 55.54455162110251, 499.55759124046466, 3915.338157230303, 1071.5776624604046, 116.69133380259028, 519.9272174130091, 4023.6879853394007, 314.95120890556717, 621.2365017114121, 251.68711900420263, 2536.2622471361683, 3375.3936186870255, 1633.9359251946858, 264.9428349830485, 236.21379010916266, 154.8510604021259, 134.38471914767877, 111.12995961981338, 88.21792963609948, 72.28890489083703, 61.17234546475593, 57.60735288731468, 51.61942466511932, 48.454648588109784, 44.00357019415866, 35.98056539143426, 35.261929097440486, 31.17112620950917, 26.930575503659192, 23.656009405435327, 21.696716317373614, 21.579981940920664, 17.963184288792952, 17.21678570710564, 13.268088902250316, 12.988052783125974, 12.976187052017151, 8.86925133426792, 8.01168589968866, 7.787734982436988, 7.1828368443505575, 6.2577416854682015, 6.0101843760154035, 267.53422518184334, 136.94405711983936], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.0325, -5.0956, -5.1604, -5.2179, -5.241, -5.2587, -5.4277, -5.555, -5.5965, -5.6208, -5.6585, -5.6777, -5.6885, -5.7036, -5.7118, -5.7477, -5.7696, -5.7896, -5.8035, -5.8054, -5.8707, -5.8834, -5.9205, -5.9404, -5.943, -5.9657, -6.0303, -6.2158, -6.1937, -6.2451, -4.5136, -5.4868, -5.0624, -4.7148, -4.7234, -5.2182, -5.0439, -4.5211, -2.7171, -4.3201, -4.04, -4.8009, -3.8389, -4.7309, -4.8246, -4.2593, -5.5201, -5.2095, -5.4685, -4.3787, -3.6587, -4.8743, -5.0482, -4.1399, -4.821, -4.3086, -3.9422, -4.5471, -3.3004, -4.4137, -4.5589, -4.9324, -4.2081, -4.9009, -4.9202, -4.6438, -4.8838, -5.0237, -2.5907, -2.6125, -3.2031, -3.7905, -3.9833, -4.0062, -4.4429, -4.6768, -4.7093, -4.7968, -4.8263, -5.0655, -5.2986, -5.3407, -5.573, -5.7252, -5.7786, -5.8281, -5.9242, -5.9783, -6.1407, -6.1447, -6.176, -6.1825, -6.2361, -6.284, -6.2925, -6.3159, -6.3306, -2.7721, -3.6678, -2.6838, -4.711, -4.1538, -5.3046, -4.5886, -2.6605, -4.2581, -4.4652, -4.3115, -4.938, -3.9863, -4.94, -4.6417, -4.4175, -4.8635, -5.1144, -2.9951, -3.48, -3.5693, -3.8181, -3.8452, -3.9364, -4.3404, -4.7349, -4.9585, -5.0646, -5.1116, -5.1257, -5.1315, -5.1376, -5.1677, -5.2099, -5.213, -5.303, -5.3048, -5.3221, -5.3934, -5.4007, -5.4184, -5.4497, -5.6347, -5.6952, -5.7128, -5.7224, -5.7467, -5.8133, -3.1457, -4.3622, -4.7915, -4.6471, -4.5304, -3.8091, -3.9411, -4.3404, -5.0133, -5.1583, -4.4506, -4.7114, -4.5904, -4.83, -4.6705, -4.6994, -4.9556, -4.7651, -5.1183, -3.6199, -4.0024, -4.2339, -4.2561, -4.376, -4.4652, -4.5124, -4.5901, -4.7307, -4.8697, -4.879, -4.9905, -4.9909, -5.0713, -5.1115, -5.1822, -5.3279, -5.3466, -5.3782, -5.3845, -5.4047, -5.4116, -5.4157, -5.4233, -5.5384, -5.5522, -5.5871, -5.6034, -5.6143, -5.6298, -3.5579, -4.0068, -4.1437, -2.7351, -4.481, -4.1683, -4.7149, -4.4096, -4.8889, -4.5974, -4.1228, -4.1829, -4.0429, -4.262, -4.8143, -4.6816, -4.7049, -4.918, -4.896, -3.0429, -3.8507, -4.1204, -3.4885, -4.388, -4.4012, -4.4827, -4.5344, -4.5709, -4.642, -4.6523, -4.6827, -4.7223, -4.7588, -4.8683, -4.9266, -4.9433, -4.9834, -5.0076, -5.0498, -5.1125, -5.1232, -5.1447, -5.1871, -5.2384, -5.2746, -5.3365, -5.3508, -5.3675, -5.4592, -4.5316, -3.539, -4.5709, -5.0431, -3.8205, -3.8212, -4.9493, -3.9879, -4.441, -4.405, -4.197, -4.7525, -4.7077, -4.9367, -2.5421, -2.9302, -3.4327, -3.6359, -3.7552, -3.7867, -4.0805, -4.1095, -4.1908, -4.2377, -4.3878, -4.5284, -4.6005, -4.6222, -4.7538, -4.8254, -4.8262, -4.9798, -4.9863, -4.9895, -4.9993, -5.0026, -5.0361, -5.0985, -5.1014, -5.1283, -5.1639, -5.1653, -5.3057, -5.3588, -3.278, -4.9175, -4.8182, -4.4537, -4.4758, -4.4941, -4.5576, -4.0485, -4.8255, -4.89, -3.0446, -3.0963, -3.2224, -3.368, -3.8102, -4.2397, -4.4569, -4.7406, -4.9246, -4.9982, -5.0106, -5.0419, -5.0608, -5.11, -5.1228, -5.1678, -5.1707, -5.199, -5.2389, -5.2756, -5.3042, -5.3568, -5.456, -5.5007, -5.5449, -5.6408, -5.6529, -5.7338, -5.7846, -5.7944, -2.4957, -4.3418, -4.6629, -3.9499, -3.4498, -4.2628, -4.0356, -4.4054, -3.7991, -2.9772, -4.0061, -4.0996, -4.6441, -4.8058, -5.0708, -5.0868, -2.0948, -3.5868, -3.8266, -3.9514, -4.0868, -4.2255, -4.3516, -4.383, -4.3987, -4.408, -4.4097, -4.4465, -4.4522, -4.6778, -4.6912, -4.7708, -4.7801, -4.8359, -4.9058, -5.0036, -5.0067, -5.0381, -5.1291, -5.1708, -5.1839, -5.1927, -5.2983, -5.3169, -5.3282, -5.4274, -5.1255, -3.2306, -4.3719, -2.8639, -4.6545, -4.0273, -4.7125, -4.5872, -4.8213, -4.8841, -4.8692, -3.5232, -3.8301, -3.9149, -3.9287, -3.9336, -4.0626, -4.2217, -4.4086, -4.4228, -4.5129, -4.5437, -4.6494, -4.6911, -4.8691, -4.8801, -5.0826, -5.1022, -5.2472, -5.2355, -5.2652, -5.3607, -5.4504, -5.4624, -5.4918, -5.6053, -5.6587, -5.6683, -5.6677, -5.6771, -5.7023, -3.5435, -2.1383, -3.2425, -5.0943, -4.1062, -2.6779, -4.5527, -4.1631, -4.8119, -3.8622, -3.901, -4.3709, -2.6258, -2.741, -3.1652, -3.3078, -3.4992, -3.7322, -3.9335, -4.1027, -4.1637, -4.2752, -4.3396, -4.4378, -4.6437, -4.6644, -4.791, -4.9418, -5.0763, -5.1661, -5.1717, -5.3637, -5.4084, -5.685, -5.7079, -5.7089, -6.1236, -6.2371, -6.2694, -6.3605, -6.5195, -6.5664, -3.9668, -6.0835], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.4976, 1.4975, 1.4974, 1.4973, 1.4973, 1.4973, 1.497, 1.4968, 1.4968, 1.4967, 1.4966, 1.4966, 1.4966, 1.4965, 1.4965, 1.4965, 1.4964, 1.4964, 1.4963, 1.4963, 1.4962, 1.4961, 1.496, 1.496, 1.496, 1.4959, 1.4957, 1.4952, 1.4951, 1.4951, 1.4893, 1.4924, 1.4811, 1.4576, 1.4499, 1.4673, 1.4587, 1.4277, 1.3306, 1.4052, 1.3869, 1.4148, 1.3235, 1.3925, 1.4005, 1.3258, 1.4715, 1.4242, 1.4607, 1.2652, 1.0948, 1.3393, 1.362, 1.0977, 1.2798, 1.0539, 0.9039, 1.1271, 0.5651, 0.9757, 1.0573, 1.2723, 0.6996, 1.1964, 1.0864, -0.2589, 0.7498, 0.4284, 1.7044, 1.7044, 1.7043, 1.7042, 1.7041, 1.7041, 1.7038, 1.7036, 1.7036, 1.7035, 1.7035, 1.7032, 1.7028, 1.7027, 1.7022, 1.7019, 1.7017, 1.7016, 1.7013, 1.7011, 1.7005, 1.7005, 1.7003, 1.7003, 1.7001, 1.6999, 1.6998, 1.6997, 1.6996, 1.6994, 1.6862, 1.6195, 1.6412, 1.4935, 1.6016, 1.5128, 1.2049, 1.4155, 1.2356, 0.9642, 1.051, 0.3986, 1.0467, 0.5959, 0.0405, 0.0644, 0.7236, 1.8267, 1.8266, 1.8265, 1.8264, 1.8264, 1.8264, 1.8262, 1.8258, 1.8256, 1.8254, 1.8253, 1.8253, 1.8253, 1.8253, 1.8253, 1.8252, 1.8252, 1.825, 1.825, 1.825, 1.8248, 1.8248, 1.8248, 1.8247, 1.8243, 1.8241, 1.8241, 1.8241, 1.824, 1.8238, 1.5934, 1.6234, 1.6798, 1.6272, 1.425, 1.1462, 0.9458, 0.9353, 1.4591, 1.5275, 0.3954, 0.6511, 0.3174, 0.7861, 0.2574, 0.0541, 0.7577, -0.7174, 1.3346, 2.0468, 2.0466, 2.0465, 2.0464, 2.0463, 2.0462, 2.0462, 2.0461, 2.0459, 2.0457, 2.0457, 2.0455, 2.0455, 2.0454, 2.0453, 2.0452, 2.0448, 2.0448, 2.0447, 2.0447, 2.0446, 2.0446, 2.0446, 2.0446, 2.0442, 2.0442, 2.0441, 2.044, 2.044, 2.044, 2.0388, 1.8797, 1.8863, 1.723, 1.8119, 1.7218, 1.7702, 1.6046, 1.618, 1.2807, 0.785, 0.745, 0.342, 0.4772, 0.8194, 0.0719, 0.1411, 0.5341, -0.8483, 2.5575, 2.5569, 2.5567, 2.5565, 2.5563, 2.5563, 2.5561, 2.556, 2.556, 2.5558, 2.5558, 2.5557, 2.5556, 2.5556, 2.5553, 2.5551, 2.5551, 2.555, 2.5549, 2.5548, 2.5546, 2.5546, 2.5545, 2.5543, 2.5541, 2.554, 2.5538, 2.5537, 2.5536, 2.5532, 2.5115, 2.4425, 2.4735, 2.5036, 2.2795, 2.2578, 2.4648, 1.8501, 2.0085, 1.971, 1.4868, 1.7429, 1.5215, 1.8892, 2.6055, 2.6054, 2.6052, 2.605, 2.6049, 2.6049, 2.6046, 2.6045, 2.6044, 2.6044, 2.6041, 2.6039, 2.6037, 2.6037, 2.6034, 2.6032, 2.6032, 2.6028, 2.6028, 2.6028, 2.6027, 2.6027, 2.6026, 2.6024, 2.6024, 2.6023, 2.6022, 2.6022, 2.6016, 2.6014, 2.5845, 2.5596, 2.4828, 2.008, 2.0196, 1.9453, 1.6934, 0.3364, 1.2535, 0.7538, 2.7111, 2.7111, 2.711, 2.7109, 2.7105, 2.71, 2.7096, 2.7089, 2.7083, 2.7081, 2.7081, 2.7079, 2.7079, 2.7077, 2.7076, 2.7074, 2.7074, 2.7073, 2.7071, 2.707, 2.7068, 2.7066, 2.7061, 2.7058, 2.7055, 2.7049, 2.7049, 2.7043, 2.7039, 2.7038, 2.608, 2.6595, 2.6601, 2.5317, 2.4302, 2.5216, 2.2558, 2.3618, 1.8847, 1.4077, 1.9806, 0.2037, 1.3449, 1.0842, 0.6425, 0.7913, 2.9779, 2.977, 2.9767, 2.9765, 2.9763, 2.976, 2.9757, 2.9757, 2.9756, 2.9756, 2.9756, 2.9755, 2.9755, 2.9748, 2.9748, 2.9745, 2.9745, 2.9743, 2.974, 2.9736, 2.9735, 2.9734, 2.9729, 2.9727, 2.9727, 2.9726, 2.972, 2.9719, 2.9718, 2.9711, 2.9574, 2.8366, 2.8994, 2.023, 2.7029, 1.6859, 2.3436, 1.3682, 1.9748, 2.0733, 0.8044, 3.5214, 3.5208, 3.5206, 3.5205, 3.5205, 3.5202, 3.5197, 3.519, 3.5189, 3.5186, 3.5184, 3.5179, 3.5177, 3.5166, 3.5166, 3.5151, 3.515, 3.5137, 3.5137, 3.5136, 3.5125, 3.5116, 3.5115, 3.5112, 3.5097, 3.509, 3.5089, 3.5088, 3.5088, 3.5084, 3.4707, 2.8169, 3.0085, 3.3741, 2.868, 2.25, 2.9228, 2.633, 2.8878, 1.5273, 1.2026, 1.4582, 5.0226, 5.0222, 5.0202, 5.0194, 5.018, 5.0159, 5.0137, 5.0115, 5.0106, 5.0088, 5.0076, 5.0058, 5.0012, 5.0007, 4.9974, 4.9928, 4.988, 4.9846, 4.9844, 4.9758, 4.9736, 4.9575, 4.956, 4.9559, 4.9217, 4.9099, 4.906, 4.8956, 4.8746, 4.868, 3.6719, 2.2248]}, \"token.table\": {\"Topic\": [6, 9, 3, 4, 6, 8, 6, 1, 3, 4, 7, 8, 1, 1, 3, 4, 5, 8, 7, 7, 1, 8, 6, 5, 1, 6, 7, 9, 5, 3, 9, 9, 10, 3, 9, 2, 2, 5, 1, 8, 2, 10, 10, 2, 3, 4, 5, 6, 3, 5, 3, 9, 1, 3, 2, 6, 7, 3, 1, 4, 6, 4, 8, 1, 2, 3, 4, 5, 7, 5, 8, 3, 6, 7, 1, 2, 4, 2, 6, 7, 2, 3, 1, 2, 6, 9, 4, 4, 4, 4, 6, 7, 3, 1, 1, 6, 3, 5, 8, 1, 2, 3, 4, 6, 2, 5, 6, 10, 10, 10, 5, 3, 8, 3, 4, 3, 8, 3, 1, 2, 3, 8, 9, 3, 6, 6, 3, 10, 6, 7, 4, 5, 1, 2, 3, 8, 4, 2, 2, 4, 10, 4, 6, 7, 5, 4, 7, 2, 5, 1, 4, 2, 3, 5, 10, 4, 5, 1, 2, 3, 4, 6, 10, 9, 1, 1, 2, 3, 5, 7, 8, 9, 1, 2, 3, 4, 7, 6, 8, 6, 4, 3, 4, 6, 1, 8, 9, 7, 7, 2, 4, 7, 6, 9, 9, 5, 2, 2, 2, 3, 1, 2, 2, 3, 8, 3, 1, 3, 4, 5, 1, 5, 7, 2, 4, 1, 2, 3, 4, 5, 7, 8, 2, 3, 2, 5, 2, 1, 2, 3, 4, 5, 6, 7, 8, 1, 6, 3, 5, 6, 10, 6, 1, 2, 3, 4, 5, 10, 4, 1, 2, 3, 4, 5, 7, 6, 4, 8, 1, 2, 6, 7, 1, 5, 4, 6, 2, 9, 7, 9, 3, 4, 4, 1, 3, 5, 4, 1, 3, 4, 5, 6, 10, 2, 6, 2, 2, 3, 10, 2, 3, 8, 9, 2, 3, 4, 5, 7, 4, 5, 6, 10, 2, 5, 6, 10, 8, 1, 2, 2, 5, 7, 9, 1, 4, 6, 5, 2, 4, 7, 1, 2, 3, 4, 4, 6, 1, 2, 3, 4, 5, 4, 5, 6, 10, 9, 4, 9, 8, 1, 2, 4, 6, 7, 3, 2, 3, 1, 2, 3, 4, 7, 4, 8, 1, 4, 5, 3, 8, 1, 2, 3, 4, 5, 6, 9, 1, 3, 5, 4, 1, 3, 5, 1, 7, 7, 5, 6, 5, 3, 2, 3, 5, 8, 5, 6, 8, 9, 1, 4, 5, 6, 2, 10, 2, 7, 9, 1, 6, 7, 3, 8, 2, 7, 1, 10, 5, 3, 7, 6, 5, 10, 7, 2, 7, 9, 8, 6, 9, 4, 2, 9, 4, 9, 1, 3, 6, 1, 1, 2, 3, 7, 9, 8, 4, 1, 1, 8, 5, 10, 10, 3, 1, 3, 5, 8, 8, 6, 9, 9, 8, 2, 3, 4, 6, 7, 8, 9, 9, 9, 1, 5, 10, 6, 2, 5, 5, 2, 5, 9, 6, 8, 1, 7, 5, 2, 6, 5, 8, 2, 3, 5, 7, 9, 2, 3, 5, 7, 7, 9, 3, 2, 1, 2, 4, 6, 7, 7, 7, 10, 2, 1, 2, 10, 6, 1, 2, 8, 1, 2, 3, 4, 5, 1, 2, 3, 5, 3, 7, 3, 5, 4, 4, 4, 1, 2, 3, 4, 8, 9, 10, 6, 7, 2, 9, 2, 8, 1, 2, 2, 2, 1, 1, 6, 10, 5, 3, 8, 5, 6, 5, 5, 1, 1, 3, 4, 3, 4, 5, 10, 2, 3, 10, 3, 9, 6, 7, 1, 6, 3, 4, 4, 7, 1, 9, 1, 2, 3, 1, 6, 9, 1, 2, 3, 4, 7, 8, 3, 7, 7, 2, 5, 2, 6, 8, 1, 2, 3, 4, 5, 7, 8, 2, 3, 5, 3, 9, 6, 1, 2, 3, 4, 5, 8, 3, 4, 4, 5, 6, 9, 3, 9, 10, 3, 3, 4, 7, 2, 1, 1, 4, 8, 7, 1, 2, 3, 5, 7, 8, 10, 6, 1, 2, 3, 8, 9, 9, 9, 2, 9, 3, 9, 1, 2, 3, 4, 7, 6, 2, 7, 8, 7, 10, 1], \"Freq\": [0.47891318565499236, 0.5193034543246905, 0.14269504762565066, 0.7582885588701931, 0.09906102479797235, 0.9941200579308136, 0.9955745772738245, 0.9524036591737508, 0.04058293017331486, 0.006044266196025617, 0.9881218008569909, 0.9937258842115455, 0.9979740111513279, 0.9311969927737138, 0.024831919807299034, 0.017244388755068774, 0.026211470907704537, 0.9985944844549798, 0.9983964302198378, 0.9983931581676692, 0.9982925545152581, 0.9910048927177104, 0.9975387624910709, 0.9943480036288366, 0.9958313409143641, 0.2698830726682044, 0.1809804134363253, 0.5492914302541101, 0.9950788644902884, 0.9966828913960133, 0.9916814471435665, 0.9874305204219952, 0.9624291338837831, 0.9959657121560803, 0.9987962794385586, 0.9986689733390497, 0.9972382030337537, 0.999506225694656, 0.9964959149068021, 0.9985377271752623, 0.9971510136209754, 0.9247708862315296, 0.7990102901836067, 0.32780033255894186, 0.08948309913865266, 0.1798792911256589, 0.35519311800954984, 0.04839392096274072, 0.81590030559506, 0.18321971774766257, 0.9989840018561206, 0.996813059266792, 0.9602022358007557, 0.039684908218180374, 0.9386166807789418, 0.06092231481403473, 0.9963202345954539, 0.9972047974649975, 0.9612909330918099, 0.03840363278269403, 0.9993535471505613, 0.9961580196467832, 0.9990403567697248, 0.6898538863103545, 0.00891148637331005, 0.21754510852492182, 0.061856199532387406, 0.019919793069751876, 0.0020968203231317762, 0.9947901152744851, 0.9970613431817903, 0.9998036185802499, 0.9992017762655457, 0.9961355581606515, 0.4731425551417362, 0.23405456185202908, 0.2929457096728622, 0.13203638526957803, 0.03976999556312591, 0.827215907713019, 0.9977005372439055, 0.9991547539276128, 0.9994173143584733, 0.0413417182993296, 0.9540396530614523, 0.992419642273733, 0.9994628646937882, 0.9990295861728509, 0.9976575712033907, 0.999256260112758, 0.9974167194096684, 0.9893791742638788, 0.998191866097215, 0.9980229934926403, 0.48223010689892226, 0.5171579884733769, 0.4222608189779288, 0.5770518469590563, 0.9912069098204764, 0.44986302749645196, 0.039700960195951096, 0.2209127048940346, 0.28302156900426484, 0.006332668497513671, 0.04777657385919231, 0.9107409391908534, 0.03881846626059375, 0.9731240766322939, 0.923926026508789, 0.9971371808482482, 0.9965289633136616, 0.632557258579634, 0.3669797836034518, 0.15362457052623904, 0.8459074453027086, 0.9978539943739643, 0.9987267317341013, 0.9993667950285747, 0.14144929916219962, 0.0400733259009267, 0.41432956601136717, 0.3847516349892546, 0.01932106784508966, 0.9985575754966586, 0.9984591143601077, 0.9987232982800195, 0.9988180302102705, 0.8319212335570428, 0.9995896624273753, 0.9930776164566857, 0.9976475355344827, 0.9969737495068921, 0.8531545973887674, 0.059334250187206165, 0.05304122365219945, 0.03506114783789455, 0.9973158686159777, 0.9986568550371823, 0.8251703886824722, 0.17355092887576823, 0.901992704738289, 0.998663227766149, 0.049069351579320894, 0.9486741305335372, 0.9983889443043861, 0.9983963786390301, 0.9959551011621126, 0.043271478256916, 0.9540330682358146, 0.9981570177255016, 0.998011529423832, 0.09360881698344861, 0.8185995526001577, 0.08692247291320228, 0.9821700869202072, 0.9973364237730548, 0.9950619770443935, 0.022214813515165036, 0.18936963409081245, 0.050022167495686304, 0.722991203493553, 0.01537948627972964, 0.9894570248957156, 0.9935784984037829, 0.9965831817971781, 0.42290520047024877, 0.06426200586016804, 0.23379129751032568, 0.08568267448022407, 0.034273069792089626, 0.031824993378368936, 0.12668795441004557, 0.06170690915315397, 0.2307581290206487, 0.0957742652481244, 0.4647301595596909, 0.14655390923874068, 0.4676351504116476, 0.5302648580560647, 0.9969799066125336, 0.9994765541487147, 0.6109659280547832, 0.18500276699976612, 0.20327464522196526, 0.998045881500884, 0.9921384576618766, 0.9956823910277122, 0.9961904034532086, 0.9897003041586377, 0.08065979538830961, 0.7220352651695456, 0.1964456307037863, 0.40127749491594944, 0.5981834284677293, 0.9975572901105246, 0.99763293159439, 0.9994740310738925, 0.9988406740062985, 0.9955441786915931, 0.9987887450573709, 0.3931380552467012, 0.6067570946608472, 0.03883042118897923, 0.03624172644304729, 0.9241640242977058, 0.9977130668189819, 0.9108152897353529, 0.022066996722843197, 0.06675266508660066, 0.9969059626731831, 0.9988667574628887, 0.9973109015957086, 0.9965593640554556, 0.9019911040623937, 0.09702693151268776, 0.342958263483138, 0.16749124495688136, 0.21366677614800153, 0.22038321704852812, 0.04533597607855435, 0.010074661350789856, 0.9970036669510051, 0.13608983144939152, 0.8630654267987052, 0.0007595146993576668, 0.9987618296553319, 0.9980311682289739, 0.5516303165583598, 0.05266707048917507, 0.23906270256825554, 0.14861273368467226, 0.0027478471559569603, 0.00022898726299641335, 0.005266707048917507, 0.9947816862076534, 0.9976586591925013, 0.9950443972597741, 0.24233590536514238, 0.7566978766865208, 0.9977501923724234, 0.9642127039064325, 0.9976311382275538, 0.6621660091270449, 0.010963013396143128, 0.3040409048530361, 0.021195159232543382, 0.0014617351194857503, 0.9727473601160336, 0.9984386993334534, 0.80312986886805, 0.1060131426905826, 0.04095962331227055, 0.041762753181138604, 0.0024093896066041502, 0.0056219090820763505, 0.9979894557265545, 0.9955123495549728, 0.9989642412861045, 0.7917441658555247, 0.006610580703996031, 0.15712841827190566, 0.04424004009597344, 0.9935510936571608, 0.0038659575628683302, 0.020882568648185447, 0.9789495060225117, 0.9947956283828171, 0.005038534363445734, 0.9967671414345505, 0.9944777558017008, 0.9985560208638241, 0.9996553248074987, 0.9992564312214228, 0.9634721135321611, 0.03125768307260828, 0.005516061718695579, 0.995741831630406, 0.7974889523043751, 0.07752127245863759, 0.12474687522079611, 0.9982340116499775, 0.9988506476465951, 0.8988492823377401, 0.981802781001086, 0.017885554537616682, 0.996574227347599, 0.30625862466700826, 0.6928664702542582, 0.9880001633273165, 0.25803487024635635, 0.7409492922157943, 0.9969008270896526, 0.9989793705097175, 0.09617504038982072, 0.2454121720291977, 0.04145475878871583, 0.5123808186285276, 0.10280780179601526, 0.9970608218149211, 0.9983874006984074, 0.9346882419876575, 0.058418015124228596, 0.9976475852125599, 0.6242191999415772, 0.11587302513885564, 0.25791092692196904, 0.9962572503263388, 0.37412247427329615, 0.625511244756216, 0.3747809731143617, 0.4927104061701164, 0.10619823290884714, 0.025932126640532443, 0.9990730063835038, 0.7902174211144172, 0.20923244524581244, 0.9974569244401702, 0.8098236066815008, 0.1898263274640947, 0.9961539277449605, 0.894379080169478, 0.005730575342046998, 0.07286017220602613, 0.026606242659503922, 0.9966770975176446, 0.9991672215519127, 0.90657323614802, 0.03455344330353537, 0.0009093011395667201, 0.023641829628734724, 0.033644142163968646, 0.9960143340312125, 0.4420732297992822, 0.5564641464050264, 0.8353245563024473, 0.9971075769667185, 0.9949446009552179, 0.9948151877234364, 0.997271987732002, 0.9976202230276001, 0.3061619354657468, 0.6508954533523752, 0.04218766827283913, 0.99588720033684, 0.9987922994794408, 0.9955376830853144, 0.9964833218658857, 0.09406872183638192, 0.5198912565613779, 0.02297861907453604, 0.10771227691188769, 0.2549190553581342, 0.9970870298702825, 0.9966117861168292, 0.35720184715793685, 0.6422268262303524, 0.9970474139338007, 0.997503006179311, 0.992191474801649, 0.0029823386017312653, 0.19385200911253225, 0.2082666456875667, 0.27188986919116703, 0.00994112867243755, 0.03305425283585486, 0.280091300345928, 0.6430154559710141, 0.35316387351023387, 0.0034623909167669988, 0.998300635652705, 0.9197456416149425, 0.0756695728654801, 0.004504141241992863, 0.9977711980005335, 0.9994334796678878, 0.9926284070300528, 0.11467653977123085, 0.8854095628848522, 0.998119702804359, 0.9974637004555658, 0.12637403828746754, 0.6693657852149381, 0.00416617708640003, 0.1999765001472014, 0.52390694197027, 0.06998034964945846, 0.40475121148605697, 0.9979233009433447, 0.8721005407226592, 0.1269305680349165, 0.9986896993486618, 0.9978424088195983, 0.9952990140886515, 0.9808353684030087, 0.9184567201927132, 0.08143951213531447, 0.987253572037486, 0.9985058782907618, 0.9957793394018457, 0.9944521352544032, 0.998022280226138, 0.9962631632910535, 0.9996282234173447, 0.9984979121252028, 0.9974700534971755, 0.9898320882714339, 0.9987028326111174, 0.16335782845928346, 0.8355929211118743, 0.9996826295304744, 0.9978440864699526, 0.9654453911119445, 0.9889261278745306, 0.5180048652857899, 0.4814651030042197, 0.9942072417802794, 0.9975277262012476, 0.9959566534701356, 0.9828246603493046, 0.9980190947564082, 0.12854425012722784, 0.8655312841900007, 0.9966323472754983, 0.9834579999099468, 0.969766831510103, 0.03026018344979519, 0.9973458694747135, 0.9986175986041648, 0.9909975812852153, 0.008737671546967364, 0.9963140369222354, 0.9015244868489378, 0.0983588989923915, 0.9988679955352252, 0.9988704096068709, 0.9980994322577432, 0.9987239951937149, 0.9980742669208387, 0.9987921064446684, 0.9861940804876802, 0.9044256553002713, 0.9993540047182498, 0.9277051641226611, 0.025997637342670445, 0.04515379117411182, 0.001368296702245813, 0.9998702018700975, 0.05004428005572258, 0.9488395498565002, 0.9965664417728369, 0.994214323104719, 0.7491092262762964, 0.04714673452088579, 0.030907303741469574, 0.036145829799345774, 0.016763283385203838, 0.11367601545591352, 0.006286231269451438, 0.9933147066981812, 0.9901961289593771, 0.9990358745654087, 0.998817028262499, 0.9771934370386182, 0.9966265436438924, 0.07840707891263013, 0.918188160950537, 0.9960340868135308, 0.051146125420400146, 0.9479081911247493, 0.994493181923941, 0.23701266119428824, 0.7590048316817087, 0.9991683261862861, 0.996122932102733, 0.9940795803232687, 0.4493151461400978, 0.5495469864328889, 0.017485180421857812, 0.9791701036240374, 0.47681560101935977, 0.40995584884690345, 0.06228513755013039, 0.029207154896388827, 0.021465499381683358, 0.031224023035544366, 0.18840257967209822, 0.342405811932156, 0.43713632249762113, 0.9485396281572777, 0.04940310563319155, 0.9996510703371503, 0.9998870630234032, 0.1723988555947075, 0.2708712337484684, 0.1817840529260777, 0.10338155829632377, 0.2714487843534758, 0.9946165992355275, 0.9964170268506577, 0.8737237190329723, 0.9997636150733684, 0.998197269982943, 0.996731443113034, 0.9964413644811009, 0.9968185405291563, 0.669682366514879, 0.3299295017630208, 0.9939200765730393, 0.8994355999582242, 0.0016428047487821447, 0.015606645113430373, 0.050926947212246484, 0.032034692601251816, 0.9827762194844302, 0.007502108545682673, 0.0012503514242804457, 0.008752459969963119, 0.9995916341112037, 0.9997273821017312, 0.9991260874651963, 0.997387998691167, 0.9989082257349795, 0.9989399287673598, 0.9992955582514762, 0.5926043340735443, 0.007491338887157246, 0.19398624907796658, 0.055987901156648895, 0.014194115786192677, 0.13602694295101317, 0.994861476509895, 0.29398460898729584, 0.705250311985481, 0.9960570112427575, 0.9970318042721779, 0.9989269998666701, 0.9980767379927098, 0.9984719188749386, 0.997304669011229, 0.9995424428293452, 0.9998710848890455, 0.9969048325791269, 0.998529954584605, 0.9985832687586051, 0.9945040066247152, 0.9986514854628882, 0.9991570294634378, 0.9953577374491902, 0.740926837012481, 0.25849939488558454, 0.9967717434308311, 0.9992286282601537, 0.9980465447007525, 0.9726607879763222, 0.00978531979855455, 0.01565651167768728, 0.3193555459021682, 0.12372383731971417, 0.5562285335911936, 0.9293256169992606, 0.9982473195426503, 0.9984600748506586, 0.9906170284718286, 0.9997221146236731, 0.9977240673685824, 0.9984310684014893, 0.9992223657931472, 0.9970226178694266, 0.9993457291333502, 0.7918213955699788, 0.20803831364897313, 0.24469090454985445, 0.7546782635063932, 0.5875379167104323, 0.4104716952360555, 0.7394779694110623, 0.0016006016653919096, 0.2584971689607934, 0.9968481164715857, 0.9998008693737132, 0.9816876534643291, 0.11500734893642733, 0.07794336918440337, 0.34338687123198686, 0.06213667193721666, 0.12645357797749357, 0.2747094969855895, 0.9990252151383132, 0.9976996541702152, 0.9923935933398702, 0.995609916479167, 0.9971415377844448, 0.9980560594652083, 0.1312376964637522, 0.8681878381448221, 0.6678197707417559, 0.022337203960415095, 0.16992975723156903, 0.1386159199038843, 0.0012525534931073885, 0.9973479824461613, 0.9963367322682941, 0.9991033022497403, 0.10833617536625044, 0.8909224947882438, 0.9985829883487727, 0.9983043237856731, 0.9982090295866884, 0.8453965152351446, 0.01648950901348569, 0.07853128667672561, 0.055239855195177066, 0.0015458914700142838, 0.002679545214691425, 0.008246838759236657, 0.991561083757631, 0.9949741419708794, 0.9981412084633241, 0.994894234854137, 0.9830799800075889, 0.5062142579792036, 0.49344396892826503, 0.946380092008861, 0.9986700555513973, 0.34589763615157076, 0.019432451469189368, 0.6334979178955734, 0.9978243396736891, 0.9988902120456492, 0.1481402692967859, 0.8514324568673856, 0.9931259430362098, 0.996974347062601, 0.6409585821005567, 0.028401757530204307, 0.3085812574903279, 0.017271339038637753, 0.0015352301367678005, 0.003070460273535601, 0.9678883980791269, 0.9981079427216459, 0.8414887872066195, 0.0014384423712933666, 0.1553517760996836, 0.0014384423712933666, 0.991586981444717, 0.9891777578327345, 0.9945863036875723, 0.999551798461347, 0.9979615708466122, 0.46883607101891317, 0.5284338766569107, 0.8392654939385953, 0.038962531354693304, 0.01791019586465741, 0.09080783517343843, 0.01256855850151397, 0.9982642429115905, 0.9993778610852369, 0.9993709353192789, 0.9949077467193506, 0.9960253282589063, 0.972268805182137, 0.9988206912451343], \"Term\": [\"_\", \"_\", \"access\", \"access\", \"access\", \"achieve\", \"action\", \"add\", \"add\", \"add\", \"aggregate\", \"airflow\", \"already\", \"also\", \"also\", \"also\", \"also\", \"analysis\", \"ansible\", \"ansible_galaxy\", \"answer\", \"apache_airflow\", \"api\", \"appear\", \"append\", \"apply\", \"apply\", \"apply\", \"approach\", \"apr\", \"area\", \"arguments_parameter\", \"asset\", \"assign\", \"attribute\", \"bam\", \"barcode\", \"base\", \"bash\", \"batch\", \"bed\", \"buy\", \"bzip\", \"call\", \"call\", \"call\", \"call\", \"call\", \"case\", \"case\", \"cast\", \"cell\", \"change\", \"change\", \"channel\", \"channel\", \"channel_frompath\", \"character\", \"check\", \"check\", \"class\", \"client\", \"cluster\", \"code\", \"code\", \"code\", \"code\", \"code\", \"code\", \"collect\", \"collection\", \"column\", \"com\", \"combine\", \"command\", \"command\", \"command\", \"complete\", \"complete\", \"complete\", \"conda_environment\", \"condition\", \"config\", \"configuration\", \"configuration\", \"confirm\", \"conflict\", \"connect\", \"connection\", \"container\", \"context\", \"control\", \"convert\", \"copy\", \"core\", \"core\", \"count\", \"count\", \"cpu\", \"create\", \"create\", \"create\", \"create\", \"create\", \"current\", \"current\", \"current\", \"cust\", \"custid\", \"customer\", \"d\", \"data\", \"data\", \"database\", \"database\", \"dataframe\", \"dataset\", \"date\", \"datum\", \"datum\", \"datum\", \"datum\", \"datum\", \"day\", \"debug\", \"debug_nextflow\", \"decimal\", \"decompress\", \"def\", \"def_require\", \"deploy\", \"df\", \"different\", \"different\", \"different\", \"different\", \"dir\", \"directive\", \"directory\", \"directory\", \"dist\", \"docker\", \"documentation\", \"documentation\", \"dplyr\", \"driver\", \"dsl\", \"else\", \"else\", \"empty\", \"enable\", \"end\", \"end\", \"end\", \"entire\", \"env\", \"err\", \"error\", \"error\", \"error\", \"error\", \"error\", \"escape\", \"essentially\", \"even\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"example\", \"execute\", \"execute\", \"execute\", \"execute\", \"execute\", \"execution\", \"execution\", \"executor\", \"expand\", \"expect\", \"expect\", \"expect\", \"export\", \"external\", \"extra\", \"extract\", \"fa\", \"fail\", \"fail\", \"fail\", \"false\", \"false\", \"false_false\", \"fast\", \"fastq\", \"fastq_gz\", \"fastqc\", \"field\", \"file\", \"file\", \"filter\", \"filter\", \"filter\", \"final\", \"find\", \"find\", \"find\", \"finish\", \"fix\", \"flag\", \"flow\", \"folder\", \"folder\", \"follow\", \"follow\", \"follow\", \"follow\", \"follow\", \"follow\", \"foo\", \"format\", \"format\", \"function\", \"function\", \"genome\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"get\", \"ggplot\", \"good\", \"groovy\", \"group\", \"group\", \"gt\", \"hadoop\", \"happen\", \"help\", \"help\", \"help\", \"help\", \"help\", \"hide\", \"host\", \"however\", \"however\", \"however\", \"however\", \"however\", \"however\", \"html\", \"https\", \"image\", \"import\", \"import\", \"import\", \"import\", \"include\", \"include\", \"info\", \"info\", \"input\", \"input\", \"input_file\", \"inputs_parameter\", \"insert\", \"instal\", \"install\", \"instead\", \"instead\", \"instead\", \"int\", \"issue\", \"issue\", \"issue\", \"item\", \"java\", \"java_home\", \"job\", \"job\", \"jobid\", \"join\", \"join\", \"keplergl\", \"key\", \"key\", \"knime\", \"label\", \"last\", \"last\", \"last\", \"last\", \"last\", \"late\", \"length\", \"lib\", \"lib\", \"lib_python\", \"library\", \"library\", \"library\", \"limit\", \"line\", \"line\", \"list\", \"list\", \"list\", \"list\", \"load\", \"local\", \"local\", \"location\", \"log\", \"log\", \"luigi\", \"m\", \"m\", \"m\", \"m\", \"machine\", \"main\", \"make\", \"make\", \"make\", \"make\", \"make\", \"manager\", \"map\", \"map\", \"mapper\", \"mark\", \"master\", \"matrix\", \"memory\", \"merge\", \"message\", \"message\", \"message\", \"metaflow\", \"min\", \"miniconda\", \"minute\", \"miss\", \"miss\", \"miss\", \"miss\", \"miss\", \"mode\", \"model\", \"module\", \"module\", \"monetdb\", \"month\", \"move\", \"name\", \"name\", \"name\", \"name\", \"name\", \"name\", \"name\", \"need\", \"need\", \"need\", \"network\", \"new\", \"new\", \"new\", \"next\", \"nextflow\", \"nf\", \"node\", \"node\", \"none\", \"null\", \"number\", \"number\", \"number\", \"number\", \"object\", \"object\", \"object\", \"operator\", \"option\", \"option\", \"order\", \"org\", \"os_path\", \"other\", \"output\", \"output\", \"outputs_artifact\", \"package\", \"page\", \"pair\", \"panda\", \"parallel\", \"param\", \"parameter\", \"parse\", \"partial\", \"partition\", \"pass\", \"pass\", \"path\", \"pattern\", \"php\", \"pickle\", \"pipeline\", \"pipeline\", \"play\", \"plot\", \"plugin\", \"png\", \"pool\", \"pop\", \"pop\", \"port\", \"portion\", \"possible\", \"possible\", \"prefix\", \"previous\", \"problem\", \"problem\", \"procedure\", \"process\", \"process\", \"processing\", \"profile\", \"project\", \"provide\", \"pull\", \"python\", \"python_site\", \"quantity\", \"query\", \"question\", \"question\", \"question\", \"question\", \"quot\", \"range\", \"range\", \"rapidminer\", \"raw\", \"read\", \"read\", \"read\", \"read\", \"read\", \"read\", \"read\", \"ready\", \"realize\", \"reason\", \"record\", \"reduce\", \"regular\", \"remove\", \"remove\", \"rename\", \"replace\", \"replace\", \"represent\", \"request\", \"request\", \"require\", \"rerun\", \"reshape\", \"resource\", \"resource\", \"response\", \"response\", \"result\", \"result\", \"result\", \"result\", \"result\", \"return\", \"return\", \"return\", \"return\", \"role\", \"role\", \"row\", \"rule\", \"run\", \"run\", \"run\", \"run\", \"run\", \"running\", \"runtime\", \"safety\", \"sample\", \"save\", \"sbatch\", \"scale\", \"scheduler\", \"script\", \"script\", \"section\", \"see\", \"see\", \"see\", \"see\", \"see\", \"seem\", \"seem\", \"seem\", \"seem\", \"select\", \"self\", \"separate\", \"sequence\", \"server\", \"service\", \"session\", \"set\", \"set\", \"set\", \"set\", \"set\", \"set\", \"share\", \"shell\", \"shell\", \"singularity\", \"site\", \"site_package\", \"size\", \"skip\", \"smk\", \"snakefile\", \"snakemake\", \"software\", \"solve\", \"spark\", \"specie\", \"split\", \"sql\", \"sqlalchemy\", \"start\", \"start\", \"stat\", \"statement\", \"status\", \"still\", \"still\", \"still\", \"string\", \"string\", \"string\", \"subject\", \"submit\", \"sum\", \"symlink\", \"table\", \"tag\", \"target\", \"task\", \"tell\", \"template\", \"teradata\", \"teradata\", \"test\", \"test\", \"text\", \"text\", \"thank\", \"thank\", \"thank\", \"thing\", \"thread\", \"threshold\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"timestamp\", \"tmp\", \"token\", \"touch\", \"traceback\", \"trim\", \"true\", \"true\", \"try\", \"try\", \"try\", \"try\", \"try\", \"tsv\", \"tuple\", \"txt\", \"type\", \"type\", \"unique\", \"upload\", \"url\", \"use\", \"use\", \"use\", \"use\", \"use\", \"use\", \"user\", \"user\", \"username\", \"usr_local\", \"util\", \"validation\", \"value\", \"value\", \"valueerror\", \"varchar\", \"variable\", \"variable\", \"variable\", \"vcf\", \"ve\", \"version\", \"version\", \"volume\", \"wait\", \"want\", \"want\", \"want\", \"want\", \"want\", \"want\", \"warehouse\", \"warn\", \"way\", \"way\", \"way\", \"way\", \"weight\", \"whalesay\", \"width\", \"wildcard\", \"window_size\", \"word\", \"word\", \"work\", \"work\", \"work\", \"work\", \"work\", \"worker\", \"workflow\", \"would_require\", \"wrapper\", \"yaml\", \"yesterday\", \"zip\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 9, 3, 8, 1, 6, 5, 10, 4, 7]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el168631405316247683685416992945\", ldavis_el168631405316247683685416992945_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el168631405316247683685416992945\", ldavis_el168631405316247683685416992945_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el168631405316247683685416992945\", ldavis_el168631405316247683685416992945_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "1     -0.265836 -0.115559       1        1  22.343811\n",
       "8     -0.206402  0.127309       2        1  18.185187\n",
       "2     -0.133672 -0.192989       3        1  16.091887\n",
       "7     -0.184859  0.057083       4        1  12.908425\n",
       "0      0.094685 -0.050125       5        1   7.746939\n",
       "5      0.131173  0.154267       6        1   7.384442\n",
       "4     -0.001880  0.259821       7        1   6.642998\n",
       "9      0.142534 -0.232646       8        1   5.088799\n",
       "3      0.187845 -0.003381       9        1   2.950956\n",
       "6      0.236412 -0.003780      10        1   0.656557, topic_info=            Term          Freq         Total Category  logprob  loglift\n",
       "2910   snakemake   7575.000000   7575.000000  Default  30.0000  30.0000\n",
       "1093        rule   7412.000000   7412.000000  Default  29.0000  29.0000\n",
       "18          file  11642.000000  11642.000000  Default  28.0000  28.0000\n",
       "120        value   3915.000000   3915.000000  Default  27.0000  27.0000\n",
       "202       output   7514.000000   7514.000000  Default  26.0000  26.0000\n",
       "...          ...           ...           ...      ...      ...      ...\n",
       "7284      mapper      6.305530      7.182837  Topic10  -6.3605   4.8956\n",
       "7628        bzip      5.378976      6.257742  Topic10  -6.5195   4.8746\n",
       "7648  decompress      5.132397      6.010184  Topic10  -6.5664   4.8680\n",
       "303      library     69.074350    267.534225  Topic10  -3.9668   3.6719\n",
       "470          lib      8.318413    136.944057  Topic10  -6.0835   2.2248\n",
       "\n",
       "[488 rows x 6 columns], token_table=      Topic      Freq           Term\n",
       "term                                \n",
       "866       6  0.478913              _\n",
       "866       9  0.519303              _\n",
       "64        3  0.142695         access\n",
       "64        4  0.758289         access\n",
       "64        6  0.099061         access\n",
       "...     ...       ...            ...\n",
       "4470      7  0.999371  would_require\n",
       "3222      8  0.994908        wrapper\n",
       "4446      7  0.996025           yaml\n",
       "2473     10  0.972269      yesterday\n",
       "3385      1  0.998821            zip\n",
       "\n",
       "[664 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[2, 9, 3, 8, 1, 6, 5, 10, 4, 7])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ea7442",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
